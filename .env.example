# =============================================================================
# TASI AI Platform (Ra'd AI) - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values.
#
# Legend:
#   [REQUIRED]  - Must be set for the app to start
#   [REQUIRED*] - Required only in specific contexts (see description)
#   [optional]  - Has a sensible default; override if needed
#
# Run `python scripts/validate_config.py` to check your configuration.

# ---------------------------------------------------------------------------
# Environment (controls production safety checks)
# ---------------------------------------------------------------------------
# Values: development (default), staging, production
# In production, AUTH_JWT_SECRET is required and stricter validation applies.
# [optional]
ENVIRONMENT=development

# ---------------------------------------------------------------------------
# LLM Provider - Google Gemini (active)
# ---------------------------------------------------------------------------
# [REQUIRED] Gemini API key from https://aistudio.google.com/apikey
GEMINI_API_KEY=your-gemini-api-key-here
# [optional] Gemini model name
GEMINI_MODEL=gemini-2.5-flash

# ---------------------------------------------------------------------------
# LLM Provider - Anthropic (legacy, kept for backward compatibility)
# ---------------------------------------------------------------------------
# [optional] Used only if GEMINI_API_KEY is not set
ANTHROPIC_API_KEY=
# [optional] Overrides ANTHROPIC_API_KEY if set
LLM_API_KEY=
# [optional] LLM model identifier
LLM_MODEL=gemini-2.5-flash
# [optional] Maximum tool call iterations per query
LLM_MAX_TOOL_ITERATIONS=10

# ---------------------------------------------------------------------------
# Database Settings (prefix: DB_)
# ---------------------------------------------------------------------------
# [optional] Backend: "sqlite" (default) or "postgres"
DB_BACKEND=sqlite
# [optional] SQLite: path relative to project root (or absolute)
DB_SQLITE_PATH=saudi_stocks.db

# ---------------------------------------------------------------------------
# PostgreSQL Settings (used when DB_BACKEND=postgres)
# Both DB_PG_* and POSTGRES_* naming conventions are accepted.
# DB_PG_* takes priority if both are set. POSTGRES_* is Docker-compatible.
# ---------------------------------------------------------------------------
# [REQUIRED*] Required when DB_BACKEND=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=tasi_platform
POSTGRES_USER=tasi_user
# IMPORTANT: Use a strong password in production (required by docker-compose)
POSTGRES_PASSWORD=changeme

# ---------------------------------------------------------------------------
# Server Settings (prefix: SERVER_)
# ---------------------------------------------------------------------------
# [optional] Server bind address
SERVER_HOST=0.0.0.0
# [optional] Server port
SERVER_PORT=8084
# [optional] Enable debug mode (true = dev logging, false = JSON logging)
SERVER_DEBUG=false

# ---------------------------------------------------------------------------
# Authentication Settings (prefix: AUTH_)
# ---------------------------------------------------------------------------
# [REQUIRED*] REQUIRED in production. In development, a random secret is
# generated on each startup (sessions lost on restart).
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
AUTH_JWT_SECRET=change-me-to-a-stable-secret
# [optional] JWT signing algorithm
AUTH_JWT_ALGORITHM=HS256
# [optional] Access token lifetime in minutes
AUTH_ACCESS_TOKEN_EXPIRE_MINUTES=30
# [optional] Refresh token lifetime in days
AUTH_REFRESH_TOKEN_EXPIRE_DAYS=7

# ---------------------------------------------------------------------------
# Connection Pool Settings (PostgreSQL only, prefix: PG_POOL_)
# ---------------------------------------------------------------------------
# [optional] Minimum pool connections
PG_POOL_MIN=2
# [optional] Maximum pool connections
PG_POOL_MAX=10

# ---------------------------------------------------------------------------
# Redis / Cache Settings (prefix: CACHE_)
# ---------------------------------------------------------------------------
# [optional] Enable caching (master switch)
CACHE_ENABLED=false
# [optional] Redis connection URL (db=0 for cache, db=1 used by rate-limiter)
CACHE_REDIS_URL=redis://localhost:6379/0
# [optional] Redis password (leave empty for development)
CACHE_REDIS_PASSWORD=
# [optional] Redis connection pool size
CACHE_REDIS_MAX_CONNECTIONS=20
# [optional] Default cache TTL in seconds
CACHE_DEFAULT_TTL=300
# [optional] TTL for live market-data queries (seconds)
CACHE_MARKET_TTL=60
# [optional] TTL for historical / financial-statement queries (seconds)
CACHE_HISTORICAL_TTL=3600
# [optional] TTL for schema / metadata queries (seconds)
CACHE_SCHEMA_TTL=86400
# [optional] Byte threshold for gzip compression before caching
CACHE_COMPRESSION_THRESHOLD=1024
# [optional] gzip compression level (1=fastest, 9=smallest)
CACHE_COMPRESSION_LEVEL=6
# [optional] Pre-warm common queries at startup
CACHE_WARM_ON_STARTUP=false
# [optional] Seconds between maintenance cycles (stats collection)
CACHE_MAINTENANCE_INTERVAL=300

# ---------------------------------------------------------------------------
# Middleware Settings (prefix: MW_)
# ---------------------------------------------------------------------------
# [optional] Comma-separated allowed CORS origins
# In production, add your deployment domain, e.g.:
# MW_CORS_ORIGINS=http://localhost:3000,http://localhost:8084,https://raid-ai-app-production.up.railway.app
MW_CORS_ORIGINS=http://localhost:3000,http://localhost:8084
# [optional] Rate limit: max requests per minute per IP
MW_RATE_LIMIT_PER_MINUTE=60
# [optional] Paths to skip in request logging
MW_LOG_SKIP_PATHS=/health,/favicon.ico

# ---------------------------------------------------------------------------
# Rate Limiting - Backend (prefix: RATELIMIT_)
# ---------------------------------------------------------------------------
# These control the Redis-backed rate limiter in backend/middleware/.
# The existing MW_RATE_LIMIT_PER_MINUTE above controls the legacy in-memory limiter.
# [optional] Enable rate limiting middleware
RATELIMIT_ENABLED=true
# [optional] Default requests per window (catch-all)
RATELIMIT_DEFAULT_LIMIT=60
# [optional] Default sliding window in seconds
RATELIMIT_DEFAULT_WINDOW=60
# [optional] Redis URL for rate limiting (uses db=1, separate from cache on db=0)
RATELIMIT_REDIS_URL=redis://localhost:6379/1
# [optional] Additional paths to skip (comma-separated, merged with built-in health/docs)
RATELIMIT_SKIP_PATHS=
# Endpoint-specific limits are defined in code (backend/middleware/rate_limit_config.py):
#   /api/v1/query:  50 req / 3600s (LLM queries)
#   /api/auth:      20 req / 60s   (brute-force protection)
#   /api/v1/export: 10 req / 3600s (data exports)
#   /api/v1:        1000 req / 3600s (general API)

# ---------------------------------------------------------------------------
# SQL Security Settings (prefix: SECURITY_)
# ---------------------------------------------------------------------------
# [optional] Maximum allowed SQL query length in characters
SECURITY_MAX_QUERY_LENGTH=5000
# [optional] Maximum rows returned from a single query
SECURITY_MAX_RESULT_ROWS=1000
# [optional] Log all validated queries for audit purposes
SECURITY_ENABLE_QUERY_LOGGING=true
# [optional] Comma-separated additional regex patterns to block in SQL
SECURITY_BLOCKED_SQL_PATTERNS=
# [optional] Path to the allowed tables JSON config file
SECURITY_ALLOWED_TABLES_PATH=config/allowed_tables.json
# [optional] Strict mode: reject queries with any risk score > 0
SECURITY_ENABLE_STRICT_MODE=false

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------
# [optional] Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
# [optional] Log format: "json" (default, for production aggregators) or "text" (human-readable)
LOG_FORMAT=json

# ---------------------------------------------------------------------------
# pgAdmin (optional, only with: docker compose --profile tools up)
# ---------------------------------------------------------------------------
# [optional] pgAdmin login email
PGADMIN_DEFAULT_EMAIL=admin@tasi.local
# [REQUIRED*] Required when using pgAdmin profile
PGADMIN_DEFAULT_PASSWORD=

# ---------------------------------------------------------------------------
# Resilience Settings (prefix: RESILIENCE_)
# ---------------------------------------------------------------------------
# [optional] Consecutive failures before opening circuit breaker
RESILIENCE_CB_FAILURE_THRESHOLD=5
# [optional] Seconds to wait in OPEN state before probing recovery
RESILIENCE_CB_RECOVERY_TIMEOUT=30.0
# [optional] Max probe calls in HALF_OPEN state
RESILIENCE_CB_HALF_OPEN_MAX_CALLS=3
# [optional] Consecutive successes in HALF_OPEN to close circuit
RESILIENCE_CB_SUCCESS_THRESHOLD=2
# [optional] Max retry attempts (including first call)
RESILIENCE_RETRY_MAX_ATTEMPTS=3
# [optional] Initial retry delay in seconds
RESILIENCE_RETRY_BASE_DELAY=1.0
# [optional] Maximum retry delay cap in seconds
RESILIENCE_RETRY_MAX_DELAY=30.0
# [optional] Add random jitter to retry delays
RESILIENCE_RETRY_JITTER=true
# [optional] Default query timeout in seconds
RESILIENCE_QUERY_TIMEOUT=30.0
# [optional] Log queries slower than this (seconds)
RESILIENCE_QUERY_SLOW_THRESHOLD=5.0
# [optional] Absolute maximum query timeout
RESILIENCE_QUERY_MAX_TIMEOUT=120.0
# [optional] Cancel PG backend on timeout
RESILIENCE_QUERY_CANCEL_ON_TIMEOUT=true
# [optional] Enable graceful degradation with fallbacks
RESILIENCE_DEGRADATION_ENABLED=true

# ---------------------------------------------------------------------------
# News Scraper Settings (prefix: SCRAPER_)
# ---------------------------------------------------------------------------
# [optional] HTTP timeout for source page requests (seconds)
SCRAPER_REQUEST_TIMEOUT=10
# [optional] HTTP timeout for individual article fetches (seconds)
SCRAPER_ARTICLE_FETCH_TIMEOUT=5
# [optional] Delay between consecutive HTTP requests (seconds)
SCRAPER_INTER_REQUEST_DELAY=1.5
# [optional] Maximum articles returned per source
SCRAPER_MAX_ARTICLES_PER_SOURCE=10
# [optional] Maximum full-article body fetches per source
SCRAPER_MAX_FULL_ARTICLE_FETCHES=5
# [optional] Interval between news fetch cycles (seconds, 1800 = 30 minutes)
SCRAPER_FETCH_INTERVAL_SECONDS=1800
# [optional] Delete articles older than this many days
SCRAPER_CLEANUP_AGE_DAYS=7
# [optional] Title similarity threshold for deduplication (0.0 - 1.0)
SCRAPER_DEDUP_THRESHOLD=0.55

# ---------------------------------------------------------------------------
# Ingestion Pipeline Settings
# ---------------------------------------------------------------------------
# [optional] Batch size for processing tickers
INGESTION_BATCH_SIZE=10
# [optional] Seconds to sleep between batches for rate limiting
INGESTION_RATE_LIMIT_SECONDS=2
