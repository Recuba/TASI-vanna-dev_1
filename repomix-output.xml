This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: *.png, *.jpg, *.jpeg, *.gif, *.svg, *.ico, *.db, *.db-shm, *.db-wal, *.csv, *.xml, *.log, *.map, *.lock, .coverage, .env, .mcp.json, vanna_docs/**, .planning/**, .playwright-mcp/**, .benchmarks/**, .claude/**, .ruff_cache/**, .pytest_cache/**, __pycache__/**, node_modules/**, .next/**, frontend/test-results/**, frontend/.next/**, 2f183a4e64493af3/**, 9f86d081884c7d65/**, db21b9b5e1df8ae3/**, e3b98a4da31a127d/**, f85ac825d102b9f2/**, f8c88490871f6169/**, analysis/**, nav-screenshots/**, new_designs/**, navigate.mjs, frontend/navigate.mjs, sse_out.txt, nul, start.bat, plan.md, repo.md, agent_teams_documentation.md, FRONTEND_UI_UX_SUMMARY.md, PRODUCTION_READINESS_PLAN.md, TEST_REPORT.md, TEST_RESULTS.md, UX_UI_DEEP_DIVE_RECOMMENDATIONS.md, TECHNICAL_SUMMARY.md, STATUS_BACKEND.md, STATUS_FRONTEND.md, ANALYSIS.md, PRIVACY.md, SECURITY.md, SLA.md, RUNBOOK.md, load-test-requirements.txt, test_app_assembly.py, pywinauto-mcp.log, cache/**, frontend/.env*, package-lock.json, *.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.dockerignore
.github/pull_request_template.md
.github/workflows/ci.yml
.github/workflows/deploy.yml
.gitignore
AGENTS.md
api/__init__.py
api/db_helper.py
api/dependencies.py
api/models/__init__.py
api/models/widgets.py
api/routes/__init__.py
api/routes/announcements.py
api/routes/auth.py
api/routes/charts_analytics.py
api/routes/charts.py
api/routes/entities.py
api/routes/health.py
api/routes/market_analytics.py
api/routes/market_movers.py
api/routes/market_overview.py
api/routes/news_feed.py
api/routes/news_stream.py
api/routes/news.py
api/routes/reports.py
api/routes/sqlite_entities.py
api/routes/stock_data.py
api/routes/stock_ohlcv.py
api/routes/tasi_index.py
api/routes/watchlists.py
api/routes/widgets_stream.py
api/schemas/__init__.py
api/schemas/announcements.py
api/schemas/charts.py
api/schemas/common.py
api/schemas/entities.py
api/schemas/health.py
api/schemas/news.py
api/schemas/reports.py
api/schemas/watchlists.py
app.py
ARCHITECTURE.md
auth/__init__.py
auth/dependencies.py
auth/jwt_handler.py
auth/models.py
auth/password.py
backend/__init__.py
backend/middleware/__init__.py
backend/middleware/cost_controller.py
backend/middleware/models.py
backend/middleware/rate_limit_config.py
backend/middleware/rate_limit_middleware.py
backend/middleware/rate_limiter.py
backend/middleware/register.py
backend/routes/__init__.py
backend/routes/health.py
backend/security/__init__.py
backend/security/allowlist.py
backend/security/config.py
backend/security/models.py
backend/security/sanitizer.py
backend/security/sql_validator.py
backend/security/vanna_hook.py
backend/services/__init__.py
backend/services/audit/__init__.py
backend/services/audit/config.py
backend/services/audit/correlation.py
backend/services/audit/migrations/001_audit_table.sql
backend/services/audit/migrations/002_security_events_table.sql
backend/services/audit/models.py
backend/services/audit/query_audit.py
backend/services/audit/security_events.py
backend/services/audit/structured_logger.py
backend/services/cache/__init__.py
backend/services/cache/compression.py
backend/services/cache/config.py
backend/services/cache/db_pool.py
backend/services/cache/maintenance.py
backend/services/cache/models.py
backend/services/cache/query_cache.py
backend/services/cache/redis_client.py
backend/services/resilience/__init__.py
backend/services/resilience/circuit_breaker.py
backend/services/resilience/config.py
backend/services/resilience/degradation.py
backend/services/resilience/retry.py
backend/services/resilience/timeout_manager.py
CHANGELOG.md
chart_engine/__init__.py
chart_engine/raid_chart_generator.py
CLAUDE.md
config/__init__.py
config/allowed_tables.json
config/env_validator.py
config/error_tracking.py
config/lifecycle.py
config/logging_config.py
config/prompts.py
config/settings.py
csv_to_sqlite.py
database/__init__.py
database/csv_to_postgres.py
database/manager.py
database/migrate_sqlite_to_pg.py
database/pool.py
database/postgres_utils.py
database/queries.py
database/schema.sql
DEVELOPMENT.md
docker-compose.yml
Dockerfile
docs/API_ERROR_AUDIT.md
docs/api-contracts.md
docs/ARCHITECTURE.md
docs/data-freshness.md
docs/DATABASE_AUDIT.md
docs/DEPLOYMENT_CHECKLIST.md
docs/DEPLOYMENT_RUNBOOK.md
docs/METRICS_AND_MONITORING.md
docs/MIGRATION_STRATEGY.md
docs/ui-transition.md
entrypoint.sh
frontend/.eslintrc.json
frontend/.gitignore
frontend/.lighthouserc.js
frontend/docs/BUNDLE_REPORT.md
frontend/docs/charts.md
frontend/docs/MOBILE_AUDIT.md
frontend/docs/tradingview-integration.md
frontend/e2e/.auth/.gitkeep
frontend/e2e/.gitignore
frontend/e2e/global-setup.ts
frontend/e2e/load-tests/locust-frontend.py
frontend/e2e/load-tests/README.md
frontend/e2e/playwright.config.ts
frontend/e2e/tests/admin.spec.ts
frontend/e2e/tests/auth.spec.ts
frontend/e2e/tests/markets-page.spec.ts
frontend/e2e/tests/news-portal.spec.ts
frontend/e2e/tests/query-flow.spec.ts
frontend/e2e/tests/stock-detail.spec.ts
frontend/next-cdn.config.md
frontend/next.config.mjs
frontend/package.json
frontend/postcss.config.mjs
frontend/public/api-docs/openapi.yaml
frontend/README.md
frontend/scripts/analyze-bundle.js
frontend/scripts/lint-rtl.js
frontend/scripts/security-audit.sh
frontend/sentry.client.config.ts
frontend/sentry.edge.config.ts
frontend/sentry.server.config.ts
frontend/src/app/admin/AdminPage.tsx
frontend/src/app/admin/layout.tsx
frontend/src/app/admin/page.tsx
frontend/src/app/announcements/error.tsx
frontend/src/app/announcements/page.tsx
frontend/src/app/api-docs/page.tsx
frontend/src/app/charts/components/AnalyticsTab.tsx
frontend/src/app/charts/components/ChartTabNavigation.tsx
frontend/src/app/charts/components/CompareTab.tsx
frontend/src/app/charts/components/index.ts
frontend/src/app/charts/components/PopularStocks.tsx
frontend/src/app/charts/components/RecentSearches.tsx
frontend/src/app/charts/components/StockChartPanel.tsx
frontend/src/app/charts/components/StocksTab.tsx
frontend/src/app/charts/components/types.ts
frontend/src/app/charts/error.tsx
frontend/src/app/charts/loading.tsx
frontend/src/app/charts/page.tsx
frontend/src/app/chat/error.tsx
frontend/src/app/chat/loading.tsx
frontend/src/app/chat/page.tsx
frontend/src/app/error.tsx
frontend/src/app/favicon.ico
frontend/src/app/fonts/GeistMonoVF.woff
frontend/src/app/fonts/GeistVF.woff
frontend/src/app/globals.css
frontend/src/app/layout.tsx
frontend/src/app/loading.tsx
frontend/src/app/login/error.tsx
frontend/src/app/login/page.tsx
frontend/src/app/market/components/index.ts
frontend/src/app/market/components/MarketMoversPanel.tsx
frontend/src/app/market/error.tsx
frontend/src/app/market/loading.tsx
frontend/src/app/market/page.tsx
frontend/src/app/markets/components/CategoryLegend.tsx
frontend/src/app/markets/components/CentralHub.tsx
frontend/src/app/markets/components/constants.ts
frontend/src/app/markets/components/ConstellationCanvas.tsx
frontend/src/app/markets/components/EdgeTooltip.tsx
frontend/src/app/markets/components/index.ts
frontend/src/app/markets/components/InstrumentNode.tsx
frontend/src/app/markets/components/MarketHeader.tsx
frontend/src/app/markets/components/MobileCard.tsx
frontend/src/app/markets/components/MobileSummary.tsx
frontend/src/app/markets/components/Sparkline.tsx
frontend/src/app/markets/components/StatBadge.tsx
frontend/src/app/markets/error.tsx
frontend/src/app/markets/loading.tsx
frontend/src/app/markets/MarketOverviewClient.tsx
frontend/src/app/markets/page.tsx
frontend/src/app/news/[id]/loading.tsx
frontend/src/app/news/[id]/page.tsx
frontend/src/app/news/components/ArticleCard.tsx
frontend/src/app/news/components/FilterBar.tsx
frontend/src/app/news/components/index.ts
frontend/src/app/news/components/NewArticlesBanner.tsx
frontend/src/app/news/components/SearchInput.tsx
frontend/src/app/news/components/SkeletonCard.tsx
frontend/src/app/news/error.tsx
frontend/src/app/news/hooks/useNewsFilters.ts
frontend/src/app/news/loading.tsx
frontend/src/app/news/page.tsx
frontend/src/app/news/utils.ts
frontend/src/app/not-found.tsx
frontend/src/app/page.tsx
frontend/src/app/reports/error.tsx
frontend/src/app/reports/page.tsx
frontend/src/app/stock/__tests__/StockDetailClient.test.tsx
frontend/src/app/stock/[ticker]/components/StockDividends.tsx
frontend/src/app/stock/[ticker]/components/StockFinancials.tsx
frontend/src/app/stock/[ticker]/components/StockNewsSection.tsx
frontend/src/app/stock/[ticker]/components/StockReportsSection.tsx
frontend/src/app/stock/[ticker]/error.tsx
frontend/src/app/stock/[ticker]/loading.tsx
frontend/src/app/stock/[ticker]/page.tsx
frontend/src/app/stock/[ticker]/StockDetailClient.tsx
frontend/src/app/watchlist/error.tsx
frontend/src/app/watchlist/loading.tsx
frontend/src/app/watchlist/page.tsx
frontend/src/components/auth/AccessDenied.tsx
frontend/src/components/auth/index.ts
frontend/src/components/auth/PermissionGuard.tsx
frontend/src/components/auth/RoleGuard.tsx
frontend/src/components/charts/__tests__/CandlestickChart.test.tsx
frontend/src/components/charts/__tests__/ChartWrapper.test.tsx
frontend/src/components/charts/__tests__/DataSourceBadge.test.tsx
frontend/src/components/charts/AreaChart.tsx
frontend/src/components/charts/CandlestickChart.tsx
frontend/src/components/charts/chart-config.ts
frontend/src/components/charts/chart-types.ts
frontend/src/components/charts/ChartEmpty.tsx
frontend/src/components/charts/ChartError.tsx
frontend/src/components/charts/ChartErrorBoundary.tsx
frontend/src/components/charts/ChartSkeleton.tsx
frontend/src/components/charts/ChartWrapper.tsx
frontend/src/components/charts/DataSourceBadge.tsx
frontend/src/components/charts/index.tsx
frontend/src/components/charts/LazySparkline.tsx
frontend/src/components/charts/LineChart.tsx
frontend/src/components/charts/MiniSparkline.tsx
frontend/src/components/charts/PreBuiltCharts.tsx
frontend/src/components/charts/StockComparisonChart.tsx
frontend/src/components/charts/StockOHLCVChart.tsx
frontend/src/components/charts/tasi/ChartExportButton.tsx
frontend/src/components/charts/tasi/IndicatorToggleBar.tsx
frontend/src/components/charts/tasi/PeriodSelector.tsx
frontend/src/components/charts/tasi/useChartIndicators.ts
frontend/src/components/charts/TASIIndexChart.tsx
frontend/src/components/charts/TradingViewAttribution.tsx
frontend/src/components/charts/TradingViewWidget.tsx
frontend/src/components/charts/useChart.ts
frontend/src/components/chat/AIChatInterface.tsx
frontend/src/components/chat/AssistantContent.tsx
frontend/src/components/chat/ChartBlock.tsx
frontend/src/components/chat/DataTable.tsx
frontend/src/components/chat/HelpPanel.tsx
frontend/src/components/chat/hooks/useConversationHistory.ts
frontend/src/components/chat/LoadingDots.tsx
frontend/src/components/chat/MessageBubble.tsx
frontend/src/components/chat/MessageThread.tsx
frontend/src/components/chat/SQLBlock.tsx
frontend/src/components/common/BackToTop.tsx
frontend/src/components/common/Breadcrumb.tsx
frontend/src/components/common/CommandPalette.tsx
frontend/src/components/common/ConnectionStatusBadge.tsx
frontend/src/components/common/error-boundary.tsx
frontend/src/components/common/error-display.tsx
frontend/src/components/common/GlobalKeyboardShortcuts.tsx
frontend/src/components/common/loading-spinner.tsx
frontend/src/components/common/MobileBottomNav.tsx
frontend/src/components/common/OfflineBanner.tsx
frontend/src/components/common/RetryButton.tsx
frontend/src/components/common/ScrollToTop.tsx
frontend/src/components/common/Toast.tsx
frontend/src/components/layout/AppShell.tsx
frontend/src/components/layout/Footer.tsx
frontend/src/components/layout/Header.tsx
frontend/src/components/layout/Sidebar.tsx
frontend/src/components/monitoring/ErrorBoundary.tsx
frontend/src/components/monitoring/ErrorFallback.tsx
frontend/src/components/performance/Skeletons.tsx
frontend/src/components/queries/ExportButton.tsx
frontend/src/components/queries/QueryHistory.tsx
frontend/src/components/queries/QueryHistoryItem.tsx
frontend/src/components/queries/QuerySuggestions.tsx
frontend/src/components/queries/SavedQueries.tsx
frontend/src/components/queries/SaveQueryModal.tsx
frontend/src/components/ui/Button.tsx
frontend/src/components/ui/index.ts
frontend/src/components/ui/Tooltip.tsx
frontend/src/components/visualization/AutoChart.tsx
frontend/src/components/visualization/chart-types/BarChart.tsx
frontend/src/components/visualization/chart-types/index.ts
frontend/src/components/visualization/chart-types/KPICard.tsx
frontend/src/components/visualization/chart-types/LineChart.tsx
frontend/src/components/visualization/chart-types/PieChart.tsx
frontend/src/components/visualization/chart-types/ScatterChart.tsx
frontend/src/components/visualization/DataTable.tsx
frontend/src/components/visualization/DataTableHeader.tsx
frontend/src/components/visualization/MobileQueryInput.tsx
frontend/src/components/visualization/QueryResultView.tsx
frontend/src/components/visualization/ResponsiveWrapper.tsx
frontend/src/components/visualization/ResultToolbar.tsx
frontend/src/components/widgets/LiveMarketWidgets.tsx
frontend/src/config/auth.ts
frontend/src/config/cdn.ts
frontend/src/config/monitoring.ts
frontend/src/config/security.ts
frontend/src/contexts/AuthContext.tsx
frontend/src/lib/__tests__/api-modules.test.ts
frontend/src/lib/__tests__/api-remaining.test.ts
frontend/src/lib/__tests__/auth-config.test.ts
frontend/src/lib/__tests__/auth-guards.test.tsx
frontend/src/lib/__tests__/auth-types.test.ts
frontend/src/lib/__tests__/news-feed.test.ts
frontend/src/lib/__tests__/session-manager.test.ts
frontend/src/lib/__tests__/tradingview-utils.test.ts
frontend/src/lib/__tests__/use-auth.test.tsx
frontend/src/lib/__tests__/useMarketIndex.test.ts
frontend/src/lib/api-client.ts
frontend/src/lib/api/auth.ts
frontend/src/lib/api/charts.ts
frontend/src/lib/api/client-base.ts
frontend/src/lib/api/entities.ts
frontend/src/lib/api/health.ts
frontend/src/lib/api/index.ts
frontend/src/lib/api/market-movers.ts
frontend/src/lib/api/market.ts
frontend/src/lib/api/news.ts
frontend/src/lib/api/stocks.ts
frontend/src/lib/auth/index.ts
frontend/src/lib/auth/route-protection.ts
frontend/src/lib/auth/session.ts
frontend/src/lib/chart-cache-provider.tsx
frontend/src/lib/chart-cache.ts
frontend/src/lib/chart-utils.ts
frontend/src/lib/config.ts
frontend/src/lib/cookies.ts
frontend/src/lib/csrf.ts
frontend/src/lib/export/__tests__/exporters.test.ts
frontend/src/lib/export/exporters.ts
frontend/src/lib/formatters.ts
frontend/src/lib/hooks/use-api.ts
frontend/src/lib/hooks/use-auth.tsx
frontend/src/lib/hooks/use-chart-data.ts
frontend/src/lib/hooks/use-keyboard-nav.ts
frontend/src/lib/hooks/use-market-data.ts
frontend/src/lib/hooks/useAsyncWithRetry.ts
frontend/src/lib/hooks/useBreakpoint.ts
frontend/src/lib/hooks/useFormatters.ts
frontend/src/lib/hooks/useLocalStorageTTL.ts
frontend/src/lib/hooks/useWatchlist.ts
frontend/src/lib/market-graph/data.ts
frontend/src/lib/market-graph/index.ts
frontend/src/lib/market-graph/layout.ts
frontend/src/lib/market-graph/quant.ts
frontend/src/lib/market-graph/types.ts
frontend/src/lib/monitoring/index.ts
frontend/src/lib/monitoring/metrics-collector.ts
frontend/src/lib/monitoring/swr-middleware.ts
frontend/src/lib/monitoring/web-vitals.ts
frontend/src/lib/performance/utils.ts
frontend/src/lib/queries/__tests__/query-store.test.ts
frontend/src/lib/queries/__tests__/suggestions.test.ts
frontend/src/lib/queries/query-store.ts
frontend/src/lib/queries/suggestions.ts
frontend/src/lib/stock-translations.ts
frontend/src/lib/tradingview-utils.ts
frontend/src/lib/types.ts
frontend/src/lib/use-sse-chat.ts
frontend/src/lib/utils.ts
frontend/src/lib/utils/arabic-numbers.ts
frontend/src/lib/validators.ts
frontend/src/middleware.ts
frontend/src/providers/LanguageProvider.tsx
frontend/src/providers/ThemeProvider.tsx
frontend/src/styles/design-system.ts
frontend/src/test/chart-test-utils.ts
frontend/src/test/integration/chart-data-flow.test.tsx
frontend/src/test/integration/chart-page-integration.test.tsx
frontend/src/test/integration/swr-cache.test.tsx
frontend/src/test/msw-handlers.ts
frontend/src/test/msw-server.ts
frontend/src/test/setup.ts
frontend/src/types/auth.ts
frontend/src/types/plotly.d.ts
frontend/src/types/queries.ts
frontend/tailwind.config.ts
frontend/tsconfig.json
frontend/vitest.config.ts
infrastructure/database/backup.sh
infrastructure/database/health_checks.sql
infrastructure/database/indexes.sql
infrastructure/database/README.md
infrastructure/database/restore.sh
infrastructure/database/tuning.md
infrastructure/database/wal_config.sql
ingestion/__init__.py
ingestion/config.py
ingestion/price_loader.py
ingestion/scheduler.py
ingestion/validators.py
ingestion/xbrl_processor.py
middleware/__init__.py
middleware/chat_auth.py
middleware/cors.py
middleware/error_handler.py
middleware/rate_limit.py
middleware/request_context.py
middleware/request_logging.py
models/__init__.py
models/api_responses.py
models/validators.py
pyproject.toml
railway.toml
README.md
requirements-dev.txt
requirements-test.txt
requirements.in
requirements.txt
ruff.toml
scripts/build_check.sh
scripts/coverage_report.sh
scripts/DEPENDENCY_AUDIT.md
scripts/deploy_checklist.md
scripts/ENDPOINT_AUTH_AUDIT.md
scripts/export_openapi.py
scripts/generate_system_prompt.py
scripts/run_pg_tests.sh
scripts/SECURITY_AUDIT.md
scripts/SECURITY_REMEDIATION.md
scripts/smoke_test.py
scripts/smoke_test.sh
scripts/test_news_api.py
scripts/test_news_scraper.py
scripts/test_pg.sh
scripts/validate_charts.py
scripts/validate_config.py
services/__init__.py
services/announcement_service.py
services/audit_service.py
services/auth_service.py
services/cache_utils.py
services/db_compat.py
services/health_service.py
services/news_paraphraser.py
services/news_scheduler.py
services/news_scraper.py
services/news_service.py
services/news_store.py
services/reports_service.py
services/sqlite_pool.py
services/stock_ohlcv.py
services/tasi_index.py
services/user_service.py
services/widgets/__init__.py
services/widgets/providers/__init__.py
services/widgets/providers/crypto.py
services/widgets/providers/indices.py
services/widgets/providers/metals.py
services/widgets/providers/oil.py
services/widgets/quotes_hub.py
services/yfinance_base.py
templates/favicon.svg
templates/index.html
templates/logo.svg
templates/raid-enhancements.css
templates/raid-features.js
tests/__init__.py
tests/conftest.py
tests/COVERAGE_BASELINE.md
tests/integration/__init__.py
tests/integration/test_api_chain.py
tests/integration/test_auth_flow.py
tests/integration/test_health.py
tests/integration/test_pg_path.py
tests/integration/test_query_flow.py
tests/integration/test_rate_limiting.py
tests/performance/__init__.py
tests/performance/test_concurrent_queries.py
tests/performance/test_load.py
tests/README.md
tests/security/__init__.py
tests/security/test_auth_bypass.py
tests/security/test_sql_injection.py
tests/test_api_routes.py
tests/test_app_assembly_v2.py
tests/test_auth_service.py
tests/test_auth.py
tests/test_backend_audit.py
tests/test_backend_cache.py
tests/test_backend_middleware.py
tests/test_backend_resilience.py
tests/test_backend_security.py
tests/test_cache.py
tests/test_chart_engine.py
tests/test_connection_pool.py
tests/test_database.py
tests/test_db_compat.py
tests/test_health_config.py
tests/test_ingestion.py
tests/test_middleware.py
tests/test_news_feed_api.py
tests/test_news_scraper.py
tests/test_news_store.py
tests/test_price_loader.py
tests/test_query_router.py
tests/test_rate_limiting.py
tests/test_redis_scheduler.py
tests/test_schemas.py
tests/test_services.py
tests/test_stock_ohlcv.py
tests/test_tasi_endpoint.py
tests/test_tasi_index.py
tests/test_ui_enhancements.py
tests/test_vanna_pipeline.py
tests/test_widgets.py
tests/test_xbrl_processor.py
vanna-skill/references/architecture.md
vanna-skill/references/database_integrations.md
vanna-skill/references/enterprise.md
vanna-skill/references/getting_started.md
vanna-skill/references/index.md
vanna-skill/references/llm_integrations.md
vanna-skill/references/migration.md
vanna-skill/references/other.md
vanna-skill/references/tools.md
vanna-skill/references/web_ui.md
vanna-skill/SKILL.md
vercel.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="api/routes/market_movers.py">
"""
Market movers API route.

Returns top gainers, top losers, and most active stocks from the TASI market.
Works with both SQLite and PostgreSQL backends via ``api.db_helper``.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/market", tags=["market-movers"])
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class MoverStock(BaseModel)
⋮----
ticker: str
short_name: Optional[str] = None
sector: Optional[str] = None
current_price: Optional[float] = None
change_pct: Optional[float] = None
volume: Optional[int] = None
market_cap: Optional[float] = None
week_52_high: Optional[float] = None
week_52_low: Optional[float] = None
⋮----
class MarketMoversResponse(BaseModel)
⋮----
top_gainers: List[MoverStock]
top_losers: List[MoverStock]
most_active: List[MoverStock]
timestamp: str
⋮----
# SQL queries
⋮----
_BASE_SELECT = """
⋮----
_TOP_GAINERS_SQL = (
⋮----
_TOP_LOSERS_SQL = (
⋮----
_MOST_ACTIVE_SQL = (
⋮----
# Helpers
⋮----
def _row_to_mover(row: dict) -> MoverStock
⋮----
# Route
⋮----
@router.get("/movers", response_model=MarketMoversResponse)
@cache_response(ttl=60)
async def get_market_movers() -> dict
⋮----
"""Return top gainers, top losers, and most active stocks."""
</file>

<file path="frontend/src/app/market/components/index.ts">

</file>

<file path="frontend/src/app/market/components/MarketMoversPanel.tsx">
import { useState, useEffect, useCallback } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { fetchMarketMovers, type MoverStock, type MarketMoversData } from '@/lib/api/market-movers';
⋮----
type TabKey = 'gainers' | 'losers' | 'active';
⋮----
function fmtVol(v: number | null): string
⋮----
function MoverRow(
⋮----
<span className=
⋮----
{/* Tabs */}
⋮----
{/* Content */}
</file>

<file path="frontend/src/components/charts/LazySparkline.tsx">
import { useRef, useState, useEffect } from 'react';
import { MiniSparkline } from '@/components/charts';
import { useMiniChartData } from '@/lib/hooks/use-chart-data';
⋮----
export function LazySparkline(
</file>

<file path="frontend/src/components/common/BackToTop.tsx">
import { useState, useEffect, useCallback } from 'react';
import { cn } from '@/lib/utils';
⋮----
const onScroll = ()
</file>

<file path="frontend/src/lib/api/market-movers.ts">
/**
 * Market movers API: top gainers, top losers, most active.
 *
 * Fetches from /api/v1/market/movers which returns the combined
 * MarketMoversData shape ({top_gainers, top_losers, most_active, timestamp}).
 */
⋮----
import { request } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface MoverStock {
  ticker: string;
  short_name: string | null;
  sector: string | null;
  current_price: number | null;
  change_pct: number | null;
  volume: number | null;
  market_cap: number | null;
  week_52_high: number | null;
  week_52_low: number | null;
}
⋮----
export interface MarketMoversData {
  top_gainers: MoverStock[];
  top_losers: MoverStock[];
  most_active: MoverStock[];
  timestamp: string;
}
⋮----
// ---------------------------------------------------------------------------
// API
// ---------------------------------------------------------------------------
⋮----
export function fetchMarketMovers(signal?: AbortSignal): Promise<MarketMoversData>
</file>

<file path="frontend/src/lib/hooks/use-keyboard-nav.ts">
import { useEffect, RefObject } from 'react';
⋮----
interface UseKeyboardNavOptions {
  /** Ref to the search input element to focus when '/' is pressed */
  searchRef?: RefObject<HTMLInputElement | null>;
  /** Array of row elements for j/k navigation */
  rowRefs?: RefObject<HTMLElement[]>;
  /** Called with the new active row index when j/k is pressed */
  onRowChange?: (index: number) => void;
  /** Whether the hook is enabled */
  enabled?: boolean;
}
⋮----
/** Ref to the search input element to focus when '/' is pressed */
⋮----
/** Array of row elements for j/k navigation */
⋮----
/** Called with the new active row index when j/k is pressed */
⋮----
/** Whether the hook is enabled */
⋮----
/**
 * Keyboard navigation for market table:
 * - '/' -> focus search input
 * - 'j' -> next row
 * - 'k' -> prev row
 * - 'Enter' on row -> click the row link
 * - 'Escape' -> blur search
 */
export function useKeyboardNav({
  searchRef,
  rowRefs,
  onRowChange,
  enabled = true,
}: UseKeyboardNavOptions =
⋮----
const handler = (e: KeyboardEvent) =>
⋮----
// Don't intercept if user is typing in an input/textarea
⋮----
// Focus search input
</file>

<file path="frontend/src/lib/utils/arabic-numbers.ts">
/**
 * Format a number, optionally converting to Eastern Arabic-Indic numerals.
 * Used for price and percentage display in Arabic locale.
 */
⋮----
/** Convert Western digits to Eastern Arabic-Indic numerals */
export function toEasternArabic(str: string): string
⋮----
/**
 * Format a number for display.
 * @param value - The number to format
 * @param opts.decimals - Decimal places (default: 2)
 * @param opts.locale - 'ar' converts to Eastern Arabic numerals
 * @param opts.prefix - Optional prefix (e.g. '+' for positive change)
 */
export function formatNumber(
  value: number | null | undefined,
  opts: { decimals?: number; locale?: string; prefix?: string } = {}
): string
⋮----
/**
 * Format market cap in human-readable form (B/M suffix).
 */
export function formatMarketCap(value: number | null | undefined, locale?: string): string
</file>

<file path="templates/favicon.svg">
<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" width="32" height="32">
  <defs>
    <linearGradient id="favGold" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" stop-color="#E8C872"/>
      <stop offset="50%" stop-color="#D4A84B"/>
      <stop offset="100%" stop-color="#B8860B"/>
    </linearGradient>
  </defs>

  <!-- Dark circular background -->
  <circle cx="16" cy="16" r="15" fill="#1A1A1A"/>
  <circle cx="16" cy="16" r="15" fill="none" stroke="#D4A84B" stroke-width="1" opacity="0.4"/>

  <!-- Lightning bolt centered -->
  <path d="M18.5,5 L11,17.5 L15,17.5 L12.5,27 L22,14.5 L18,14.5 L20.5,5 Z"
        fill="url(#favGold)"/>
</svg>
</file>

<file path="templates/logo.svg">
<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 200 60" width="200" height="60">
  <defs>
    <linearGradient id="goldGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" stop-color="#B8860B"/>
      <stop offset="50%" stop-color="#D4A84B"/>
      <stop offset="100%" stop-color="#E8C872"/>
    </linearGradient>
    <linearGradient id="goldGradVert" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" stop-color="#E8C872"/>
      <stop offset="50%" stop-color="#D4A84B"/>
      <stop offset="100%" stop-color="#B8860B"/>
    </linearGradient>
    <linearGradient id="boltGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" stop-color="#E8C872"/>
      <stop offset="100%" stop-color="#B8860B"/>
    </linearGradient>
  </defs>

  <!-- Octagonal frame inspired by Islamic geometric patterns -->
  <polygon points="30,4 42,4 52,14 52,26 52,36 52,46 42,56 30,56 20,56 8,46 8,36 8,26 8,14 20,4"
           fill="none" stroke="url(#goldGrad)" stroke-width="1.5" opacity="0.6"/>

  <!-- Inner octagonal accent -->
  <polygon points="30,10 38,10 46,18 46,30 46,42 38,50 30,50 22,50 14,42 14,30 14,18 22,10"
           fill="none" stroke="url(#goldGrad)" stroke-width="0.8" opacity="0.3"/>

  <!-- Lightning bolt icon centered in octagon -->
  <path d="M34,14 L24,32 L30,32 L26,46 L38,26 L32,26 L36,14 Z"
        fill="url(#boltGrad)" opacity="0.95"/>

  <!-- Small chart bars at bottom-right of icon area -->
  <rect x="40" y="40" width="3" height="8" rx="0.5" fill="url(#goldGrad)" opacity="0.5"/>
  <rect x="44.5" y="36" width="3" height="12" rx="0.5" fill="url(#goldGrad)" opacity="0.6"/>
  <rect x="49" y="32" width="3" height="16" rx="0.5" fill="url(#goldGrad)" opacity="0.7"/>

  <!-- "Ra'd" text in stylized lettering -->
  <text x="68" y="38" font-family="'Segoe UI', 'Helvetica Neue', Arial, sans-serif"
        font-size="28" font-weight="700" fill="url(#goldGrad)" letter-spacing="1">
    Ra'd
  </text>

  <!-- Subtitle -->
  <text x="68" y="52" font-family="'Segoe UI', 'Helvetica Neue', Arial, sans-serif"
        font-size="8.5" font-weight="400" fill="#D4A84B" letter-spacing="3" opacity="0.7">
    SAUDI STOCKS AI
  </text>

  <!-- Thin decorative line under main text -->
  <line x1="68" y1="42" x2="130" y2="42" stroke="url(#goldGrad)" stroke-width="0.5" opacity="0.4"/>
</svg>
</file>

<file path=".github/pull_request_template.md">
## Changes
<!-- Describe what changed and why -->

## Checklist
- [ ] All Python tests pass (`python -m unittest discover -s tests`)
- [ ] Database tests pass (`python test_database.py`)
- [ ] Assembly tests pass (`python test_app_assembly_v2.py`)
- [ ] Frontend builds (`cd frontend && npm run build`)
- [ ] Frontend tests pass (`cd frontend && npx vitest run`)
- [ ] No new ESLint warnings
- [ ] API changes documented in docs/api-contracts.md
</file>

<file path=".gitignore">
# Python
__pycache__/
.pytest_cache/
.benchmarks/
*.pyc
*.pyo
*.pyd

# Environment files
.env
.env.*
.env.local
.env.production
.env.staging
frontend/.env.local
frontend/.env.*

# Credentials and keys
*.pem
*.key
*.p12
*.pfx
credentials.json

# IDE and OS
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store
Thumbs.db

# Database files (generated)
*.db-shm
*.db-wal

# Build artifacts
node_modules/
.next/
dist/
build/

# Logs
*.log

# Project-specific generated dirs
nul
f8c88490871f6169/
</file>

<file path="api/__init__.py">
# API package for TASI AI platform
</file>

<file path="api/models/__init__.py">
"""API models package."""
</file>

<file path="api/models/widgets.py">
"""Pydantic models for live market widgets."""
⋮----
class QuoteItem(BaseModel)
⋮----
"""A single market quote for display in live widgets."""
⋮----
symbol: str
name: str
asset_class: Literal["crypto", "metal", "oil", "index", "fx", "other"]
price: float
currency: str
change: Optional[float] = None
change_pct: Optional[float] = None
ts_iso: str
source: str
is_delayed: bool = False
delay_minutes: int = 0
</file>

<file path="api/routes/__init__.py">
# API route modules for TASI AI platform
</file>

<file path="api/routes/market_overview.py">
"""
Market Overview API route for the World 360 page.

Provides real-time prices, daily change, sparkline data (30-day),
and historical closes (90-day) for 10 global instruments via yfinance.
Works with any backend (no database dependency).
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/market-overview", tags=["market-overview"])
⋮----
# ---------------------------------------------------------------------------
# Instrument definitions
⋮----
INSTRUMENTS: Dict[str, Dict[str, str]] = {
⋮----
# Response models
⋮----
class InstrumentData(BaseModel)
⋮----
"""Data for a single instrument."""
⋮----
key: str
ticker: str
nameAr: str
nameEn: str
category: str
value: Optional[float] = None
change: Optional[float] = None
sparkline: List[float] = []
historical_closes: List[float] = []
currency: str = "USD"
error: Optional[str] = None
⋮----
class MarketOverviewResponse(BaseModel)
⋮----
"""Response from the market overview endpoint."""
⋮----
instruments: List[InstrumentData]
timestamp: str
count: int
⋮----
# yfinance data fetcher (synchronous -- called via asyncio.to_thread)
⋮----
def _fetch_instrument_sync(symbol: str, info: Dict[str, str]) -> dict
⋮----
"""Fetch price, change, and historical data for one instrument.

    This is a synchronous function meant to be called via asyncio.to_thread().
    """
⋮----
tkr = yf.Ticker(info["ticker"])
⋮----
# Fetch 90 days of daily history (covers both sparkline and correlation)
hist = tkr.history(period="90d", interval="1d", auto_adjust=True, timeout=10)
⋮----
closes = hist["Close"].dropna().tolist()
⋮----
# Current price: last close
price = closes[-1] if closes else None
⋮----
# Daily change %: compare last two closes
change_pct = None
⋮----
prev = closes[-2]
⋮----
change_pct = round(((closes[-1] - prev) / prev) * 100, 2)
⋮----
# Sparkline: last 30 closes
sparkline = closes[-30:] if len(closes) >= 30 else closes
⋮----
# Determine currency from ticker info
currency = "USD"
⋮----
currency = "SAR"
⋮----
# Route
⋮----
@router.get("", response_model=MarketOverviewResponse)
@cache_response(ttl=60)
async def get_market_overview() -> dict
⋮----
"""Return live prices for 10 global instruments with sparkline and historical data.

    Data is cached for 60 seconds. Each instrument fetch runs concurrently
    via asyncio.to_thread to avoid blocking the event loop.
    """
tasks = [
results = await asyncio.gather(*tasks, return_exceptions=True)
⋮----
instruments = []
</file>

<file path="api/schemas/announcements.py">
"""Pydantic schemas for announcement endpoints."""
⋮----
class AnnouncementCreate(BaseModel)
⋮----
"""Request body for creating an announcement."""
⋮----
ticker: Optional[str] = Field(None, max_length=20)
title_ar: Optional[str] = Field(None, max_length=500)
title_en: Optional[str] = Field(None, max_length=500)
body_ar: Optional[str] = None
body_en: Optional[str] = None
source: Optional[str] = Field(None, max_length=50)
announcement_date: Optional[datetime] = None
category: Optional[str] = Field(None, max_length=100)
classification: Optional[str] = Field(None, max_length=100)
is_material: bool = False
source_url: Optional[str] = Field(None, max_length=2000)
⋮----
class AnnouncementResponse(BaseModel)
⋮----
"""Response model for a single announcement."""
⋮----
id: str
ticker: Optional[str] = None
title_ar: Optional[str] = None
title_en: Optional[str] = None
⋮----
source: Optional[str] = None
⋮----
category: Optional[str] = None
classification: Optional[str] = None
⋮----
source_url: Optional[str] = None
created_at: Optional[datetime] = None
</file>

<file path="api/schemas/common.py">
"""Common schema types shared across all API route modules."""
⋮----
T = TypeVar("T")
⋮----
class PaginationParams
⋮----
"""Injectable FastAPI dependency for standard pagination query parameters.

    Usage::

        @router.get("/items")
        async def list_items(pagination: PaginationParams = Depends()):
            items = svc.get_items(limit=pagination.limit, offset=pagination.offset)
    """
⋮----
@property
    def offset(self) -> int
⋮----
@property
    def limit(self) -> int
⋮----
class PaginatedResponse(BaseModel, Generic[T])
⋮----
"""Generic paginated response wrapper."""
⋮----
items: List[T]
total: int
page: int
page_size: int
total_pages: int
⋮----
"""Construct a paginated response with computed total_pages."""
⋮----
class ErrorResponse(BaseModel)
⋮----
"""Standard error response body."""
⋮----
detail: str
code: Optional[str] = None
</file>

<file path="api/schemas/news.py">
"""Pydantic schemas for news article endpoints."""
⋮----
class NewsCreate(BaseModel)
⋮----
"""Request body for creating a news article."""
⋮----
title: str = Field(..., min_length=1, max_length=500)
content: str = Field(..., min_length=1)
ticker: Optional[str] = Field(None, max_length=20)
source: Optional[str] = Field(None, max_length=200)
source_url: Optional[str] = Field(None, max_length=2000)
language: str = Field("ar", max_length=5)
sentiment_score: Optional[float] = Field(None, ge=-1.0, le=1.0)
sentiment_label: Optional[str] = Field(None, max_length=20)
⋮----
class NewsUpdate(BaseModel)
⋮----
"""Request body for updating a news article. All fields optional."""
⋮----
title: Optional[str] = Field(None, min_length=1, max_length=500)
content: Optional[str] = Field(None, min_length=1)
⋮----
language: Optional[str] = Field(None, max_length=5)
⋮----
class NewsResponse(BaseModel)
⋮----
"""Response model for a single news article."""
⋮----
id: str
ticker: Optional[str] = None
title: str
body: Optional[str] = None
source_name: Optional[str] = None
source_url: Optional[str] = None
published_at: Optional[datetime] = None
sentiment_score: Optional[float] = None
sentiment_label: Optional[str] = None
language: str = "ar"
created_at: Optional[datetime] = None
</file>

<file path="api/schemas/reports.py">
"""Pydantic schemas for technical report endpoints."""
⋮----
class ReportCreate(BaseModel)
⋮----
"""Request body for creating a technical report."""
⋮----
title: str = Field(..., min_length=1, max_length=500)
ticker: Optional[str] = Field(None, max_length=20)
summary: Optional[str] = Field(None, max_length=5000)
author: Optional[str] = Field(None, max_length=200)
source_name: Optional[str] = Field(None, max_length=200)
source_url: Optional[str] = Field(None, max_length=2000)
published_at: Optional[datetime] = None
recommendation: Optional[str] = Field(None, max_length=50)
target_price: Optional[float] = Field(None, ge=0)
current_price_at_report: Optional[float] = Field(None, ge=0)
report_type: Optional[str] = Field(None, max_length=50)
⋮----
class ReportUpdate(BaseModel)
⋮----
"""Request body for updating a technical report. All fields optional."""
⋮----
title: Optional[str] = Field(None, min_length=1, max_length=500)
⋮----
class ReportResponse(BaseModel)
⋮----
"""Response model for a single technical report."""
⋮----
id: str
ticker: Optional[str] = None
title: str
summary: Optional[str] = None
author: Optional[str] = None
source_name: Optional[str] = None
source_url: Optional[str] = None
⋮----
recommendation: Optional[str] = None
target_price: Optional[float] = None
current_price_at_report: Optional[float] = None
report_type: Optional[str] = None
created_at: Optional[datetime] = None
</file>

<file path="api/schemas/watchlists.py">
"""Pydantic schemas for watchlist and alert endpoints."""
⋮----
class WatchlistCreateRequest(BaseModel)
⋮----
"""Request body for creating a watchlist."""
⋮----
name: str = Field("Default", min_length=1, max_length=100)
tickers: List[str] = Field(default_factory=list)
⋮----
class WatchlistAddRequest(BaseModel)
⋮----
"""Request body for adding a single ticker to a watchlist."""
⋮----
ticker: str = Field(..., min_length=1, max_length=20)
⋮----
class WatchlistUpdateRequest(BaseModel)
⋮----
"""Request body for updating a watchlist."""
⋮----
name: Optional[str] = Field(None, min_length=1, max_length=100)
tickers: Optional[List[str]] = None
⋮----
class WatchlistResponse(BaseModel)
⋮----
"""Response model for a watchlist."""
⋮----
id: str
user_id: str
name: str
tickers: List[str]
⋮----
class AlertCreateRequest(BaseModel)
⋮----
"""Request body for creating a price/volume alert."""
⋮----
alert_type: str = Field(..., min_length=1, max_length=50)
threshold_value: Optional[float] = None
⋮----
class AlertResponse(BaseModel)
⋮----
"""Response model for a user alert."""
⋮----
ticker: str
alert_type: str
⋮----
is_active: bool = True
</file>

<file path="ARCHITECTURE.md">
# ARCHITECTURE.md

> System architecture documentation for the Ra'd AI TASI Platform.
> Last updated: 2026-02-13

---

## System Diagram

```
                           +-------------------+
                           |    Client / UI    |
                           | (Next.js / Legacy)|
                           +--------+----------+
                                    |
                                    | HTTPS (port 8084)
                                    v
+-----------------------------------------------------------------------+
|                          MIDDLEWARE PIPELINE                           |
|                                                                       |
|  +--------+    +-----------+    +--------+    +--------+    +------+  |
|  |  CORS  | -> | Correlat. | -> | Error  | -> |  Rate  | -> | Auth |  |
|  |        |    |    ID     |    |Handler |    |Limiter |    | JWT  |  |
|  +--------+    +-----------+    +--------+    +--------+    +------+  |
|       |              |               |             |            |     |
|       |         X-Request-ID    Safe JSON      429 + HDR    401/403  |
|       |         on all resp     errors         Retry-After           |
+-------+------+-------+--------+-------+---------+-------+----+------+
               |                                           |
               v                                           v
+------------------------------+          +-------------------------------+
|      FastAPI Router          |          |      Vanna 2.0 Agent          |
|                              |          |                               |
|  /health, /ready, /metrics   |          |  NL Query -> LLM -> SQL      |
|  /api/auth/* (login, reg.)   |          |                               |
|  /api/v1/news/*              |          |  +-------------------------+  |
|  /api/v1/tasi-index/*        |          |  | SQL Validation Pipeline |  |
|  /api/v1/charts/*            |          |  |                         |  |
|  /api/v1/entities/*          |          |  |  Sanitizer -> Validator |  |
|  /api/v1/stock-ohlcv/*       |          |  |  -> Allowlist -> Exec   |  |
|  /api/v1/announcements/*     |          |  +------------+------------+  |
+------------------------------+          +---------------+---------------+
               |                                          |
               v                                          v
+-----------------------------------------------------------------------+
|                         SERVICE LAYER                                 |
|                                                                       |
|  +----------+  +---------+  +---------+  +-----------+  +----------+  |
|  |  Query   |  | Circuit |  |  Retry  |  |  Timeout  |  |   Cost   |  |
|  |  Cache   |  | Breaker |  |  Logic  |  |  Manager  |  | Control  |  |
|  +----+-----+  +----+----+  +----+----+  +-----+-----+  +----+-----+  |
|       |             |            |              |             |        |
|  +----+-----+  +----+----+  +---+---+                                 |
|  | Compress.|  |  Maint.  |  | Audit |                                |
|  +----------+  +----------+  | Logger|                                |
|                              +---+---+                                |
+-----------------------------------------------------------------------+
               |                   |
               v                   v
+---------------------------+  +---------------------------+
|      DATA LAYER           |  |      AUDIT / SECURITY     |
|                           |  |                           |
|  SQLite (dev)             |  |  query_audit_log (PG)     |
|    saudi_stocks.db        |  |  security_events (PG)     |
|    10 normalized tables   |  |  Structured JSON logs     |
|                           |  |                           |
|  PostgreSQL (prod)        |  +---------------------------+
|    All core + PG-only     |
|    Connection pool        |  +---------------------------+
|    WAL archiving          |  |      EXTERNAL SERVICES    |
|                           |  |                           |
|  Redis                    |  |  Gemini / Anthropic LLM   |
|    db=0: Query cache      |  |  Yahoo Finance (TASI)     |
|    db=1: Rate limits +    |  |  5 Arabic news sources    |
|          cost tracking    |  +---------------------------+
+---------------------------+
```

---

## Component Descriptions

### Middleware Pipeline

Middleware executes in registration order for every HTTP request:

| Order | Component | Module | Purpose |
|---|---|---|---|
| 1 | **CORS** | `middleware/cors.py` | Validates `Origin` header against `MW_CORS_ORIGINS`; blocks unauthorized cross-origin requests |
| 2 | **Correlation ID** | `backend/services/audit/correlation.py` | Assigns UUID4 `X-Request-ID`; stores in contextvar for request-scoped tracing |
| 3 | **Error Handler** | `middleware/error_handler.py` | Catches unhandled exceptions; returns uniform `{"error": {...}}` JSON; hides stack traces in production |
| 4 | **Request Logging** | `middleware/request_logging.py` | Logs method, path, status, duration, IP for every request (JSON format) |
| 5 | **Rate Limiter** (existing) | `middleware/rate_limit.py` | In-memory sliding window per IP with tiered path limits |
| 6 | **Rate Limiter** (new) | `backend/middleware/rate_limit_middleware.py` | Redis-backed sliding window; JWT user ID or IP identification; `X-RateLimit-*` headers |
| 7 | **GZip Compression** | `backend/services/cache/compression.py` | Compresses responses above threshold; `Content-Encoding: gzip` |

### FastAPI Routes

| Router | Prefix | Auth | Description |
|---|---|---|---|
| `api/routes/health.py` | `/health`, `/health/live`, `/health/ready` | No | Liveness, readiness, full health report |
| `backend/routes/health.py` | `/health`, `/ready`, `/metrics/basic` | No | Enhanced health + operational metrics + circuit breaker states |
| `api/routes/auth.py` | `/api/auth` | No | Register, login, refresh, logout |
| `api/routes/news.py` | `/api/v1/news` | Optional | News articles CRUD |
| `api/routes/news_feed.py` | `/api/v1/news/feed` | No | Scraped Arabic news feed |
| `api/routes/announcements.py` | `/api/v1/announcements` | Optional | CMA/Tadawul announcements |
| `api/routes/tasi_index.py` | `/api/v1/tasi-index` | No | TASI index data (OHLCV + health) |
| `api/routes/charts.py` | `/api/v1/charts` | No | Chart generation |
| `api/routes/charts_analytics.py` | `/api/v1/charts/analytics` | No | Chart analytics |
| `api/routes/entities.py` | `/api/v1/entities` | No | Company entity lookup (PG) |
| `api/routes/sqlite_entities.py` | `/api/v1/sqlite/entities` | No | Company entity lookup (SQLite) |
| `api/routes/stock_ohlcv.py` | `/api/v1/stock-ohlcv` | No | Individual stock OHLCV data |
| `api/routes/stock_data.py` | `/api/v1/stock-data` | No | Stock data endpoints |
| `api/routes/market_analytics.py` | `/api/v1/market-analytics` | No | Market analytics |
| `api/routes/reports.py` | `/api/v1/reports` | Optional | Technical reports |
| `api/routes/watchlists.py` | `/api/v1/watchlists` | Yes | User watchlists (JWT required) |
| Vanna SSE | `/api/v1/chat` (SSE) | No | Natural language to SQL chat (Vanna 2.0 agent) |

### Vanna 2.0 Agent

The core AI pipeline assembled in `app.py`:

```
User NL Query
    |
    v
SaudiStocksSystemPromptBuilder  -- schema docs + constraints
    |
    v
AnthropicLlmService / Gemini   -- NL -> SQL generation
    |
    v
ToolRegistry
    |-- RunSqlTool              -- executes validated SQL
    |-- VisualizeDataTool       -- Plotly chart generation
    |
    v
Agent (stream_responses=True, max_tool_iterations=10)
    |
    v
SSE Response
```

### SQL Validation Pipeline

Three-stage defense for AI-generated SQL:

```
Stage 1: Input Sanitizer (backend/security/sanitizer.py)
    - Strip control chars, Unicode NFC normalize
    - HTML escape, truncate to 2000 chars
    - Reject raw SQL input
         |
         v
Stage 2: SqlQueryValidator (backend/security/sql_validator.py)
    - sqlparse parsing + analysis
    - Forbidden operation detection (28+ operations)
    - Injection pattern matching (9 regex patterns)
    - Stacked query detection
    - Comment injection scanning
    - Schema probing detection
    - Risk score calculation (0.0-1.0)
         |
         v
Stage 3: QueryAllowlist (backend/security/allowlist.py)
    - Table allowlist enforcement (hot-reload from JSON)
    - Operation allowlist (SELECT only by default)
    - Blocked table rejection
         |
         v
validate_vanna_output() -> ValidatedQuery {is_safe, sql, reason, risk_score}
```

### Resilience Pipeline

Protection against cascading failures:

```
External Call (LLM, DB, Yahoo Finance)
    |
    v
Circuit Breaker (backend/services/resilience/circuit_breaker.py)
    - States: CLOSED -> OPEN -> HALF_OPEN -> CLOSED
    - failure_threshold=5, recovery_timeout=30s
    - Global registry for health reporting
         |
         v
Retry with Backoff (backend/services/resilience/retry.py)
    - @with_retry: exponential backoff + jitter
    - max_attempts=3, base_delay=1s, max_delay=30s
    - Configurable retryable_exceptions
         |
         v
Timeout Manager (backend/services/resilience/timeout_manager.py)
    - Default: 30s, Max: 120s
    - Slow query logging (threshold: 5s)
    - PG backend cancellation on timeout
         |
         v
Result or CircuitBreakerOpen / TimeoutError / RetryExhausted
```

### Caching Pipeline

```
SQL Query
    |
    v
QueryCache.get(sql)  -- SHA-256 key, msgpack deserialization
    |
    +-- HIT  -> Return cached data (hit_rate tracked)
    |
    +-- MISS -> Execute query
                    |
                    v
               classify_tier(sql)
                    |
                    +-- MARKET:     60s TTL
                    +-- HISTORICAL: 3600s TTL
                    +-- SCHEMA:     86400s TTL
                    |
                    v
               QueryCache.set(sql, data, tier)
                    |
                    v
               compress_large_response() if > 1024 bytes
                    |
                    v
               RedisManager.set(key, packed, ttl)  (db=0)
```

### Data Layer

**SQLite (development):**
- 10 normalized tables from 1062-column CSV
- 500 TASI-listed companies
- 7 single-row tables + 3 financial statement tables (unpivoted)
- Path: `saudi_stocks.db` (script-relative)

**PostgreSQL (production):**
- All SQLite tables + enrichment tables (sectors, entities, filings, XBRL)
- User management (users, watchlists, alerts)
- Content tables (news_articles, technical_reports, announcements)
- Audit tables (query_audit_log, security_events)
- Connection pool: SQLAlchemy async engine (pool_size=5, max_overflow=10)
- Indexes: 9 production indexes for audit/security tables

**Redis:**
- db=0: Query cache (msgpack serialized, tiered TTLs)
- db=1: Rate limiting (sorted sets) + cost tracking (hash maps)
- Optional: all Redis-dependent features fall back to in-memory

### Audit System

```
Every Request
    |
    v
CorrelationMiddleware -> request_id (contextvar)
    |
    +-> Structured Logger (JSON)
    |     - timestamp, level, logger, message, request_id
    |     - Extra fields merged from caller
    |
    +-> QueryAuditLogger
    |     - Logs to tasi.audit.query
    |     - Persists to query_audit_log (PG, best-effort)
    |     - Fields: nl_query, generated_sql, validation_result,
    |       execution_time_ms, row_count, risk_score, ip_address
    |
    +-> SecurityEventLogger
          - Logs to tasi.audit.security
          - Persists to security_events (PG, best-effort)
          - Types: sql_injection_attempt, forbidden_keyword,
            rate_limit_exceeded, auth_failure, invalid_input,
            suspicious_pattern, unauthorized_access
          - Severity: LOW, MEDIUM, HIGH, CRITICAL
```

---

## Technology Stack

| Layer | Technology | Version | Rationale |
|---|---|---|---|
| **Runtime** | Python | 3.11+ | Vanna 2.0 + FastAPI compatibility |
| **Web Framework** | FastAPI | 0.115.6+ | Async, OpenAPI docs, SSE streaming |
| **AI Agent** | Vanna 2.0 | 2.0.2 | NL-to-SQL with tool calling |
| **LLM** | Gemini 2.5 Flash / Claude Sonnet 4.5 | Current | Primary: Gemini; fallback: Anthropic |
| **Frontend** | Next.js 14 + TypeScript + Tailwind CSS | 14.x | RSC, app router, RTL Arabic support |
| **Legacy UI** | HTML + vanna-chat web component | - | CDN-loaded, gold/dark theme |
| **Database (dev)** | SQLite | 3.x | Zero-config local development |
| **Database (prod)** | PostgreSQL | 16 | Full schema, extensions, connection pooling |
| **Cache** | Redis | 5.0+ | Query cache (db=0), rate limiting (db=1) |
| **SQL Parsing** | sqlparse | 0.5+ | Query validation and table extraction |
| **Auth** | PyJWT + bcrypt | - | HS256 tokens, password hashing |
| **Serialization** | msgpack | - | Compact cache storage |
| **ORM/Pool** | SQLAlchemy (async) | 2.x | Async connection pool with QueuePool |
| **Config** | pydantic-settings | 2.x | Typed env var management |
| **Container** | Docker + Docker Compose | - | PostgreSQL + app + pgAdmin |
| **Deployment** | Railway | - | PaaS hosting with auto-deploy |
| **CI/CD** | GitHub Actions | - | CI (lint, test) + Deploy workflows |
| **Charts** | Plotly + lightweight-charts + TradingView | - | AI charts, TASI index, individual stocks |
| **News Scraping** | requests + BeautifulSoup4 + lxml | - | 5 Arabic news sources, 30min scheduler |

---

## Key Design Decisions

1. **Dual database backend**: SQLite for zero-config development; PostgreSQL for production. Controlled by `DB_BACKEND` env var with shared service interfaces.

2. **Redis is optional**: All Redis-dependent features (caching, distributed rate limiting, cost tracking) fall back to in-memory implementations. Development works without Redis.

3. **Defense-in-depth SQL security**: Three-stage validation pipeline (sanitizer + validator + allowlist) prevents SQL injection even when the LLM generates malicious output.

4. **Fire-and-forget audit logging**: Audit persistence failures are logged but never block requests. The audit trail is a reliability asset, not a liability.

5. **Circuit breaker registry**: All circuit breakers register globally, enabling health endpoints to report their states for monitoring dashboards.

6. **Correlation ID everywhere**: A single `X-Request-ID` flows through middleware, logs, audit tables, and error responses, enabling end-to-end request tracing.

7. **Tiered caching**: Query results are automatically classified into market (60s), historical (1h), or schema (24h) tiers based on SQL content heuristics.

8. **Cost controls**: Per-user daily/monthly token budgets prevent runaway LLM costs with Redis-backed tracking.
</file>

<file path="auth/password.py">
"""
Password hashing utilities using bcrypt.

All passwords are hashed with bcrypt before storage. Each hash includes
a unique salt, so identical passwords produce different hashes.
"""
⋮----
def hash_password(password: str) -> str
⋮----
"""Hash a plaintext password with bcrypt. Returns the hash string."""
⋮----
def verify_password(plain_password: str, hashed_password: str) -> bool
⋮----
"""Verify a plaintext password against a bcrypt hash."""
</file>

<file path="backend/__init__.py">

</file>

<file path="backend/middleware/models.py">
"""
Pydantic models for rate limiting.

Defines the RateLimitResult returned by the RateLimiter after evaluating
a request, and related configuration types.
"""
⋮----
class RateLimitResult(BaseModel)
⋮----
"""Result of a rate limit check for a single request.

    Attributes
    ----------
    allowed : bool
        Whether the request is allowed to proceed.
    limit : int
        Maximum requests allowed in the current window.
    remaining : int
        Requests remaining in the current window.
    reset_after : int
        Seconds until the window resets (used for Retry-After header).
    identifier : str
        The rate limit key (user ID or IP address).
    bucket : str
        The rate limit bucket that was matched (e.g. path prefix or "_default").
    """
⋮----
allowed: bool
limit: int
remaining: int = Field(ge=0)
reset_after: int = Field(ge=0)
identifier: str
bucket: str = "_default"
</file>

<file path="backend/middleware/rate_limit_middleware.py">
"""
FastAPI rate limiting middleware.

Integrates with RateLimiter to enforce per-request rate limits.
Extracts client identity from JWT bearer token (user_id) or falls back
to the client IP address. Sets standard X-RateLimit-* response headers
and returns 429 JSON when the limit is exceeded.

Health and liveness endpoints are skipped by default.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Default paths that bypass rate limiting
_DEFAULT_SKIP_PATHS: Set[str] = {
⋮----
class RateLimitMiddleware(BaseHTTPMiddleware)
⋮----
"""FastAPI middleware that enforces rate limits per client.

    Parameters
    ----------
    app : ASGIApp
        The ASGI application.
    limiter : RateLimiter
        The rate limiter instance (Redis or in-memory).
    default_limit : int
        Default requests per window if no path rule matches.
    default_window : int
        Default sliding window size in seconds.
    skip_paths : set[str] or None
        Exact paths that bypass rate limiting. Merged with built-in defaults.
    path_limits : dict[str, tuple[int, int]] or None
        Mapping of path prefix -> (limit, window_seconds).
        Example: {"/api/auth": (20, 60), "/api/v1/query": (50, 3600)}
        Longest matching prefix wins.
    """
⋮----
# Sort path_limits by prefix length descending for longest-match-first
⋮----
def _resolve_limit(self, path: str) -> tuple
⋮----
"""Return (bucket, limit, window) for the given request path.

        Matches the longest prefix from path_limits, or falls back to
        the default limit/window.
        """
⋮----
def _extract_identifier(self, request: Request) -> str
⋮----
"""Extract client identity: JWT user_id if present, else IP address.

        Reads the Authorization header for a Bearer token. On decode failure
        (expired, invalid, missing), falls back to the client IP.
        """
auth_header = request.headers.get("authorization", "")
⋮----
token = auth_header[7:]
⋮----
payload = decode_token(token, expected_type="access")
user_id = payload.get("sub")
⋮----
# Token invalid/expired -- fall through to IP
⋮----
# Fallback: client IP
⋮----
async def dispatch(self, request: Request, call_next) -> Response
⋮----
"""Process each request through the rate limiter."""
path = request.url.path
⋮----
# Skip health/docs endpoints
⋮----
identifier = self._extract_identifier(request)
⋮----
result = self.limiter.check(
⋮----
request_id = getattr(request.state, "request_id", "unknown")
⋮----
# Request allowed -- add rate limit headers to the response
response = await call_next(request)
</file>

<file path="backend/middleware/register.py">
"""
Middleware registration for the Ra'd AI TASI Platform.

Provides a single ``register_middleware(app)`` function that app.py can call
to wire up rate limiting and cost control middleware.

Usage in app.py::

    from backend.middleware.register import register_middleware

    app = FastAPI()
    register_middleware(app)

Environment variables used (add to .env.example if not present):

    # Rate limiting (backend/middleware)
    RATELIMIT_ENABLED=true
    RATELIMIT_DEFAULT_LIMIT=60
    RATELIMIT_DEFAULT_WINDOW=60
    RATELIMIT_REDIS_URL=redis://localhost:6379/1
    RATELIMIT_SKIP_PATHS=
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Module-level singletons for shutdown access
_rate_limiter: Optional[RateLimiter] = None
_cost_controller: Optional[CostController] = None
⋮----
def register_middleware(app: FastAPI) -> None
⋮----
"""Register rate limiting middleware on the FastAPI application.

    Reads configuration from environment variables (RATELIMIT_* prefix).
    If rate limiting is disabled via ``RATELIMIT_ENABLED=false``, this
    function is a no-op.

    Parameters
    ----------
    app : FastAPI
        The FastAPI application instance.
    """
⋮----
config = RateLimitConfig()
⋮----
# Initialize rate limiter (Redis db=1 with in-memory fallback)
_rate_limiter = RateLimiter(redis_url=config.redis_url)
⋮----
# Initialize cost controller on same Redis instance
_cost_controller = CostController(redis_url=config.redis_url)
⋮----
# Register the middleware
⋮----
def get_rate_limiter() -> Optional[RateLimiter]
⋮----
"""Return the global RateLimiter instance, or None if not initialized."""
⋮----
def get_cost_controller() -> Optional[CostController]
⋮----
"""Return the global CostController instance, or None if not initialized."""
⋮----
def shutdown_middleware() -> None
⋮----
"""Close middleware connections. Call during app shutdown."""
⋮----
_rate_limiter = None
⋮----
_cost_controller = None
</file>

<file path="backend/routes/__init__.py">
"""
Backend API routes for Ra'd AI TASI platform.

Provides health check, readiness, and metrics endpoints.
"""
⋮----
__all__ = ["health_router"]
</file>

<file path="backend/security/__init__.py">
"""SQL security module for Ra'd AI TASI Platform.

Provides SQL injection prevention, query validation, input sanitization,
and Vanna integration hooks for AI-generated SQL queries.
"""
⋮----
__all__ = [
</file>

<file path="backend/security/config.py">
"""Security configuration for Ra'd AI SQL validation.

Pydantic Settings model with SECURITY_ prefix for environment variables.
Controls SQL validation behavior, query limits, and security strictness.
"""
⋮----
class SecurityConfig(BaseSettings)
⋮----
"""SQL security settings loaded from environment variables.

    All env vars use the SECURITY_ prefix.

    Attributes:
        max_query_length: Maximum allowed SQL query length in characters.
        max_result_rows: Maximum rows returned from a single query.
        enable_query_logging: Whether to log all validated queries.
        blocked_sql_patterns: Comma-separated additional regex patterns to block.
        allowed_tables_path: Path to the allowed_tables.json config file.
        enable_strict_mode: When True, rejects queries with any risk score > 0.
    """
⋮----
model_config = SettingsConfigDict(env_prefix="SECURITY_")
⋮----
max_query_length: int = Field(
max_result_rows: int = Field(
enable_query_logging: bool = Field(
blocked_sql_patterns: str = Field(
allowed_tables_path: str = Field(
enable_strict_mode: bool = Field(
⋮----
@property
    def blocked_patterns_list(self) -> list[str]
⋮----
"""Parse comma-separated blocked patterns into a list."""
⋮----
@property
    def resolved_allowed_tables_path(self) -> Path
⋮----
"""Return absolute path to allowed_tables.json, resolved relative to project root."""
p = Path(self.allowed_tables_path)
</file>

<file path="backend/security/models.py">
"""Pydantic models for SQL security validation results."""
⋮----
class ValidationResult(BaseModel)
⋮----
"""Result of SQL query validation.

    Attributes:
        is_valid: Whether the query passed all validation checks.
        violations: List of specific violation descriptions found.
        sanitized_sql: The query after whitespace normalization (or empty if invalid).
        risk_score: Numeric risk score from 0.0 (safe) to 1.0 (dangerous).
        tables_accessed: List of table names referenced in the query.
    """
⋮----
is_valid: bool = True
violations: list[str] = Field(default_factory=list)
sanitized_sql: str = ""
risk_score: float = 0.0
tables_accessed: list[str] = Field(default_factory=list)
⋮----
class ValidatedQuery(BaseModel)
⋮----
"""Result of the Vanna output validation pipeline.

    This is the output of the single entry point for validating
    AI-generated SQL before execution.

    Attributes:
        is_safe: Whether the query is safe to execute.
        sql: The validated (possibly sanitized) SQL string.
        reason: Human-readable explanation of the validation outcome.
        risk_score: Numeric risk score from 0.0 (safe) to 1.0 (dangerous).
        validation_time_ms: Time taken for validation in milliseconds.
    """
⋮----
is_safe: bool = True
sql: str = ""
reason: str = ""
⋮----
validation_time_ms: float = 0.0
</file>

<file path="backend/services/__init__.py">
"""Backend services for Ra'd AI TASI Platform."""
</file>

<file path="backend/services/audit/__init__.py">
"""Audit and structured logging subsystem for Ra'd AI TASI Platform.

Provides:
- Structured JSON logging with automatic request-ID injection
- Correlation ID middleware for end-to-end request tracing
- Query audit logging for NL-to-SQL lifecycle tracking
- Security event logging for threat detection and compliance

Quick start::

    from backend.services.audit import (
        configure_logging,
        get_logger,
        CorrelationMiddleware,
        get_current_request_id,
        QueryAuditLogger,
        SecurityEventLogger,
        QueryAuditEvent,
        SecurityEvent,
        AuditConfig,
    )
"""
⋮----
__all__ = [
</file>

<file path="backend/services/audit/config.py">
"""Audit subsystem configuration.

Pydantic Settings model for the audit module. All environment variables use
the ``AUDIT_`` prefix.

Usage::

    from backend.services.audit.config import AuditConfig

    cfg = AuditConfig()          # reads from env / .env
    if cfg.enable_query_audit:
        ...
"""
⋮----
class AuditConfig(BaseSettings)
⋮----
"""Configuration for the audit and logging subsystem.

    Environment variables are prefixed with ``AUDIT_``.
    """
⋮----
model_config = SettingsConfigDict(env_prefix="AUDIT_")
⋮----
enable_query_audit: bool = Field(
enable_security_events: bool = Field(
enable_request_logging: bool = Field(
log_level: str = Field(
log_format: Literal["json", "text"] = Field(
retention_days: int = Field(
</file>

<file path="backend/services/audit/correlation.py">
"""Request correlation ID middleware using contextvars.

Assigns a UUID4 ``request_id`` to every incoming HTTP request, stores it in
a :mod:`contextvars` variable so it is accessible anywhere in the call stack
(including background tasks spawned by the same request), and returns it in
the ``X-Request-ID`` response header.

If the client sends an ``X-Request-ID`` header, that value is reused instead
of generating a new one, allowing end-to-end tracing across services.

Usage::

    from backend.services.audit.correlation import (
        CorrelationMiddleware,
        get_current_request_id,
    )

    # Register as ASGI middleware (add FIRST so it runs before others):
    app.add_middleware(CorrelationMiddleware)

    # Anywhere in request-handling code:
    rid = get_current_request_id()   # returns str | None
"""
⋮----
# Thread- and async-safe storage for the current request ID.
_request_id_ctx: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar(
⋮----
_HEADER_NAME = "X-Request-ID"
⋮----
def get_current_request_id() -> Optional[str]
⋮----
"""Return the correlation request ID for the current context, or *None*."""
⋮----
class CorrelationMiddleware(BaseHTTPMiddleware)
⋮----
"""ASGI middleware that manages per-request correlation IDs.

    For every request:
    1. Reads ``X-Request-ID`` from the incoming headers, or generates a UUID4.
    2. Stores the ID in a :mod:`contextvars` variable (accessible via
       :func:`get_current_request_id`).
    3. Attaches the ID to ``request.state.request_id`` for downstream
       middleware that reads it from there (e.g. ``RequestLoggingMiddleware``).
    4. Sets ``X-Request-ID`` on the response headers for the caller.
    """
⋮----
async def dispatch(self, request: Request, call_next) -> Response
⋮----
# Prefer client-supplied ID for distributed tracing; fall back to UUID4.
request_id: str = request.headers.get(_HEADER_NAME) or uuid.uuid4().hex
⋮----
# Store in contextvar (available to loggers, services, etc.).
token = _request_id_ctx.set(request_id)
⋮----
# Also set on request.state for middleware that reads it there.
⋮----
response: Response = await call_next(request)
⋮----
# Reset contextvar to prevent leakage between requests.
</file>

<file path="backend/services/audit/migrations/001_audit_table.sql">
-- Migration 001: Query Audit Log table
-- Extended version of the query_audit_log table from database/schema.sql
-- with additional columns for correlation IDs, validation results, and risk scoring.
--
-- This migration is idempotent (uses IF NOT EXISTS).

CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS query_audit_log (
    id                      UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    request_id              VARCHAR(64),
    user_id                 UUID,
    natural_language_query   TEXT NOT NULL,
    generated_sql           TEXT,
    validation_result       VARCHAR(64),
    execution_time_ms       INTEGER,
    row_count               INTEGER,
    was_successful          BOOLEAN DEFAULT TRUE,
    error_message           TEXT,
    ip_address              INET,
    risk_score              DOUBLE PRECISION,
    created_at              TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes for common query patterns
CREATE INDEX IF NOT EXISTS idx_audit_request_id
    ON query_audit_log(request_id);
CREATE INDEX IF NOT EXISTS idx_audit_user_date
    ON query_audit_log(user_id, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_audit_created
    ON query_audit_log(created_at);
CREATE INDEX IF NOT EXISTS idx_audit_risk_score
    ON query_audit_log(risk_score)
    WHERE risk_score IS NOT NULL;
</file>

<file path="backend/services/audit/migrations/002_security_events_table.sql">
-- Migration 002: Security Events table
-- Records security-relevant occurrences such as SQL injection attempts,
-- rate-limit breaches, authentication failures, and suspicious patterns.
--
-- This migration is idempotent (uses IF NOT EXISTS).

CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS security_events (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    timestamp           TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    event_type          VARCHAR(64) NOT NULL,
    severity            VARCHAR(16) NOT NULL,
    user_id             UUID,
    ip_address          INET,
    details             TEXT,
    request_id          VARCHAR(64)
);

-- Indexes for security event queries
CREATE INDEX IF NOT EXISTS idx_security_events_type
    ON security_events(event_type);
CREATE INDEX IF NOT EXISTS idx_security_events_severity
    ON security_events(severity);
CREATE INDEX IF NOT EXISTS idx_security_events_timestamp
    ON security_events(timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_security_events_request_id
    ON security_events(request_id);
CREATE INDEX IF NOT EXISTS idx_security_events_ip
    ON security_events(ip_address)
    WHERE ip_address IS NOT NULL;
</file>

<file path="backend/services/audit/query_audit.py">
"""Query audit logger.

Records every NL-to-SQL query lifecycle event as a structured log entry and
(when a database connection is available) persists it to the ``query_audit_log``
table.

The logger is intentionally fire-and-forget: failures to persist an audit
record are logged as warnings but never raise exceptions or block the
request.  This keeps the audit trail from becoming a reliability liability.

Usage::

    from backend.services.audit.query_audit import QueryAuditLogger

    audit = QueryAuditLogger()
    audit.log(QueryAuditEvent(nl_query="show me top 10 stocks", ...))
"""
⋮----
_log = logging.getLogger("tasi.audit.query")
⋮----
class QueryAuditLogger
⋮----
"""Logs query audit events to structured logging and optionally to a database.

    Parameters
    ----------
    db_connection_factory:
        Optional callable that returns a DB-API 2.0 connection (e.g. psycopg2).
        If *None*, events are only written to the structured logger.
    """
⋮----
_INSERT_SQL = """
⋮----
def log(self, event: QueryAuditEvent) -> None
⋮----
"""Record a query audit event.

        Always emits a structured log line.  If a database factory was
        provided, also persists the event to PostgreSQL.

        Args:
            event: The query audit event to record.
        """
# Auto-fill request_id from correlation context if not set.
⋮----
def _emit_log(self, event: QueryAuditEvent) -> None
⋮----
"""Write the event to the structured logger."""
extra = event.model_dump(exclude_none=True)
level = logging.WARNING if event.error else logging.INFO
⋮----
def _persist(self, event: QueryAuditEvent) -> None
⋮----
"""Persist the event to the query_audit_log table (best-effort)."""
⋮----
conn = self._db_factory()
⋮----
params = event.model_dump()
</file>

<file path="backend/services/audit/security_events.py">
"""Security event logger.

Records security-relevant events (SQL injection attempts, rate-limit hits,
auth failures, etc.) as structured log entries and optionally persists them
to the ``security_events`` database table.

Like :mod:`query_audit`, this logger is fire-and-forget: persistence failures
are logged but never propagated.

Usage::

    from backend.services.audit.security_events import SecurityEventLogger
    from backend.services.audit.models import SecurityEvent, SecurityEventType, SecuritySeverity

    sec = SecurityEventLogger()
    sec.log(SecurityEvent(
        event_type=SecurityEventType.SQL_INJECTION_ATTEMPT,
        severity=SecuritySeverity.HIGH,
        details="DROP TABLE detected in user input",
    ))
"""
⋮----
_log = logging.getLogger("tasi.audit.security")
⋮----
# Map severity to Python log level.
_SEVERITY_TO_LEVEL: dict[SecuritySeverity, int] = {
⋮----
class SecurityEventLogger
⋮----
"""Logs security events to structured logging and optionally to a database.

    Parameters
    ----------
    db_connection_factory:
        Optional callable returning a DB-API 2.0 connection.  When *None*,
        events are only written to the structured logger.
    """
⋮----
_INSERT_SQL = """
⋮----
def log(self, event: SecurityEvent) -> None
⋮----
"""Record a security event.

        Always emits a structured log line at a level matching the event's
        severity.  If a database factory was provided, also persists the
        event to PostgreSQL.

        Args:
            event: The security event to record.
        """
# Auto-fill request_id from correlation context if not set.
⋮----
def _emit_log(self, event: SecurityEvent) -> None
⋮----
"""Write the event to the structured logger at appropriate severity."""
extra = event.model_dump(exclude_none=True, mode="json")
level = _SEVERITY_TO_LEVEL.get(event.severity, logging.WARNING)
⋮----
def _persist(self, event: SecurityEvent) -> None
⋮----
"""Persist the event to the security_events table (best-effort)."""
⋮----
conn = self._db_factory()
⋮----
params = event.model_dump(mode="json")
</file>

<file path="backend/services/cache/__init__.py">
"""Redis caching layer for Ra'd AI TASI Platform.

Provides async Redis connection management with connection pooling,
tiered query caching, database pool management, response compression,
cache maintenance, and centralized configuration.
"""
⋮----
__all__ = [
</file>

<file path="backend/services/cache/compression.py">
"""Response compression utilities for the cache layer.

Provides gzip helpers that can be used both as a FastAPI middleware and as
standalone functions for compressing large payloads before Redis storage.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_DEFAULT_THRESHOLD = 1024  # bytes
_DEFAULT_LEVEL = 6
⋮----
"""Gzip-compress raw bytes.

    Args:
        data: The input bytes.
        level: Compression level (1=fastest, 9=smallest).

    Returns:
        The gzip-compressed bytes.
    """
⋮----
def decompress_bytes(data: bytes) -> bytes
⋮----
"""Decompress gzip bytes.

    Args:
        data: Gzip-compressed bytes.

    Returns:
        The decompressed bytes.
    """
⋮----
"""Conditionally compress a response body.

    Args:
        body: The raw response body.
        threshold: Minimum size in bytes to trigger compression.
        level: gzip compression level.

    Returns:
        A tuple of ``(payload, was_compressed)``.
    """
⋮----
compressed = compress_bytes(body, level=level)
# Only use compressed version if it is actually smaller
⋮----
class GZipCacheMiddleware(BaseHTTPMiddleware)
⋮----
"""FastAPI middleware that gzip-compresses responses above a size threshold.

    This is complementary to (not a replacement for) standard GZip middleware
    shipped with Starlette. It adds ``X-Compressed: true`` and
    ``Content-Encoding: gzip`` headers when compression is applied.

    Args:
        app: The ASGI application.
        threshold: Minimum response body size to trigger compression.
        level: gzip compression level.
    """
⋮----
# Skip if client doesn't accept gzip
accept_encoding = request.headers.get("accept-encoding", "")
⋮----
response = await call_next(request)
⋮----
# Only compress JSON / text responses
content_type = response.headers.get("content-type", "")
⋮----
# Read the body from the streaming response
body_parts: list[bytes] = []
async for chunk in response.body_iterator:  # type: ignore[union-attr]
⋮----
chunk = chunk.encode("utf-8")
⋮----
body = b"".join(body_parts)
⋮----
headers = dict(response.headers)
</file>

<file path="backend/services/cache/config.py">
"""Cache configuration via Pydantic Settings.

All settings are loaded from environment variables with the ``CACHE_`` prefix
(e.g. ``CACHE_ENABLED=true``, ``CACHE_DEFAULT_TTL=300``).
"""
⋮----
class CacheConfig(BaseSettings)
⋮----
"""Configuration for the Redis caching subsystem.

    Attributes:
        enabled: Master switch for the cache layer.
        redis_url: Redis connection URL (database 0 for caching).
        redis_password: Optional Redis auth password.
        redis_max_connections: Maximum connection pool size.
        default_ttl: Default TTL in seconds when no tier applies.
        market_ttl: TTL for live market-data queries (seconds).
        historical_ttl: TTL for historical / financial-statement queries.
        schema_ttl: TTL for schema / metadata queries.
        compression_threshold: Byte threshold above which responses are gzip-compressed before caching.
        compression_level: gzip compression level (1-9).
        warm_on_startup: Whether to pre-warm common queries at startup.
        maintenance_interval: Seconds between maintenance cycles (stats, cleanup).
    """
⋮----
model_config = SettingsConfigDict(
⋮----
enabled: bool = False
redis_url: str = "redis://localhost:6379/0"
redis_password: str = ""
redis_max_connections: int = 20
default_ttl: int = 300
market_ttl: int = 60
historical_ttl: int = 3600
schema_ttl: int = 86400
compression_threshold: int = 1024
compression_level: int = 6
warm_on_startup: bool = False
maintenance_interval: int = 300
</file>

<file path="backend/services/cache/db_pool.py">
"""Async database connection pool manager using SQLAlchemy.

Wraps SQLAlchemy's async engine and session factory to provide connection
pool statistics, health checks, and graceful lifecycle management.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
class DatabasePoolManager
⋮----
"""Manages an async SQLAlchemy engine with pool monitoring.

    Args:
        config: Pool configuration (URL, sizes, timeouts).
    """
⋮----
def __init__(self, config: PoolConfig | None = None) -> None
⋮----
@property
    def engine(self) -> AsyncEngine | None
⋮----
"""The underlying async engine, or None if not connected."""
⋮----
async def connect(self) -> None
⋮----
"""Create the async engine and session factory."""
⋮----
cfg = self._config
⋮----
async def disconnect(self) -> None
⋮----
"""Dispose of the engine and release all pooled connections."""
⋮----
def get_session(self) -> AsyncSession
⋮----
"""Return a new async session from the pool.

        Raises:
            RuntimeError: If connect() has not been called.
        """
⋮----
def pool_stats(self) -> PoolStats
⋮----
"""Return a snapshot of pool utilization metrics.

        Returns an empty PoolStats if the engine is not initialized or if
        the underlying pool does not support status reporting (e.g. NullPool
        or StaticPool used by aiosqlite).
        """
⋮----
pool = self._engine.pool
⋮----
async def health_check(self) -> dict[str, Any]
⋮----
"""Run a lightweight connectivity check against the database.

        Returns:
            Dict with ``status``, ``latency_ms``, and pool stats.
        """
start = time.monotonic()
⋮----
latency_ms = round((time.monotonic() - start) * 1000, 2)
stats = self.pool_stats()
</file>

<file path="backend/services/resilience/__init__.py">
"""
Resilience services for Ra'd AI TASI platform.

Provides circuit breaking, retry logic, timeout management,
and graceful degradation for external service calls.
"""
⋮----
__all__ = [
</file>

<file path="backend/services/resilience/config.py">
"""
Resilience configuration via environment variables.

All settings use the ``RESILIENCE_`` prefix. Example ``.env`` entries::

    RESILIENCE_CB_FAILURE_THRESHOLD=5
    RESILIENCE_CB_RECOVERY_TIMEOUT=30.0
    RESILIENCE_RETRY_MAX_ATTEMPTS=3
    RESILIENCE_QUERY_TIMEOUT=30.0
"""
⋮----
class ResilienceConfig(BaseSettings)
⋮----
"""Resilience settings loaded from environment variables.

    Prefix: ``RESILIENCE_``
    """
⋮----
model_config = {"env_prefix": "RESILIENCE_"}
⋮----
# --- Circuit Breaker ---
cb_failure_threshold: int = Field(
cb_recovery_timeout: float = Field(
cb_half_open_max_calls: int = Field(
cb_success_threshold: int = Field(
⋮----
# --- Retry ---
retry_max_attempts: int = Field(
retry_base_delay: float = Field(
retry_max_delay: float = Field(
retry_jitter: bool = Field(
⋮----
# --- Query Timeout ---
query_timeout: float = Field(
query_slow_threshold: float = Field(
query_max_timeout: float = Field(
query_cancel_on_timeout: bool = Field(
⋮----
# --- Degradation ---
degradation_enabled: bool = Field(
⋮----
_config: ResilienceConfig | None = None
⋮----
def get_resilience_config() -> ResilienceConfig
⋮----
"""Return the cached resilience configuration singleton."""
⋮----
_config = ResilienceConfig()
</file>

<file path="backend/services/resilience/timeout_manager.py">
"""
Query timeout manager for database operations.

Provides deadline-enforced query execution with:
- Configurable timeouts per query type
- Slow query logging with duration and SQL preview
- PostgreSQL backend cancellation via pg_cancel_backend
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
T = TypeVar("T")
⋮----
class QueryTimeoutConfig(BaseModel)
⋮----
"""Configuration for the query timeout manager."""
⋮----
default_timeout: float = Field(
slow_query_threshold: float = Field(
max_timeout: float = Field(
cancel_on_timeout: bool = Field(
⋮----
class QueryTimeoutManager
⋮----
"""Manages query execution with timeouts and slow-query logging.

    For async query functions, wraps execution in ``asyncio.wait_for``.
    For synchronous queries, runs them in a thread executor with a deadline.

    When a PostgreSQL query exceeds the timeout and ``cancel_on_timeout`` is
    enabled, attempts to cancel the backend process via ``pg_cancel_backend``.

    Args:
        config: Timeout configuration. Uses defaults if not provided.
    """
⋮----
def __init__(self, config: Optional[QueryTimeoutConfig] = None) -> None
⋮----
@property
    def slow_query_count(self) -> int
⋮----
"""Number of queries that exceeded the slow query threshold."""
⋮----
@property
    def timeout_count(self) -> int
⋮----
"""Number of queries that timed out."""
⋮----
@property
    def total_queries(self) -> int
⋮----
"""Total number of queries executed through this manager."""
⋮----
"""Execute a query function with a timeout deadline.

        Args:
            func: Async or sync callable to execute.
            *args: Positional arguments for *func*.
            timeout: Override timeout in seconds. Clamped to ``max_timeout``.
            query_label: Label for logging (e.g., SQL preview or query name).
            pg_pid: PostgreSQL backend PID for cancellation on timeout.
            **kwargs: Keyword arguments for *func*.

        Returns:
            The return value of *func*.

        Raises:
            asyncio.TimeoutError: If the query exceeds the timeout.
        """
effective_timeout = min(
⋮----
start = time.monotonic()
⋮----
result = await asyncio.wait_for(
⋮----
loop = asyncio.get_running_loop()
⋮----
elapsed = time.monotonic() - start
⋮----
async def _cancel_pg_backend(self, pid: int, query_label: str) -> None
⋮----
"""Attempt to cancel a PostgreSQL backend process.

        Issues ``SELECT pg_cancel_backend(pid)`` to gracefully cancel a
        long-running query. Requires a superuser or the same role as the
        backend process.

        This is a best-effort operation; failures are logged but not raised.
        """
⋮----
settings = get_settings()
⋮----
conn = psycopg2.connect(
⋮----
cur = conn.cursor()
⋮----
cancelled = cur.fetchone()
⋮----
def _log_if_slow(self, query_label: str, elapsed: float) -> None
⋮----
"""Log a warning if the query exceeded the slow query threshold."""
⋮----
def get_stats(self) -> dict
⋮----
"""Return a summary of timeout manager statistics."""
</file>

<file path="chart_engine/__init__.py">
__all__ = ["RaidChartGenerator"]
</file>

<file path="config/allowed_tables.json">
{
    "description": "Allowlist configuration for Ra'd AI SQL security. Tables and operations listed here are permitted for AI-generated queries.",
    "allowed_tables": [
        "companies",
        "market_data",
        "valuation_metrics",
        "profitability_metrics",
        "dividend_data",
        "financial_summary",
        "analyst_data",
        "balance_sheet",
        "income_statement",
        "cash_flow",
        "news_articles",
        "news_feed"
    ],
    "allowed_operations": [
        "SELECT"
    ],
    "blocked_tables": [
        "sqlite_master",
        "sqlite_sequence",
        "information_schema",
        "pg_catalog",
        "pg_tables",
        "users",
        "user_watchlists",
        "user_alerts",
        "query_audit_log"
    ]
}
</file>

<file path="config/error_tracking.py">
"""
Pluggable error tracking for TASI AI Platform.

Provides an ErrorTracker interface with two implementations:
- LogErrorTracker (default): Reports errors via structured logging.
- SentryErrorTracker (optional): Forwards errors to Sentry.

Usage:
    from config.error_tracking import init_error_tracking, get_error_tracker

    init_error_tracking()  # Call once at startup
    tracker = get_error_tracker()
    tracker.capture_exception(exc)

Environment variables:
    ERROR_TRACKER   - "log" (default) or "sentry"
    SENTRY_DSN      - Sentry DSN (required when ERROR_TRACKER=sentry)
    ENVIRONMENT     - Passed to Sentry as environment tag
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
class ErrorTracker(ABC)
⋮----
"""Abstract interface for error tracking backends."""
⋮----
"""Report an exception to the tracking backend.

        Args:
            exc: The exception to report.
            context: Optional dict of extra context (user, request path, etc.).
        """
⋮----
"""Report a message (without an exception) to the tracking backend.

        Args:
            message: Human-readable description.
            level: Severity level (debug, info, warning, error, critical).
            context: Optional dict of extra context.
        """
⋮----
class LogErrorTracker(ErrorTracker)
⋮----
"""Default error tracker that logs errors via Python logging.

    Uses structured logging from config/logging_config.py so errors
    appear in JSON format in production and pretty format in dev.
    """
⋮----
def __init__(self) -> None
⋮----
extra = {"error_context": context} if context else {}
⋮----
log_level = getattr(logging, level.upper(), logging.ERROR)
⋮----
# ---------------------------------------------------------------------------
# Sentry integration (opt-in)
⋮----
# To enable Sentry:
# 1. pip install sentry-sdk[fastapi]
# 2. Set ERROR_TRACKER=sentry and SENTRY_DSN=https://...@sentry.io/...
# 3. Optionally set ENVIRONMENT=production
#
# class SentryErrorTracker(ErrorTracker):
#     """Error tracker that forwards to Sentry."""
⋮----
#     def __init__(self, dsn: str, environment: str = "production") -> None:
#         import sentry_sdk
#         from sentry_sdk.integrations.fastapi import FastApiIntegration
#         from sentry_sdk.integrations.logging import LoggingIntegration
⋮----
#         sentry_sdk.init(
#             dsn=dsn,
#             environment=environment,
#             traces_sample_rate=0.1,  # 10% of transactions
#             integrations=[
#                 FastApiIntegration(transaction_style="endpoint"),
#                 LoggingIntegration(level=logging.WARNING, event_level=logging.ERROR),
#             ],
#         )
#         self._dsn = dsn
⋮----
#     def capture_exception(
#         self,
#         exc: BaseException,
#         *,
#         context: Optional[dict[str, Any]] = None,
#     ) -> None:
⋮----
#         with sentry_sdk.push_scope() as scope:
#             if context:
#                 for key, value in context.items():
#                     scope.set_extra(key, value)
#             sentry_sdk.capture_exception(exc)
⋮----
#     def capture_message(
⋮----
#         message: str,
⋮----
#         level: str = "error",
⋮----
#             sentry_sdk.capture_message(message, level=level)
⋮----
# Singleton management
⋮----
_tracker: Optional[ErrorTracker] = None
⋮----
def init_error_tracking() -> ErrorTracker
⋮----
"""Initialize the error tracker based on environment configuration.

    Reads ERROR_TRACKER env var:
    - "log" (default): Uses LogErrorTracker (structured logging).
    - "sentry": Would use SentryErrorTracker (uncomment class above).

    Returns:
        The initialized ErrorTracker instance.
    """
⋮----
backend = os.environ.get("ERROR_TRACKER", "log").lower()
⋮----
dsn = os.environ.get("SENTRY_DSN", "")
⋮----
_tracker = LogErrorTracker()
⋮----
# Uncomment SentryErrorTracker class above and this block to enable:
# environment = os.environ.get("ENVIRONMENT", "production")
# _tracker = SentryErrorTracker(dsn=dsn, environment=environment)
⋮----
def get_error_tracker() -> ErrorTracker
⋮----
"""Return the current error tracker, initializing if needed.

    Returns:
        The active ErrorTracker instance.
    """
</file>

<file path="config/prompts.py">
"""
System prompt definitions for the Saudi Stocks AI platform.

Extracted from app.py to improve readability and maintainability.
"""
⋮----
SAUDI_STOCKS_SYSTEM_PROMPT = """\
⋮----
PG_NOTES = """
</file>

<file path="csv_to_sqlite.py">
"""
csv_to_sqlite.py
================
Converts the flat denormalized CSV file 'saudi_stocks_yahoo_data.csv' (500 stocks,
1062 columns) into a normalized SQLite database 'saudi_stocks.db'.

Tables created:
  - companies             (core company info)
  - market_data           (price / volume / shares)
  - valuation_metrics     (PE, PB, EV ratios)
  - profitability_metrics  (margins, growth)
  - dividend_data         (dividends)
  - financial_summary     (key financial aggregates)
  - analyst_data          (targets, recommendations)
  - balance_sheet         (unpivoted, multiple rows per ticker)
  - income_statement      (unpivoted, multiple rows per ticker)
  - cash_flow             (unpivoted, multiple rows per ticker)

Usage:
    python csv_to_sqlite.py
"""
⋮----
# ---------------------------------------------------------------------------
# Configuration
⋮----
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_PATH = os.path.join(SCRIPT_DIR, "saudi_stocks_yahoo_data.csv")
DB_PATH = os.path.join(SCRIPT_DIR, "saudi_stocks.db")
⋮----
# Column mappings for the simple (one-row-per-ticker) tables
# Keys   = target column name in SQLite
# Values = source column name in the CSV
⋮----
COMPANIES_COLS = {
⋮----
MARKET_DATA_COLS = {
⋮----
VALUATION_COLS = {
⋮----
PROFITABILITY_COLS = {
⋮----
DIVIDEND_COLS = {
⋮----
# Note the renamed columns: info_total_debt -> total_debt, etc.
FINANCIAL_SUMMARY_COLS = {
⋮----
ANALYST_COLS = {
⋮----
# Financial statement field lists (after stripping the prefix + 'date')
# These are the Mixed_Case names as they appear in the CSV after the prefix.
⋮----
BS_FIELDS = [
⋮----
IS_FIELDS = [
⋮----
CF_FIELDS = [
⋮----
# Period prefix -> (period_type, period_index)
BS_PERIODS = {
⋮----
IS_PERIODS = {
⋮----
CF_PERIODS = {
⋮----
# DDL statements
⋮----
DDL_COMPANIES = """
⋮----
DDL_MARKET_DATA = """
⋮----
DDL_VALUATION = """
⋮----
DDL_PROFITABILITY = """
⋮----
DDL_DIVIDEND = """
⋮----
DDL_FINANCIAL_SUMMARY = """
⋮----
DDL_ANALYST = """
⋮----
DDL_BALANCE_SHEET = """
⋮----
DDL_INCOME_STATEMENT = """
⋮----
DDL_CASH_FLOW = """
⋮----
INDEX_DDL = [
⋮----
# Helper functions
⋮----
def extract_simple_table(df: pd.DataFrame, col_map: dict) -> pd.DataFrame
⋮----
"""Extract a subset of columns from the master DataFrame, renaming as needed."""
src_cols = list(col_map.values())
# Only keep columns that actually exist in the CSV
existing = [c for c in src_cols if c in df.columns]
missing = [c for c in src_cols if c not in df.columns]
⋮----
sub = df[existing].copy()
# Build reverse map for renaming (src -> target)
rename_map = {v: k for k, v in col_map.items() if v in existing}
⋮----
"""
    Unpivot flat financial columns into normalized rows.

    For each ticker and each period prefix, extracts one row containing
    ticker, period_type, period_index, period_date, and all field values.
    Skips rows where period_date is null/empty.
    """
all_rows = []
tickers = df["ticker"].values
total = len(tickers)
⋮----
date_col = f"{prefix}_date"
⋮----
# Build the list of source column names for this prefix
src_cols = [f"{prefix}_{f}" for f in fields]
⋮----
# Check which source columns exist
existing_src = [c for c in src_cols if c in df.columns]
existing_tgt = [
missing_src = [c for c in src_cols if c not in df.columns]
⋮----
# Get date values
dates = df[date_col].values
⋮----
# Get data for existing columns as a numpy array for speed
⋮----
data_block = df[existing_src].values
⋮----
date_val = dates[row_idx]
# Skip if date is empty / NaN
⋮----
row_dict = {
⋮----
val = data_block[row_idx, col_idx]
⋮----
# Fill missing columns with None
⋮----
field_name = c.split(f"{prefix}_", 1)[1].lower()
⋮----
result = pd.DataFrame(all_rows)
⋮----
def safe_to_sql(df: pd.DataFrame, table_name: str, conn: sqlite3.Connection)
⋮----
"""Write DataFrame to SQLite, replacing NaN with None for proper NULL storage."""
df = df.where(pd.notnull(df), None)
⋮----
# Main
⋮----
def main()
⋮----
t_start = time.time()
⋮----
# -- Read CSV --------------------------------------------------------
⋮----
df = pd.read_csv(CSV_PATH, encoding="utf-8-sig", low_memory=False)
⋮----
# -- Remove existing DB if present -----------------------------------
⋮----
# -- Connect to SQLite -----------------------------------------------
conn = sqlite3.connect(DB_PATH)
⋮----
cur = conn.cursor()
⋮----
# -- Create tables ---------------------------------------------------
⋮----
# -- Populate simple tables ------------------------------------------
simple_tables = [
⋮----
sub = extract_simple_table(df, col_map)
⋮----
count = cur.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
⋮----
# -- Populate financial statement tables (unpivot) -------------------
fin_tables = [
⋮----
result_df = unpivot_financial(df, periods, fields, table_name)
⋮----
# Drop the 'id' column if accidentally present; AUTOINCREMENT handles it
⋮----
# -- Create indexes --------------------------------------------------
⋮----
# -- Summary statistics ----------------------------------------------
elapsed = time.time() - t_start
⋮----
all_tables = [
total_rows = 0
⋮----
count = cur.execute(f"SELECT COUNT(*) FROM {tbl}").fetchone()[0]
</file>

<file path="database/schema.sql">
-- =============================================================================
-- Ra'd AI - TASI Saudi Stock Market Platform
-- PostgreSQL Schema
-- =============================================================================
-- This schema preserves all 10 existing SQLite tables with IDENTICAL column
-- names and adds new tables for XBRL data, price history, news/announcements,
-- user management, and audit logging.
--
-- Type mapping from SQLite:
--   REAL          -> NUMERIC(20,4)    (financial precision)
--   INTEGER       -> BIGINT           (volumes, counts)
--   AUTOINCREMENT -> SERIAL           (migrated tables)
--   TEXT          -> TEXT             (unchanged)
--
-- New tables use UUID primary keys via uuid-ossp extension.
-- =============================================================================

-- ---------------------------------------------------------------------------
-- Extensions
-- ---------------------------------------------------------------------------
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- ===========================================================================
-- SECTION 1: Existing SQLite Tables (10 tables, identical column names)
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- companies (core company info, 500 rows)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS companies (
    ticker                  TEXT PRIMARY KEY,
    short_name              TEXT,
    sector                  TEXT,
    industry                TEXT,
    exchange                TEXT,
    quote_type              TEXT,
    currency                TEXT,
    financial_currency      TEXT,
    market                  TEXT
);

-- ---------------------------------------------------------------------------
-- market_data (price / volume / shares, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS market_data (
    ticker                      TEXT PRIMARY KEY REFERENCES companies(ticker),
    current_price               NUMERIC(20,4),
    previous_close              NUMERIC(20,4),
    open_price                  NUMERIC(20,4),
    day_high                    NUMERIC(20,4),
    day_low                     NUMERIC(20,4),
    week_52_high                NUMERIC(20,4),
    week_52_low                 NUMERIC(20,4),
    avg_50d                     NUMERIC(20,4),
    avg_200d                    NUMERIC(20,4),
    volume                      BIGINT,
    avg_volume                  BIGINT,
    avg_volume_10d              BIGINT,
    beta                        NUMERIC(20,4),
    market_cap                  NUMERIC(20,4),
    shares_outstanding          NUMERIC(20,4),
    float_shares                NUMERIC(20,4),
    implied_shares_outstanding  NUMERIC(20,4),
    pct_held_insiders           NUMERIC(20,4),
    pct_held_institutions       NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- valuation_metrics (PE, PB, EV ratios, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS valuation_metrics (
    ticker              TEXT PRIMARY KEY REFERENCES companies(ticker),
    trailing_pe         NUMERIC(20,4),
    forward_pe          NUMERIC(20,4),
    price_to_book       NUMERIC(20,4),
    price_to_sales      NUMERIC(20,4),
    enterprise_value    NUMERIC(20,4),
    ev_to_revenue       NUMERIC(20,4),
    ev_to_ebitda        NUMERIC(20,4),
    peg_ratio           NUMERIC(20,4),
    trailing_eps        NUMERIC(20,4),
    forward_eps         NUMERIC(20,4),
    book_value          NUMERIC(20,4),
    revenue_per_share   NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- profitability_metrics (margins, growth, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS profitability_metrics (
    ticker                      TEXT PRIMARY KEY REFERENCES companies(ticker),
    roa                         NUMERIC(20,4),
    roe                         NUMERIC(20,4),
    profit_margin               NUMERIC(20,4),
    operating_margin            NUMERIC(20,4),
    gross_margin                NUMERIC(20,4),
    ebitda_margin               NUMERIC(20,4),
    earnings_growth             NUMERIC(20,4),
    revenue_growth              NUMERIC(20,4),
    earnings_quarterly_growth   NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- dividend_data (dividends, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS dividend_data (
    ticker                          TEXT PRIMARY KEY REFERENCES companies(ticker),
    dividend_rate                   NUMERIC(20,4),
    dividend_yield                  NUMERIC(20,4),
    ex_dividend_date                TEXT,
    payout_ratio                    NUMERIC(20,4),
    avg_dividend_yield_5y           NUMERIC(20,4),
    last_dividend_value             NUMERIC(20,4),
    last_dividend_date              TEXT,
    trailing_annual_dividend_rate   NUMERIC(20,4),
    trailing_annual_dividend_yield  NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- financial_summary (key financial aggregates, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS financial_summary (
    ticker                  TEXT PRIMARY KEY REFERENCES companies(ticker),
    total_revenue           NUMERIC(20,4),
    total_cash              NUMERIC(20,4),
    total_cash_per_share    NUMERIC(20,4),
    total_debt              NUMERIC(20,4),
    debt_to_equity          NUMERIC(20,4),
    current_ratio           NUMERIC(20,4),
    quick_ratio             NUMERIC(20,4),
    operating_cashflow      NUMERIC(20,4),
    free_cashflow           NUMERIC(20,4),
    ebitda                  NUMERIC(20,4),
    gross_profits           NUMERIC(20,4),
    net_income_to_common    NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- analyst_data (targets, recommendations, 1 row per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS analyst_data (
    ticker                  TEXT PRIMARY KEY REFERENCES companies(ticker),
    target_mean_price       NUMERIC(20,4),
    target_high_price       NUMERIC(20,4),
    target_low_price        NUMERIC(20,4),
    target_median_price     NUMERIC(20,4),
    analyst_count           BIGINT,
    recommendation          TEXT,
    recommendation_score    NUMERIC(20,4),
    most_recent_quarter     TEXT,
    last_fiscal_year_end    TEXT
);

-- ---------------------------------------------------------------------------
-- balance_sheet (unpivoted, multiple rows per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS balance_sheet (
    id                                              SERIAL PRIMARY KEY,
    ticker                                          TEXT REFERENCES companies(ticker),
    period_type                                     TEXT,
    period_index                                    INTEGER,
    period_date                                     TEXT,
    total_assets                                    NUMERIC(20,4),
    current_assets                                  NUMERIC(20,4),
    cash_and_cash_equivalents                       NUMERIC(20,4),
    cash_cash_equivalents_and_short_term_investments NUMERIC(20,4),
    accounts_receivable                             NUMERIC(20,4),
    inventory                                       NUMERIC(20,4),
    other_current_assets                            NUMERIC(20,4),
    total_non_current_assets                        NUMERIC(20,4),
    net_ppe                                         NUMERIC(20,4),
    goodwill_and_other_intangible_assets            NUMERIC(20,4),
    goodwill                                        NUMERIC(20,4),
    other_intangible_assets                         NUMERIC(20,4),
    long_term_equity_investment                     NUMERIC(20,4),
    other_non_current_assets                        NUMERIC(20,4),
    total_liabilities_net_minority_interest          NUMERIC(20,4),
    current_liabilities                             NUMERIC(20,4),
    current_debt                                    NUMERIC(20,4),
    accounts_payable                                NUMERIC(20,4),
    other_current_liabilities                       NUMERIC(20,4),
    total_non_current_liabilities_net_minority_interest NUMERIC(20,4),
    long_term_debt                                  NUMERIC(20,4),
    long_term_capital_lease_obligation              NUMERIC(20,4),
    capital_lease_obligations                       NUMERIC(20,4),
    other_non_current_liabilities                   NUMERIC(20,4),
    total_equity_gross_minority_interest            NUMERIC(20,4),
    stockholders_equity                             NUMERIC(20,4),
    common_stock_equity                             NUMERIC(20,4),
    retained_earnings                               NUMERIC(20,4),
    common_stock                                    NUMERIC(20,4),
    additional_paid_in_capital                      NUMERIC(20,4),
    treasury_stock                                  NUMERIC(20,4),
    minority_interest                               NUMERIC(20,4),
    total_capitalization                            NUMERIC(20,4),
    net_tangible_assets                             NUMERIC(20,4),
    working_capital                                 NUMERIC(20,4),
    invested_capital                                NUMERIC(20,4),
    tangible_book_value                             NUMERIC(20,4),
    total_debt                                      NUMERIC(20,4),
    net_debt                                        NUMERIC(20,4),
    share_issued                                    NUMERIC(20,4),
    ordinary_shares_number                          NUMERIC(20,4),
    treasury_shares_number                          NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- income_statement (unpivoted, multiple rows per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS income_statement (
    id                                          SERIAL PRIMARY KEY,
    ticker                                      TEXT REFERENCES companies(ticker),
    period_type                                 TEXT,
    period_index                                INTEGER,
    period_date                                 TEXT,
    total_revenue                               NUMERIC(20,4),
    operating_revenue                           NUMERIC(20,4),
    cost_of_revenue                             NUMERIC(20,4),
    gross_profit                                NUMERIC(20,4),
    operating_expense                           NUMERIC(20,4),
    selling_general_and_administration          NUMERIC(20,4),
    general_and_administrative_expense          NUMERIC(20,4),
    research_and_development                    NUMERIC(20,4),
    operating_income                            NUMERIC(20,4),
    net_non_operating_interest_income_expense   NUMERIC(20,4),
    interest_income                             NUMERIC(20,4),
    interest_expense                            NUMERIC(20,4),
    other_non_operating_income_expenses         NUMERIC(20,4),
    pretax_income                               NUMERIC(20,4),
    tax_provision                               NUMERIC(20,4),
    tax_rate_for_calcs                          NUMERIC(20,4),
    net_income                                  NUMERIC(20,4),
    net_income_common_stockholders              NUMERIC(20,4),
    net_income_continuous_operations            NUMERIC(20,4),
    net_income_including_noncontrolling_interests NUMERIC(20,4),
    diluted_eps                                 NUMERIC(20,4),
    basic_eps                                   NUMERIC(20,4),
    diluted_average_shares                      NUMERIC(20,4),
    basic_average_shares                        NUMERIC(20,4),
    ebitda                                      NUMERIC(20,4),
    ebit                                        NUMERIC(20,4),
    reconciled_depreciation                     NUMERIC(20,4),
    total_operating_income_as_reported          NUMERIC(20,4),
    normalized_ebitda                           NUMERIC(20,4),
    normalized_income                           NUMERIC(20,4),
    net_interest_income                         NUMERIC(20,4),
    total_expenses                              NUMERIC(20,4),
    minority_interests                          NUMERIC(20,4)
);

-- ---------------------------------------------------------------------------
-- cash_flow (unpivoted, multiple rows per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS cash_flow (
    id                                  SERIAL PRIMARY KEY,
    ticker                              TEXT REFERENCES companies(ticker),
    period_type                         TEXT,
    period_index                        INTEGER,
    period_date                         TEXT,
    operating_cash_flow                 NUMERIC(20,4),
    investing_cash_flow                 NUMERIC(20,4),
    financing_cash_flow                 NUMERIC(20,4),
    free_cash_flow                      NUMERIC(20,4),
    capital_expenditure                 NUMERIC(20,4),
    depreciation_and_amortization       NUMERIC(20,4),
    change_in_working_capital           NUMERIC(20,4),
    change_in_receivables               NUMERIC(20,4),
    change_in_inventory                 NUMERIC(20,4),
    change_in_payable                   NUMERIC(20,4),
    change_in_prepaid_assets            NUMERIC(20,4),
    stock_based_compensation            NUMERIC(20,4),
    net_income_from_continuing_operations NUMERIC(20,4),
    dividends_received_cfi              NUMERIC(20,4),
    interest_paid_cfo                   NUMERIC(20,4),
    interest_received_cfo               NUMERIC(20,4),
    taxes_refund_paid                   NUMERIC(20,4),
    purchase_of_business                NUMERIC(20,4),
    purchase_of_investment              NUMERIC(20,4),
    sale_of_investment                  NUMERIC(20,4),
    net_investment_purchase_and_sale     NUMERIC(20,4),
    purchase_of_ppe                     NUMERIC(20,4),
    sale_of_ppe                         NUMERIC(20,4),
    net_ppe_purchase_and_sale           NUMERIC(20,4),
    issuance_of_debt                    NUMERIC(20,4),
    long_term_debt_issuance             NUMERIC(20,4),
    long_term_debt_payments             NUMERIC(20,4),
    repayment_of_debt                   NUMERIC(20,4),
    issuance_of_capital_stock           NUMERIC(20,4),
    common_stock_issuance               NUMERIC(20,4),
    net_other_financing_charges         NUMERIC(20,4),
    net_other_investing_changes         NUMERIC(20,4),
    beginning_cash_position             NUMERIC(20,4),
    end_cash_position                   NUMERIC(20,4),
    changes_in_cash                     NUMERIC(20,4),
    other_non_cash_items                NUMERIC(20,4)
);


-- ===========================================================================
-- SECTION 2: New Reference Tables
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- sectors (reference table with Arabic/English names)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS sectors (
    id          SERIAL PRIMARY KEY,
    name_en     TEXT UNIQUE NOT NULL,
    name_ar     TEXT,
    code        TEXT UNIQUE,
    created_at  TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- entities (enhanced company info with Arabic names, identifiers)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS entities (
    id              UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker          TEXT UNIQUE NOT NULL REFERENCES companies(ticker),
    name_ar         TEXT,
    name_en         TEXT,
    sector_id       INTEGER REFERENCES sectors(id),
    listing_date    DATE,
    isin            TEXT UNIQUE,
    cma_id          TEXT,
    tadawul_id      TEXT,
    website         TEXT,
    description_ar  TEXT,
    description_en  TEXT,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at      TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


-- ===========================================================================
-- SECTION 3: New Financial Data Tables
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- filings (filing metadata - must be created before xbrl_facts for FK)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS filings (
    id              UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker          TEXT NOT NULL REFERENCES companies(ticker),
    filing_type     TEXT NOT NULL,       -- 'annual', 'quarterly', 'interim'
    filing_date     DATE NOT NULL,
    period_start    DATE,
    period_end      DATE,
    source          TEXT,                -- 'CMA', 'Tadawul'
    source_url      TEXT,
    document_url    TEXT,
    status          TEXT NOT NULL DEFAULT 'pending',  -- 'pending', 'processing', 'completed', 'failed'
    processed_at    TIMESTAMPTZ,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- xbrl_facts (XBRL financial data with concept identification)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS xbrl_facts (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker              TEXT NOT NULL REFERENCES companies(ticker),
    filing_id           UUID REFERENCES filings(id),
    concept             TEXT NOT NULL,       -- XBRL concept (e.g., 'ifrs-full:Revenue')
    label_en            TEXT,
    label_ar            TEXT,
    value_numeric       NUMERIC(20,4),
    value_text          TEXT,
    value_boolean       BOOLEAN,
    unit                TEXT,                -- e.g., 'SAR', 'shares'
    decimals            INTEGER,
    period_start        DATE,
    period_end          DATE,
    period_instant      DATE,
    dimension_member    TEXT,                -- XBRL dimension member
    dimension_value     TEXT,                -- XBRL dimension value
    source_url          TEXT,
    content_hash        TEXT UNIQUE,         -- SHA-256 for dedup
    created_at          TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- computed_metrics (derived ratios, growth rates, custom calculations)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS computed_metrics (
    id              SERIAL PRIMARY KEY,
    ticker          TEXT NOT NULL REFERENCES companies(ticker),
    metric_name     TEXT NOT NULL,
    metric_value    NUMERIC(20,4),
    period_date     DATE,
    period_type     TEXT,                -- 'annual', 'quarterly', 'ttm'
    computed_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE (ticker, metric_name, period_date, period_type)
);

-- ---------------------------------------------------------------------------
-- price_history (daily OHLCV with computed changes and moving averages)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS price_history (
    id              SERIAL PRIMARY KEY,
    ticker          TEXT NOT NULL REFERENCES companies(ticker),
    trade_date      DATE NOT NULL,
    open_price      NUMERIC(12,4),
    high_price      NUMERIC(12,4),
    low_price       NUMERIC(12,4),
    close_price     NUMERIC(12,4),
    volume          BIGINT,
    value_traded    NUMERIC(20,4),
    num_trades      INTEGER,
    change_amount   NUMERIC(12,4),
    change_pct      NUMERIC(8,4),
    ma_5            NUMERIC(12,4),
    ma_10           NUMERIC(12,4),
    ma_20           NUMERIC(12,4),
    ma_50           NUMERIC(12,4),
    ma_200          NUMERIC(12,4),
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE (ticker, trade_date)
);


-- ===========================================================================
-- SECTION 4: New Content Tables
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- announcements (CMA/Tadawul with Arabic content, classification)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS announcements (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker              TEXT REFERENCES companies(ticker),
    title_ar            TEXT,
    title_en            TEXT,
    body_ar             TEXT,
    body_en             TEXT,
    source              TEXT,                -- 'CMA', 'Tadawul'
    announcement_date   TIMESTAMPTZ NOT NULL,
    category            TEXT,
    classification      TEXT,
    is_material         BOOLEAN NOT NULL DEFAULT FALSE,
    embedding_flag      BOOLEAN NOT NULL DEFAULT FALSE,
    source_url          TEXT,
    created_at          TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- news_articles (multi-source with sentiment, entity extraction)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS news_articles (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker              TEXT REFERENCES companies(ticker),
    title               TEXT NOT NULL,
    body                TEXT,
    source_name         TEXT,
    source_url          TEXT,
    published_at        TIMESTAMPTZ,
    sentiment_score     NUMERIC(5,4),
    sentiment_label     TEXT,                -- 'positive', 'negative', 'neutral'
    entities_extracted  JSONB,
    language            TEXT NOT NULL DEFAULT 'ar',
    created_at          TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- technical_reports (analyst research with recommendations/targets)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS technical_reports (
    id                      UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker                  TEXT REFERENCES companies(ticker),
    title                   TEXT NOT NULL,
    summary                 TEXT,
    author                  TEXT,
    source_name             TEXT,
    source_url              TEXT,
    published_at            TIMESTAMPTZ,
    recommendation          TEXT,            -- 'buy', 'hold', 'sell', 'overweight', etc.
    target_price            NUMERIC(12,4),
    current_price_at_report NUMERIC(12,4),
    report_type             TEXT,            -- 'initiation', 'update', 'sector_review'
    created_at              TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


-- ===========================================================================
-- SECTION 5: New User / Auth Tables
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- users (auth provider, profile, subscription tier, usage tracking)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS users (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    auth_provider       TEXT NOT NULL,       -- 'local', 'google', 'microsoft'
    auth_provider_id    TEXT,
    email               TEXT UNIQUE NOT NULL,
    display_name        TEXT,
    avatar_url          TEXT,
    subscription_tier   TEXT NOT NULL DEFAULT 'free',  -- 'free', 'pro', 'enterprise'
    usage_count         INTEGER NOT NULL DEFAULT 0,
    last_query_at       TIMESTAMPTZ,
    is_active           BOOLEAN NOT NULL DEFAULT TRUE,
    created_at          TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at          TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE (auth_provider, auth_provider_id)
);

-- ---------------------------------------------------------------------------
-- user_watchlists (ticker arrays per user)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS user_watchlists (
    id          UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id     UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    name        TEXT NOT NULL DEFAULT 'Default',
    tickers     TEXT[] NOT NULL DEFAULT '{}',
    created_at  TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at  TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE (user_id, name)
);

-- ---------------------------------------------------------------------------
-- user_alerts (price/event alerts per user per ticker)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS user_alerts (
    id                  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id             UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    ticker              TEXT NOT NULL REFERENCES companies(ticker),
    alert_type          TEXT NOT NULL,       -- 'price_above', 'price_below', 'volume_spike', 'event'
    threshold_value     NUMERIC(20,4),
    is_active           BOOLEAN NOT NULL DEFAULT TRUE,
    last_triggered_at   TIMESTAMPTZ,
    created_at          TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ---------------------------------------------------------------------------
-- query_audit_log (full query logging for analytics and debugging)
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS query_audit_log (
    id                      UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id                 UUID REFERENCES users(id) ON DELETE SET NULL,
    natural_language_query   TEXT NOT NULL,
    generated_sql           TEXT,
    execution_time_ms       INTEGER,
    row_count               INTEGER,
    was_successful          BOOLEAN,
    error_message           TEXT,
    ip_address              INET,
    user_agent              TEXT,
    created_at              TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


-- ===========================================================================
-- SECTION 6: Indexes
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- Existing table indexes (matching SQLite originals)
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_bs_ticker
    ON balance_sheet(ticker);
CREATE INDEX IF NOT EXISTS idx_bs_ticker_period
    ON balance_sheet(ticker, period_type, period_date);
CREATE INDEX IF NOT EXISTS idx_bs_period_type
    ON balance_sheet(period_type);

CREATE INDEX IF NOT EXISTS idx_is_ticker
    ON income_statement(ticker);
CREATE INDEX IF NOT EXISTS idx_is_ticker_period
    ON income_statement(ticker, period_type, period_date);
CREATE INDEX IF NOT EXISTS idx_is_period_type
    ON income_statement(period_type);

CREATE INDEX IF NOT EXISTS idx_cf_ticker
    ON cash_flow(ticker);
CREATE INDEX IF NOT EXISTS idx_cf_ticker_period
    ON cash_flow(ticker, period_type, period_date);
CREATE INDEX IF NOT EXISTS idx_cf_period_type
    ON cash_flow(period_type);

-- ---------------------------------------------------------------------------
-- companies indexes (sector/industry lookups)
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_companies_sector
    ON companies(sector);
CREATE INDEX IF NOT EXISTS idx_companies_industry
    ON companies(industry);

-- ---------------------------------------------------------------------------
-- entities indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_entities_sector_id
    ON entities(sector_id);
CREATE INDEX IF NOT EXISTS idx_entities_name_ar_trgm
    ON entities USING GIN (name_ar gin_trgm_ops);
CREATE INDEX IF NOT EXISTS idx_entities_name_en_trgm
    ON entities USING GIN (name_en gin_trgm_ops);

-- ---------------------------------------------------------------------------
-- filings indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_filings_ticker_date
    ON filings(ticker, filing_date DESC);
CREATE INDEX IF NOT EXISTS idx_filings_status
    ON filings(status);

-- ---------------------------------------------------------------------------
-- xbrl_facts indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_xbrl_ticker_concept
    ON xbrl_facts(ticker, concept);
CREATE INDEX IF NOT EXISTS idx_xbrl_filing_id
    ON xbrl_facts(filing_id);
CREATE INDEX IF NOT EXISTS idx_xbrl_period_end
    ON xbrl_facts(period_end);

-- ---------------------------------------------------------------------------
-- computed_metrics indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_computed_ticker_metric
    ON computed_metrics(ticker, metric_name);

-- ---------------------------------------------------------------------------
-- price_history indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_price_ticker_date
    ON price_history(ticker, trade_date DESC);
CREATE INDEX IF NOT EXISTS idx_price_trade_date
    ON price_history(trade_date);

-- ---------------------------------------------------------------------------
-- announcements indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_announcements_ticker_date
    ON announcements(ticker, announcement_date DESC);
CREATE INDEX IF NOT EXISTS idx_announcements_title_ar_trgm
    ON announcements USING GIN (title_ar gin_trgm_ops);
CREATE INDEX IF NOT EXISTS idx_announcements_body_ar_trgm
    ON announcements USING GIN (body_ar gin_trgm_ops);
CREATE INDEX IF NOT EXISTS idx_announcements_category
    ON announcements(category);

-- ---------------------------------------------------------------------------
-- news_articles indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_news_ticker_date
    ON news_articles(ticker, published_at DESC);
CREATE INDEX IF NOT EXISTS idx_news_sentiment
    ON news_articles(sentiment_label);
CREATE INDEX IF NOT EXISTS idx_news_entities_gin
    ON news_articles USING GIN (entities_extracted);

-- ---------------------------------------------------------------------------
-- technical_reports indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_reports_ticker_date
    ON technical_reports(ticker, published_at DESC);
CREATE INDEX IF NOT EXISTS idx_reports_recommendation
    ON technical_reports(recommendation);

-- ---------------------------------------------------------------------------
-- users indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_users_email
    ON users(email);
CREATE INDEX IF NOT EXISTS idx_users_auth_provider
    ON users(auth_provider, auth_provider_id);

-- ---------------------------------------------------------------------------
-- user_alerts indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_alerts_user_active
    ON user_alerts(user_id, is_active);
CREATE INDEX IF NOT EXISTS idx_alerts_ticker
    ON user_alerts(ticker);

-- ---------------------------------------------------------------------------
-- query_audit_log indexes
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_audit_user_date
    ON query_audit_log(user_id, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_audit_created
    ON query_audit_log(created_at);


-- ===========================================================================
-- SECTION 7: Views
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- v_latest_annual_metrics
-- Joins companies with the most recent annual financial statements
-- (period_type='annual', period_index=0) for a single-row-per-company snapshot.
-- ---------------------------------------------------------------------------
CREATE OR REPLACE VIEW v_latest_annual_metrics AS
SELECT
    c.ticker,
    c.short_name,
    c.sector,
    c.industry,
    -- Balance sheet (latest annual)
    bs.period_date          AS bs_period_date,
    bs.total_assets,
    bs.current_assets,
    bs.cash_and_cash_equivalents,
    bs.total_liabilities_net_minority_interest,
    bs.current_liabilities,
    bs.total_equity_gross_minority_interest,
    bs.stockholders_equity,
    bs.retained_earnings,
    bs.total_debt           AS bs_total_debt,
    bs.net_debt,
    bs.working_capital,
    -- Income statement (latest annual)
    ist.period_date         AS is_period_date,
    ist.total_revenue,
    ist.gross_profit,
    ist.operating_income,
    ist.net_income,
    ist.net_income_common_stockholders,
    ist.ebitda,
    ist.ebit,
    ist.diluted_eps,
    ist.basic_eps,
    -- Cash flow (latest annual)
    cf.period_date          AS cf_period_date,
    cf.operating_cash_flow,
    cf.investing_cash_flow,
    cf.financing_cash_flow,
    cf.free_cash_flow,
    cf.capital_expenditure
FROM companies c
LEFT JOIN balance_sheet bs
    ON bs.ticker = c.ticker
    AND bs.period_type = 'annual'
    AND bs.period_index = 0
LEFT JOIN income_statement ist
    ON ist.ticker = c.ticker
    AND ist.period_type = 'annual'
    AND ist.period_index = 0
LEFT JOIN cash_flow cf
    ON cf.ticker = c.ticker
    AND cf.period_type = 'annual'
    AND cf.period_index = 0;

-- ---------------------------------------------------------------------------
-- v_company_summary
-- Comprehensive company overview joining core tables plus entity details.
-- ---------------------------------------------------------------------------
CREATE OR REPLACE VIEW v_company_summary AS
SELECT
    c.ticker,
    c.short_name,
    c.sector,
    c.industry,
    c.currency,
    -- Entity details
    e.name_ar,
    e.name_en,
    e.listing_date,
    e.isin,
    s.name_en               AS sector_name_en,
    s.name_ar               AS sector_name_ar,
    -- Market data
    md.current_price,
    md.previous_close,
    md.market_cap,
    md.volume,
    md.avg_volume,
    md.beta,
    md.week_52_high,
    md.week_52_low,
    md.shares_outstanding,
    -- Valuation
    vm.trailing_pe,
    vm.forward_pe,
    vm.price_to_book,
    vm.price_to_sales,
    vm.ev_to_ebitda,
    vm.trailing_eps,
    vm.forward_eps,
    -- Profitability
    pm.roa,
    pm.roe,
    pm.profit_margin,
    pm.operating_margin,
    pm.gross_margin,
    pm.revenue_growth,
    pm.earnings_growth
FROM companies c
LEFT JOIN entities e        ON e.ticker = c.ticker
LEFT JOIN sectors s         ON s.id = e.sector_id
LEFT JOIN market_data md    ON md.ticker = c.ticker
LEFT JOIN valuation_metrics vm  ON vm.ticker = c.ticker
LEFT JOIN profitability_metrics pm ON pm.ticker = c.ticker;
</file>

<file path="docs/API_ERROR_AUDIT.md">
# API Error Handling Consistency Audit

**Date**: 2026-02-13
**Scope**: All route handlers in `app.py` and `api/routes/` (16 files), plus `middleware/error_handler.py`

---

## 1. Executive Summary

The API has **three distinct error response shapes** used inconsistently across endpoints:

| Shape | Example | Used By |
|-------|---------|---------|
| **A**: `{"error": {"code": "...", "message": "..."}}` | ErrorHandlerMiddleware | Unhandled exceptions only |
| **B**: `{"detail": "..."}` | FastAPI HTTPException default | Most route handlers |
| **C**: `{"detail": "...", "endpoint_prefix": "..."}` | SQLite stub routes | app.py stub handlers |

Additionally, the `RateLimitMiddleware` returns `{"detail": "Too many requests"}` (shape B variant).

**No endpoint includes a `request_id` in error responses.**

---

## 2. Middleware Assessment

### 2.1 ErrorHandlerMiddleware (`middleware/error_handler.py`)

- **Response format**: `{"error": {"code": "...", "message": "..."}}`
- **Exception mapping**: `ValueError` -> 400, `PermissionError` -> 403, `FileNotFoundError`/`KeyError` -> 404
- **Catch-all**: 500 with generic message (debug shows detail)
- **Missing**: `request_id` field, `ConnectionError` handling, `RequestValidationError` handling
- **Issue**: Shape A conflicts with FastAPI's default HTTPException handler (shape B)

### 2.2 RateLimitMiddleware (`middleware/rate_limit.py`)

- Returns `{"detail": "Too many requests"}` with `Retry-After` header
- Uses shape B (matches HTTPException default)
- No `request_id`

### 2.3 RequestLoggingMiddleware (`middleware/request_logging.py`)

- Logs: method, path, status_code, duration_ms, client_ip
- **Missing**: `request_id`, JSON structured format, IP anonymization
- Skips configurable paths (default: `/health`, `/favicon.ico`)
- Does NOT skip `/docs` or `/openapi.json`

### 2.4 CORS Middleware (`middleware/cors.py`)

- Standard FastAPI CORSMiddleware wrapper, no error handling concerns

---

## 3. Per-Endpoint Audit

### 3.1 `app.py` (Direct Routes)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /` | custom_index | No try/except | HTMLResponse | File read can raise `FileNotFoundError`; caught by middleware (shape A) |
| `GET /favicon.ico` | favicon | Checks `exists()` | HTMLResponse/FileResponse | No error possible; returns empty HTML if missing |
| `POST /api/vanna/v2/chat_sse` | PG auth middleware | Returns 401 JSONResponse | `{"detail": "..."}` (shape B) | Only active in postgres mode |
| SQLite stub routes (`/api/news`, etc.) | GET | Returns 503 | `{"detail": "...", "endpoint_prefix": "..."}` (shape C) | Non-standard extra field |

### 3.2 `api/routes/health.py`

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /health` | health_check | No try/except | `HealthResponse` model / JSONResponse(503) | Service errors propagate as unhandled exceptions (caught by middleware as shape A) |

**Issue**: If `get_health()` raises, middleware returns shape A while explicit 503 uses Pydantic model -- two different shapes for same endpoint.

### 3.3 `api/routes/news.py` (PG-only)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/news` | list_news | No try/except | `PaginatedResponse[NewsResponse]` | Service exceptions unhandled (shape A via middleware) |
| `GET /api/news/ticker/{ticker}` | news_by_ticker | No try/except | `PaginatedResponse[NewsResponse]` | Same |
| `GET /api/news/sector/{sector}` | news_by_sector | No try/except | `PaginatedResponse[NewsResponse]` | Same |
| `GET /api/news/{article_id}` | get_article | HTTPException(404) | `{"detail": "Article not found"}` (shape B) | OK for 404; service exceptions unhandled |
| `POST /api/news` | create_article | Auth dependency | `NewsResponse` | Auth errors handled by dependency; service exceptions unhandled |

### 3.4 `api/routes/reports.py` (PG-only)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/reports` | list_reports | No try/except | `PaginatedResponse[ReportResponse]` | Service exceptions unhandled |
| `GET /api/reports/ticker/{ticker}` | reports_by_ticker | No try/except | `PaginatedResponse[ReportResponse]` | Same |
| `GET /api/reports/{report_id}` | get_report | HTTPException(404) | `{"detail": "Report not found"}` (shape B) | OK for 404 |
| `POST /api/reports` | create_report | Auth dependency | `ReportResponse` | Service exceptions unhandled |

### 3.5 `api/routes/announcements.py` (PG-only)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/announcements` | list_announcements | No try/except | `PaginatedResponse[AnnouncementResponse]` | Service exceptions unhandled |
| `GET /api/announcements/material` | material_events | No try/except | `PaginatedResponse[AnnouncementResponse]` | Same |
| `GET /api/announcements/sector/{sector}` | announcements_by_sector | No try/except | Same | Same |
| `GET /api/announcements/{id}` | get_announcement | HTTPException(404) | `{"detail": "..."}` (shape B) | OK |
| `POST /api/announcements` | create_announcement | Auth dependency | `AnnouncementResponse` | Service exceptions unhandled |

### 3.6 `api/routes/watchlists.py` (PG-only, all authenticated)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/watchlists` | list_watchlists | Auth dependency | `List[WatchlistResponse]` | Service exceptions unhandled |
| `POST /api/watchlists` | create_watchlist | Auth dependency | `WatchlistResponse` | Same |
| `POST /api/watchlists/{id}/tickers` | add_ticker | Auth + HTTPException(404) | shape B | OK |
| `PATCH /api/watchlists/{id}` | update_watchlist | Auth + HTTPException(404) | shape B | OK |
| `DELETE /api/watchlists/{id}` | delete_watchlist | Auth + HTTPException(404) | shape B | OK |
| `GET /api/watchlists/alerts` | list_alerts | Auth dependency | `List[AlertResponse]` | Service exceptions unhandled |
| `POST /api/watchlists/alerts` | create_alert | Auth dependency | `AlertResponse` | Same |
| `DELETE /api/watchlists/alerts/{id}` | deactivate_alert | Auth + HTTPException(404) | shape B | OK |

### 3.7 `api/routes/auth.py`

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `POST /api/auth/register` | register | HTTPException(503, 409) | shape B | Good error handling |
| `POST /api/auth/login` | login | HTTPException(503, 401) | shape B | Good error handling |
| `POST /api/auth/guest` | guest_login | No try/except | `AuthResponse` | Token generation could fail |
| `POST /api/auth/refresh` | refresh_token | HTTPException(401) | shape B | Good; catches jwt exceptions |
| `GET /api/auth/me` | get_me | Auth dependency | `UserProfile` | OK |

### 3.8 `api/routes/charts.py` (PG-only)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/charts/sector-market-cap` | sector_market_cap | **No try/except** | `ChartResponse` | psycopg2 errors unhandled |
| `GET /api/charts/top-companies` | top_companies | **No try/except** | `ChartResponse` | Same |
| `GET /api/charts/sector-pe` | sector_avg_pe | **No try/except** | `ChartResponse` | Same |
| `GET /api/charts/dividend-yield-top` | top_dividend_yields | **No try/except** | `ChartResponse` | Same |

**Critical**: These PG-only chart routes have zero error handling. Database connection failures will produce shape A middleware errors.

### 3.9 `api/routes/entities.py` (PG-only)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/entities` | list_entities | **No try/except** | `EntityListResponse` | psycopg2 errors unhandled |
| `GET /api/entities/sectors` | list_sectors | **No try/except** | `List[SectorInfo]` | Same |
| `GET /api/entities/{ticker}` | get_entity | HTTPException(404) | shape B | DB errors still unhandled |

### 3.10 `api/routes/news_feed.py` (SQLite-backed)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/v1/news/feed` | get_news_feed | No try/except | `NewsFeedResponse` | SQLite errors unhandled |
| `GET /api/v1/news/feed/{id}` | get_article | HTTPException(404) | shape B | OK for 404 |
| `GET /api/v1/news/search` | search_articles | No try/except | `NewsFeedResponse` | SQLite errors unhandled |
| `GET /api/v1/news/sources` | get_sources | No try/except | `NewsSourcesResponse` | SQLite errors unhandled |

### 3.11 `api/routes/charts_analytics.py` (Dual-backend)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/charts/sector-market-cap` | sector_market_cap | try/except -> HTTPException(503) | shape B with detail `"Database query failed: {exc}"` | **Leaks exception details** in error message |
| `GET /api/charts/top-companies` | top_companies | try/except -> HTTPException(503) | Same | Same info leak |
| `GET /api/charts/sector-pe` | sector_avg_pe | try/except -> HTTPException(503) | Same | Same info leak |
| `GET /api/charts/dividend-yield-top` | top_dividend_yields | try/except -> HTTPException(503) | Same | Same info leak |

**Issue**: Error messages include raw exception text (`f"Database query failed: {exc}"`), leaking implementation details.

### 3.12 `api/routes/sqlite_entities.py` (Dual-backend)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/entities` | list_entities | try/except -> HTTPException(503) | shape B with detail `"Database query failed: {exc}"` | Leaks exception details |
| `GET /api/entities/sectors` | list_sectors | try/except -> HTTPException(503) | Same | Same |
| `GET /api/entities/{ticker}` | get_entity | try/except + HTTPException(404) | shape B | 404 OK; 503 leaks details |

### 3.13 `api/routes/market_analytics.py` (Dual-backend)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/v1/market/movers` | get_movers | try/except -> HTTPException(503) | shape B with detail leaking | Info leak |
| `GET /api/v1/market/summary` | get_market_summary | try/except -> HTTPException(503) | Same | Same |
| `GET /api/v1/market/sectors` | get_sector_analytics | try/except -> HTTPException(503) | Same | Same |
| `GET /api/v1/market/heatmap` | get_heatmap | try/except -> HTTPException(503) | Same | Same |

### 3.14 `api/routes/stock_data.py` (Dual-backend)

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/v1/stocks/{ticker}/dividends` | get_dividends | HTTPException(404) | shape B | **No try/except around DB calls** |
| `GET /api/v1/stocks/{ticker}/summary` | get_financial_summary | HTTPException(404) | shape B | Same |
| `GET /api/v1/stocks/{ticker}/financials` | get_financials | HTTPException(400, 404) | shape B | Same |
| `GET /api/v1/stocks/compare` | compare_stocks | HTTPException(400) | shape B | No try/except around DB calls |
| `GET /api/v1/stocks/quotes` | get_batch_quotes | HTTPException(400) | shape B | No try/except around DB calls |

### 3.15 `api/routes/stock_ohlcv.py`

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/v1/charts/{ticker}/ohlcv` | get_stock_ohlcv | HTTPException(400) | shape B | Service exceptions unhandled |
| `GET /api/v1/charts/{ticker}/health` | stock_ohlcv_health | No try/except | `StockHealthResponse` | Import error silently handled, but other exceptions unhandled |

### 3.16 `api/routes/tasi_index.py`

| Endpoint | Method | Error Handling | Response Format | Issues |
|----------|--------|----------------|-----------------|--------|
| `GET /api/v1/charts/tasi/index` | get_tasi_index | HTTPException(400) | shape B | Service exceptions unhandled |
| `GET /api/v1/charts/tasi/health` | tasi_health | No try/except | `TASIHealthResponse` | Similar to stock_ohlcv health |

---

## 4. Issues Summary

### 4.1 Inconsistent Error Response Shapes

| Priority | Issue | Count | Impact |
|----------|-------|-------|--------|
| **HIGH** | Three different error response shapes (A, B, C) | All endpoints | Clients cannot reliably parse error responses |
| **HIGH** | No `request_id` in any error response | All endpoints | Cannot correlate client errors with server logs |
| **MEDIUM** | `RequestValidationError` not handled by ErrorHandlerMiddleware | All endpoints | FastAPI default 422 response uses shape B, not shape A |
| **MEDIUM** | `HTTPException` not intercepted by ErrorHandlerMiddleware | All endpoints | Shape B bypasses middleware, producing inconsistent format |

### 4.2 Missing Error Handling

| Priority | Issue | Affected Endpoints | Impact |
|----------|-------|--------------------|--------|
| **HIGH** | PG-only routes (`charts.py`, `entities.py`) have zero try/except | 7 endpoints | Unhandled psycopg2 errors produce 500 with shape A |
| **HIGH** | `stock_data.py` DB calls have no try/except | 5 endpoints | DB errors produce 500 |
| **MEDIUM** | PG service routes (`news.py`, `reports.py`, `announcements.py`, `watchlists.py`) rely entirely on middleware | ~18 endpoints | Inconsistent error shape (A vs B) |
| **LOW** | `news_feed.py` SQLite operations unprotected | 4 endpoints | SQLite errors produce 500 |

### 4.3 Information Leakage

| Priority | Issue | Affected Files | Impact |
|----------|-------|----------------|--------|
| **HIGH** | `f"Database query failed: {exc}"` exposes raw exceptions | `charts_analytics.py`, `sqlite_entities.py`, `market_analytics.py` | Leaks DB structure, query details, connection info |
| **MEDIUM** | No distinction between debug/production error detail | Same files | Always leaks in production |

### 4.4 Status Code Inconsistencies

| Priority | Issue | Details |
|----------|-------|---------|
| **MEDIUM** | Dual-backend routes use 503 for DB errors | `charts_analytics.py`, `sqlite_entities.py`, `market_analytics.py` use 503 |
| **MEDIUM** | PG-only routes let DB errors become 500 | `charts.py`, `entities.py` let psycopg2 errors become 500 via middleware |
| **LOW** | SQLite stub routes use non-standard error shape | app.py stubs add `endpoint_prefix` field |

---

## 5. Recommendations

1. **Unify error response shape**: Adopt `{"error": {"code": "...", "message": "...", "request_id": "..."}}` everywhere by handling `HTTPException` and `RequestValidationError` in `ErrorHandlerMiddleware`
2. **Add `request_id`**: Generate UUID per request in middleware, include in all error responses and logs
3. **Remove info leaks**: Replace `f"Database query failed: {exc}"` with generic messages; log details server-side
4. **Add `ConnectionError` handling**: Map to 503 in ErrorHandlerMiddleware
5. **Add response models**: Create `models/api_responses.py` with standardized Pydantic models for all response types
6. **Protect unguarded routes**: Add try/except to `charts.py`, `entities.py`, `stock_data.py`, `news_feed.py`
</file>

<file path="docs/api-contracts.md">
# Ra'd AI -- API Contracts

> Authoritative reference for every HTTP endpoint exposed by the Ra'd AI backend.
> Generated from source code on 2026-02-10.

---

## Table of Contents

1. [Conventions](#conventions)
2. [Vanna Chat (SSE)](#vanna-chat-sse)
3. [TASI Index Charts](#tasi-index-charts)
4. [Charts (PostgreSQL)](#charts-postgresql)
5. [Entities](#entities)
6. [News](#news)
7. [Reports](#reports)
8. [Announcements](#announcements)
9. [Watchlists & Alerts](#watchlists--alerts)
10. [Auth](#auth)
11. [Health](#health)
12. [Static / UI](#static--ui)
13. [Pagination](#pagination)
14. [Error Responses](#error-responses)

---

## Conventions

| Convention | Detail |
|---|---|
| Base URL | `http://localhost:8084` (dev) or `https://raid-ai-app-production.up.railway.app` (prod) |
| Content-Type | `application/json` for all JSON endpoints |
| Auth header | `Authorization: Bearer <access_token>` (JWT, where noted) |
| User header | `X-User-Id: <user_id>` (watchlist read endpoints) |
| Rate limiting | 60 req/min per IP (disabled in debug mode). `/health` is exempt. |
| CORS origins | `http://localhost:3000`, `http://localhost:8084` (configurable) |
| Backend modes | **SQLite** (default) -- limited to Vanna chat + TASI index + static routes. **PostgreSQL** -- all routes enabled. |

---

## Vanna Chat (SSE)

Provided by `VannaFastAPIServer`. The SSE endpoint streams AI assistant responses.

### POST /api/vanna/v2/chat_sse

**Auth required:** No | **Rate limited:** Yes | **DB backend:** SQLite + PostgreSQL

Stream a natural language query to the AI assistant.

**Request body:**

```json
{
  "message": "Show top 10 companies by market cap"
}
```

**Response:** Server-Sent Events (SSE) stream. Each event is a `data:` line with JSON:

```
data: {"type": "text", "data": {"content": "Let me query..."}}
data: {"type": "sql", "data": {"query": "SELECT ...", "result_file": "abc.csv"}}
data: {"type": "chart", "data": {"html": "<div id='plotly-chart'>...</div>"}}
data: {"type": "text", "data": {"content": "The top 10 companies..."}}
data: [DONE]
```

**SSE event types:**

| type | data shape | description |
|---|---|---|
| `text` | `{content: string}` | Incremental text from the LLM |
| `sql` | `{query: string, result_file: string}` | SQL executed + CSV filename |
| `chart` | `{html: string}` | Plotly chart HTML fragment |
| `error` | `{message: string}` | Error during tool execution |

**Frontend consumer:** `frontend/src/lib/use-sse-chat.ts` -> `AIChatInterface` component.
Also consumed by legacy `templates/index.html` native SSE chat UI.

**curl example:**

```bash
curl -N -X POST http://localhost:8084/api/vanna/v2/chat_sse \
  -H "Content-Type: application/json" \
  -d '{"message": "What are the top 5 companies by market cap?"}'
```

---

## TASI Index Charts

Prefix: `/api/v1/charts/tasi` | Tag: `tasi-index`

Works with **both** SQLite and PostgreSQL backends (no database dependency).

### GET /api/v1/charts/tasi/index

**Auth required:** No | **Rate limited:** Yes | **DB backend:** Any (no DB required)

Return TASI index OHLCV data for TradingView chart rendering.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `period` | string | `1y` | Data period. One of: `1mo`, `3mo`, `6mo`, `1y`, `2y`, `5y` |

**Response (200):**

```json
{
  "data": [
    {
      "time": "2025-02-10",
      "open": 11523.45,
      "high": 11598.20,
      "low": 11501.30,
      "close": 11580.10,
      "volume": 185000000
    }
  ],
  "source": "real",
  "last_updated": "2026-02-10T12:00:00.000Z",
  "symbol": "^TASI",
  "period": "1y",
  "count": 252
}
```

**Response fields:**

| Field | Type | Description |
|---|---|---|
| `data` | `TASIOHLCVPoint[]` | Array of OHLCV data points |
| `source` | `"real" \| "mock" \| "cached"` | Data origin |
| `last_updated` | string (ISO 8601) | Timestamp of the data |
| `symbol` | string | Yahoo Finance symbol used (`^TASI` or `TASI.SR`) |
| `period` | string | Echo of the requested period |
| `count` | int | Number of data points |

**Error (400):**

```json
{
  "detail": "Invalid period 'bad'. Must be one of: 1mo, 3mo, 6mo, 1y, 2y, 5y"
}
```

**Data pipeline:** yfinance (^TASI, then TASI.SR fallback) -> 5-minute in-memory cache -> stale cache fallback -> deterministic mock data.

**Frontend consumer:** `useMarketIndex()` hook in `frontend/src/lib/hooks/use-chart-data.ts`.

**curl example:**

```bash
curl "http://localhost:8084/api/v1/charts/tasi/index?period=6mo"
```

---

### GET /api/v1/charts/tasi/health

**Auth required:** No | **Rate limited:** Yes | **DB backend:** Any

Return health/diagnostic status for the TASI data pipeline.

**Response (200):**

```json
{
  "status": "ok",
  "yfinance_available": true,
  "cache_status": "fresh",
  "cache_age_seconds": 42,
  "last_updated": "2026-02-10T12:00:00.000Z"
}
```

| Field | Type | Description |
|---|---|---|
| `status` | `"ok" \| "degraded"` | `degraded` when cache is not fresh AND yfinance is unavailable |
| `yfinance_available` | bool | Whether `yfinance` is importable |
| `cache_status` | `"fresh" \| "stale" \| "empty"` | Current cache state |
| `cache_age_seconds` | int or null | Seconds since last cache write |
| `last_updated` | string or null | ISO timestamp of cached data |

**curl example:**

```bash
curl "http://localhost:8084/api/v1/charts/tasi/health"
```

---

## Charts (PostgreSQL)

Prefix: `/api/charts` | Tag: `charts`

**DB backend:** PostgreSQL only. Pre-built data endpoints for frontend charting.

### GET /api/charts/sector-market-cap

**Auth required:** No | **Rate limited:** Yes

Total market cap by sector (for pie/bar chart).

**Response (200):**

```json
{
  "chart_type": "bar",
  "title": "Market Cap by Sector (SAR)",
  "data": [
    {"label": "Energy", "value": 7500000000000.0},
    {"label": "Financial Services", "value": 2100000000000.0}
  ]
}
```

**Frontend consumer:** `getChartSectorMarketCap()` in `api-client.ts`.

**curl example:**

```bash
curl "http://localhost:8084/api/charts/sector-market-cap"
```

---

### GET /api/charts/top-companies

**Auth required:** No | **Rate limited:** Yes

Top N companies by market cap.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `limit` | int (1-50) | `10` | Number of companies |
| `sector` | string (optional) | - | Filter by sector (ILIKE match) |

**Response:** Same `ChartResponse` schema as above.

**curl example:**

```bash
curl "http://localhost:8084/api/charts/top-companies?limit=5&sector=Energy"
```

---

### GET /api/charts/sector-pe

**Auth required:** No | **Rate limited:** Yes

Average trailing P/E ratio by sector (excludes P/E > 200 and <= 0).

**Response:** Same `ChartResponse` schema.

---

### GET /api/charts/dividend-yield-top

**Auth required:** No | **Rate limited:** Yes

Top N companies by dividend yield (as percentage).

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `limit` | int (1-50) | `15` | Number of companies |

**Response:** Same `ChartResponse` schema.

---

## Entities

Prefix: `/api/entities` | Tag: `entities` | **DB backend:** PostgreSQL only

### GET /api/entities

**Auth required:** No | **Rate limited:** Yes

List companies with basic market data.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `limit` | int (1-500) | `50` | Max results |
| `offset` | int (>=0) | `0` | Offset for pagination |
| `sector` | string (optional) | - | Filter by sector (ILIKE match) |
| `search` | string (optional) | - | Search by ticker or company name (ILIKE) |

**Response (200):**

```json
{
  "items": [
    {
      "ticker": "2222.SR",
      "short_name": "Saudi Aramco",
      "sector": "Energy",
      "industry": "Oil & Gas Integrated",
      "current_price": 28.45,
      "market_cap": 7500000000000.0,
      "change_pct": 1.23
    }
  ],
  "count": 50
}
```

**Frontend consumer:** `useEntities()` hook, `getEntities()` in `api-client.ts`.

**curl example:**

```bash
curl "http://localhost:8084/api/entities?search=aramco&limit=10"
```

---

### GET /api/entities/sectors

**Auth required:** No | **Rate limited:** Yes

Return all sectors with company counts.

**Response (200):**

```json
[
  {"sector": "Financial Services", "company_count": 85},
  {"sector": "Energy", "company_count": 12}
]
```

**Frontend consumer:** `useSectors()` hook, `getSectors()` in `api-client.ts`.

---

### GET /api/entities/{ticker}

**Auth required:** No | **Rate limited:** Yes

Detailed company information with market data, valuation, and profitability.

**Path parameters:**

| Param | Type | Description |
|---|---|---|
| `ticker` | string | Stock ticker (e.g., `2222.SR`) |

**Response (200):**

```json
{
  "ticker": "2222.SR",
  "short_name": "Saudi Aramco",
  "sector": "Energy",
  "industry": "Oil & Gas Integrated",
  "exchange": "SAU",
  "currency": "SAR",
  "current_price": 28.45,
  "previous_close": 28.10,
  "day_high": 28.60,
  "day_low": 28.05,
  "week_52_high": 35.20,
  "week_52_low": 26.50,
  "volume": 15000000,
  "market_cap": 7500000000000.0,
  "beta": 0.85,
  "trailing_pe": 15.2,
  "forward_pe": 13.8,
  "price_to_book": 2.1,
  "trailing_eps": 1.87,
  "roe": 0.32,
  "profit_margin": 0.28,
  "revenue_growth": 0.05,
  "recommendation": "buy",
  "target_mean_price": 32.50,
  "analyst_count": 18
}
```

**Error (404):** `{"detail": "Company not found"}`

**Frontend consumer:** `useStockDetail()` hook, `getEntityDetail()` in `api-client.ts`. Used by `/stock/[ticker]` page.

---

## News

Prefix: `/api/news` | Tag: `news` | **DB backend:** PostgreSQL only

### GET /api/news

**Auth required:** No | **Rate limited:** Yes

Return the latest news articles across all tickers. Supports pagination.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `page` | int (>=1) | `1` | Page number |
| `page_size` | int (1-100) | `20` | Items per page |
| `language` | string (optional) | - | Filter by language code (e.g., `ar`, `en`) |

**Response (200):** `PaginatedResponse<NewsResponse>`

```json
{
  "items": [
    {
      "id": "uuid",
      "ticker": "2222.SR",
      "title": "Aramco Q4 results exceed expectations",
      "body": "...",
      "source_name": "Bloomberg",
      "source_url": "https://...",
      "published_at": "2026-02-10T08:00:00Z",
      "sentiment_score": 0.75,
      "sentiment_label": "positive",
      "language": "en",
      "created_at": "2026-02-10T08:01:00Z"
    }
  ],
  "total": 150,
  "page": 1,
  "page_size": 20,
  "total_pages": 8
}
```

**Frontend consumer:** `useNews()` hook -> `/news` page.

**curl example:**

```bash
curl "http://localhost:8084/api/news?page=1&page_size=10&language=en"
```

---

### GET /api/news/ticker/{ticker}

**Auth required:** No | **Rate limited:** Yes

News articles for a specific ticker.

**Additional query parameters:**

| Param | Type | Description |
|---|---|---|
| `sentiment` | string (optional) | Filter by sentiment label (e.g., `positive`, `negative`, `neutral`) |
| `since` | datetime (optional) | Only articles after this timestamp |

---

### GET /api/news/sector/{sector}

**Auth required:** No | **Rate limited:** Yes

News articles for all companies in a sector.

**Additional query parameters:**

| Param | Type | Description |
|---|---|---|
| `since` | datetime (optional) | Only articles after this timestamp |

---

### GET /api/news/{article_id}

**Auth required:** No | **Rate limited:** Yes

Return a single news article by ID.

**Error (404):** `{"detail": "Article not found"}`

---

### POST /api/news

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 201

Create a new news article.

**Request body:**

```json
{
  "title": "Aramco announces dividend increase",
  "content": "Saudi Aramco has announced...",
  "ticker": "2222.SR",
  "source": "Reuters",
  "source_url": "https://...",
  "language": "en",
  "sentiment_score": 0.65,
  "sentiment_label": "positive"
}
```

**Validation:**

| Field | Constraints |
|---|---|
| `title` | Required, 1-500 chars |
| `content` | Required, min 1 char |
| `ticker` | Optional, max 20 chars |
| `source` | Optional, max 200 chars |
| `source_url` | Optional, max 2000 chars |
| `language` | Default `"ar"`, max 5 chars |
| `sentiment_score` | Optional, -1.0 to 1.0 |
| `sentiment_label` | Optional, max 20 chars |

---

## Reports

Prefix: `/api/reports` | Tag: `reports` | **DB backend:** PostgreSQL only

### GET /api/reports

**Auth required:** No | **Rate limited:** Yes

Return the latest technical reports. Supports pagination.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `page` | int (>=1) | `1` | Page number |
| `page_size` | int (1-100) | `20` | Items per page |
| `recommendation` | string (optional) | - | Filter by recommendation (e.g., `buy`, `hold`, `sell`) |
| `report_type` | string (optional) | - | Filter by type (e.g., `technical`, `fundamental`, `sector`, `macro`) |
| `since` | datetime (optional) | - | Only reports after this timestamp |

**Response (200):** `PaginatedResponse<ReportResponse>`

```json
{
  "items": [
    {
      "id": "uuid",
      "ticker": "1180.SR",
      "title": "Al Rajhi Bank Technical Analysis",
      "summary": "...",
      "author": "Research Dept",
      "source_name": "NCB Capital",
      "source_url": "https://...",
      "published_at": "2026-02-09T10:00:00Z",
      "recommendation": "buy",
      "target_price": 95.50,
      "current_price_at_report": 88.20,
      "report_type": "technical",
      "created_at": "2026-02-09T10:01:00Z"
    }
  ],
  "total": 45,
  "page": 1,
  "page_size": 20,
  "total_pages": 3
}
```

**Frontend consumer:** `useReports()` hook -> `/reports` page.

---

### GET /api/reports/ticker/{ticker}

**Auth required:** No | **Rate limited:** Yes

Reports for a specific ticker.

---

### GET /api/reports/{report_id}

**Auth required:** No | **Rate limited:** Yes

Single report by ID.

**Error (404):** `{"detail": "Report not found"}`

---

### POST /api/reports

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 201

Create a new technical report.

---

## Announcements

Prefix: `/api/announcements` | Tag: `announcements` | **DB backend:** PostgreSQL only

### GET /api/announcements

**Auth required:** No | **Rate limited:** Yes

Return announcements with optional filters.

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `page` | int (>=1) | `1` | Page number |
| `page_size` | int (1-100) | `20` | Items per page |
| `ticker` | string (optional) | - | Filter by ticker |
| `category` | string (optional) | - | Filter by category |
| `source` | string (optional) | - | Filter by source |
| `since` | datetime (optional) | - | Only announcements after this timestamp |

**Response (200):** `PaginatedResponse<AnnouncementResponse>`

```json
{
  "items": [
    {
      "id": "uuid",
      "ticker": "2222.SR",
      "title_ar": "...",
      "title_en": "Dividend Declaration",
      "body_ar": "...",
      "body_en": "...",
      "source": "Tadawul",
      "announcement_date": "2026-02-09",
      "category": "dividend",
      "classification": "financial",
      "is_material": true,
      "source_url": "https://...",
      "created_at": "2026-02-09T06:00:00Z"
    }
  ],
  "total": 200,
  "page": 1,
  "page_size": 20,
  "total_pages": 10
}
```

---

### GET /api/announcements/material

**Auth required:** No | **Rate limited:** Yes

Return only material announcements (CMA-classified).

**Query parameters:**

| Param | Type | Description |
|---|---|---|
| `ticker` | string (optional) | Filter by ticker |
| `since` | datetime (optional) | Only after this timestamp |

---

### GET /api/announcements/sector/{sector}

**Auth required:** No | **Rate limited:** Yes

Announcements for all companies in a sector.

---

### GET /api/announcements/{announcement_id}

**Auth required:** No | **Rate limited:** Yes

Single announcement by ID.

**Error (404):** `{"detail": "Announcement not found"}`

---

### POST /api/announcements

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 201

Create a new announcement.

---

## Watchlists & Alerts

Prefix: `/api/watchlists` | Tag: `watchlists` | **DB backend:** PostgreSQL only

### GET /api/watchlists

**Auth required:** No (uses `X-User-Id` header) | **Rate limited:** Yes

Return all watchlists for the specified user.

**Required headers:** `X-User-Id: <user_id>`

**Response (200):**

```json
[
  {
    "id": "uuid",
    "user_id": "user-123",
    "name": "My Watchlist",
    "tickers": ["2222.SR", "1180.SR", "2010.SR"]
  }
]
```

**Frontend consumer:** `getWatchlists()` in `api-client.ts` -> `/watchlist` page.

**curl example:**

```bash
curl -H "X-User-Id: user-123" "http://localhost:8084/api/watchlists"
```

---

### POST /api/watchlists

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 201

Create a new watchlist.

**Request body:**

```json
{
  "name": "Banking Sector",
  "tickers": ["1180.SR", "1010.SR"]
}
```

---

### POST /api/watchlists/{watchlist_id}/tickers

**Auth required:** Yes (JWT) | **Rate limited:** Yes

Add a single ticker to an existing watchlist.

**Request body:**

```json
{
  "ticker": "2222.SR"
}
```

---

### PATCH /api/watchlists/{watchlist_id}

**Auth required:** Yes (JWT) | **Rate limited:** Yes

Update a watchlist's name and/or tickers.

**Request body:**

```json
{
  "name": "Updated Name",
  "tickers": ["2222.SR", "1180.SR"]
}
```

---

### DELETE /api/watchlists/{watchlist_id}

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 204

Delete a watchlist. Returns empty body on success.

**Error (404):** `{"detail": "Watchlist not found"}`

---

### GET /api/watchlists/alerts

**Auth required:** No (uses `X-User-Id` header) | **Rate limited:** Yes

Return active alerts for the specified user.

**Required headers:** `X-User-Id: <user_id>`

**Query parameters:**

| Param | Type | Description |
|---|---|---|
| `ticker` | string (optional) | Filter by ticker |

**Response (200):**

```json
[
  {
    "id": "uuid",
    "user_id": "user-123",
    "ticker": "2222.SR",
    "alert_type": "price_above",
    "threshold_value": 30.0,
    "is_active": true
  }
]
```

---

### POST /api/watchlists/alerts

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 201

Create a new alert.

**Request body:**

```json
{
  "ticker": "2222.SR",
  "alert_type": "price_above",
  "threshold_value": 30.0
}
```

---

### DELETE /api/watchlists/alerts/{alert_id}

**Auth required:** Yes (JWT) | **Rate limited:** Yes | **Status:** 204

Deactivate an alert (soft-delete).

---

## Auth

Prefix: `/api/auth` | Tag: `auth` | **DB backend:** PostgreSQL only

### POST /api/auth/register

**Auth required:** No | **Rate limited:** Yes | **Status:** 201

Register a new user account with bcrypt-hashed password.

**Request body:**

```json
{
  "email": "user@example.com",
  "password": "securepass123",
  "display_name": "Ahmed"
}
```

**Validation:**

| Field | Constraints |
|---|---|
| `email` | Required, valid email format |
| `password` | Required, 8-128 characters |
| `display_name` | Optional, max 100 characters |

**Response (201):**

```json
{
  "access_token": "eyJ...",
  "refresh_token": "eyJ...",
  "token_type": "bearer"
}
```

**Error (409):** `{"detail": "Email already registered"}`

**curl example:**

```bash
curl -X POST http://localhost:8084/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email":"user@example.com","password":"securepass123","display_name":"Ahmed"}'
```

---

### POST /api/auth/login

**Auth required:** No | **Rate limited:** Yes

Authenticate and receive tokens. Only works for `auth_provider='local'` users.

**Request body:**

```json
{
  "email": "user@example.com",
  "password": "securepass123"
}
```

**Response (200):** Same `TokenResponse` as register.

**Error (401):** `{"detail": "Invalid email or password"}` or `{"detail": "Account is deactivated"}`

---

### POST /api/auth/refresh

**Auth required:** No | **Rate limited:** Yes

Exchange a valid refresh token for new access/refresh tokens.

**Request body:**

```json
{
  "refresh_token": "eyJ..."
}
```

**Response (200):** Same `TokenResponse`.

**Error (401):** `{"detail": "Refresh token has expired"}` / `{"detail": "Invalid refresh token"}` / `{"detail": "User not found"}` / `{"detail": "Account is deactivated"}`

---

### GET /api/auth/me

**Auth required:** Yes (JWT) | **Rate limited:** Yes

Return the authenticated user's profile.

**Response (200):**

```json
{
  "id": "uuid",
  "email": "user@example.com",
  "display_name": "Ahmed",
  "subscription_tier": "free",
  "usage_count": 42,
  "is_active": true,
  "created_at": "2026-01-15T10:00:00Z"
}
```

**curl example:**

```bash
curl -H "Authorization: Bearer eyJ..." "http://localhost:8084/api/auth/me"
```

---

## Health

Tag: `health` | **DB backend:** PostgreSQL only

### GET /health

**Auth required:** No | **Rate limited:** No (skip path)

Structured health status for all platform components. Returns `503` when any component is `UNHEALTHY`.

**Response (200):**

```json
{
  "status": "healthy",
  "components": [
    {"name": "database", "status": "healthy", "latency_ms": 2.34, "message": ""},
    {"name": "llm", "status": "healthy", "latency_ms": null, "message": ""},
    {"name": "redis", "status": "degraded", "latency_ms": null, "message": "Not configured"}
  ]
}
```

**Frontend consumer:** `getHealth()` in `api-client.ts`.

---

## Static / UI

### GET /

**Auth required:** No | **Rate limited:** No

Serves the legacy `templates/index.html` frontend (Ra'd AI chat UI with gold/dark theme).

### GET /favicon.ico

**Auth required:** No | **Rate limited:** No

Serves `templates/favicon.svg` or empty response.

### GET /static/{path}

Static file serving from the `templates/` directory.

---

## Pagination

All paginated endpoints use a consistent pattern:

**Query parameters:**

| Param | Type | Default | Description |
|---|---|---|---|
| `page` | int | `1` | 1-indexed page number |
| `page_size` | int | `20` | Items per page (max 100) |

**Response wrapper:**

```json
{
  "items": [...],
  "total": 150,
  "page": 1,
  "page_size": 20,
  "total_pages": 8
}
```

`total_pages` is computed server-side as `ceil(total / page_size)`.

---

## Error Responses

All endpoints return errors in a consistent format:

```json
{
  "detail": "Human-readable error message",
  "code": "OPTIONAL_ERROR_CODE"
}
```

**Standard HTTP status codes:**

| Code | Meaning |
|---|---|
| `400` | Bad request (invalid parameters) |
| `401` | Unauthorized (missing/invalid JWT) |
| `404` | Resource not found |
| `409` | Conflict (e.g., duplicate email) |
| `422` | Validation error (Pydantic) |
| `429` | Rate limit exceeded |
| `500` | Internal server error |
| `503` | Service unavailable (health check failure) |

Pydantic validation errors (422) include detailed field-level errors:

```json
{
  "detail": [
    {
      "loc": ["body", "email"],
      "msg": "value is not a valid email address",
      "type": "value_error"
    }
  ]
}
```
</file>

<file path="docs/ARCHITECTURE.md">
# Architecture

System architecture documentation for the Ra'd AI TASI Platform.

## System Overview

```
+---------------------------------------------------------------------+
|                          CLIENTS                                     |
|  +------------------+    +-------------------+    +---------------+  |
|  | Next.js Frontend |    | Legacy Template   |    | API Consumers |  |
|  | (Vercel/port 3k) |    | (served by app)   |    | (curl, etc.)  |  |
|  +--------+---------+    +--------+----------+    +-------+-------+  |
|           |                       |                       |          |
+-----------+-----------------------+-----------------------+----------+
            |                       |                       |
            v                       v                       v
+---------------------------------------------------------------------+
|                      MIDDLEWARE STACK                                 |
|  CORS -> RateLimit -> RequestLogging -> ErrorHandler                 |
+---------------------------------------------------------------------+
            |
            v
+---------------------------------------------------------------------+
|                      FastAPI APPLICATION (app.py)                     |
|                                                                      |
|  +-------------------+  +------------------+  +------------------+   |
|  | Vanna 2.0 Agent   |  | API Routes       |  | Auth Module      |   |
|  | (text-to-SQL +    |  | /api/v1/*        |  | JWT + bcrypt     |   |
|  |  chart generation)|  |                  |  |                  |   |
|  +--------+----------+  +--------+---------+  +--------+---------+   |
|           |                      |                      |            |
+---------------------------------------------------------------------+
            |                      |                      |
            v                      v                      v
+---------------------------------------------------------------------+
|                        DATA LAYER                                    |
|                                                                      |
|  +------------------+  +------------------+  +------------------+    |
|  | SQLite (dev)     |  | PostgreSQL (prod)|  | Redis (optional) |    |
|  | saudi_stocks.db  |  | Connection Pool  |  | Response Cache   |    |
|  +------------------+  +------------------+  +------------------+    |
|                                                                      |
+---------------------------------------------------------------------+
            |
            v
+---------------------------------------------------------------------+
|                    EXTERNAL SERVICES                                  |
|                                                                      |
|  +------------------+  +------------------+  +------------------+    |
|  | Anthropic /      |  | Yahoo Finance    |  | Arabic News      |    |
|  | Gemini LLM API   |  | (yfinance)       |  | Sources (5)      |    |
|  +------------------+  +------------------+  +------------------+    |
+---------------------------------------------------------------------+
```

## Component Inventory

### Backend Core (`app.py`)

The central FastAPI application that assembles and wires all components:

| Component | Role | Module |
|-----------|------|--------|
| Vanna Agent | Text-to-SQL + chart generation | `vanna.Agent` |
| AnthropicLlmService | LLM integration (Claude/Gemini) | `vanna.integrations.anthropic` |
| SqliteRunner | SQLite query execution | `vanna.integrations.sqlite` |
| PostgresRunner | PostgreSQL query execution | `vanna.integrations.postgres` |
| ToolRegistry | Registers RunSqlTool + VisualizeDataTool | `vanna.ToolRegistry` |
| DemoAgentMemory | In-memory conversation history | `vanna.integrations.local` |
| SystemPromptBuilder | Schema documentation for LLM | `config/prompts.py` |
| VannaFastAPIServer | Creates FastAPI app with SSE chat | `vanna.servers.fastapi` |

### API Routes (`api/routes/`)

| Route Module | Prefix | DB Requirement | Description |
|-------------|--------|----------------|-------------|
| `health.py` | `/api/v1/health` | PostgreSQL | Full platform health check |
| `charts.py` | `/api/v1/charts` | Any | Chart generation endpoints |
| `tasi_index.py` | `/api/v1/charts/tasi` | Any | TASI index OHLCV data |
| `stock_ohlcv.py` | `/api/v1/charts/{ticker}` | Any | Per-stock OHLCV data |
| `charts_analytics.py` | `/api/v1/charts` | Any | Technical analysis overlays |
| `market_analytics.py` | `/api/v1/market` | Any | Market-wide analytics |
| `entities.py` | `/api/entities` | PostgreSQL | Company CRUD (PG) |
| `sqlite_entities.py` | `/api/v1/entities` | SQLite | Company listings (SQLite) |
| `stock_data.py` | `/api/v1/stock` | Any | Individual stock data |
| `news.py` | `/api/news` | PostgreSQL | News articles (PG) |
| `news_feed.py` | `/api/v1/news` | SQLite | News feed (scraped, SQLite) |
| `reports.py` | `/api/reports` | PostgreSQL | Technical reports |
| `announcements.py` | `/api/announcements` | PostgreSQL | CMA/Tadawul announcements |
| `auth.py` | `/api/auth` | Any | Login, register, refresh |
| `watchlists.py` | `/api/v1/watchlists` | Any | User watchlists |

### Services (`services/`)

| Service | Purpose | Dependencies |
|---------|---------|-------------|
| `health_service.py` | DB + LLM health checks | psycopg2 |
| `news_service.py` | PG-backed news CRUD | psycopg2 |
| `reports_service.py` | PG-backed reports CRUD | psycopg2 |
| `announcement_service.py` | PG-backed announcement CRUD | psycopg2 |
| `auth_service.py` | User registration/login | psycopg2, bcrypt |
| `user_service.py` | User management | psycopg2 |
| `tasi_index.py` | TASI index OHLCV via yfinance | yfinance |
| `stock_ohlcv.py` | Per-stock OHLCV via yfinance | yfinance |
| `news_scraper.py` | 5-source Arabic news scraper | requests, bs4 |
| `news_store.py` | SQLite news storage | sqlite3 |
| `news_scheduler.py` | Background news fetch (30min) | threading |
| `news_paraphraser.py` | Arabic synonym substitution | (built-in) |
| `db_compat.py` | SQLite/PG compatibility layer | sqlite3, psycopg2 |
| `audit_service.py` | Audit logging | psycopg2 |

### Authentication (`auth/`)

| Module | Purpose |
|--------|---------|
| `jwt_handler.py` | JWT token creation/validation |
| `password.py` | bcrypt password hashing |
| `models.py` | Auth data models |
| `dependencies.py` | FastAPI auth dependencies |

### Configuration (`config/`)

| Module | Purpose |
|--------|---------|
| `settings.py` | Pydantic settings (DB, LLM, Server, Auth, Pool, Cache, Middleware) |
| `logging_config.py` | Structured logging (JSON prod, pretty dev) |
| `error_tracking.py` | Pluggable error tracking (log/Sentry) |
| `prompts.py` | System prompt for Vanna agent |
| `lifecycle.py` | Startup diagnostics and env validation |

### Middleware (`middleware/`)

| Module | Purpose | Order |
|--------|---------|-------|
| `cors.py` | CORS header management | 1st |
| `rate_limit.py` | Per-IP rate limiting | 2nd |
| `request_logging.py` | Request/response logging | 3rd |
| `error_handler.py` | Global exception handler | 4th |

Middleware executes in reverse registration order (last registered = outermost). The error handler wraps everything to catch unhandled exceptions.

### Chart Engine (`chart_engine/`)

| Module | Purpose |
|--------|---------|
| `raid_chart_generator.py` | Custom Plotly chart generator for Vanna |

### Cache (`cache/`)

| Module | Purpose |
|--------|---------|
| `redis_client.py` | Redis connection management |
| `decorators.py` | `@cached` decorator for route handlers |

### Database (`database/`)

| Module | Purpose |
|--------|---------|
| `pool.py` | PG connection pool (psycopg2) |
| `manager.py` | Database manager abstraction |
| `schema.sql` | Full PostgreSQL DDL |
| `csv_to_postgres.py` | CSV data loader for PG |
| `migrate_sqlite_to_pg.py` | SQLite to PG migration |

### Frontend (`frontend/`)

Next.js 14 application with TypeScript and Tailwind CSS:

| Page | Route | Description |
|------|-------|-------------|
| Home | `/` | Dashboard landing page |
| Chat | `/chat` | AI chat interface (SSE) |
| Market | `/market` | TASI index chart (lightweight-charts) |
| Charts | `/charts` | TradingView embedded charts |
| Stock | `/stock/[ticker]` | Individual stock view |
| News | `/news` | Arabic news feed |
| News Detail | `/news/[id]` | Single news article |
| Reports | `/reports` | Technical reports |
| Announcements | `/announcements` | CMA/Tadawul announcements |
| Watchlist | `/watchlist` | User stock watchlists |
| Login | `/login` | Authentication page |

## Data Flow Diagrams

### Text-to-SQL Chat Flow

```
User Query (Arabic/English)
       |
       v
[Next.js Frontend] -- POST /api/vanna/v2/chat_sse -->
       |
       v
[FastAPI SSE Endpoint]
       |
       v
[Vanna Agent]
  |-- 1. SystemPromptBuilder: inject schema + instructions
  |-- 2. LLM Service: generate SQL from natural language
  |-- 3. RunSqlTool: execute SQL against SQLite/PostgreSQL
  |-- 4. VisualizeDataTool: generate Plotly chart (if applicable)
  |-- 5. LLM Service: summarize results in natural language
       |
       v
[SSE Stream] -- text/event-stream -->
       |
       v
[Frontend: render text + chart]
```

### Chart Request Flow (TASI Index)

```
[Frontend TASIIndexChart.tsx]
       |
       | GET /api/v1/charts/tasi/index?period=1mo
       v
[tasi_index route]
       |
       v
[tasi_index service]
  |-- Check circuit breaker state
  |-- If CLOSED: call yfinance (^TASI.SR)
  |   |-- Success: return OHLCV data, reset failure count
  |   |-- Failure: increment failures, open breaker if threshold hit
  |-- If OPEN: return cached/mock data
       |
       v
[JSON response: {dates, open, high, low, close, volume}]
       |
       v
[Frontend: render candlestick + volume with lightweight-charts]
```

### Authentication Flow

```
[Login Page]
       |
       | POST /api/auth/login {username, password}
       v
[auth route]
       |
       v
[auth_service]
  |-- Look up user in database
  |-- Verify password (bcrypt)
  |-- Generate JWT access token + refresh token
       |
       v
[Response: {access_token, refresh_token, token_type}]
       |
       v
[Frontend: store tokens, attach to subsequent requests]
       |
       | Authorization: Bearer <access_token>
       v
[Auth dependency: verify JWT, extract user]
       |
       v
[Protected route handler]
```

### News Scraping Flow

```
[NewsScheduler (daemon thread)]
  |-- Startup: immediate fetch
  |-- Then: every 30 minutes
       |
       v
[NewsScraper]
  |-- For each of 5 sources:
  |   |-- HTTP GET with requests + BeautifulSoup
  |   |-- Parse title, body, date, source URL
  |   |-- 1.5s delay between sources
       |
       v
[NewsParaphraser]
  |-- Apply Arabic synonym substitutions (~30 pairs)
       |
       v
[NewsStore (SQLite)]
  |-- INSERT OR IGNORE (dedup by title + source_name)
       |
       v
[/api/v1/news/feed]
  |-- Frontend polls for latest articles
  |-- Supports source filtering, search, pagination
```

## Technology Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| AI Framework | Vanna 2.0 | Purpose-built for text-to-SQL with tool calling and agent memory |
| LLM | Gemini 2.5 Flash / Claude Sonnet 4.5 | Fast, accurate SQL generation; supports tool use |
| Backend Framework | FastAPI | Async, auto-docs, SSE support, Python ecosystem |
| Dev Database | SQLite | Zero-config local development, single-file portability |
| Prod Database | PostgreSQL 16 | ACID compliance, connection pooling, Railway plugin |
| Frontend | Next.js 14 | App router, RSC, TypeScript, Vercel deployment |
| Charts (TASI) | lightweight-charts v4 | TradingView open-source, candlestick + volume |
| Charts (Stocks) | TradingView Widget | Rich embedded charts for TADAWUL symbols |
| Charts (AI) | Plotly | Vanna-native visualization via VisualizeDataTool |
| Cache | Redis (optional) | Low-latency response caching, graceful degradation |
| Auth | JWT + bcrypt | Stateless tokens, industry-standard password hashing |
| Deployment | Railway + Docker | PaaS with PostgreSQL plugin, Dockerfile-based builds |
| CI/CD | GitHub Actions | Two-stage: CI (test) -> Deploy (Railway) |
| Language (UI) | Arabic (RTL) | Target audience is Saudi market participants |
| Design | Gold/dark theme | Financial industry aesthetic, brand identity |

## Database Schema

10 normalized tables derived from a 1062-column flat CSV of ~500 TASI-listed companies:

```
+-------------------+     +---------------------+     +--------------------+
| companies         |     | market_data         |     | valuation_metrics  |
| (ticker, name,    |<--->| (price, volume,     |<--->| (pe, pb, ps,       |
|  sector, industry)|     |  market_cap, beta)  |     |  ev_ebitda)        |
+-------------------+     +---------------------+     +--------------------+
         |
         +---> profitability_metrics (margins, ROE, ROA)
         +---> dividend_data (yield, payout, growth)
         +---> financial_summary (revenue, earnings, debt)
         +---> analyst_data (target price, recommendations)
         +---> balance_sheet (multi-period, annual/quarterly/TTM)
         +---> income_statement (multi-period, annual/quarterly/TTM)
         +---> cash_flow (multi-period, annual/quarterly/TTM)
```

Financial statement tables use `period_type` (annual/quarterly/ttm) and `period_index` (0=most recent) for time-series queries.

## Known Limitations

1. **yfinance dependency**: Yahoo Finance API is unofficial and may be rate-limited or blocked. Circuit breaker pattern mitigates this with fallback to cached/mock data.

2. **News scraping fragility**: The 5 Arabic news sources may change their HTML structure, breaking scrapers. Each source is independent; failures are isolated.

3. **Single-node deployment**: Currently runs as a single Railway instance. No horizontal scaling or load balancing configured.

4. **In-memory agent memory**: `DemoAgentMemory` does not persist across restarts. Conversation history is lost on redeployment.

5. **SQLite concurrency**: SQLite backend does not support concurrent writes. Suitable for development only.

6. **TradingView TASI**: TADAWUL:TASI is not available in TradingView embedded widgets. TASI index uses lightweight-charts with yfinance data instead.

7. **No database migrations**: Schema changes require manual DDL or a fresh init. No migration framework (Alembic) is integrated.

8. **JWT secret rotation**: Changing `AUTH_JWT_SECRET` invalidates all existing tokens. No graceful rotation mechanism.

## Related Documents

- [Deployment Runbook](./DEPLOYMENT_RUNBOOK.md) - Full deployment procedures
- [Deployment Checklist](./DEPLOYMENT_CHECKLIST.md) - Pre/post deploy steps
- [Metrics and Monitoring](./METRICS_AND_MONITORING.md) - Observability guide
- [API Contracts](./api-contracts.md) - API endpoint specifications
- [Data Freshness](./data-freshness.md) - Data pipeline documentation
- [UI Transition](./ui-transition.md) - Frontend migration notes
</file>

<file path="docs/data-freshness.md">
# Data Freshness Assessment

**Date:** 2026-02-10
**Status:** Mixed -- static CSV snapshot for Vanna queries, live yfinance for index charts

---

## 1. Current Data Architecture

```
                     DATA FLOW DIAGRAM
                     =================

  saudi_stocks_yahoo_data.csv (one-time fetch, ~Feb 2026)
          |
          v
  csv_to_sqlite.py / csv_to_postgres.py
          |
          v
  +-----------------------+
  | SQLite / PostgreSQL   |  <-- STATIC: 10 normalized tables
  | (companies,           |      500 tickers, 11,263 rows
  |  market_data,         |      Financial periods: FY2021-FY2024 (annual)
  |  balance_sheet,       |                         Q0-Q3 ~2025 (quarterly)
  |  income_statement,    |      Price data: single snapshot, NOT updated
  |  cash_flow, ...)      |
  +-----------------------+
          |
          v
  Vanna AI (text-to-SQL)  --> Returns STALE data from static DB
          |
          v
  User sees static prices, static financials


  yfinance API (live, on-demand)
          |
          v
  services/tasi_index.py  --> 5-min in-memory cache, circuit breaker
          |
          v
  /api/v1/charts/tasi/index  --> Returns LIVE TASI index OHLCV
          |
          v
  TradingView candlestick charts on frontend
```

### The Staleness Gap

When a user asks Vanna "What is the current price of SABIC?", the system:
1. Generates SQL: `SELECT current_price FROM market_data WHERE ticker = '2010.SR'`
2. Returns the price snapshot from the CSV fetch date (~early Feb 2026)
3. This price is NOT live -- it was frozen at CSV extraction time

Meanwhile, the TradingView charts on the market page show live TASI index data
fetched in real-time via yfinance. This creates user confusion: charts show
today's prices while Vanna reports stale prices from the database.

---

## 2. CSV Data Staleness

**File:** `saudi_stocks_yahoo_data.csv` (501 lines: 1 header + 500 tickers)

### Date Evidence

Examining date columns across multiple tickers:

| Field | Example Value | Meaning |
|-------|--------------|---------|
| `ex_dividend_date` (1020.SR) | 2026-02-09 | Yesterday -- CSV fetched very recently |
| `most_recent_quarter` | 2025-09-30 | Q3 2025 is latest quarterly data |
| `last_fiscal_year_end` | 2024-12-31 | FY 2024 is latest annual data |
| `bs_q0_date` | 2025-06-30 | Most recent quarterly balance sheet |
| `is_ttm_date` | 2025-09-30 | TTM income statement through Q3 2025 |
| `cf_ttm_date` | 2025-09-30 | TTM cash flow through Q3 2025 |
| `bs_y0_date` | 2024-12-31 | Most recent annual balance sheet |

### Conclusions

- **CSV fetch date:** Approximately early February 2026 (based on ex-dividend dates)
- **Price data:** Single point-in-time snapshot from the fetch date; NOT updated
- **Financial statements:** Quarterly data up to Q3 2025 (6 months old for some companies); Annual data up to FY 2024 (14 months old)
- **TTM data:** Trailing twelve months through Q3 2025
- **Coverage:** Some smaller companies have NO financial statement data at all (empty columns), e.g., 9650.SR, 9651.SR

---

## 3. Vanna Query Freshness

**Source:** `app.py` lines 106-324 (SYSTEM_PROMPT)

The system prompt tells the LLM about all 10 database tables but provides NO
warning about data staleness. When Vanna generates SQL queries:

- `market_data.current_price` -- stale (snapshot from CSV fetch date)
- `market_data.volume` -- stale (daily volume from a single day)
- `market_data.market_cap` -- stale
- `balance_sheet`, `income_statement`, `cash_flow` -- reasonably fresh for fundamental analysis (Q3 2025 quarterly, FY2024 annual)
- `analyst_data.recommendation` -- stale (snapshot)
- `dividend_data.dividend_yield` -- stale (uses snapshot price)

**Key risk:** Users expect "current price" to mean today's price. The database
column is literally named `current_price`, reinforcing this misconception.

---

## 4. TASI Index (Live Data)

**Source:** `services/tasi_index.py` + `api/routes/tasi_index.py`

The TASI index endpoint is the ONLY live data source currently operational:

- **Symbols tried:** `^TASI` (primary), `TASI.SR` (fallback)
- **Cache TTL:** 300 seconds (5 minutes), thread-safe with `threading.Lock`
- **Circuit breaker:** Opens after 5 consecutive failures, 15-minute cooldown
- **Fallback chain:** Fresh cache -> yfinance -> Stale cache -> Deterministic mock
- **`.SR` suffix:** Works for Saudi stocks via yfinance (confirmed in code)

This endpoint serves the TradingView charts on the market overview page. It does
NOT serve per-stock price data -- only the TASI index.

---

## 5. Ingestion Pipeline Assessment

### 5.1 ingestion/config.py -- OPERATIONAL (utility)

| Attribute | Value |
|-----------|-------|
| **Purpose** | Configuration dataclass for batch size, rate limits, retries |
| **Status** | Operational utility class, used by price_loader and scheduler |
| **Data source** | N/A (configuration only) |
| **Error handling** | Environment variable fallback defaults |
| **Scheduling** | N/A |

### 5.2 ingestion/validators.py -- OPERATIONAL (utility)

| Attribute | Value |
|-----------|-------|
| **Purpose** | Validation functions for price data, XBRL facts, ticker format |
| **Status** | Operational utility module; all functions are complete and testable |
| **Data source** | N/A (validation only) |
| **Error handling** | Returns lists of error strings |
| **Scheduling** | N/A |

### 5.3 ingestion/price_loader.py -- BUILT, NEVER RUN IN PRODUCTION

| Attribute | Value |
|-----------|-------|
| **Purpose** | Fetch daily OHLCV prices via yfinance into `price_history` PG table |
| **Status** | Fully implemented (~670 lines), CLI-ready, but never deployed |
| **Data source** | yfinance (`.SR` suffix tickers) or CSV files |
| **Error handling** | Exponential backoff, batch processing, partial failure handling, NaN/Inf cleanup |
| **Scheduling** | Designed for scheduler.py (daily at 17:00), also runnable via CLI |
| **Database target** | PostgreSQL `price_history` table (not SQLite) |
| **Why not running** | No `price_history` table in SQLite schema; PG deployment not active; no `data/` directory exists |

Key features that ARE implemented:
- Batch processing with configurable size (default: 10 tickers per batch)
- Rate limiting between batches (default: 2s sleep)
- ON CONFLICT DO NOTHING for incremental/idempotent loads
- CSV import mode (single file or directory)
- Dry run mode
- Change amount/percentage computation

### 5.4 ingestion/xbrl_processor.py -- BUILT, NEVER RUN IN PRODUCTION

| Attribute | Value |
|-----------|-------|
| **Purpose** | Parse XBRL XML filings and Excel workbooks into `xbrl_facts` PG table |
| **Status** | Fully implemented (~1190 lines), supports XML (lxml) and Excel (openpyxl) |
| **Data source** | XBRL XML filings, Excel workbooks, or URLs |
| **Error handling** | Per-file error tracking, SHA-256 content hash dedup, filing status tracking |
| **Scheduling** | Designed for scheduler.py (weekly Friday at 20:00) |
| **Database target** | PostgreSQL `xbrl_facts` and `filings` tables |
| **Why not running** | No `data/filings/` directory exists; no XBRL filing sources configured; PG not active |

Key features that ARE implemented:
- IFRS taxonomy awareness (28 concept mappings)
- Arabic text detection
- Context/period parsing from XBRL elements
- URL download capability
- Batch insert with dedup by content_hash

### 5.5 ingestion/scheduler.py -- BUILT, NEVER RUN IN PRODUCTION

| Attribute | Value |
|-----------|-------|
| **Purpose** | APScheduler-based cron scheduler for automated data loading |
| **Status** | Fully implemented, but never deployed |
| **Schedule** | Price loader: daily 17:00 (after Saudi market close); XBRL: weekly Friday 20:00 |
| **Data source** | Orchestrates price_loader and xbrl_processor |
| **Error handling** | Signal handlers (SIGINT/SIGTERM), per-job try/finally with connection cleanup |
| **Why not running** | Requires PostgreSQL; not containerized; not referenced in Docker setup |

---

## 6. Per-Stock yfinance Capability

From `services/tasi_index.py` lines 215-216, the code tries two symbols:
```python
symbols = ["^TASI", "TASI.SR"]
```

The `.SR` suffix is the standard yfinance convention for Tadawul-listed stocks.
For individual stocks, the format is `{4-digit-number}.SR` (e.g., `2222.SR` for
Saudi Aramco, `1010.SR` for Riyad Bank). This is the same format used in the
database `ticker` column and the CSV.

The `ingestion/price_loader.py` already uses this format correctly:
```python
cur.execute("SELECT ticker FROM companies WHERE ticker LIKE %s", ("%.SR",))
```

yfinance `.SR` support for Saudi stocks is functional -- the entire CSV was
originally sourced from yfinance using these exact tickers.

---

## 7. Risk Summary

| Risk | Severity | Description |
|------|----------|-------------|
| **Stale price confusion** | HIGH | Vanna returns `current_price` from static DB while charts show live data |
| **No automated refresh** | HIGH | CSV loaded once; no recurring pipeline running |
| **User trust erosion** | MEDIUM | Financial professionals will notice stale prices immediately |
| **Missing price_history** | MEDIUM | SQLite schema has no `price_history` table; ingestion pipeline only targets PG |
| **XBRL never used** | LOW | Built but blocked on: no filing sources, no PG deployment |
| **TTM data aging** | LOW | TTM through Q3 2025 is acceptable for fundamental analysis |

---

## 8. Recommended Strategy

### Short Term (this week)

1. **Add staleness disclaimer to system prompt:** Update `SYSTEM_PROMPT` in
   `app.py` to warn the LLM that `market_data` prices are snapshots, not live.
   The LLM should caveat price queries with the data date.

2. **Rename misleading columns:** Consider renaming `current_price` to
   `snapshot_price` or adding a `data_as_of` column to `market_data`.

3. **Wire per-stock OHLCV endpoint:** The stock OHLCV endpoint (task #2) will
   provide live per-stock prices via yfinance, similar to the TASI index
   endpoint. This closes the gap for chart data.

### Medium Term (1-2 weeks)

4. **Deploy PostgreSQL + price_history pipeline:** The ingestion modules are
   fully built. To activate them:
   - Deploy with `DB_BACKEND=postgres`
   - Run `price_loader.py --all --from-date 2024-01-01` for initial backfill
   - Start `scheduler.py` to run daily at 17:00

5. **Add "live price" tool to Vanna:** Register a new Vanna tool that fetches
   real-time price from yfinance when the user asks about current prices,
   instead of querying the static database.

6. **Re-fetch CSV periodically:** Run the original Yahoo Finance scraper
   monthly to refresh fundamental data, analyst ratings, and financial summaries.

### Long Term (1+ month)

7. **Hybrid query routing:** Teach the system prompt to route price queries to
   the live yfinance tool and fundamental queries to the database. This gives
   users real-time prices AND historical financial data.

8. **XBRL pipeline activation:** Source XBRL filings from Tadawul/CMA
   disclosure portal. The processor is ready; it needs filing URLs and a
   scheduled scraper to discover new filings.

9. **Data freshness metadata:** Add a `data_sources` table tracking when each
   table was last refreshed, and expose this in the UI so users know how fresh
   their answers are.

---

## 9. Ingestion Pipeline Dependency Map

```
CURRENTLY RUNNING:
  services/tasi_index.py  -->  /api/v1/charts/tasi/index  (LIVE, yfinance)

BUILT BUT NOT RUNNING (requires PostgreSQL):
  ingestion/scheduler.py
      |
      +-- job_load_prices()  -->  ingestion/price_loader.py  -->  price_history table
      |                              uses: yfinance, ingestion/config.py
      |                              validates: ingestion/validators.py
      |
      +-- job_process_xbrl() -->  ingestion/xbrl_processor.py  -->  xbrl_facts table
                                     uses: lxml, openpyxl, requests
                                     needs: data/filings/ directory (does not exist)

NOT BUILT:
  - Per-stock live price tool for Vanna (planned)
  - Automated CSV re-fetch pipeline
  - Tadawul filing scraper/discovery
```
</file>

<file path="docs/DATABASE_AUDIT.md">
# Database Audit Report

## Overview

The Ra'd AI TASI platform uses a **dual-backend** architecture:

- **SQLite** (default, `DB_BACKEND=sqlite`): Local development, file-based `saudi_stocks.db`
- **PostgreSQL** (`DB_BACKEND=postgres`): Production (Railway/Docker), full schema in `database/schema.sql`

Configuration is managed by `config/settings.py` (`DatabaseSettings`) with `pydantic-settings`. The `api/db_helper.py` module provides a unified interface with automatic `?` to `%s` parameter conversion for PostgreSQL.

---

## 1. Database Schema: Tables, Columns, and Types

### Section 1: Core SQLite Tables (10 tables)

These tables exist in both SQLite and PostgreSQL with identical column names. Type mapping: `REAL` -> `NUMERIC(20,4)`, `INTEGER` -> `BIGINT`, `AUTOINCREMENT` -> `SERIAL`.

| Table | PK | Row Type | SQLite Types | PG Types |
|---|---|---|---|---|
| **companies** | `ticker TEXT` | 1/ticker | TEXT | TEXT |
| **market_data** | `ticker TEXT` (FK) | 1/ticker | REAL, INTEGER | NUMERIC(20,4), BIGINT |
| **valuation_metrics** | `ticker TEXT` (FK) | 1/ticker | REAL | NUMERIC(20,4) |
| **profitability_metrics** | `ticker TEXT` (FK) | 1/ticker | REAL | NUMERIC(20,4) |
| **dividend_data** | `ticker TEXT` (FK) | 1/ticker | REAL, TEXT | NUMERIC(20,4), TEXT |
| **financial_summary** | `ticker TEXT` (FK) | 1/ticker | REAL | NUMERIC(20,4) |
| **analyst_data** | `ticker TEXT` (FK) | 1/ticker | REAL, INTEGER, TEXT | NUMERIC(20,4), BIGINT, TEXT |
| **balance_sheet** | `id INTEGER AUTOINCREMENT` / `SERIAL` | N/ticker | REAL, TEXT, INTEGER | NUMERIC(20,4), TEXT, INTEGER |
| **income_statement** | `id INTEGER AUTOINCREMENT` / `SERIAL` | N/ticker | REAL, TEXT, INTEGER | NUMERIC(20,4), TEXT, INTEGER |
| **cash_flow** | `id INTEGER AUTOINCREMENT` / `SERIAL` | N/ticker | REAL, TEXT, INTEGER | NUMERIC(20,4), TEXT, INTEGER |

### Section 2: PG-Only Reference Tables

| Table | PK | Notes |
|---|---|---|
| **sectors** | `id SERIAL` | Reference: name_en, name_ar, code |
| **entities** | `id UUID` | Enhanced company info with Arabic names, FK to companies(ticker), sectors(id) |

### Section 3: PG-Only Financial Data Tables

| Table | PK | Notes |
|---|---|---|
| **filings** | `id UUID` | Filing metadata, FK to companies(ticker) |
| **xbrl_facts** | `id UUID` | XBRL financial data, FK to companies(ticker), filings(id) |
| **computed_metrics** | `id SERIAL` | Derived ratios, UNIQUE(ticker, metric_name, period_date, period_type) |
| **price_history** | `id SERIAL` | Daily OHLCV, UNIQUE(ticker, trade_date) |

### Section 4: PG-Only Content Tables

| Table | PK | Notes |
|---|---|---|
| **announcements** | `id UUID` | CMA/Tadawul announcements, FK to companies(ticker) |
| **news_articles** (PG version) | `id UUID` | Multi-source with sentiment, JSONB entities_extracted |
| **technical_reports** | `id UUID` | Analyst research with recommendations/targets |

### Section 5: PG-Only User/Auth Tables

| Table | PK | Notes |
|---|---|---|
| **users** | `id UUID` | Auth, profile, subscription tier |
| **user_watchlists** | `id UUID` | Ticker arrays (TEXT[]), FK to users(id) CASCADE |
| **user_alerts** | `id UUID` | Price/event alerts, FK to users(id) CASCADE |
| **query_audit_log** | `id UUID` | Query logging, INET type for ip_address |

### SQLite-Only Table (news_store)

| Table | PK | Notes |
|---|---|---|
| **news_articles** (SQLite version) | `id TEXT` | Different schema from PG version; includes `priority` column, UNIQUE(title, source_name) |

---

## 2. Query Locations by File

### `app.py` (FastAPI Server)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 59-62 | `AnthropicLlmService()` | N/A | LLM config, no DB |
| 71-93 | `_create_sql_runner()` | Both | Creates `SqliteRunner` or `PostgresRunner` |
| 96 | `sql_runner = _create_sql_runner()` | Both | Module-level runner |
| 102-108 | `tools.register_local_tool(RunSqlTool(...))` | Both | Vanna SQL tool |
| 503-516 | `init_pool(...)` | PG only | Connection pool init in lifespan |
| 566-567 | `NewsStore(str(_HERE / "saudi_stocks.db"))` | SQLite | News scheduler uses SQLite store |
| 606-613 | `close_pool()` | PG only | Connection pool close in lifespan |

### `csv_to_sqlite.py` (Data Pipeline)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 314-573 | DDL statements | SQLite only | `AUTOINCREMENT`, `REAL`, `INTEGER` types |
| 685-688 | `safe_to_sql()` with `pd.to_sql()` | SQLite only | Bulk insert via pandas |
| 715-717 | `PRAGMA journal_mode=WAL`, `PRAGMA foreign_keys=ON` | SQLite only | SQLite-specific PRAGMAs |
| 753 | `SELECT COUNT(*) FROM {table_name}` | SQLite only | Unparameterized table name (safe: hardcoded list) |

### `config/settings.py`

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 18-67 | `DatabaseSettings` | Both | Accepts `DB_PG_*` and `POSTGRES_*` env vars |
| 79-85 | `PoolSettings` | PG only | `PG_POOL_MIN`, `PG_POOL_MAX` env vars |

### `api/db_helper.py` (Dual-Backend Helper)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 42-66 | `get_conn()` | Both | Returns SQLite or PG connection |
| 74-78 | `_convert_sql()` | Both | Converts `?` to `%s` for PG |
| 84-99 | `fetchall()` | Both | Dict-based result rows |
| 102-115 | `fetchone()` | Both | Dict-based single row |

### `database/pool.py` (Connection Pool)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 46-87 | `init_pool()` | PG only | `ThreadedConnectionPool(minconn, maxconn, ...)` |
| 100-123 | `get_connection()` | PG only | Context manager with commit/rollback |
| 177-194 | `get_pool_connection()` | PG only | Returns `_PooledConnection` wrapper |
| 197-207 | `close_pool()` | PG only | Closes all pool connections |

### `services/news_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 107-117 | `INSERT INTO news_articles ... ON CONFLICT (id) DO NOTHING` | PG | Uses `%(name)s` params, `psycopg2.extras.Json` |
| 156-161 | `SELECT ... ORDER BY n.published_at DESC NULLS LAST LIMIT ... OFFSET` | PG | PG-compatible `NULLS LAST` |
| 198-204 | `SELECT ... WHERE ticker = ... ORDER BY published_at DESC NULLS LAST` | PG | Ticker filter |
| 238-245 | `SELECT ... JOIN companies ... WHERE sector ILIKE ...` | PG | `ILIKE` (PG-specific) |
| 257 | `SELECT * FROM news_articles WHERE id = %(id)s` | PG | Single article lookup |
| 292 | `SELECT COUNT(*) FROM news_articles n ...` | PG | Count with optional joins |

### `services/reports_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 107-118 | `INSERT INTO technical_reports ... ON CONFLICT (id) DO NOTHING` | PG | `%(name)s` params |
| 141-152 | Bulk `INSERT ... ON CONFLICT DO NOTHING` | PG | `executemany` |
| 193-199 | `SELECT ... WHERE recommendation ILIKE ...` | PG | `ILIKE` (PG-specific) |
| 235-240 | `SELECT ... ORDER BY published_at DESC NULLS LAST` | PG | Ticker filter |
| 253 | `SELECT * FROM technical_reports WHERE id = %(id)s` | PG | Single report lookup |
| 285 | `SELECT COUNT(*) FROM technical_reports ...` | PG | Count with filters |

### `services/announcement_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 108-118 | `INSERT INTO announcements ... ON CONFLICT (id) DO NOTHING` | PG | `%(name)s` params |
| 167-173 | `SELECT ... WHERE category ILIKE ... ORDER BY announcement_date DESC NULLS LAST` | PG | `ILIKE`, `NULLS LAST` |
| 191 | `WHERE a.is_material = TRUE` | PG | Boolean literal (also valid in SQLite) |
| 241-248 | `SELECT ... JOIN companies ... WHERE sector ILIKE ...` | PG | `ILIKE` (PG-specific) |
| 260 | `SELECT * FROM announcements WHERE id = %(id)s` | PG | Single announcement lookup |
| 292 | `SELECT COUNT(*) FROM announcements a ...` | PG | Count with filters |

### `services/audit_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 110-119 | `INSERT INTO query_audit_log ... %(ip_address)s::inet` | PG | `::inet` cast (PG-specific) |
| 170-176 | `SELECT ... ORDER BY created_at DESC LIMIT ... OFFSET` | PG | User query history |
| 192 | `WHERE q.created_at >= NOW() - %(interval)s::interval` | PG | `::interval` cast (PG-specific) |
| 201-213 | `TO_CHAR(...)`, `COUNT(*) FILTER (WHERE ...)` | PG | PG-specific: `TO_CHAR`, `FILTER` clause |
| 250-262 | Monthly stats with `TO_CHAR`, `FILTER`, `::interval` | PG | PG-specific functions |
| 303 | `SELECT COUNT(*) FROM query_audit_log q ...` | PG | Count with filters |

### `services/auth_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 49 | `SELECT id FROM users WHERE email = %s` | PG | `%s` positional params |
| 55-59 | `INSERT INTO users ... RETURNING id` | PG | `RETURNING` clause (PG-specific) |
| 78-83 | `SELECT id, auth_provider_id, is_active FROM users WHERE email = %s` | PG | Login query |
| 106 | `SELECT is_active FROM users WHERE id = %s` | PG | Active check |

### `services/user_service.py` (PG-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 153-158 | `INSERT INTO users ... ON CONFLICT (email) DO NOTHING` | PG | Upsert pattern |
| 159 | `SELECT * FROM users WHERE email = %(email)s` | PG | Select after upsert |
| 187 | `SELECT * FROM users WHERE id = %(id)s` | PG | User by ID |
| 202 | `SELECT * FROM users WHERE email = %(email)s` | PG | User by email |
| 217-222 | `UPDATE users SET usage_count = usage_count + 1, last_query_at = NOW()` | PG | `NOW()` function |
| 245-249 | `SELECT * FROM user_watchlists WHERE user_id = ...` | PG | Watchlist list |
| 272-276 | `INSERT INTO user_watchlists ... RETURNING *` | PG | `RETURNING *` (PG-specific) |
| 321-326 | `UPDATE user_watchlists SET ... RETURNING *` | PG | `RETURNING *` (PG-specific) |
| 347-349 | `DELETE FROM user_watchlists WHERE id = ... AND user_id = ...` | PG | Watchlist delete |
| 379-386 | `INSERT INTO user_alerts ... RETURNING *` | PG | `RETURNING *` (PG-specific) |
| 431-436 | `SELECT ... FROM user_alerts WHERE user_id = ... AND is_active = TRUE` | PG | Active alerts |
| 448-452 | `UPDATE user_alerts SET is_active = FALSE WHERE ...` | PG | Deactivate alert |

### `services/health_service.py` (Both Backends)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 88 | `SELECT 1` | PG | Health check ping via pool |
| 103-104 | `SELECT 1` | PG | Health check ping via direct connection |
| 117-118 | `SELECT 1` (via `conn.execute`) | SQLite | Health check ping |
| 234 | `SELECT COUNT(*) FROM companies` | SQLite | Entity count (always SQLite) |
| 264-269 | `SELECT COUNT(*) FROM market_data ...` | SQLite | Market data count (always SQLite) |
| 311-312 | `SELECT name FROM sqlite_master WHERE type='table' AND name='news_articles'` | SQLite | **SQLite-specific**: `sqlite_master` |
| 323-327 | `SELECT COUNT(*) FROM news_articles`, `SELECT COUNT(DISTINCT source_name)` | SQLite | Article counts |
| 461-462 | `SELECT MAX(created_at) FROM news_articles` | SQLite | News scraper check |
| 473-476 | `WHERE created_at > datetime('now', '-1 day')` | SQLite | **SQLite-specific**: `datetime()` function |

### `services/news_store.py` (SQLite-Only)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 27-43 | `CREATE TABLE IF NOT EXISTS news_articles (...)` | SQLite | `TEXT PRIMARY KEY`, `UNIQUE(title, source_name)` |
| 64 | `PRAGMA journal_mode=WAL` | SQLite | SQLite-specific PRAGMA |
| 99 | `INSERT OR IGNORE INTO news_articles ...` | SQLite | **SQLite-specific**: `INSERT OR IGNORE` |
| 118 | `SELECT changes()` | SQLite | **SQLite-specific**: `changes()` function |
| 144-157 | `SELECT * FROM news_articles ... ORDER BY priority ASC, created_at DESC` | SQLite | `?` params |
| 166-168 | `SELECT * FROM news_articles WHERE id = ?` | SQLite | Single article |
| 200-207 | `WHERE title LIKE ? OR body LIKE ?` | SQLite | Case-insensitive LIKE (SQLite default) |
| 216-220 | `SELECT source_name, COUNT(*) ... GROUP BY source_name` | SQLite | Source list |
| 230 | `DELETE FROM news_articles WHERE created_at < ?` | SQLite | Cleanup |
| 231 | `SELECT changes()` | SQLite | **SQLite-specific**: `changes()` function |

### `api/routes/market_analytics.py` (Dual-Backend via db_helper)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 76-91 | `_MOVERS_SQL`: `CASE WHEN ... THEN ... END AS change_pct` | Both | Standard SQL, compatible |
| 122 | `ORDER BY change_pct {order} LIMIT ?` | Both | `?` converted to `%s` by db_helper |
| 147-158 | Market summary aggregates with `COALESCE`, `SUM`, `CASE WHEN` | Both | Standard SQL |
| 190-206 | Sector analytics with `AVG`, `SUM`, `COUNT`, `GROUP BY` | Both | Standard SQL |
| 242-256 | Heatmap data with `CASE WHEN` | Both | Standard SQL |

### `api/routes/stock_data.py` (Dual-Backend via db_helper)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 115 | `SELECT 1 FROM companies WHERE ticker = ?` | Both | Existence check |
| 198 | `SELECT * FROM dividend_data WHERE ticker = ?` | Both | Single row |
| 228 | `SELECT * FROM financial_summary WHERE ticker = ?` | Both | Single row |
| 282 | `SELECT * FROM {statement} WHERE ticker = ? AND period_type = ?` | Both | Table name from whitelist |
| 345-348 | `SELECT ticker, short_name FROM companies WHERE ticker IN (...)` | Both | Dynamic placeholders |
| 358-360 | `SELECT {col_list} FROM {table} WHERE ticker IN (...)` | Both | Table/col from whitelist |
| 395-409 | `SELECT ... CASE WHEN ... END AS change_pct ... WHERE ticker IN (...)` | Both | Batch quotes |

### `api/routes/sqlite_entities.py` (Dual-Backend via db_helper)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 139-145 | `WHERE c.sector LIKE ? ... c.ticker LIKE ? OR c.short_name LIKE ?` | Both | `LIKE` (compatible) |
| 155-160 | `SELECT COUNT(*) AS cnt FROM companies c LEFT JOIN market_data m ...` | Both | Count query |
| 164-177 | `SELECT ... FROM companies c LEFT JOIN market_data m ... ORDER BY m.market_cap DESC LIMIT ? OFFSET ?` | Both | Paginated list |
| 207-215 | `SELECT c.sector, COUNT(*) ... GROUP BY c.sector` | Both | Sector counts |
| 250-278 | 7-table JOIN query for full entity detail | Both | `CASE WHEN` for change_pct |

### `api/routes/charts_analytics.py` (Dual-Backend via db_helper)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 54-60 | `SELECT c.sector AS label, SUM(m.market_cap) AS value ... GROUP BY c.sector` | Both | Sector market cap |
| 96-103 | `SELECT c.short_name AS label, m.market_cap AS value ... LIMIT ?` | Both | Top companies |
| 134-141 | `SELECT c.sector AS label, AVG(v.trailing_pe) AS value ... GROUP BY c.sector` | Both | Sector P/E |
| 170-176 | `SELECT c.short_name AS label, d.dividend_yield AS value ... LIMIT ?` | Both | Top dividend yields |

### `api/routes/news_feed.py` (SQLite-Only via NewsStore)

| Line | Query/Action | Backend | Notes |
|---|---|---|---|
| 28 | `NewsStore(_DB_PATH)` | SQLite | Singleton store init |
| All routes | Delegates to `services/news_store.py` | SQLite | Uses SQLite `?` params |

---

## 3. SQLite-Specific vs PG-Compatible Queries

### SQLite-Specific Constructs (NOT compatible with PostgreSQL)

| File | Line | Construct | PG Equivalent |
|---|---|---|---|
| `csv_to_sqlite.py:436` | `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` |
| `csv_to_sqlite.py:715-717` | `PRAGMA journal_mode=WAL; PRAGMA foreign_keys=ON;` | No equivalent needed |
| `services/news_store.py:27-43` | `CREATE TABLE ... id TEXT PRIMARY KEY ...` | `id UUID PRIMARY KEY DEFAULT uuid_generate_v4()` |
| `services/news_store.py:64` | `PRAGMA journal_mode=WAL` | No equivalent needed |
| `services/news_store.py:99` | `INSERT OR IGNORE INTO ...` | `INSERT ... ON CONFLICT DO NOTHING` |
| `services/news_store.py:118,231` | `SELECT changes()` | Use cursor's `rowcount` attribute |
| `services/health_service.py:311` | `SELECT name FROM sqlite_master WHERE type='table'` | `SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname='public'` |
| `services/health_service.py:474` | `datetime('now', '-1 day')` | `NOW() - INTERVAL '1 day'` |

### PG-Specific Constructs (NOT compatible with SQLite)

| File | Line | Construct | Notes |
|---|---|---|---|
| `services/audit_service.py:119` | `%(ip_address)s::inet` | PostgreSQL type cast |
| `services/audit_service.py:192` | `%(interval)s::interval` | PostgreSQL type cast |
| `services/audit_service.py:203-206` | `TO_CHAR(...)`, `COUNT(*) FILTER (WHERE ...)` | PG-specific functions |
| `services/news_service.py:126` | `psycopg2.extras.Json(...)` | PG JSONB adapter |
| `services/news_service.py:225` | `ILIKE` | PG case-insensitive LIKE |
| `services/reports_service.py:180` | `ILIKE` | PG case-insensitive LIKE |
| `services/announcement_service.py:155,228` | `ILIKE` | PG case-insensitive LIKE |
| `services/auth_service.py:57` | `INSERT ... RETURNING id` | PG-specific `RETURNING` |
| `services/user_service.py:276,326,386` | `INSERT/UPDATE ... RETURNING *` | PG-specific `RETURNING` |
| `services/user_service.py:220` | `NOW()` | PG function (but also supported in SQLite via custom function) |
| `database/schema.sql:21-22` | `CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; "pg_trgm"` | PG extensions |
| `database/schema.sql:595` | `USING GIN (... gin_trgm_ops)` | PG GIN index |

### Cross-Compatible Queries (work on both backends)

All queries in these files use standard SQL compatible with both backends (via `api/db_helper.py`):
- `api/routes/market_analytics.py` -- Standard `CASE WHEN`, `SUM`, `AVG`, `COUNT`, `GROUP BY`
- `api/routes/stock_data.py` -- Standard `SELECT`, `JOIN`, `WHERE IN`
- `api/routes/sqlite_entities.py` -- Standard `SELECT`, `JOIN`, `LIKE`, `COUNT`
- `api/routes/charts_analytics.py` -- Standard `SELECT`, `JOIN`, `GROUP BY`, `ORDER BY`

---

## 4. Connection Pooling Configuration

### Current Implementation (`database/pool.py`)

- **Pool type**: `psycopg2.pool.ThreadedConnectionPool`
- **Default min**: 2 connections (`PG_POOL_MIN` env var)
- **Default max**: 10 connections (`PG_POOL_MAX` env var)
- **Key strategy**: UUID-based unique keys (avoids async handler thread-sharing issue)
- **Connection wrapper**: `_PooledConnection` class returns connections to pool on `close()`
- **Lifecycle**: Initialized in `app.py` lifespan (`_lifespan`), closed on shutdown
- **Health check**: `is_pool_initialized()` function, used by `health_service.py`

### Architecture Layers

```
Route handler
  -> api/dependencies.py:get_db_connection()
    -> database/manager.py:DatabaseManager._get_raw_connection()
      -> database/pool.py:get_pool_connection() [if pool initialized]
      -> psycopg2.connect() [fallback if pool not initialized]
```

The `DatabaseManager` (singleton via `get_database_manager()`) is the centralized connection factory. For PostgreSQL, it checks if the pool is initialized and delegates to `get_pool_connection()`. If the pool is unavailable (e.g., import error or not yet initialized), it falls back to a direct `psycopg2.connect()`.

### Pool Usage Paths

| Consumer | Connection Source | Pattern |
|---|---|---|
| `api/dependencies.py:get_db_connection()` | `DatabaseManager._get_raw_connection()` | Pool-based when PG, delegates to pool |
| `api/dependencies.py:get_db_connection_dep()` | Generator wrapping `get_db_connection()` | FastAPI `Depends()` with auto-close |
| `api/db_helper.py:get_conn()` | `api.dependencies.get_db_connection()` | Dual-backend routes |
| `services/db_compat.py:get_read_connection()` | `database.pool.get_pool_connection()` | Backend-aware health checks |
| Service constructors (News, Reports, etc.) | `get_conn=get_db_connection` | Lazy connection per operation |

### Config (`config/settings.py:79-85`)

```python
class PoolSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="PG_POOL_")
    min: int = 2
    max: int = 10
```

### Pool Initialization (`app.py:553-569`)

```python
if DB_BACKEND == "postgres":
    from database.pool import init_pool
    _pool_min = _settings.pool.min if _settings else 2
    _pool_max = _settings.pool.max if _settings else 10
    init_pool(_db_settings, min_connections=_pool_min, max_connections=_pool_max)
```

### Production Sizing Recommendations

| Deployment | `PG_POOL_MIN` | `PG_POOL_MAX` | Rationale |
|---|---|---|---|
| Development (single user) | 1 | 5 | Low concurrency, minimal resource use |
| Railway (1 dyno, 512MB) | 2 | 10 | Default values, suitable for moderate traffic |
| Railway (1 dyno, 1GB) | 3 | 15 | Higher concurrency, more headroom |
| Multi-instance (2+ dynos) | 2 | 8 | Per-instance; total = N * max; keep under PG `max_connections` (default 100) |

**Key considerations**:
- PostgreSQL default `max_connections` is 100. With connection pooling across N app instances, ensure `N * PG_POOL_MAX` stays well below this limit.
- Each idle connection in the min pool consumes ~5-10MB of PG memory. Set `PG_POOL_MIN` to match steady-state concurrent query count.
- The UUID-based key strategy is correct for async FastAPI. Without it, all async handlers sharing the event loop thread would get the same connection.
- `_PooledConnection.close()` calls `rollback()` before returning to pool, which is defensive but correct -- prevents uncommitted state from leaking between checkouts.

### Connection Lifecycle Correctness

| Scenario | Behavior | Status |
|---|---|---|
| Clean exit from `get_connection()` context manager | `conn.commit()` then `putconn()` | Correct |
| Exception in `get_connection()` context manager | `conn.rollback()` then `putconn()` | Correct |
| `get_pool_connection()` caller calls `close()` | `rollback()` then `putconn()` | Correct |
| Pool not initialized | `RuntimeError` raised | Correct |
| Pool already initialized (double init) | Warning logged, skipped | Correct |
| `close_pool()` when not initialized | No-op | Correct |
| `close_pool()` exception in `closeall()` | Exception logged, pool set to None | Correct |
| `DatabaseManager` with no pool | Falls back to direct `psycopg2.connect()` | Correct |

### Tests (`tests/test_connection_pool.py`)

- 8 test methods using mocked `psycopg2` (no real PG required)
- Coverage: init, double-init skip, init failure, get_connection commit, get_connection rollback on exception, get_pool_connection wrapper close, close_pool, close_pool error handling
- `reset_pool` autouse fixture ensures clean state per test

### Identified Issues and Recommendations

1. **No connection timeout on pool checkout**: `ThreadedConnectionPool.getconn()` can block indefinitely if all connections are in use. Consider wrapping with a timeout or switching to a pool implementation that supports `timeout` (e.g., `psycopg2.pool.SimpleConnectionPool` with manual locking, or `psycogreen`/`psycopg3`).

2. **No connection validation/keepalive**: Idle connections may be terminated by PG (via `idle_in_transaction_session_timeout` or network timeouts). The pool does not validate connections before returning them. Consider adding a `SELECT 1` health check on checkout or using `keepalives` parameters:
   ```python
   psycopg2.connect(..., keepalives=1, keepalives_idle=30, keepalives_interval=10, keepalives_count=5)
   ```

3. **No pool metrics exposure**: The pool size, active connections, and available connections are not exposed to the health endpoint or monitoring. Consider adding a `pool_status()` function that returns `{ "min": N, "max": N, "active": N, "available": N }`.

4. **Env var documentation**: The `PG_POOL_MIN` and `PG_POOL_MAX` env vars are not documented in `.env.example`. They should be added for operator visibility.

---

## 5. Skipped PostgreSQL Tests

### `test_database.py` (23 skipped tests)

- **Condition**: `@unittest.skipUnless(_pg_available(), "PostgreSQL not available (set POSTGRES_HOST)")`
- **Class**: `TestDatabaseIntegrityPG` (line 447)
- **Inherits**: `_DatabaseTestMixin` with 23 test methods covering:
  - Table existence, row counts, column presence
  - Foreign key relationships, index existence
  - Data integrity (non-null tickers, valid period types)
  - Financial statement validation
  - Cross-table consistency
- **PG check**: Requires `POSTGRES_HOST` env var and reachable PG server

### `test_app_assembly_v2.py` (3 skipped tests)

- **Test 3.2** (line 258): `PostgresRunner` construction -- skipped if `PG_AVAILABLE` is False
- **Test 7.4** (line 596): Full agent assembly with `PostgresRunner` -- skipped if `PG_AVAILABLE` is False
- **Test 11.2** (line 840): PG env vars present when `DB_BACKEND=postgres` -- skipped if backend is sqlite
- **PG check**: Same `_pg_available()` function checking `POSTGRES_HOST` env var

### `tests/test_services.py` (3 skipped test classes)

- **`TestNewsServicePG`** (line 406): `@unittest.skipUnless(PG_AVAILABLE, ...)`
  - 3 test methods: store_articles, get_latest_news, get_news_by_ticker
- **`TestUserServicePG`** (line 451): `@unittest.skipUnless(PG_AVAILABLE, ...)`
  - 3 test methods: get_or_create_user, increment_usage, watchlist CRUD
- **`TestAuditServicePG`** (line 504): `@unittest.skipUnless(PG_AVAILABLE, ...)`
  - 3 test methods: log_query, get_user_query_history, get_usage_stats_daily

### `tests/test_api_routes.py` (1 skipped test class)

- **`TestAPIRoutesIntegrationPG`** (line 342): `@unittest.skipUnless(PG_AVAILABLE, ...)`
  - Tests API routes against live PostgreSQL with `TestClient`
  - Sets `DB_BACKEND=postgres` before importing app
  - Tests: health, news, reports, announcements endpoints

### Summary of Skipped PG Tests

| File | Test Class/Function | Approx Tests Skipped | Skip Condition |
|---|---|---|---|
| `test_database.py` | `TestDatabaseIntegrityPG` | 23 | `POSTGRES_HOST` not set |
| `test_app_assembly_v2.py` | Tests 3.2, 7.4, 11.2 | 3 | `POSTGRES_HOST` not set |
| `tests/test_services.py` | 3 classes (News, User, Audit) | ~9 | `POSTGRES_HOST` not set |
| `tests/test_api_routes.py` | `TestAPIRoutesIntegrationPG` | ~5+ | `POSTGRES_HOST` not set |
| **Total** | | **~40** | |

### PG Test Infrastructure

**Shared fixtures** (`tests/conftest.py`):
- `pg_conn` (session-scoped): Live PG connection, auto-skips when `POSTGRES_HOST` not set
- `pg_cursor`: PG cursor with SAVEPOINT/ROLLBACK isolation per test
- `pg_conn_factory`: Callable returning new PG connections (for services accepting `get_conn`)
- `PG_AVAILABLE`: Module-level boolean for skip conditions

**Runner scripts**:
- `scripts/run_pg_tests.sh`: Starts PG via docker-compose, sets env vars, initializes schema, runs all test suites. Supports `--down` (teardown after) and `--pg-only` (PG tests only).
- `scripts/test_pg.sh`: Similar runner created by test-integr team.

---

## 6. Backend-Specific Service Classification

### SQLite-Only Services

| Service | File | DB Interaction |
|---|---|---|
| `NewsStore` | `services/news_store.py` | SQLite `news_articles` table, `INSERT OR IGNORE`, `SELECT changes()`, `PRAGMA journal_mode=WAL` |
| `NewsScheduler` | `services/news_scheduler.py` | Delegates to `NewsStore` |
| `NewsScraper` | `services/news_scraper.py` | No direct DB (passes articles to `NewsStore`) |
| `NewsParaphraser` | `services/news_paraphraser.py` | No DB access |

### PG-Only Services

| Service | File | DB Interaction |
|---|---|---|
| `NewsAggregationService` | `services/news_service.py` | `psycopg2`, `%(name)s` params, `ON CONFLICT`, `ILIKE`, `NULLS LAST` |
| `TechnicalReportsService` | `services/reports_service.py` | `psycopg2`, `%(name)s` params, `ON CONFLICT`, `ILIKE`, `NULLS LAST` |
| `AnnouncementService` | `services/announcement_service.py` | `psycopg2`, `%(name)s` params, `ON CONFLICT`, `ILIKE`, `NULLS LAST` |
| `AuditService` | `services/audit_service.py` | `psycopg2`, `::inet`, `::interval`, `TO_CHAR`, `FILTER` |
| `AuthService` | `services/auth_service.py` | `psycopg2`, `%s` params, `RETURNING` |
| `UserService` | `services/user_service.py` | `psycopg2`, `%(name)s` params, `RETURNING *`, `ON CONFLICT`, `NOW()` |

### No-DB Services

| Service | File | Notes |
|---|---|---|
| `TASIIndex` | `services/tasi_index.py` | yfinance only, in-memory cache |
| `StockOHLCV` | `services/stock_ohlcv.py` | yfinance only, in-memory cache |

### Dual-Backend (via db_helper)

| Route File | Uses | Notes |
|---|---|---|
| `api/routes/market_analytics.py` | `api.db_helper` | `?` params auto-converted |
| `api/routes/stock_data.py` | `api.db_helper` | `?` params auto-converted |
| `api/routes/sqlite_entities.py` | `api.db_helper` | `?` params auto-converted |
| `api/routes/charts_analytics.py` | `api.db_helper` | `?` params auto-converted |

### Health Service (Both Backends)

| Check | Backend | File:Line |
|---|---|---|
| `check_database()` | Both | `services/health_service.py:71-136` |
| `check_entities()` | SQLite always | `services/health_service.py:222-249` |
| `check_market_data()` | SQLite always | `services/health_service.py:252-291` |
| `check_news()` | SQLite always | `services/health_service.py:294-350` |
| `check_news_scraper()` | SQLite always | `services/health_service.py:415-522` |
| `check_tasi_index()` | No DB | `services/health_service.py:353-412` |
| `check_llm()` | No DB | `services/health_service.py:139-158` |
| `check_redis()` | No DB | `services/health_service.py:161-204` |

**Note**: `check_entities()`, `check_market_data()`, `check_news()`, and `check_news_scraper()` always query the SQLite database regardless of `DB_BACKEND` setting. This is a known limitation -- in PG mode, these health checks still read from `saudi_stocks.db`.

---

## 7. Identified Issues and Recommendations

### Issue 1: Health Checks Hardcoded to SQLite

`check_entities()`, `check_market_data()`, `check_news()`, and `check_news_scraper()` in `services/health_service.py` always use `_get_sqlite_path()` and `_sqlite_query()`, even when running with PostgreSQL backend. These should be made backend-aware.

### Issue 2: `news_articles` Table Schema Divergence

The SQLite version (in `services/news_store.py`) and PostgreSQL version (in `database/schema.sql`) have different schemas:
- SQLite: `id TEXT`, `priority INTEGER`, `UNIQUE(title, source_name)`, no `entities_extracted`
- PG: `id UUID`, `entities_extracted JSONB`, no `priority`, PK on `id` only

This means the same table name is used for different purposes in each backend.

### Issue 3: No `change_pct` Column in `market_data`

Several route queries compute `change_pct` dynamically via `CASE WHEN`. The health check `check_market_data()` references `change_pct` as if it were a real column, but it only checks for non-null values. The PG `market_data` table schema does not include `change_pct` -- this works because it's always computed inline.

### Issue 4: `LIKE` vs `ILIKE` Inconsistency

Dual-backend routes use `LIKE` (case-sensitive in PG, case-insensitive in SQLite by default). PG-only services use `ILIKE`. If dual-backend routes need case-insensitive search on PG, they should use `ILIKE` or `LOWER()`.

### Issue 5: Pool Settings Env Prefix

`PoolSettings` uses `PG_POOL_` prefix, but `app.py` references `_settings.pool.min` / `_settings.pool.max`. Env vars are `PG_POOL_MIN` and `PG_POOL_MAX`. This is correct but could be confusing alongside `POSTGRES_*` prefixes.
</file>

<file path="docs/DEPLOYMENT_CHECKLIST.md">
# Deployment Checklist

Pre-deploy, deploy, and post-deploy steps for the Ra'd AI TASI Platform.

## Pre-Deploy

### Tests (all must pass)

- [ ] Python unit/integration tests: `python -m unittest discover -s tests -p "test_*.py"`
- [ ] Database integrity tests: `python test_database.py`
- [ ] Vanna assembly tests: `python test_app_assembly_v2.py`
- [ ] Frontend build: `cd frontend && npm run build`
- [ ] Frontend tests: `cd frontend && npx vitest run`

### Configuration

- [ ] `.env` exists with production values (copy from `.env.example`)
- [ ] `ANTHROPIC_API_KEY` set to valid key
- [ ] `DB_BACKEND=postgres` for production
- [ ] `POSTGRES_*` env vars configured (host, port, db, user, password)
- [ ] `AUTH_JWT_SECRET` set to a stable, random secret (not the default)
- [ ] `MW_CORS_ORIGINS` includes production domain: `https://raid-ai-app-production.up.railway.app`
- [ ] `CACHE_ENABLED=true` and `REDIS_URL` configured (if Redis is available)
- [ ] `SERVER_DEBUG=false`

### Secrets (GitHub Actions)

- [ ] `RAILWAY_TOKEN` configured in repository secrets
- [ ] `RAILWAY_PROJECT_ID` configured in repository secrets
- [ ] `DEPLOY_URL` configured in repository variables (optional, defaults to Railway URL)

### Build Artifacts

- [ ] `saudi_stocks_yahoo_data.csv` present in repo root (needed for DB initialization)
- [ ] `database/schema.sql` up to date with any schema changes
- [ ] `requirements.txt` includes all Python dependencies
- [ ] `frontend/package-lock.json` exists (needed for `npm ci` in CI)

## Deploy

### Automatic (CI/CD)

1. Push to `master` branch
2. CI pipeline runs: lint, Python tests (SQLite + PG), frontend build + test
3. On CI success, deploy pipeline triggers: Railway CLI deploys `raid-ai-app` service
4. Post-deploy health check runs automatically (5 retries, 15s intervals)

### Manual (Railway CLI)

```bash
# Ensure Railway CLI is installed and authenticated
npm install -g @railway/cli

# Deploy from local
RAILWAY_TOKEN=<token> RAILWAY_PROJECT_ID=<project-id> railway up --service raid-ai-app
```

### Manual (Docker Compose, local/staging)

```bash
# Start PostgreSQL + app
docker compose up -d

# With pgAdmin
docker compose --profile tools up -d

# First run: entrypoint.sh auto-initializes PG schema + loads CSV data
```

## Post-Deploy

### Health Checks

- [ ] Backend health: `curl https://raid-ai-app-production.up.railway.app/api/v1/health`
- [ ] TASI data pipeline: `curl https://raid-ai-app-production.up.railway.app/api/v1/charts/tasi/health`
- [ ] Stock OHLCV pipeline: `curl https://raid-ai-app-production.up.railway.app/api/v1/charts/2222/health`
- [ ] Chat endpoint responds: `curl -X POST https://raid-ai-app-production.up.railway.app/api/vanna/v2/chat_sse`

### Smoke Test

```bash
# Run the automated smoke test script
bash scripts/smoke_test.sh https://raid-ai-app-production.up.railway.app
```

### Verify Functionality

- [ ] Homepage loads at `/`
- [ ] Chat interface responds to natural language queries
- [ ] Candlestick chart shows data on `/stock/2222` (source should be "real" or "cached", not "mock")
- [ ] TASI index chart renders on `/market`
- [ ] CORS headers present for frontend origin

## Rollback

### Quick Rollback (Railway)

1. Go to Railway dashboard: https://railway.com/project/b91140a9-417a-4edc-b625-8282366860bd
2. Navigate to the `raid-ai-app` service
3. Click on the previous successful deployment
4. Click "Redeploy" to roll back to that version

### Git Rollback

```bash
# Find the last known good commit
git log --oneline -10

# Revert to previous commit (creates a new commit)
git revert HEAD
git push origin master
# CI/CD will auto-deploy the reverted state
```

### Database Rollback

- The `entrypoint.sh` script only initializes the database if the `companies` table does not exist
- Schema changes require manual migration or a fresh init
- For data-only issues: re-run `python database/csv_to_postgres.py` to reload from CSV
- For schema issues: drop and recreate with `database/schema.sql`, then reload data

## Service Dependencies

| Service | Required | Notes |
|---------|----------|-------|
| PostgreSQL 16 | Yes (production) | `database/schema.sql` for schema, `csv_to_postgres.py` for data |
| Redis 7 | Optional | Set `CACHE_ENABLED=true`; app degrades gracefully without it |
| yfinance (Yahoo Finance API) | Optional | TASI index + stock OHLCV charts; falls back to mock data |
| Anthropic API | Yes | Claude Sonnet 4.5 for natural language SQL generation |

## Endpoints (DB-agnostic, always available)

| Endpoint | Description |
|----------|-------------|
| `GET /` | Homepage (legacy template UI) |
| `GET /api/v1/charts/tasi/index` | TASI index OHLCV data |
| `GET /api/v1/charts/tasi/health` | TASI data pipeline health |
| `GET /api/v1/charts/{ticker}/ohlcv` | Per-stock OHLCV data |
| `GET /api/v1/charts/{ticker}/health` | Per-stock data pipeline health |
| `POST /api/vanna/v2/chat_sse` | Vanna chat (SSE streaming) |

## Endpoints (PostgreSQL-only)

| Endpoint | Description |
|----------|-------------|
| `GET /api/v1/health` | Full platform health check |
| `GET /api/news` | News articles |
| `GET /api/reports` | Technical reports |
| `GET /api/announcements` | CMA/Tadawul announcements |
| `GET /api/entities` | Company listings |
| `POST /api/auth/login` | JWT authentication |
</file>

<file path="docs/DEPLOYMENT_RUNBOOK.md">
# Deployment Runbook

Step-by-step procedures for deploying, validating, monitoring, and rolling back the Ra'd AI TASI Platform.

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [First Deployment (Railway)](#first-deployment-railway)
3. [Environment Configuration](#environment-configuration)
4. [PostgreSQL Setup on Railway](#postgresql-setup-on-railway)
5. [CI/CD Automatic Deployment](#cicd-automatic-deployment)
6. [Manual Deployment](#manual-deployment)
7. [Post-Deploy Validation](#post-deploy-validation)
8. [Monitoring Setup](#monitoring-setup)
9. [Rollback Procedure](#rollback-procedure)
10. [Troubleshooting](#troubleshooting)

---

## Prerequisites

### Accounts and Access

- GitHub repository access (push to `master`)
- Railway account with project access
- Railway CLI installed: `npm install -g @railway/cli`
- Anthropic or Gemini API key for LLM service

### Local Tools

```bash
# Verify Railway CLI
railway version

# Verify Docker (for local testing)
docker --version
docker compose version

# Verify Python environment
python --version   # 3.11+
pip install -r requirements.txt
```

### Repository Secrets (GitHub Actions)

Configure these in GitHub repository Settings > Secrets and Variables > Actions:

| Secret | Required | Description |
|--------|----------|-------------|
| `RAILWAY_TOKEN` | Yes | Railway API token for deployment |
| `RAILWAY_PROJECT_ID` | Yes | Railway project ID |

| Variable | Required | Description |
|----------|----------|-------------|
| `DEPLOY_URL` | No | Deployment URL (defaults to `https://raid-ai-app-production.up.railway.app`) |

---

## First Deployment (Railway)

### Option A: Railway Dashboard

1. Log in to [Railway](https://railway.com)
2. Create a new project or select the existing `raid-ai-tasi` project
3. Add a **PostgreSQL** service (see [PostgreSQL Setup](#postgresql-setup-on-railway))
4. Add a service from GitHub:
   - Connect the repository
   - Railway auto-detects `Dockerfile` and `railway.toml`
5. Configure environment variables (see [Environment Configuration](#environment-configuration))
6. Deploy triggers automatically on push to `master`

### Option B: Railway CLI

```bash
# Authenticate
railway login

# Link to project (interactive)
railway link

# Deploy
railway up --service raid-ai-app

# Check deployment status
railway status
```

### What Happens on First Deploy

1. Docker image builds from `Dockerfile` (multi-stage: builder + runtime)
2. `entrypoint.sh` runs:
   - Checks if `companies` table exists in PostgreSQL
   - If not: runs `database/schema.sql` to create tables, then `csv_to_postgres.py` to load data
3. Uvicorn starts on `$PORT` (Railway assigns this automatically)
4. Health check validates at `/health`

---

## Environment Configuration

### Required Variables (Production)

Set these in Railway service variables:

```bash
# Database
DB_BACKEND=postgres
POSTGRES_HOST=<railway-pg-host>       # e.g., postgres.railway.internal
POSTGRES_PORT=5432
POSTGRES_DB=raid_ai
POSTGRES_USER=raid
POSTGRES_PASSWORD=<strong-password>

# LLM (one of these)
GEMINI_API_KEY=<your-gemini-key>      # Preferred
# or
ANTHROPIC_API_KEY=<your-anthropic-key>

# Authentication
AUTH_JWT_SECRET=<stable-random-secret>
# Generate: python -c "import secrets; print(secrets.token_urlsafe(32))"

# Server
SERVER_DEBUG=false
ENVIRONMENT=production

# CORS (include your frontend domains)
MW_CORS_ORIGINS=http://localhost:3000,https://raid-ai-app-production.up.railway.app,https://frontend-two-nu-83.vercel.app
```

### Optional Variables

```bash
# Logging
LOG_LEVEL=INFO                        # DEBUG, INFO, WARNING, ERROR

# Connection pool
PG_POOL_MIN=2
PG_POOL_MAX=10

# Rate limiting
MW_RATE_LIMIT_PER_MINUTE=60

# Cache (if Redis available)
CACHE_ENABLED=false
REDIS_URL=redis://localhost:6379/0

# Error tracking
ERROR_TRACKER=log                     # "log" (default) or "sentry"
SENTRY_DSN=                           # Required if ERROR_TRACKER=sentry
```

### Variable Validation

Run the config validation script locally before deploying:

```bash
python scripts/validate_config.py
```

---

## PostgreSQL Setup on Railway

### Add PostgreSQL Plugin

1. In Railway project dashboard, click **+ New** > **Database** > **PostgreSQL**
2. Railway provisions a PostgreSQL 16 instance
3. Note the internal connection variables:
   - `POSTGRES_HOST` = `postgres.railway.internal` (internal networking)
   - `POSTGRES_PORT` = `5432`
   - `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASSWORD` (auto-generated)

### Link to App Service

Railway automatically exposes database variables to linked services. Ensure:
- The app service references the PostgreSQL service
- `DB_BACKEND=postgres` is set in app service variables

### Schema Initialization

Schema is auto-initialized by `entrypoint.sh` on first boot. To manually reinitialize:

```bash
# Via Railway CLI (connects to PG)
railway run psql -f database/schema.sql

# Reload data
railway run python database/csv_to_postgres.py --csv-path saudi_stocks_yahoo_data.csv
```

### Verify Database

```bash
# Connect to Railway PG
railway run psql

# Check tables exist
\dt

# Verify data
SELECT COUNT(*) FROM companies;
-- Expected: ~500 rows
```

---

## CI/CD Automatic Deployment

The project uses a two-workflow CI/CD pipeline:

### Workflow 1: CI (`.github/workflows/ci.yml`)

Triggered on: push to `master`, pull requests

Steps:
1. Lint (ruff)
2. Python tests (SQLite + PostgreSQL)
3. Frontend build + Vitest

### Workflow 2: Deploy (`.github/workflows/deploy.yml`)

Triggered on: CI workflow completes successfully on `master`

Steps:
1. Installs Railway CLI
2. Deploys to Railway (`railway up --service raid-ai-app`)
3. Waits 30s for stabilization
4. Runs health check (5 retries, 15s intervals)

### Deploy Flow

```
Push to master -> CI runs -> CI passes -> Deploy triggers -> Railway builds -> Health check
```

If CI fails, deployment does **not** proceed.

---

## Manual Deployment

### When to Use

- Hotfix that needs immediate deployment
- CI is broken but change is verified locally
- Testing a specific commit on Railway

### Steps

```bash
# 1. Ensure you're on the correct commit
git log --oneline -3

# 2. Run tests locally first
python -m pytest tests/ -x -q
python test_database.py
python test_app_assembly_v2.py

# 3. Deploy via Railway CLI
export RAILWAY_TOKEN=<your-token>
export RAILWAY_PROJECT_ID=<your-project-id>
railway up --service raid-ai-app

# 4. Monitor deployment logs
railway logs --follow

# 5. Run post-deploy validation
bash scripts/smoke_test.sh https://raid-ai-app-production.up.railway.app
```

---

## Post-Deploy Validation

### Automated Smoke Test

```bash
bash scripts/smoke_test.sh https://raid-ai-app-production.up.railway.app
```

### Manual Health Checks

```bash
BASE_URL="https://raid-ai-app-production.up.railway.app"

# 1. Basic health
curl -s "$BASE_URL/health" | python -m json.tool

# 2. Full platform health (PG-only)
curl -s "$BASE_URL/api/v1/health" | python -m json.tool

# 3. TASI data pipeline
curl -s "$BASE_URL/api/v1/charts/tasi/health" | python -m json.tool

# 4. Stock OHLCV pipeline
curl -s "$BASE_URL/api/v1/charts/2222/health" | python -m json.tool

# 5. Chat endpoint (SSE)
curl -s -X POST "$BASE_URL/api/vanna/v2/chat_sse" \
  -H "Content-Type: application/json" \
  -d '{"message": "كم عدد الشركات المدرجة؟"}' \
  --max-time 30
```

### Verification Checklist

- [ ] `/health` returns 200
- [ ] Homepage loads at `/`
- [ ] Chat responds to a test query
- [ ] TASI chart data endpoint returns data
- [ ] CORS headers present for configured origins
- [ ] Logs show JSON format (not dev-mode pretty)
- [ ] No ERROR level entries in startup logs

---

## Monitoring Setup

### Railway Built-in

Railway provides:
- **Deploy logs**: Real-time log streaming (structured JSON logs auto-parsed)
- **Metrics**: CPU, memory, network per service
- **Health checks**: Configured in `railway.toml` at `/health`

### Log Monitoring

```bash
# Tail live logs
railway logs --follow

# Filter errors
railway logs --follow | grep '"level":"ERROR"'

# Check last hour
railway logs --since 1h
```

### Uptime Monitoring (Recommended)

Set up an external uptime monitor (Betterstack, UptimeRobot, or similar):
- Monitor URL: `https://raid-ai-app-production.up.railway.app/health`
- Check interval: 3 minutes
- Alert on: 3 consecutive failures
- Alert channels: Email, Slack, or webhook

### Error Tracking

The platform uses `config/error_tracking.py` with structured logging by default. To enable Sentry:

1. `pip install sentry-sdk[fastapi]` (add to `requirements.txt`)
2. Set `ERROR_TRACKER=sentry` and `SENTRY_DSN=<your-dsn>` in Railway variables
3. Uncomment `SentryErrorTracker` class in `config/error_tracking.py`

See [Metrics and Monitoring](./METRICS_AND_MONITORING.md) for detailed metric definitions and dashboard layout.

---

## Rollback Procedure

### Quick Rollback (Railway Dashboard)

1. Go to Railway dashboard: [raid-ai-tasi project](https://railway.com)
2. Navigate to `raid-ai-app` service > Deployments
3. Find the last successful deployment
4. Click **Redeploy** on that deployment

### Git Rollback (Triggers CI/CD)

```bash
# Option 1: Revert the bad commit (creates a new commit)
git revert HEAD
git push origin master
# CI runs -> deploys reverted state

# Option 2: Revert multiple commits
git revert HEAD~3..HEAD --no-commit
git commit -m "Revert: roll back last 3 commits due to <reason>"
git push origin master
```

### Database Rollback

The database is **not** automatically rolled back with application rollbacks.

- **Data-only issues**: Re-run `python database/csv_to_postgres.py` to reload from CSV
- **Schema issues**: Drop and recreate with `database/schema.sql`, then reload data
- **Partial migrations**: Restore from Railway's automatic PostgreSQL backups (if enabled)

```bash
# Re-initialize schema (destructive - drops all tables)
railway run psql -f database/schema.sql

# Reload data
railway run python database/csv_to_postgres.py --csv-path saudi_stocks_yahoo_data.csv
```

---

## Troubleshooting

### Build Failures

| Symptom | Cause | Fix |
|---------|-------|-----|
| `pip install` fails | Missing system dependency | Check `Dockerfile` `apt-get install` stage |
| `psycopg2` build error | Missing `libpq-dev` | Verify builder stage has `gcc libpq-dev` |
| Docker build timeout | Large dependencies | Check Railway build timeout settings |

### Startup Failures

| Symptom | Cause | Fix |
|---------|-------|-----|
| `NEED_INIT` loop | PG connection failed | Verify `POSTGRES_*` env vars, check PG is running |
| `ModuleNotFoundError` | Missing dependency | Run `pip install -r requirements.txt`, rebuild Docker |
| Port bind error | `$PORT` conflict | Railway sets `PORT` automatically; do not hardcode |
| `AUTH_JWT_SECRET` warning | Secret not configured | Set `AUTH_JWT_SECRET` in Railway variables |

### Runtime Errors

| Symptom | Cause | Fix |
|---------|-------|-----|
| 502 Bad Gateway | App crashed or slow startup | Check `railway logs`, increase health check timeout |
| 500 on chat | LLM API key invalid/missing | Verify `GEMINI_API_KEY` or `ANTHROPIC_API_KEY` |
| Empty chart data | yfinance blocked/down | Check circuit breaker state; yfinance is optional |
| CORS errors | Origin not in allowlist | Add domain to `MW_CORS_ORIGINS` |
| Rate limit (429) | Too many requests | Adjust `MW_RATE_LIMIT_PER_MINUTE` or check for abuse |

### Database Issues

| Symptom | Cause | Fix |
|---------|-------|-----|
| `connection refused` | PG not running | Check Railway PG service status |
| `pool exhausted` | Too many concurrent queries | Increase `PG_POOL_MAX` |
| `relation does not exist` | Schema not initialized | Run `database/schema.sql` manually |
| Stale data | CSV not reloaded after update | Re-run `csv_to_postgres.py` |

### Useful Diagnostic Commands

```bash
# Check app environment
railway run env | sort

# Test database connectivity
railway run python -c "
import psycopg2, os
conn = psycopg2.connect(
    host=os.environ['POSTGRES_HOST'],
    port=os.environ['POSTGRES_PORT'],
    dbname=os.environ['POSTGRES_DB'],
    user=os.environ['POSTGRES_USER'],
    password=os.environ['POSTGRES_PASSWORD']
)
cur = conn.cursor()
cur.execute('SELECT COUNT(*) FROM companies')
print('Companies:', cur.fetchone()[0])
conn.close()
"

# Check Railway service status
railway status
```

---

## Related Documents

- [Deployment Checklist](./DEPLOYMENT_CHECKLIST.md) - Quick pre/post deploy checklist
- [Metrics and Monitoring](./METRICS_AND_MONITORING.md) - Metrics definitions and dashboards
- [Architecture](./ARCHITECTURE.md) - System architecture overview
</file>

<file path="docs/METRICS_AND_MONITORING.md">
# Metrics and Monitoring

Observability guide for the Ra'd AI TASI Platform. Covers key metrics, alerting thresholds, recommended tooling for Railway, and dashboard layout.

## Key Metrics

### HTTP Request Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `http_requests_total` | Total request count by method, path, status | `middleware/request_logging.py` |
| `http_request_duration_ms` | Response time per request (p50, p95, p99) | `middleware/request_logging.py` |
| `http_errors_total` | 4xx and 5xx error count by status code | `middleware/error_handler.py` |

### Vanna Chat Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `chat_requests_total` | Total chat/SSE queries | `/api/vanna/v2/chat_sse` |
| `chat_response_time_ms` | End-to-end chat latency (includes LLM + SQL) | SSE handler in `app.py` |
| `chat_sql_generation_errors` | Failed SQL generation attempts | Vanna agent logs |
| `tool_iterations_per_query` | Number of tool calls per chat query (max 10) | Agent config |

### Database Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `db_query_duration_ms` | SQL query execution time | `RunSqlTool` |
| `db_connections_active` | Current active PG connections | `database/pool.py` |
| `db_connections_idle` | Current idle pool connections | `database/pool.py` |
| `db_pool_exhausted_total` | Pool exhaustion events (all connections in use) | Pool error logs |

### External Service Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `yfinance_requests_total` | Total yfinance API calls | `services/tasi_index.py`, `services/stock_ohlcv.py` |
| `yfinance_errors_total` | Failed yfinance calls | Service error logs |
| `yfinance_available` | Boolean: yfinance reachable (1=yes, 0=no) | Health check endpoints |
| `circuit_breaker_state` | Circuit breaker state (closed/open/half-open) | Data pipeline services |
| `circuit_breaker_trips_total` | Number of times circuit breaker opened | Service logs |

### Cache Metrics (when Redis enabled)

| Metric | Description | Source |
|--------|-------------|--------|
| `cache_hits_total` | Successful cache reads | Cache middleware |
| `cache_misses_total` | Cache misses requiring origin fetch | Cache middleware |
| `cache_hit_rate` | Derived: hits / (hits + misses) | Computed |
| `cache_evictions_total` | Keys evicted by TTL or memory pressure | Redis INFO stats |

### News Scraper Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `news_scrape_runs_total` | Total scheduler runs | `services/news_scheduler.py` |
| `news_articles_fetched` | Articles fetched per run by source | `services/news_scraper.py` |
| `news_scrape_errors` | Scraping failures by source | Scraper error logs |
| `news_scrape_duration_ms` | Time per scraping cycle | Scheduler logs |

### Authentication Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `auth_login_attempts_total` | Login attempts (success/failure) | `api/routes/auth.py` |
| `auth_token_refresh_total` | Token refresh events | Auth middleware |
| `auth_invalid_tokens_total` | Rejected invalid/expired JWTs | Auth middleware |

## Alert Thresholds

### Critical (page immediately)

| Condition | Threshold | Action |
|-----------|-----------|--------|
| Health check failure | 3 consecutive failures | Investigate DB/LLM connectivity |
| Error rate (5xx) | > 5% of requests over 5 min | Check logs, rollback if needed |
| Response time p99 | > 30s sustained for 5 min | Check LLM latency, DB pool |
| DB pool exhausted | Any occurrence | Scale pool or investigate leaks |

### Warning (investigate within 1 hour)

| Condition | Threshold | Action |
|-----------|-----------|--------|
| Error rate (4xx) | > 20% of requests over 15 min | Check for client bugs or abuse |
| yfinance unavailable | > 10 min continuous | Circuit breaker should handle; verify |
| Cache hit rate drop | Below 50% sustained | Check Redis connectivity, TTL config |
| Response time p95 | > 10s sustained for 10 min | Profile slow queries |
| News scraper failures | 3+ consecutive failures per source | Check source availability |

### Informational (review daily)

| Condition | Threshold | Action |
|-----------|-----------|--------|
| Chat query volume | Track trend | Capacity planning |
| Auth failures spike | > 10 in 5 min | Potential credential stuffing |
| Memory usage | > 80% of container limit | Plan vertical scaling |

## Recommended Tools for Railway

### Built-in Railway Observability

Railway provides:
- **Deploy logs**: Real-time stdout/stderr streaming (JSON logs are auto-parsed)
- **Metrics dashboard**: CPU, memory, network I/O per service
- **Health checks**: Configure in `railway.toml` or dashboard

### Structured Log Aggregation

The platform outputs structured JSON logs in production (via `config/logging_config.py`). These are automatically ingested by Railway's log viewer. For advanced querying:

- **Railway Log Explorer**: Filter by JSON fields (`level`, `logger`, `message`)
- **External**: Forward logs to Datadog, Grafana Cloud, or Betterstack via Railway log drains

### Setting Up Log Drains (Railway)

```bash
# Via Railway CLI
railway logs --filter "level=ERROR"

# Configure a log drain in Railway dashboard:
# Settings > Log Drains > Add Drain
# Supported: HTTP, Datadog, Logtail/Betterstack
```

### Application Performance Monitoring (APM)

For deeper observability beyond logs:

1. **Sentry** (recommended for error tracking)
   - See `config/error_tracking.py` for integration stub
   - Free tier: 5K errors/month, performance tracing

2. **Grafana Cloud** (recommended for metrics dashboards)
   - Free tier: 10K metrics series, 50GB logs
   - Use OpenTelemetry SDK to export metrics

3. **Betterstack** (recommended for uptime monitoring)
   - Free tier: 5 monitors, 3-min intervals
   - Simple HTTP health check to `/api/v1/health`

## Dashboard Layout

Suggested dashboard panels for a Grafana or similar monitoring tool:

```
+---------------------------------------------------+
|              Ra'd AI TASI Platform                 |
+---------------------------------------------------+
| Row 1: Overview                                    |
| +------------+ +------------+ +------------------+ |
| | Requests/s | | Error Rate | | Avg Response (ms)| |
| |   (gauge)  | |  (gauge)   | |    (gauge)       | |
| +------------+ +------------+ +------------------+ |
+---------------------------------------------------+
| Row 2: Response Times                              |
| +------------------------------------------------+ |
| | HTTP Response Time (p50, p95, p99 over time)   | |
| |                 (time series)                   | |
| +------------------------------------------------+ |
+---------------------------------------------------+
| Row 3: Error Breakdown                             |
| +-----------------------+ +----------------------+ |
| | Errors by Status Code | | Errors by Endpoint   | |
| |    (stacked bar)      | |    (table)           | |
| +-----------------------+ +----------------------+ |
+---------------------------------------------------+
| Row 4: External Dependencies                       |
| +------------+ +------------+ +-----------------+  |
| | yfinance   | | Circuit    | | DB Pool         |  |
| | Status     | | Breaker    | | Utilization     |  |
| | (indicator)| | (state map)| | (gauge)         |  |
| +------------+ +------------+ +-----------------+  |
+---------------------------------------------------+
| Row 5: Chat / LLM                                  |
| +-----------------------+ +----------------------+ |
| | Chat Queries/min      | | LLM Response Time    | |
| |   (time series)       | |   (histogram)        | |
| +-----------------------+ +----------------------+ |
+---------------------------------------------------+
| Row 6: Cache & News                                |
| +-----------------------+ +----------------------+ |
| | Cache Hit Rate        | | News Articles/hour   | |
| |   (gauge + trend)     | |   (bar chart)        | |
| +-----------------------+ +----------------------+ |
+---------------------------------------------------+
```

## Implementation Notes

### Current State

The platform uses Python `logging` with structured JSON output in production. Metrics are currently derived from log analysis rather than explicit instrumentation. The middleware stack (`middleware/request_logging.py`) logs every request with method, path, status, and duration.

### Future Instrumentation

To add explicit metrics collection (e.g., Prometheus counters/histograms):

1. Add `prometheus-fastapi-instrumentator` to `requirements.txt`
2. Initialize in `app.py` lifespan:
   ```python
   from prometheus_fastapi_instrumentator import Instrumentator
   Instrumentator().instrument(app).expose(app, endpoint="/metrics")
   ```
3. Configure Railway to scrape `/metrics` or export via push gateway

### Log-Based Metric Extraction

Until explicit instrumentation is added, derive metrics from structured logs:

```bash
# Count errors in last hour (Railway CLI)
railway logs --since 1h | jq 'select(.level == "ERROR")' | wc -l

# Average response time from request logs
railway logs --since 1h | jq 'select(.logger == "tasi.access") | .message' | grep -oP '\d+\.\d+ms' | ...
```

## Related Documents

- [Deployment Checklist](./DEPLOYMENT_CHECKLIST.md) - Pre/post deploy steps
- [Deployment Runbook](./DEPLOYMENT_RUNBOOK.md) - Full deployment procedures
- [Architecture](./ARCHITECTURE.md) - System architecture overview
</file>

<file path="docs/MIGRATION_STRATEGY.md">
# Migration Strategy: SQLite to PostgreSQL

## Overview

This document describes the migration strategy for moving the Ra'd AI TASI platform from SQLite (development) to PostgreSQL (production). It covers schema mapping, data pipelines, rollback procedures, and operational considerations.

---

## 1. Schema Comparison

### 1.1 Shared Tables (10 tables)

These tables exist in both SQLite and PostgreSQL with identical column names. The PostgreSQL version has more columns (fuller data mapping from the source CSV) and stricter types.

| Table | SQLite PK | PG PK | Type Changes |
|---|---|---|---|
| `companies` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY` | None (TEXT stays TEXT) |
| `market_data` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)`, `INTEGER` -> `BIGINT` |
| `valuation_metrics` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)` |
| `profitability_metrics` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)` |
| `dividend_data` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)` |
| `financial_summary` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)` |
| `analyst_data` | `TEXT PRIMARY KEY` | `TEXT PRIMARY KEY REFERENCES companies(ticker)` | `REAL` -> `NUMERIC(20,4)`, `INTEGER` -> `BIGINT` |
| `balance_sheet` | `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` | `REAL` -> `NUMERIC(20,4)` |
| `income_statement` | `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` | `REAL` -> `NUMERIC(20,4)` |
| `cash_flow` | `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` | `REAL` -> `NUMERIC(20,4)` |

### 1.2 PostgreSQL-Only Tables (13 tables)

| Table | PK Type | Purpose | FK References |
|---|---|---|---|
| `sectors` | `SERIAL` | Reference table (Arabic/English sector names) | None |
| `entities` | `UUID` | Enhanced company info (Arabic names, CMA/Tadawul IDs) | `companies(ticker)`, `sectors(id)` |
| `filings` | `UUID` | Filing metadata (annual, quarterly, interim) | `companies(ticker)` |
| `xbrl_facts` | `UUID` | XBRL financial data with concept identification | `companies(ticker)`, `filings(id)` |
| `computed_metrics` | `SERIAL` | Derived ratios, growth rates | `companies(ticker)` |
| `price_history` | `SERIAL` | Daily OHLCV with moving averages | `companies(ticker)` |
| `announcements` | `UUID` | CMA/Tadawul announcements (Arabic) | `companies(ticker)` |
| `news_articles` | `UUID` | Multi-source news with sentiment, entities | `companies(ticker)` |
| `technical_reports` | `UUID` | Analyst research with recommendations | `companies(ticker)` |
| `users` | `UUID` | User accounts (auth provider, subscription) | None |
| `user_watchlists` | `UUID` | Per-user ticker watchlists | `users(id)` |
| `user_alerts` | `UUID` | Price/event alerts per user | `users(id)`, `companies(ticker)` |
| `query_audit_log` | `UUID` | Query logging for analytics | `users(id)` |

### 1.3 SQLite-Only Tables

| Table | Purpose | Notes |
|---|---|---|
| `news_feed` | Scraped Arabic news (5 sources) | Created by `services/news_store.py`, uses `INSERT OR IGNORE`, `PRAGMA journal_mode=WAL` |

The `news_feed` table exists only in SQLite and is managed by the news scraper pipeline. It does not have a PostgreSQL equivalent; the PG-only `news_articles` table serves a similar but structurally different purpose.

### 1.4 PostgreSQL Extensions

```sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";   -- UUID generation
CREATE EXTENSION IF NOT EXISTS "pg_trgm";     -- Trigram text search (Arabic names)
```

### 1.5 PostgreSQL Views

| View | Purpose |
|---|---|
| `v_latest_annual_metrics` | Joins companies with most recent annual financial statements (period_index=0) |
| `v_company_summary` | Comprehensive company overview: company + entity + market + valuation + profitability |

### 1.6 PostgreSQL Indexes (30+ indexes)

See `database/schema.sql` Section 6. Key index types:
- **B-tree**: Standard lookups (ticker, sector, dates)
- **GIN trigram**: Arabic text search (`name_ar`, `title_ar`, `body_ar`) using `gin_trgm_ops`
- **GIN JSONB**: Entity extraction search on `news_articles.entities_extracted`
- **Composite**: Multi-column for common query patterns (`ticker, period_type, period_date`)

---

## 2. Migration Pipelines

### 2.1 Pipeline A: SQLite -> PostgreSQL Migration

**Script**: `database/migrate_sqlite_to_pg.py`

**What it does**:
1. Applies `database/schema.sql` to create all PG tables (unless `--skip-schema`)
2. Reads all 10 shared tables from `saudi_stocks.db`
3. Maps types: `NaN` -> `NULL`, `Infinity` -> `NULL`
4. Excludes `id` column for SERIAL tables (PG auto-generates)
5. Batch inserts via `psycopg2.extras.execute_batch` (default batch size: 250)
6. Populates `sectors` reference table from unique sectors in `companies`
7. Populates `entities` table from `companies` (ticker, short_name, sector_id)

**Usage**:
```bash
# Dry run (preview, no writes)
python database/migrate_sqlite_to_pg.py --dry-run

# Full migration
python database/migrate_sqlite_to_pg.py \
    --pg-host localhost --pg-port 5432 \
    --pg-dbname tasi_platform --pg-user tasi_user --pg-password changeme

# Skip schema (tables already exist)
python database/migrate_sqlite_to_pg.py --skip-schema
```

**Environment variables**: `PG_HOST`, `PG_PORT`, `PG_DBNAME`, `PG_USER`, `PG_PASSWORD`

### 2.2 Pipeline B: CSV -> PostgreSQL Direct

**Script**: `database/csv_to_postgres.py`

**What it does**:
1. Reads `saudi_stocks_yahoo_data.csv` (500 rows, 1062 columns)
2. Applies schema, normalizes into 10 core tables + sectors + entities
3. Supports `--upsert` mode (updates existing, inserts new)
4. Same column mappings as `csv_to_sqlite.py`

**Usage**:
```bash
# Initial load
python database/csv_to_postgres.py

# Upsert mode (incremental update)
python database/csv_to_postgres.py --upsert

# Dry run
python database/csv_to_postgres.py --dry-run
```

### 2.3 Pipeline C: CSV -> SQLite

**Script**: `csv_to_sqlite.py`

**What it does**:
- Transforms the 1062-column flat CSV into 10 normalized SQLite tables
- Financial statements use `unpivot_financial()` to convert wide to tall format
- Period types: `annual`, `quarterly`, `ttm`
- Period index: 0 = most recent

---

## 3. Migration Procedures

### 3.1 Local Development: SQLite -> Docker PG

**Prerequisites**: Docker, Docker Compose, Python with psycopg2

```bash
# 1. Start PostgreSQL
export POSTGRES_PASSWORD=changeme
docker compose up -d postgres

# 2. Wait for readiness
docker compose exec postgres pg_isready -U tasi_user -d tasi_platform

# 3. Migrate data
python database/migrate_sqlite_to_pg.py \
    --pg-host localhost --pg-port 5432 \
    --pg-dbname tasi_platform --pg-user tasi_user --pg-password changeme

# 4. Switch backend
export DB_BACKEND=postgres
export POSTGRES_HOST=localhost
python app.py

# 5. Verify
curl http://localhost:8084/health
```

### 3.2 Docker Compose (Full Stack)

```bash
# 1. Configure .env
cp .env.example .env
# Edit .env: set DB_BACKEND=postgres, POSTGRES_PASSWORD, ANTHROPIC_API_KEY

# 2. Start all services
docker compose up -d

# The app container auto-connects to postgres container
# Schema is applied via docker-entrypoint-initdb.d/01-schema.sql
```

### 3.3 Railway Deployment (Production)

The `entrypoint.sh` script handles production initialization:

1. Maps `POSTGRES_*` env vars to `PG_*` vars
2. Checks if `companies` table exists in PG
3. If not: applies `database/schema.sql` via `psql`, loads CSV data via `csv_to_postgres.py`
4. Starts `uvicorn app:app --host 0.0.0.0 --port $PORT`

**Railway environment variables** (set in Railway dashboard):
```
DB_BACKEND=postgres
POSTGRES_HOST=postgres.railway.internal
POSTGRES_PORT=5432
POSTGRES_DB=raid_ai
POSTGRES_USER=raid
POSTGRES_PASSWORD=<strong-password>
GEMINI_API_KEY=<key>
AUTH_JWT_SECRET=<stable-secret>
```

### 3.4 Data Refresh (Re-import from CSV)

When the source CSV is updated with new stock data:

```bash
# Option A: SQLite then migrate
python csv_to_sqlite.py
python database/migrate_sqlite_to_pg.py

# Option B: Direct to PG with upsert
python database/csv_to_postgres.py --upsert
```

---

## 4. Rollback Strategy

### 4.1 SQLite Fallback

SQLite remains the default backend. Rollback is immediate:

```bash
# 1. Stop PG-backed app
# 2. Unset/change env vars
export DB_BACKEND=sqlite
unset POSTGRES_HOST

# 3. Restart
python app.py
```

The SQLite database (`saudi_stocks.db`) is always present and unmodified by PG migration.

### 4.2 PostgreSQL Data Reset

To reset the PostgreSQL database and re-migrate:

```bash
# Drop all tables (use with caution)
docker compose exec postgres psql -U tasi_user -d tasi_platform -c \
    "DROP SCHEMA public CASCADE; CREATE SCHEMA public;"

# Re-apply schema and data
python database/migrate_sqlite_to_pg.py
```

### 4.3 Docker Volume Reset

For a clean start:

```bash
docker compose down -v    # Removes volumes (deletes all PG data)
docker compose up -d      # Fresh start with schema init
```

---

## 5. SQL Compatibility Notes

### 5.1 Constructs That Differ Between Backends

| Pattern | SQLite | PostgreSQL |
|---|---|---|
| Parameter placeholder | `?` | `%s` |
| Named parameter | Not standard | `%(name)s` |
| Auto-increment PK | `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` |
| Upsert | `INSERT OR IGNORE` | `ON CONFLICT ... DO NOTHING` |
| Last change count | `SELECT changes()` | `RETURNING` clause |
| Table existence | `SELECT 1 FROM sqlite_master WHERE type='table' AND name=?` | `SELECT 1 FROM information_schema.tables WHERE table_schema='public' AND table_name=%s` |
| Recent datetime | `datetime('now', '-1 day')` | `NOW() - INTERVAL '1 day'` |
| Case-insensitive search | `LIKE` (default case-insensitive) | `ILIKE` |
| Schema inspection | `PRAGMA table_info(...)` | `information_schema.columns` |
| Boolean type | `INTEGER (0/1)` | `BOOLEAN` |
| Array type | Not supported | `TEXT[]` |
| JSON type | Not supported | `JSONB` |
| IP address type | `TEXT` | `INET` |
| Trigram search | Not supported | `pg_trgm` extension with GIN indexes |

### 5.2 Compatibility Layer

The `services/db_compat.py` module provides backend-aware helpers:

```python
from services.db_compat import (
    is_postgres,          # True when DB_BACKEND=postgres
    get_read_connection,  # Pool-based for PG, file-based for SQLite
    fetchall_compat,      # Dict-based results for either backend
    fetchone_compat,      # Single row dict for either backend
    scalar_compat,        # Single scalar value for either backend
    table_exists,         # information_schema vs sqlite_master
    datetime_recent,      # NOW() - INTERVAL vs datetime('now', '-N')
)
```

For route-level queries, `api/db_helper.py` provides automatic `?` -> `%s` conversion.

### 5.3 Backend-Specific Services

| Category | Services | Backend |
|---|---|---|
| PG-only | `news_service`, `reports_service`, `announcement_service`, `user_service`, `audit_service`, `auth_service` | PostgreSQL (psycopg2) |
| SQLite-only | `news_store`, `news_scraper`, `news_scheduler`, `news_paraphraser` | SQLite |
| Dual-backend | `health_service` (via `db_compat`), routes in `api/routes/` (via `db_helper`) | Both |
| No DB | `tasi_index`, `stock_ohlcv` | yfinance API |

---

## 6. Testing the Migration

### 6.1 Automated PG Tests

```bash
# Run all tests with PG backend (starts Docker PG, sets env vars, runs tests)
bash scripts/run_pg_tests.sh

# Run and tear down after
bash scripts/run_pg_tests.sh --down

# Run only PG-specific tests
bash scripts/run_pg_tests.sh --pg-only
```

### 6.2 Test Coverage

| Test File | SQLite Tests | PG Tests | Skip Condition |
|---|---|---|---|
| `test_database.py` | 23 (always run) | 23 (skipped without PG) | `POSTGRES_HOST` not set |
| `test_app_assembly_v2.py` | ~21 | 3 | `POSTGRES_HOST` not set |
| `tests/test_services.py` | ~20 (mock) | ~9 (live PG) | `POSTGRES_HOST` not set |
| `tests/test_api_routes.py` | ~15 (mock) | ~10 (live PG) | `POSTGRES_HOST` not set |
| `tests/test_connection_pool.py` | 8 (mocked) | 0 | N/A (fully mocked) |
| **Total** | ~87 | ~45 | |

### 6.3 Manual Verification Checklist

After migration, verify:

- [ ] `GET /health` returns `"status": "healthy"` with database check passing
- [ ] `GET /api/entities?limit=5` returns 5 companies
- [ ] `GET /api/entities/2222.SR` returns Saudi Aramco details
- [ ] `GET /api/news?limit=5` returns news articles
- [ ] `GET /api/reports?limit=5` returns reports
- [ ] `GET /api/announcements?limit=5` returns announcements
- [ ] Vanna chat query "What is the market cap of Aramco?" returns correct SQL and result
- [ ] Chart endpoints (`/api/charts/sector-market-cap`, `/api/charts/top-companies`) return data
- [ ] User registration and JWT login work
- [ ] Query audit log records new queries

---

## 7. Production Readiness Checklist

### 7.1 Before Migration

- [ ] PostgreSQL 16 provisioned (Railway, Docker, or managed)
- [ ] `POSTGRES_PASSWORD` is strong (not `changeme`)
- [ ] `AUTH_JWT_SECRET` is stable and persistent
- [ ] `PG_POOL_MIN` and `PG_POOL_MAX` sized for expected concurrency
- [ ] Backup strategy defined (pg_dump schedule or managed snapshots)
- [ ] `GEMINI_API_KEY` or `ANTHROPIC_API_KEY` configured

### 7.2 During Migration

- [ ] Run `database/migrate_sqlite_to_pg.py --dry-run` first
- [ ] Verify row counts match SQLite source
- [ ] Check sectors and entities tables populated
- [ ] Run `bash scripts/run_pg_tests.sh` to validate

### 7.3 After Migration

- [ ] Switch `DB_BACKEND=postgres` in production
- [ ] Verify health endpoint
- [ ] Monitor connection pool usage
- [ ] Set up pg_dump backups (daily recommended)
- [ ] Configure `log_min_duration_statement = 1000` for slow query logging
</file>

<file path="docs/ui-transition.md">
# Ra'd AI -- UI Transition Assessment

> Evaluates the Next.js frontend against the legacy `templates/index.html` and documents
> the path to full parity. Generated from source code on 2026-02-10.

---

## Table of Contents

1. [Overview](#overview)
2. [Legacy UI Capabilities](#legacy-ui-capabilities)
3. [Next.js Page Assessment](#nextjs-page-assessment)
4. [Feature Parity Matrix](#feature-parity-matrix)
5. [API Dependency Map](#api-dependency-map)
6. [Recommended Transition Strategy](#recommended-transition-strategy)
7. [Risk Areas](#risk-areas)

---

## Overview

The Ra'd AI platform currently has two frontends:

| Frontend | Path | Status | Serves at |
|---|---|---|---|
| **Legacy** | `templates/index.html` | Production | `GET /` on the FastAPI server |
| **Next.js** | `frontend/` | In progress | Separate dev server (port 3000) |

The legacy UI is a single-page chat interface served directly by the FastAPI backend. The Next.js app is a multi-page application with dedicated pages for market browsing, stock details, news, reports, and watchlists. The Next.js frontend communicates with the same backend via `/api/*` routes (proxied through Next.js rewrites or `NEXT_PUBLIC_API_URL`).

---

## Legacy UI Capabilities

The legacy `templates/index.html` provides:

1. **Native SSE Chat** -- Full AI chat interface with streaming responses via `POST /api/vanna/v2/chat_sse`. Renders markdown (via marked.js CDN), SQL blocks, data tables, and Plotly chart HTML.

2. **Suggestion Chips** -- 6 preset queries (top 10 by market cap, profitability heatmap, Aramco revenue trend, sector valuation, market cap vs P/E, dividend yields). Collapsed by default with "Show Suggestions" toggle. Keyboard-accessible with tabindex management.

3. **Plotly Chart Display** -- Charts generated server-side by `chart_engine/raid_chart_generator.py` and returned as HTML fragments in the SSE stream. Full dark theme with gold palette.

4. **Ra'd Branding** -- Dark background (#0E0E0E), gold accent (#D4A84B), Tajawal font, gold gradient logo, animated fade-in.

5. **Shadow DOM Overrides** -- CSS injection into `vanna-chat` shadow root for consistent branding. MutationObserver hides admin/setup diagnostic messages.

6. **Accessibility** -- ARIA roles (banner, main, contentinfo), skip-to-content link, focus-visible outlines, prefers-reduced-motion, prefers-contrast support, ARIA live region for screen reader announcements.

7. **Theme Toggle** -- Dark/light mode with localStorage persistence and prefers-color-scheme detection.

8. **Onboarding Overlay** -- 3-step first-visit tutorial.

9. **CDN Fallback** -- Progressive retry (3s/6s/10s) for external JS/CSS CDNs.

10. **Print Styles** -- Clean print output hiding interactive elements.

---

## Next.js Page Assessment

### / (Home Page)

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| Hero section with branding | Yes | Static |
| Market stats bar | Yes | Static + live sector count |
| Quick action cards (4) | Yes | Static links |
| Sector list with counts | Yes | `GET /api/entities/sectors` (live) |
| Top by Market Cap (5 stocks) | Yes | `GET /api/entities?limit=5` (live) |
| Mini sparklines per stock | Yes | `useOHLCVData` -> mock fallback |
| TradingView attribution | Yes | Static |
| AI Chat CTA | Yes | Static link |
| Loading/error states | Yes | Spinner + retry |

**Notes:** Functional with real data from PostgreSQL-backed APIs. Sparklines currently fall back to mock data since the OHLCV endpoint per ticker is not yet backed by a real data source.

---

### /market

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| TASI Index area chart | Yes | `GET /api/v1/charts/tasi/index` (live yfinance) |
| DataSourceBadge (LIVE/SAMPLE) | Yes | From API response |
| Search by ticker/name | Yes | `GET /api/entities?search=` (live) |
| Sector table with filter | Yes | `GET /api/entities/sectors` (live) |
| Company cards with sparklines | Yes | `GET /api/entities` (live) + mock sparklines |
| TradingView attribution | Yes | Static |
| AI Chat CTA | Yes | Static link |

**Notes:** The most complete page. Real TASI data from yfinance, sector/company data from PostgreSQL. Per-stock sparklines still fall back to mock OHLCV.

---

### /stock/[ticker]

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| Company header (name, sector, price) | Yes | `GET /api/entities/{ticker}` (live) |
| Price change calculation | Yes | Derived from current_price/previous_close |
| CandlestickChart (OHLCV) | Yes | `useOHLCVData(ticker)` -> mock fallback |
| ChartWrapper + DataSourceBadge | Yes | Shows SAMPLE badge (mock data) |
| Key Metrics grid (6 cards) | Yes | From entity detail API |
| Valuation metrics (4 cards) | Yes | From entity detail API |
| Profitability metrics (3 cards) | Yes | From entity detail API |
| Analyst consensus | Yes | Conditional on recommendation data |
| TradingView attribution | Yes | Static |
| Loading/error states | Yes | Full-page spinner + error |

**Notes:** Company data is live from PostgreSQL. The candlestick chart falls back to mock OHLCV data because there is no per-stock OHLCV backend endpoint yet -- only TASI index has a real data source. The `getOHLCVData(ticker)` API call targets `/api/v1/charts/{ticker}/ohlcv` which is not yet implemented.

---

### /chat

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| SSE streaming chat | Yes | `POST /api/vanna/v2/chat_sse` (live) |
| User/assistant message bubbles | Yes | Component-based |
| Markdown rendering | Yes | Via AssistantContent component |
| SQL block display | Yes | SQLBlock component |
| Data table display | Yes | DataTable component |
| Chart display | Yes | ChartBlock component |
| Suggestion chips (6) | Yes | Static, same queries as legacy |
| New chat / clear | Yes | clearMessages() |
| Stop streaming | Yes | stopStreaming() via AbortController |
| Auto-scroll to bottom | Yes | messagesEndRef |
| Textarea with Shift+Enter | Yes | Multi-line input |

**Notes:** Feature-complete SSE chat with parity to the legacy UI. The chat page is the largest page (~262KB). All Vanna SSE event types (text, sql, chart) are handled. The suggestion chips match the legacy UI exactly.

---

### /news

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| News article list | Yes | `GET /api/news?limit=50` (live) |
| Article title with link | Yes | source_url -> external link |
| Body preview (2-line clamp) | Yes | article.body |
| Metadata (source, date, sentiment, language) | Yes | All fields rendered |
| Ticker badge linking to stock page | Yes | `/stock/{ticker}` |
| Loading/error states | Yes | Spinner + retry |
| Pagination | No | Fixed limit=50, no page controls |
| Language filter | No | API supports it, UI does not expose |

**Notes:** Functional but limited. Displays articles from PostgreSQL. Missing pagination controls and language filter. The API supports both but the UI only fetches the first 50 articles.

---

### /reports

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| Report list (grid layout) | Yes | `GET /api/reports?limit=50` (live) |
| Type filter chips (all/technical/fundamental/sector/macro) | Yes | Client-side filter |
| Report type badge (colored) | Yes | Per-type colors |
| Recommendation badge | Yes | Gold pill |
| Title with link | Yes | source_url -> external link |
| Summary (3-line clamp) | Yes | report.summary |
| Target price vs current price | Yes | Conditional section |
| Ticker badge | Yes | `/stock/{ticker}` |
| Loading/error states | Yes | Spinner + retry |
| Pagination | No | Fixed limit=50 |

**Notes:** Well-implemented with type filtering. Missing pagination beyond the first 50 reports.

---

### /watchlist

**Status: Functional**

| Feature | Implemented | Data Source |
|---|---|---|
| Multiple watchlists (tabs) | Yes | API or localStorage fallback |
| Create new list | Yes | Form with name input |
| Delete list | Yes | Delete button (protects "default") |
| Add ticker to list | Yes | Text input + Enter/button |
| Remove ticker | Yes | X button per row |
| Live quotes per ticker | Yes | `GET /api/entities/{ticker}` per stock |
| Price and change % | Yes | Computed from current/previous close |
| Link to stock detail | Yes | Ticker links to `/stock/{ticker}` |
| Responsive table | Yes | Hidden columns on mobile |
| Default watchlists | Yes | 2 defaults with common tickers |
| API/localStorage fallback | Yes | Tries API first, falls back to localStorage |

**Notes:** The most sophisticated page in terms of state management. Gracefully degrades when the API is unavailable (SQLite mode). Default watchlists seed with TASI blue-chip tickers.

---

## Feature Parity Matrix

| Feature | Legacy | Next.js | Gap |
|---|---|---|---|
| AI Chat (SSE streaming) | Yes | Yes | None -- full parity |
| Suggestion chips | Yes | Yes | None |
| Plotly chart rendering | Yes (HTML fragments) | Yes (ChartBlock) | None |
| SQL result display | Yes | Yes (SQLBlock + DataTable) | None |
| Markdown rendering | Yes (marked.js CDN) | Yes (component-based) | None |
| Market overview | No | Yes | Next.js adds new capability |
| Stock detail page | No | Yes | Next.js adds new capability |
| News feed | No | Yes | Next.js adds new capability |
| Reports browser | No | Yes | Next.js adds new capability |
| Watchlists | No | Yes | Next.js adds new capability |
| TradingView charts | No | Yes | Next.js adds new capability |
| Theme toggle (dark/light) | Yes | Partial | Next.js has ThemeProvider but not all pages verified |
| Onboarding overlay | Yes | No | **Gap** -- legacy has 3-step tutorial |
| Shadow DOM branding | Yes | N/A | Not needed (no vanna-chat web component) |
| Print styles | Yes | No | **Gap** -- no print media rules in Next.js |
| Skip-to-content (a11y) | Yes | Unknown | **Potential gap** -- needs verification |
| ARIA roles | Yes (banner/main/contentinfo) | Partial | Layout components may need audit |
| CDN fallback | Yes | N/A | Next.js bundles dependencies locally |
| Keyboard shortcuts (Ctrl+K) | Yes | No | **Gap** |
| prefers-reduced-motion | Yes | Unknown | **Potential gap** |
| prefers-contrast | Yes | Unknown | **Potential gap** |

---

## API Dependency Map

Which APIs each Next.js page requires:

| Page | Required APIs | Works on SQLite? |
|---|---|---|
| `/` (home) | `/api/entities/sectors`, `/api/entities?limit=5` | No (PG-only APIs) |
| `/market` | `/api/v1/charts/tasi/index`, `/api/entities/sectors`, `/api/entities` | Partial (TASI index works) |
| `/stock/[ticker]` | `/api/entities/{ticker}`, `/api/v1/charts/{ticker}/ohlcv` | No (entity API is PG-only) |
| `/chat` | `/api/vanna/v2/chat_sse` | Yes (Vanna works on both) |
| `/news` | `/api/news` | No (PG-only) |
| `/reports` | `/api/reports` | No (PG-only) |
| `/watchlist` | `/api/watchlists`, `/api/entities/{ticker}` | Degraded (localStorage fallback, no quotes) |

**Note:** Only `/chat` and the TASI index chart work without PostgreSQL. All other pages require the PostgreSQL backend with populated data.

---

## Recommended Transition Strategy

### Phase 1: Fill Critical Gaps (before switching)

1. **Per-stock OHLCV endpoint** -- Implement `GET /api/v1/charts/{ticker}/ohlcv` backed by yfinance (similar to TASI index service). This unblocks real candlestick data on `/stock/[ticker]`.

2. **Pagination UI** -- Add page controls to `/news` and `/reports` pages. The APIs already support pagination.

3. **Onboarding overlay** -- Port the 3-step tutorial from the legacy UI or implement a simpler first-visit welcome.

4. **Accessibility audit** -- Verify skip-to-content, ARIA landmarks, focus management, and motion preferences in the Next.js layout.

### Phase 2: Soft Launch (parallel running)

5. **Proxy configuration** -- Configure Next.js rewrites to proxy `/api/*` requests to the FastAPI backend. This is partially set up.

6. **A/B routing** -- Serve the legacy UI at `/` and the Next.js app at `/app` or a subdomain. Let users opt in.

7. **Print styles** -- Add `@media print` rules to the Next.js global CSS.

8. **Keyboard shortcuts** -- Add Ctrl+K (or Cmd+K) to focus the chat input.

### Phase 3: Full Cutover

9. **Replace `/` route** -- Point the root route to the Next.js build output instead of `templates/index.html`.

10. **Retire legacy files** -- Remove `templates/index.html`, `templates/raid-enhancements.css`, `templates/raid-features.js` once all users have migrated.

11. **SQLite fallback** -- Consider adding SQLite-compatible entity/news/report queries so the Next.js UI works in local development without PostgreSQL.

---

## Risk Areas

### 1. Per-stock OHLCV data gap

The `/stock/[ticker]` page shows a candlestick chart that always displays mock data. The frontend calls `GET /api/v1/charts/{ticker}/ohlcv` but this endpoint does not exist. Until it is implemented, every stock detail page will show "SAMPLE" badge data. This is the **highest-priority gap**.

### 2. PostgreSQL dependency

All Next.js pages except `/chat` require PostgreSQL-backed APIs. In SQLite mode (local development), the home page, market page, stock detail, news, reports, and watchlist pages will fail to load data. The legacy UI works fine on SQLite because it only needs the Vanna chat endpoint.

### 3. Chat page bundle size

The `/chat` page is 262KB (the largest page). This includes the SSE streaming logic, markdown rendering, SQL/chart/table display components. Monitor this for performance on mobile connections.

### 4. Mock data user confusion

When OHLCV data falls back to mock, the "SAMPLE" badge may not be prominent enough. Users could mistake generated data for real market data. Consider making the mock state more visually distinct (e.g., watermark overlay, desaturated colors).

### 5. Authentication flow

The Next.js frontend stores JWT tokens in `localStorage` (via `api-client.ts` `authHeaders()`) and user IDs for watchlist scoping. The auth flow (register/login/refresh) is not yet wired to a UI login page in the Next.js app. Watchlist writes and content creation require authentication but there is no login UI.

### 6. Real-time data staleness

The TASI index chart has a 5-minute backend cache + 6-minute frontend refresh. During market hours, data can be up to 11 minutes stale. For a platform marketing "live" data, this lag should be documented or the refresh intervals reduced.

### 7. No announcements page

The API has full announcements CRUD (`/api/announcements`) but there is no dedicated `/announcements` page in the Next.js app. Announcements are not surfaced anywhere in the frontend.
</file>

<file path="frontend/.eslintrc.json">
{
  "extends": ["next/core-web-vitals", "next/typescript"]
}
</file>

<file path="frontend/.lighthouserc.js">
/** @type {import('@lhci/cli').LighthouseConfig} */
</file>

<file path="frontend/docs/BUNDLE_REPORT.md">
# Bundle Analysis Report

Generated: 2026-02-13
Next.js: 14.2.35 | Node: 20.x | Build: production

## Summary

| Metric | Value |
|--------|-------|
| Shared JS (all pages) | 87.7 kB |
| Largest page (first-load) | /stock/[ticker] at 125 kB |
| Smallest page (first-load) | /_not-found at 87.8 kB |
| All pages under 150 kB | Yes |

## Per-Page First-Load JS

| Route | Page JS | First Load JS | Status |
|-------|---------|---------------|--------|
| / | 6.34 kB | 124 kB | OK |
| /_not-found | 138 B | 87.8 kB | OK |
| /announcements | 3.64 kB | 99.4 kB | OK |
| /charts | 5.58 kB | 118 kB | OK |
| /chat | 8.87 kB | 105 kB | OK |
| /login | 3.53 kB | 108 kB | OK |
| /market | 6.08 kB | 123 kB | OK |
| /news | 7.83 kB | 112 kB | OK |
| /news/[id] | 7.23 kB | 112 kB | OK |
| /reports | 4.2 kB | 109 kB | OK |
| /stock/[ticker] | 7.3 kB | 125 kB | OK |
| /watchlist | 8.52 kB | 113 kB | OK |

## Shared Chunks

| Chunk | Size |
|-------|------|
| chunks/117-*.js (React, Next.js runtime) | 31.7 kB |
| chunks/fd9d1056-*.js (framework) | 53.6 kB |
| Other shared chunks | 2.33 kB |

## Largest Dependencies

| Package | Loaded On | Strategy |
|---------|-----------|----------|
| plotly.js-dist-min (~240 kB) | /chat (on demand) | dynamic import, ssr: false |
| react-syntax-highlighter (~80 kB) | /chat (on demand) | dynamic import via SQLBlock |
| react-markdown (~30 kB) | /chat (on demand) | dynamic import |
| lightweight-charts v4.2.3 (~45 kB) | /charts, /market | dynamic import, ssr: false |
| react-plotly.js | /chat (on demand) | dynamic import, ssr: false |
| swr | All pages (shared) | Shared chunk |

## lightweight-charts Chunking

- Version: 4.2.3 (pinned, do NOT upgrade)
- Import strategy: `dynamic(() => import(...), { ssr: false })`
- Used in: TASIIndexChart, StockOHLCVChart, StockComparisonChart
- Separate chunk, loaded only on /charts and /stock/[ticker] pages
- ResizeObserver for responsive width

## Optimization Applied

### Chat page: 362 kB -> 105 kB (71% reduction)

The /chat page previously bundled plotly.js, react-syntax-highlighter, and
react-markdown in its first-load JS. These were converted to dynamic imports
in `AssistantContent.tsx`:

- `SQLBlock` (react-syntax-highlighter): now lazy-loaded
- `ChartBlock` (react-plotly.js): now lazy-loaded with ssr: false
- `ReactMarkdown`: now lazy-loaded

These components load on demand when the AI assistant returns SQL, charts,
or markdown content, keeping the initial chat page load fast.

## How to Run Bundle Analysis

```bash
cd frontend
ANALYZE=true npm run build
```

This opens an interactive treemap in the browser showing exact chunk composition.
Requires `@next/bundle-analyzer` (configured in `next.config.mjs`).
</file>

<file path="frontend/docs/charts.md">
# Ra'd AI -- Frontend Charts Documentation

> Architecture, components, hooks, caching, and extension guide for the TradingView Lightweight Charts integration.
> Generated from source code on 2026-02-10.

---

## Table of Contents

1. [Overview](#overview)
2. [Component Hierarchy](#component-hierarchy)
3. [Component Reference](#component-reference)
4. [Data Types](#data-types)
5. [Hooks Contract](#hooks-contract)
6. [Caching Architecture](#caching-architecture)
7. [Data Flow](#data-flow)
8. [SSR Safety](#ssr-safety)
9. [Color Theme Reference](#color-theme-reference)
10. [How to Add a New Chart Type](#how-to-add-a-new-chart-type)
11. [TradingView Attribution](#tradingview-attribution)

---

## Overview

The charts system uses **TradingView Lightweight Charts v4.2.3** (Apache 2.0 license) for client-side financial charting. All chart components are client-only (no SSR) and loaded via `next/dynamic` with skeleton fallbacks. Data flows from the Python backend through SWR-cached hooks with automatic mock data fallback when the API is unavailable.

**Key files:**

| Path | Purpose |
|---|---|
| `src/components/charts/` | All chart components (15 files) |
| `src/components/charts/index.tsx` | Barrel with dynamic imports |
| `src/components/charts/chart-config.ts` | Color constants and chart options |
| `src/components/charts/chart-types.ts` | TypeScript interfaces |
| `src/lib/hooks/use-chart-data.ts` | Data fetching hooks |
| `src/lib/chart-cache.ts` | SWR cache configuration |
| `src/lib/chart-cache-provider.tsx` | SWR cache provider |
| `src/lib/chart-utils.ts` | Mock data generators and formatters |
| `src/lib/api-client.ts` | Backend API client |

---

## Component Hierarchy

```
ChartErrorBoundary                  (catches render errors, shows ChartError fallback)
  |
  +-- ChartWrapper                  (title bar + DataSourceBadge)
        |
        +-- CandlestickChart        (full OHLCV chart with volume, MA20/MA50, toolbar)
        |     |-- ChartSkeleton     (loading state)
        |     |-- ChartError        (error state with retry button)
        |     +-- ChartEmpty        (no data state)
        |
        +-- AreaChart               (filled area chart for index/trends)
        |     |-- ChartSkeleton
        |     |-- ChartError
        |     +-- ChartEmpty
        |
        +-- LineChart               (simple line chart)
        |     |-- ChartSkeleton
        |     |-- ChartError
        |     +-- ChartEmpty
        |
        +-- MiniSparkline           (tiny inline sparkline, no axes)

TradingViewAttribution              (required Apache 2.0 attribution link)
DataSourceBadge                     (LIVE/SAMPLE/CACHED pill)
```

**State flow for each chart:**

```
loading=true, data=null  -->  ChartSkeleton (animated placeholder)
error="message"          -->  ChartError    (error message + "Retry" button)
data=[], loading=false   -->  ChartEmpty    (friendly empty state)
data=[...], loading=false-->  Actual chart render
```

---

## Component Reference

### CandlestickChart

Full-featured OHLCV chart with:
- Candlestick series (green up / red down)
- Volume histogram (on secondary price scale)
- MA20 line (gold) and MA50 line (blue) -- toggleable
- Time range selector: 1W, 1M, 3M, 6M, 1Y, All
- Crosshair tooltip bar (OHLC + Volume)
- Responsive height: 250px (mobile), 300px (tablet), 400px (desktop), 500px (XL)
- RTL support (left price scale when `dir="rtl"`)
- ResizeObserver for container width tracking

**Props:**

| Prop | Type | Default | Description |
|---|---|---|---|
| `data` | `OHLCVData[]` | required | OHLCV data points |
| `height` | number | `400` | Base chart height (responsive override) |
| `showVolume` | boolean | `true` | Initial volume visibility |
| `showMA20` | boolean | `true` | Initial MA20 visibility |
| `showMA50` | boolean | `true` | Initial MA50 visibility |
| `title` | string | - | Chart title text |
| `ticker` | string | - | Ticker symbol (shown in toolbar) |
| `className` | string | - | Additional CSS classes |
| `loading` | boolean | `false` | Show skeleton when true |
| `error` | string or null | `null` | Show error state when set |
| `refetch` | function | - | Retry callback for error state |

**Used on:** `/stock/[ticker]` page.

---

### AreaChart

Filled area chart for index data and price trends.

**Props:** `data: LineDataPoint[]`, `height`, `title`, `className`, `loading`, `error`, `refetch`

**Used on:** `/market` page (TASI Index chart).

---

### LineChart

Simple line chart.

**Props:** Similar to AreaChart. Adds `lineWidth: 1 | 2 | 3 | 4`.

---

### MiniSparkline

Tiny inline chart (no axes, no tooltips, no grid). Fixed dimensions.

**Props:** `data: LineDataPoint[]`, `width: number` (default 120), `height: number` (default 40)

**Used on:** Home page (Top by Market Cap), Market page (company cards).

---

### ChartWrapper

Container that adds a title bar with `DataSourceBadge`.

**Props:** `title?: string`, `source: DataSource | null`, `children: ReactNode`, `className?: string`

---

### ChartErrorBoundary

React class component error boundary. Catches rendering errors in child charts and shows `ChartError` fallback with "Something went wrong" message and retry button.

**Props:** `children: ReactNode`, `fallbackHeight?: number`, `onError?: (error, errorInfo) => void`

---

### DataSourceBadge

Small pill showing data origin:

| Source | Label | Color | Description |
|---|---|---|---|
| `real` | LIVE | Green (#4CAF50) | Data from yfinance/API |
| `mock` | SAMPLE | Orange (#FFA726) | Fallback mock data (hidden a11y description) |
| `cached` | CACHED | Blue (#4A9FFF) | Stale cache data (hidden a11y description) |

Returns `null` when `source` is null (loading state).

---

### TradingViewAttribution

Required Apache 2.0 attribution link. Renders "Charts by TradingView" with hover gold effect and focus outline for keyboard navigation.

---

### ChartSkeleton

Animated loading placeholder. Accepts optional `height` prop.

### ChartError

Error display with message and optional retry button. Props: `message`, `onRetry`, `height`.

### ChartEmpty

Empty state with friendly message. Props: `message`, `height`.

---

## Data Types

Defined in `src/components/charts/chart-types.ts`:

```typescript
interface OHLCVData {
  time: string;        // "YYYY-MM-DD" format
  open: number;
  high: number;
  low: number;
  close: number;
  volume?: number;
}

interface LineDataPoint {
  time: string;        // "YYYY-MM-DD" format
  value: number;
}

interface AreaDataPoint {
  time: string;
  value: number;
}

type ChartTimeRange = '1W' | '1M' | '3M' | '6M' | '1Y' | 'ALL';

type DataSource = 'real' | 'mock' | 'cached';

// The universal return type for all chart hooks
interface ChartDataResult<T> {
  data: T | null;       // null while loading
  loading: boolean;     // true during initial fetch
  error: string | null; // error message or null
  source: DataSource | null;  // data origin
  refetch: () => void;  // trigger re-fetch
}
```

---

## Hooks Contract

All hooks are in `src/lib/hooks/use-chart-data.ts`. Each returns `ChartDataResult<T>`.

### useOHLCVData(ticker: string)

Returns `ChartDataResult<OHLCVData[]>`.

- Fetches from `GET /api/v1/charts/{ticker}/ohlcv`
- Falls back to `generateMockOHLCV(ticker, 365)` on error/404
- Cache key: `['ohlcv', ticker]`

### usePriceTrend(ticker: string, days?: number)

Returns `ChartDataResult<LineDataPoint[]>`.

- Derives from OHLCV data (close prices only)
- Filters to last N days
- Falls back to mock data
- Cache key: `['priceTrend', ticker, days]`

### useMarketIndex(period?: string)

Returns `ChartDataResult<LineDataPoint[]>`.

- Fetches from `GET /api/v1/charts/tasi/index?period=1y`
- Maps OHLCV close prices to `LineDataPoint[]`
- Preserves `source` from API response (`real`, `mock`, `cached`)
- Falls back to `generateMockPriceTrend(365)` on error
- Cache key: `['marketIndex', period]`

### useMiniChartData(ticker: string)

Returns `ChartDataResult<LineDataPoint[]>`.

- Last 30 days of close prices (optimized for sparklines)
- Falls back to mock data
- Cache key: `['miniChart', ticker]`

### Common behavior (all hooks)

1. Try API first
2. On success: return `{data, source: 'real'}`
3. On failure: log warning in development, return `{data: mockData, source: 'mock'}`
4. The `source` field tracks whether displayed data is real or fallback
5. `refetch()` triggers SWR revalidation

---

## Caching Architecture

### SWR Configuration

Defined in `src/lib/chart-cache.ts`:

```typescript
const chartCacheConfig = {
  revalidateOnFocus: false,       // No refetch on tab focus
  dedupingInterval: 60_000,       // 60s dedup window (no duplicate fetches)
  refreshInterval: 360_000,       // 6min auto-refresh
  errorRetryCount: 3,             // Retry 3 times on failure
  revalidateOnReconnect: true,    // Refetch when network reconnects
  keepPreviousData: true,         // Show stale data during revalidation
};
```

### Cache Key Factories

```typescript
const chartKeys = {
  ohlcv:       (ticker) => ['ohlcv', ticker],
  priceTrend:  (ticker, days) => ['priceTrend', ticker, days],
  marketIndex: (period) => ['marketIndex', period],
  miniChart:   (ticker) => ['miniChart', ticker],
};
```

### TTL Alignment

| Layer | TTL | Purpose |
|---|---|---|
| Backend (Python) | 300s (5min) | yfinance in-memory cache in `services/tasi_index.py` |
| Frontend (SWR) | 360s (6min) | `refreshInterval` in chart-cache.ts |

The frontend refresh interval (360s) is intentionally 60s longer than the backend cache TTL (300s). This avoids a thundering-herd pattern where the frontend always hits the backend right as its cache expires. In practice, most frontend refreshes will hit the backend's fresh cache.

### `useChartCache<T>` wrapper

Generic SWR hook that merges the chart-specific defaults:

```typescript
function useChartCache<T>(
  key: readonly unknown[] | null,   // chartKeys.* result
  fetcher: () => Promise<T>,        // async data fetcher
  config?: SWRConfiguration,        // optional overrides
): ChartCacheResult<T>
```

Returns `{ data, error, isLoading, isValidating, mutate }`.

---

## Data Flow

```
Page Component
  |
  +-- useOHLCVData(ticker) / useMarketIndex(period) / ...
        |
        +-- useChartCache(key, fetcher)
              |
              +-- SWR(key, fetcher, chartCacheConfig)
                    |
                    +-- fetcher()
                          |
                          +-- api-client.ts: getOHLCVData(ticker) / getTasiIndex(period)
                                |
                                +-- fetch(`/api/v1/charts/...`)
                                      |
                                      +-- Backend (Python FastAPI)
                                            |
                                            +-- yfinance / cache / mock fallback
```

**On success:** `{ data: apiData, source: 'real' }` -> cached in SWR
**On API failure:** `{ data: mockData, source: 'mock' }` -> cached in SWR with console.warn (dev only)
**On SWR revalidation:** old data shown (`keepPreviousData: true`) while refetching

---

## SSR Safety

TradingView Lightweight Charts requires browser APIs (`window`, `document`, `ResizeObserver`). All chart components are dynamically imported with SSR disabled in the barrel file (`index.tsx`):

```typescript
// src/components/charts/index.tsx
import dynamic from 'next/dynamic';
import { ChartSkeleton } from './ChartSkeleton';

export const CandlestickChart = dynamic(() => import('./CandlestickChart'), {
  ssr: false,
  loading: () => <ChartSkeleton />,
});

export const AreaChart = dynamic(() => import('./AreaChart'), {
  ssr: false,
  loading: () => <ChartSkeleton />,
});

// ... same pattern for LineChart, MiniSparkline
```

**Why:** `createChart()` from lightweight-charts accesses `document.createElement` internally. Without `ssr: false`, Next.js SSR would crash with `ReferenceError: document is not defined`.

**How it works:**
1. During SSR, `CandlestickChart` renders `<ChartSkeleton />` (a pure HTML placeholder)
2. After hydration on the client, the actual chart component loads and replaces the skeleton
3. The skeleton provides the same height to prevent layout shift

**Important:** Always import chart components from the barrel (`@/components/charts`) and not directly from their files. The barrel handles the `dynamic()` wrapping.

---

## Color Theme Reference

All chart colors are defined in `src/components/charts/chart-config.ts`:

### Base Theme

| Constant | Value | Usage |
|---|---|---|
| `RAID_CHART_OPTIONS.layout.background` | `#1A1A1A` | Chart background |
| `RAID_CHART_OPTIONS.layout.textColor` | `#B0B0B0` | Axis labels, text |
| `RAID_CHART_OPTIONS.grid.vertLines.color` | `rgba(212, 168, 75, 0.08)` | Vertical grid |
| `RAID_CHART_OPTIONS.grid.horzLines.color` | `rgba(212, 168, 75, 0.08)` | Horizontal grid |
| `RAID_CHART_OPTIONS.crosshair` | `rgba(212, 168, 75, 0.3)` | Crosshair lines |
| `RAID_CHART_OPTIONS.crosshair.labelBackgroundColor` | `#D4A84B` | Crosshair label bg |
| `RAID_CHART_OPTIONS.timeScale.borderColor` | `rgba(212, 168, 75, 0.15)` | Time axis border |

### Candlestick Colors

| Constant | Value | Usage |
|---|---|---|
| `CANDLE_COLORS.upColor` | `#4CAF50` | Green candle body |
| `CANDLE_COLORS.downColor` | `#FF6B6B` | Red candle body |
| `VOLUME_UP_COLOR` | `rgba(76, 175, 80, 0.3)` | Green volume bar |
| `VOLUME_DOWN_COLOR` | `rgba(255, 107, 107, 0.3)` | Red volume bar |

### Overlay Colors

| Constant | Value | Usage |
|---|---|---|
| `MA20_COLOR` | `#D4A84B` | 20-day moving average (gold) |
| `MA50_COLOR` | `#4A9FFF` | 50-day moving average (blue) |
| `AREA_TOP_COLOR` | `rgba(212, 168, 75, 0.4)` | Area chart gradient top |
| `AREA_BOTTOM_COLOR` | `rgba(212, 168, 75, 0.0)` | Area chart gradient bottom |
| `LINE_COLOR` | `#D4A84B` | Line chart default color |

### Brand Color

The primary gold color used throughout: **`#D4A84B`**

---

## How to Add a New Chart Type

Follow these steps to add a new chart component (e.g., a histogram chart):

### Step 1: Create the component

Create `src/components/charts/HistogramChart.tsx`:

```typescript
'use client';

import { useEffect, useRef } from 'react';
import { createChart, type IChartApi } from 'lightweight-charts';
import { RAID_CHART_OPTIONS } from './chart-config';
import type { ChartContainerProps } from './chart-types';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';

interface HistogramChartProps extends ChartContainerProps {
  data: { time: string; value: number }[];
  loading?: boolean;
  error?: string | null;
  refetch?: () => void;
}

export function HistogramChart({ data, height = 300, loading, error, refetch }: HistogramChartProps) {
  // Handle loading/error/empty states FIRST
  if (loading) return <ChartSkeleton height={height} />;
  if (error) return <ChartError height={height} message={error} onRetry={refetch} />;
  if (!data || data.length === 0) return <ChartEmpty height={height} />;

  // Chart creation logic with useRef + useEffect...
}

export default HistogramChart; // Required for dynamic() import
```

### Step 2: Add to the barrel with SSR-safe dynamic import

In `src/components/charts/index.tsx`:

```typescript
export const HistogramChart = dynamic(() => import('./HistogramChart'), {
  ssr: false,
  loading: () => <ChartSkeleton />,
});
```

### Step 3: Add data types (if needed)

In `src/components/charts/chart-types.ts`:

```typescript
export interface HistogramDataPoint {
  time: string;
  value: number;
  color?: string;
}
```

### Step 4: Add a data hook

In `src/lib/hooks/use-chart-data.ts`:

```typescript
export function useHistogramData(ticker: string): ChartDataResult<HistogramDataPoint[]> {
  const fetcher = useCallback(async (): Promise<SourcedData<HistogramDataPoint[]>> => {
    try {
      const data = await getHistogramData(ticker);
      if (data && data.length > 0) return { data, source: 'real' };
    } catch (err) {
      warnMockFallback('useHistogramData', { ticker, reason: (err as Error).message });
    }
    return { data: generateMockHistogram(ticker), source: 'mock' };
  }, [ticker]);

  const { data, error, isLoading, mutate } = useChartCache(
    chartKeys.histogram(ticker),
    fetcher,
  );

  return toChartResult(data, isLoading, error, () => mutate());
}
```

### Step 5: Add cache key

In `src/lib/chart-cache.ts`:

```typescript
export const chartKeys = {
  // ...existing keys
  histogram: (ticker: string) => ['histogram', ticker] as const,
};
```

### Step 6: Add API client function (if new endpoint)

In `src/lib/api-client.ts`:

```typescript
export function getHistogramData(ticker: string): Promise<HistogramDataPoint[]> {
  return request(`/api/v1/charts/${encodeURIComponent(ticker)}/histogram`);
}
```

### Step 7: Use on a page

```tsx
import { HistogramChart, ChartWrapper, ChartErrorBoundary } from '@/components/charts';
import { useHistogramData } from '@/lib/hooks/use-chart-data';

function MyPage() {
  const { data, loading, error, source, refetch } = useHistogramData('2222.SR');
  return (
    <ChartErrorBoundary>
      <ChartWrapper title="Volume Distribution" source={source}>
        <HistogramChart data={data || []} loading={loading} error={error} refetch={refetch} />
      </ChartWrapper>
    </ChartErrorBoundary>
  );
}
```

### Checklist for new chart types:

- [ ] Component file with loading/error/empty states handled
- [ ] `export default` at bottom for `dynamic()` import
- [ ] Added to `index.tsx` barrel with `ssr: false`
- [ ] Data types defined in `chart-types.ts`
- [ ] Hook in `use-chart-data.ts` with mock fallback
- [ ] Cache key in `chart-cache.ts`
- [ ] API client function in `api-client.ts` (if new endpoint)
- [ ] Wrapped in `ChartErrorBoundary` + `ChartWrapper` on page
- [ ] `TradingViewAttribution` visible on the page

---

## TradingView Attribution

TradingView Lightweight Charts is licensed under **Apache License 2.0**. The license requires visible attribution when used in production. The `TradingViewAttribution` component renders:

> Charts by TradingView

This component MUST appear on every page that displays a chart. It is currently included on:

- `/` (home page) -- below the sparklines
- `/market` -- below the TASI index chart
- `/stock/[ticker]` -- below the candlestick chart

If you add charts to a new page, include `<TradingViewAttribution />` somewhere visible on that page.
</file>

<file path="frontend/docs/MOBILE_AUDIT.md">
# Mobile Viewport & Responsive Audit

Generated: 2026-02-13
Viewports tested: 375px (iPhone SE), 390px (iPhone 14), 768px (iPad)

## Layout System

| Component | Mobile Behavior | Status |
|-----------|----------------|--------|
| AppShell | `pb-16 lg:pb-0` for bottom nav spacing | OK |
| Header | `sticky top-0 z-50`, `px-4 sm:px-6` | OK |
| Sidebar | Hidden on mobile, toggle via hamburger | OK |
| MobileBottomNav | `lg:hidden`, safe-area-inset-bottom | OK |
| Footer | Hidden behind bottom nav on mobile | OK |
| Main content | `min-w-0` prevents flex overflow | OK |

## Navigation

| Feature | Status | Notes |
|---------|--------|-------|
| Bottom nav (5 items) | OK | Home, Market, Ra'd (center), News, Charts |
| Virtual keyboard detection | OK | Hides bottom nav when keyboard open |
| Sidebar overlay on mobile | OK | Backdrop + slide-in |
| Command palette (Ctrl+K) | OK | Works on mobile with touch |

## Chart Components

### TASIIndexChart
| Aspect | Status | Notes |
|--------|--------|-------|
| Responsive height | OK | 280px mobile, 350px tablet, prop height desktop |
| Width resize | OK | ResizeObserver auto-resizes chart width |
| Toolbar wrap | OK | `flex-wrap gap-2` prevents overflow |
| Export buttons | OK | `hidden sm:block` -- hidden on mobile |
| Period selector | OK | Scrollable on small screens |
| Touch interaction | OK | lightweight-charts has built-in touch pinch/pan |

### StockOHLCVChart
| Aspect | Status | Notes |
|--------|--------|-------|
| Responsive height | OK | Same pattern as TASIIndexChart |
| Toolbar layout | OK | `flex-wrap` handles small screens |
| Touch pan/zoom | OK | Built-in to lightweight-charts |

### StockComparisonChart
| Aspect | Status | Notes |
|--------|--------|-------|
| Height | OK | Uses prop height |
| Width resize | OK | ResizeObserver |
| Legend | OK | Wraps below chart area |

### Plotly Charts (Chat)
| Aspect | Status | Notes |
|--------|--------|-------|
| Responsive | OK | `useResizeHandler` + `responsive: true` config |
| Touch zoom | OK | Built-in to Plotly.js |

## Page-Specific Audit

### / (Home)
- Quick action cards: `grid-cols-1 sm:grid-cols-2 lg:grid-cols-4` -- OK
- Sector/mover cards: `grid-cols-1 lg:grid-cols-2` -- OK
- Sparklines: Fixed 60x28px, no overflow -- OK

### /market
- Search + sector dropdown: `flex-col sm:flex-row` stacking -- OK
- Sector chips: `flex-wrap` -- OK
- Company table: `overflow-x-auto` wrapper -- OK
- Pagination: Centered with flex gap -- OK

### /charts
- Tab bar: Fixed width buttons in flex container -- OK
- Search results dropdown: `z-20`, full-width -- OK
- Quick pick chips: `flex-wrap` -- OK
- Fullscreen mode: `fixed inset-0` -- OK

### /chat
- Chat messages: Full-width with max-w -- OK
- Input bar: `max-w-3xl mx-auto` centered -- OK
- SQL blocks: Horizontally scrollable with `wrapLongLines` -- OK
- Data tables: `overflow-x-auto max-h-[400px]` -- OK

### /news
- Article grid: `grid-cols-1 md:grid-cols-2 xl:grid-cols-3` -- OK
- Source filter chips: `flex-wrap` -- OK
- Sticky search: Full-width with backdrop blur -- OK
- Infinite scroll sentinel: Works on mobile -- OK

### /announcements
- Cards: Full-width, single column -- OK
- Badges: `flex-wrap` -- OK

### /reports
- Grid: `grid-cols-1 sm:grid-cols-2` -- OK
- Search: Full-width -- OK
- Type filters: `flex-wrap` -- OK

### /watchlist
- Table columns: `hidden sm:table-cell` for name/sector -- OK
- Add ticker form: Full-width flex -- OK

### /stock/[ticker]
- Tab navigation: `overflow-x-auto` -- OK
- Financial tables: `overflow-x-auto` -- OK
- Chart: Responsive height via prop -- OK

### /login
- Form: `max-w-md` centered, full-width inputs -- OK
- Mode toggle: Full-width flex -- OK

## Touch Interactions

| Interaction | Component | Status |
|------------|-----------|--------|
| Chart pan/scroll | lightweight-charts | Built-in touch support |
| Chart pinch-zoom | lightweight-charts | Built-in |
| Plotly chart interactions | react-plotly.js | Built-in touch |
| Bottom nav tap targets | MobileBottomNav | 22px icons, h-full tap area |
| Dropdown selection | All search dropdowns | 200ms blur delay for touch |

## Identified Non-Issues

- **Chart export buttons** hidden on mobile (`hidden sm:block`) is intentional --
  prevents accidental taps and saves space. Users can still screenshot.
- **Market table** requires horizontal scroll on 375px -- this is expected and
  handled with `overflow-x-auto`.
- **Stock detail tab bar** scrolls horizontally -- correct behavior for many tabs.

## Recommendations (Low Priority)

1. Consider adding `touch-action: pan-x pan-y` to chart containers if any
   scroll interference is reported.
2. The news sticky search bar uses `bg-[#0E0E0E]/95 backdrop-blur-sm` which
   may have minor performance impact on older mobile devices. Monitor.
</file>

<file path="frontend/docs/tradingview-integration.md">
# TradingView Charts Integration

## Overview

Ra'd AI now integrates the **TradingView Advanced Chart Widget** (free tier) to provide professional-grade candlestick charts for all TASI-listed stocks. Users can search for any TASI stock by symbol or company name and view interactive daily charts with full technical analysis capabilities.

## Implementation

### Components

#### 1. **TradingViewWidget** ([src/components/charts/TradingViewWidget.tsx](../src/components/charts/TradingViewWidget.tsx))

The main widget component that embeds TradingView's Advanced Chart.

**Features:**
- Dynamic symbol switching
- Configurable intervals (1min to monthly)
- Dark/light theme support
- Candlestick style by default
- Interactive toolbar with drawing tools, indicators, and studies
- Image export capability
- Responsive sizing

**Props:**
```typescript
interface TradingViewWidgetProps {
  symbol: string;              // TradingView format: "TADAWUL:2222"
  interval?: string;           // "1" | "3" | "5" | "15" | "30" | "60" | "D" | "W" | "M"
  theme?: 'light' | 'dark';   // Chart theme
  height?: number;             // Chart height in pixels
  allowSymbolChange?: boolean; // Enable symbol search
  hideTopToolbar?: boolean;    // Hide drawing tools
  hideSideToolbar?: boolean;   // Hide watchlist/indicators
  hideVolume?: boolean;        // Hide volume bars
  enableSaveImage?: boolean;   // Enable chart export
  className?: string;          // Custom CSS classes
}
```

**Usage Example:**
```tsx
import TradingViewWidget from '@/components/charts/TradingViewWidget';
import { formatTASISymbol } from '@/lib/tradingview-utils';

<TradingViewWidget
  symbol={formatTASISymbol("2222")} // "TADAWUL:2222"
  interval="D"
  theme="dark"
  height={600}
  allowSymbolChange={false}
/>
```

#### 2. **TradingView Utilities** ([src/lib/tradingview-utils.ts](../src/lib/tradingview-utils.ts))

Helper functions for working with TASI stock symbols.

**Functions:**

- `formatTASISymbol(ticker: string): string`
  - Converts TASI ticker to TradingView format
  - Example: `"2222"` → `"TADAWUL:2222"`

- `extractTicker(symbol: string): string`
  - Extracts plain ticker from TradingView format
  - Example: `"TADAWUL:2222"` → `"2222"`

- `isValidTASITicker(ticker: string): boolean`
  - Validates TASI ticker format (4-digit number)
  - Example: `"2222"` → `true`, `"ABC"` → `false`

- `getTASIStockName(ticker: string): string`
  - Returns display name for popular TASI stocks
  - Example: `"2222"` → `"Saudi Aramco"`

### Pages

#### **Charts Page** ([src/app/charts/page.tsx](../src/app/charts/page.tsx))

The main charting interface accessible at `/charts`.

**Features:**
1. **Search Bar**
   - Real-time search for stocks by ticker or company name
   - Dropdown with matching results
   - Minimum 2 characters to trigger search

2. **Quick Pick Chips**
   - 10 popular TASI stocks (Aramco, Al Rajhi, SABIC, etc.)
   - One-click selection
   - Visual feedback for selected stock

3. **Default View**
   - Shows TASI Index chart when no stock selected
   - Full TradingView functionality

4. **Stock View**
   - Selected stock's candlestick chart
   - Stock header with current price and change
   - Sector information
   - Link to full stock detail page

5. **AI Chat CTA**
   - Link to Ra'd AI chat for deeper analysis

## Symbol Format

TradingView uses specific symbol formats for each exchange:

| Exchange | Format | Example |
|----------|--------|---------|
| TADAWUL (Saudi) | `TADAWUL:XXXX` | `TADAWUL:2222` (Aramco) |
| TASI Index | `TASI` | `TASI` (Tadawul All Share Index) |

**Note:** The widget automatically handles the symbol format conversion using `formatTASISymbol()`.

## Configuration

### Widget Settings

The current implementation uses these settings:

```javascript
{
  autosize: true,              // Responsive width
  symbol: "TADAWUL:2222",     // Dynamic based on selection
  interval: "D",               // Daily candlesticks
  timezone: "Asia/Riyadh",    // Saudi timezone
  theme: "dark",               // Matches Ra'd AI design
  style: "1",                  // Candlestick style
  locale: "en",                // English interface
  allow_symbol_change: false, // Disabled in stock view
  calendar: false,             // Hide earnings calendar
  hide_top_toolbar: false,    // Show drawing tools
  hide_side_toolbar: false,   // Show indicators
  hide_volume: false,          // Show volume bars
  save_image: true            // Enable chart export
}
```

### Customization

To customize the widget appearance or behavior, modify the config in [TradingViewWidget.tsx](../src/components/charts/TradingViewWidget.tsx):

```tsx
const config = {
  // ... modify settings here
};
```

Available customization options:
- Chart type (candlestick, line, area, bars, etc.)
- Technical indicators (RSI, MACD, Bollinger Bands, etc.)
- Drawing tools (trendlines, fibonacci, etc.)
- Time intervals (1min, 5min, 1hr, daily, weekly, monthly)
- Color schemes and styles

See [TradingView Widget Documentation](https://www.tradingview.com/widget-docs/widgets/charts/advanced-chart/) for full options.

## Testing

### Unit Tests

**TradingView Utilities** ([src/lib/__tests__/tradingview-utils.test.ts](../src/lib/__tests__/tradingview-utils.test.ts))

9 tests covering:
- Symbol formatting (plain ticker → TADAWUL format)
- Ticker extraction (TADAWUL format → plain ticker)
- Ticker validation (4-digit format check)
- Stock name lookup (ticker → display name)

```bash
cd frontend
npm test tradingview-utils
```

### Manual Testing

1. **Navigate to Charts page**
   - Click "Charts" tab in sidebar
   - Should load TASI Index by default

2. **Search for stock**
   - Type "Aramco" or "2222" in search bar
   - Select from dropdown
   - Chart should update to Aramco candlestick

3. **Quick pick selection**
   - Click any quick pick chip (e.g., "Al Rajhi")
   - Chart should switch immediately
   - Chip should highlight

4. **Verify chart functionality**
   - Chart should be interactive (zoom, pan)
   - Drawing tools should be accessible
   - Volume bars should display below price
   - Indicators menu should work

5. **Test responsiveness**
   - Resize browser window
   - Chart should resize automatically
   - All controls should remain accessible

## Navigation

The Charts page is accessible through:
1. **Sidebar** - "Charts" tab
2. **Direct URL** - `/charts`
3. **Stock detail page** - "View full details" link

## Performance

- **Bundle Size**: 4.13 kB (page), 109 kB (First Load JS)
- **Loading**: Skeleton loader shown during script load
- **Rendering**: Client-side only (dynamic import with SSR disabled)
- **Caching**: TradingView handles data caching internally

## Limitations

### Free Tier Restrictions

The TradingView Advanced Chart Widget (free tier) has these limitations:

1. **No commercial use** - Only for personal/non-commercial projects
2. **Branding required** - "Powered by TradingView" attribution mandatory
3. **Data delays** - May have 15-minute delayed data for some symbols
4. **No custom datafeed** - Uses TradingView's data exclusively
5. **No whitelabeling** - TradingView branding cannot be removed

### TASI Symbol Coverage

- TradingView may not have data for all 500+ TASI stocks
- Newly listed stocks may have delayed addition
- If a symbol is not found, widget will show "Symbol not found" message

### Alternatives Considered

For production/commercial use, consider:

1. **TradingView Charting Library** (Paid)
   - Self-hosted with custom datafeed
   - No branding requirements
   - Full customization
   - Requires license (contact TradingView)

2. **Lightweight Charts** (Free, Open Source)
   - Already integrated in Ra'd AI
   - Used for sparklines and mini charts
   - Limited compared to TradingView
   - Custom data from backend required

## Troubleshooting

### Chart not loading

1. Check browser console for script loading errors
2. Verify symbol format is correct: `TADAWUL:XXXX`
3. Ensure internet connection (widget loads from TradingView CDN)
4. Try different stock symbol to rule out data availability

### Symbol not found

1. Verify ticker exists on TADAWUL
2. Check symbol format: `formatTASISymbol("2222")` → `"TADAWUL:2222"`
3. Try searching on [TradingView website](https://www.tradingview.com/) directly
4. Some stocks may not have coverage on TradingView

### Chart appears blank

1. Check height prop is set: `height={600}`
2. Verify container has dimensions
3. Check theme prop matches your design system
4. Clear browser cache and reload

## References

- [TradingView Advanced Chart Widget Documentation](https://www.tradingview.com/widget-docs/widgets/charts/advanced-chart/)
- [TradingView Widget Constructor](https://www.tradingview.com/charting-library-docs/latest/core_concepts/Widget-Constructor/)
- [TradingView Free Charting Libraries](https://www.tradingview.com/free-charting-libraries/)
- [Next.js Dynamic Imports](https://nextjs.org/docs/advanced-features/dynamic-import)

## Future Enhancements

Potential improvements for future iterations:

1. **Multiple timeframes** - Quick switcher for 1D, 1W, 1M, 3M, 1Y
2. **Comparison mode** - Compare multiple stocks on same chart
3. **Watchlist integration** - Show all watchlist stocks in side panel
4. **Custom indicators** - Pre-configured technical analysis setups
5. **Chart templates** - Save and load user chart preferences
6. **Mobile optimization** - Simplified interface for mobile devices
7. **Fullscreen mode** - Dedicated fullscreen chart view
8. **Share charts** - Generate shareable chart links or images

---

**Last Updated**: 2026-02-10
**Version**: 1.0.0
**Maintainer**: Ra'd AI Development Team
</file>

<file path="frontend/e2e/.auth/.gitkeep">

</file>

<file path="frontend/e2e/.gitignore">
# Playwright generated files
.auth/*-state.json
test-results/
playwright-report/
</file>

<file path="frontend/e2e/global-setup.ts">
/**
 * Playwright global setup.
 *
 * Verifies the dev server is reachable and prepares shared test state
 * (e.g. storage state files for authenticated sessions).
 */
⋮----
import { chromium, type FullConfig } from '@playwright/test';
⋮----
/** Simple JWT-like token for E2E test sessions (not cryptographically valid). */
function fakeJwt(payload: Record<string, unknown>): string
⋮----
export default async function globalSetup(_config: FullConfig)
⋮----
// -----------------------------------------------------------------------
// 1. Verify dev server is reachable (skip in CI -- webServer handles it)
// -----------------------------------------------------------------------
⋮----
// -----------------------------------------------------------------------
// 2. Create storage-state files for each test role so specs can reuse them
// -----------------------------------------------------------------------
⋮----
// Navigate to a page so localStorage is on the correct origin
</file>

<file path="frontend/e2e/load-tests/locust-frontend.py">
"""
Locust load test for the Ra'd AI frontend.

Usage:
    locust -f locust-frontend.py --host=http://localhost:3000

Configure via environment or Locust UI:
    - Users: 50-200 concurrent
    - Spawn rate: 10 users/second
    - Run time: 5-10 minutes for meaningful results
"""
⋮----
class FrontendUser(HttpUser)
⋮----
"""Simulates a typical user browsing the Ra'd AI frontend."""
⋮----
wait_time = between(1, 5)
⋮----
def on_start(self)
⋮----
"""Called when a simulated user starts. Fetches the home page first."""
⋮----
@task(5)
@tag("navigation")
    def visit_home(self)
⋮----
"""Visit the home page (most common action)."""
⋮----
@task(4)
@tag("navigation")
    def visit_chat(self)
⋮----
"""Visit the AI chat page."""
⋮----
@task(3)
@tag("navigation")
    def visit_market(self)
⋮----
"""Visit the market data page."""
⋮----
@task(3)
@tag("navigation")
    def visit_charts(self)
⋮----
"""Visit the charts page."""
⋮----
@task(2)
@tag("navigation")
    def visit_news(self)
⋮----
"""Visit the news page."""
⋮----
@task(1)
@tag("navigation")
    def visit_reports(self)
⋮----
"""Visit the reports page."""
⋮----
@task(1)
@tag("navigation")
    def visit_announcements(self)
⋮----
"""Visit the announcements page."""
⋮----
@task(2)
@tag("api")
    def fetch_sectors(self)
⋮----
"""Fetch sectors API (called by home page)."""
⋮----
@task(2)
@tag("api")
    def fetch_entities(self)
⋮----
"""Fetch top entities (called by home page)."""
⋮----
@task(1)
@tag("api")
    def fetch_health(self)
⋮----
"""Check health endpoint."""
⋮----
@task(1)
@tag("api")
    def fetch_news_feed(self)
⋮----
"""Fetch news feed."""
⋮----
@task(1)
@tag("auth")
    def visit_login(self)
⋮----
"""Visit the login page."""
⋮----
class AuthenticatedUser(HttpUser)
⋮----
"""Simulates an authenticated user performing heavier operations."""
⋮----
wait_time = between(2, 8)
weight = 1  # Lower weight - fewer authenticated users
⋮----
"""Simulate login by posting to auth endpoint."""
⋮----
@task(3)
@tag("authenticated")
    def browse_market(self)
⋮----
"""Browse market with sector filter."""
⋮----
@task(2)
@tag("authenticated")
    def view_stock_detail(self)
⋮----
"""View a specific stock detail page."""
⋮----
@task(1)
@tag("authenticated")
    def browse_charts(self)
⋮----
"""View charts page and fetch TASI index data."""
</file>

<file path="frontend/e2e/load-tests/README.md">
# Load Testing

Frontend load tests using [Locust](https://locust.io/).

## Setup

```bash
# From the project root
pip install -r load-test-requirements.txt
```

## Running

### Web UI (recommended)

```bash
locust -f frontend/e2e/load-tests/locust-frontend.py --host=http://localhost:3000
```

Open http://localhost:8089 to configure and start the test.

### Headless

```bash
locust -f frontend/e2e/load-tests/locust-frontend.py \
  --host=http://localhost:3000 \
  --users 100 \
  --spawn-rate 10 \
  --run-time 5m \
  --headless \
  --csv=load-test-results
```

## Configuration

| Parameter     | Recommended Range | Description                    |
|---------------|-------------------|--------------------------------|
| `--users`     | 50-200            | Total concurrent simulated users |
| `--spawn-rate`| 5-20              | Users spawned per second       |
| `--run-time`  | 5m-10m            | Duration of the test           |

## Test Scenarios

- **FrontendUser** (weight=3): Simulates anonymous browsing across all pages
- **AuthenticatedUser** (weight=1): Simulates logged-in users performing heavier operations

## Prerequisites

Make sure the frontend dev server is running:

```bash
cd frontend && npm run dev
```

For more realistic results, also run the backend:

```bash
python app.py
```
</file>

<file path="frontend/e2e/playwright.config.ts">
import { defineConfig, devices } from '@playwright/test';
</file>

<file path="frontend/e2e/tests/admin.spec.ts">
import { test, expect } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Inject an admin session into localStorage. */
async function loginAsAdmin(page: import('@playwright/test').Page)
⋮----
/** Inject a viewer (non-admin) session into localStorage. */
async function loginAsViewer(page: import('@playwright/test').Page)
⋮----
/** Mock health and admin-related API endpoints. */
async function mockAdminApis(page: import('@playwright/test').Page)
⋮----
// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------
⋮----
// Verify the health API mock works by fetching directly
⋮----
// Try navigating to /admin - it may or may not exist yet (depends on task #6)
⋮----
// If the page exists, it should not return a server error
⋮----
// If RBAC is in place, viewer should see access denied or be redirected
// The page should not show admin content
⋮----
// Check for access denied text or redirect to login/home
⋮----
// If /admin doesn't exist yet, a 404 is also acceptable
⋮----
// Home page should render the hero section
</file>

<file path="frontend/e2e/tests/auth.spec.ts">
import { test, expect } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Intercept auth API calls and return mock responses. */
function mockAuthApi(
  page: import('@playwright/test').Page,
  opts: { loginStatus?: number; loginBody?: unknown } = {},
)
⋮----
// Mock common API calls that pages may make after redirect
⋮----
// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------
⋮----
// Clear any stored auth state
⋮----
// The toggle buttons contain "Sign In" / "Register" text (or Arabic equivalents)
⋮----
// Error message container should appear
⋮----
// Click the Register toggle
⋮----
// Name input should now be visible
⋮----
// Set up an authenticated session
⋮----
// Reload to pick up the stored session
⋮----
// Simulate logout by clearing tokens (matches useAuth().logout behavior)
⋮----
// Click "Continue as Guest" button
⋮----
// Delay the API response so we can observe loading state
⋮----
// The submit button should show loading text
</file>

<file path="frontend/e2e/tests/markets-page.spec.ts">
import { test, expect, type Page, type Route } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Mock all API calls the markets page makes. */
async function mockMarketsApis(page: Page)
⋮----
// Market analytics endpoints
⋮----
// Market live data (SSE or polling)
⋮----
// Market sectors
⋮----
// Health endpoint (Header component polls this)
⋮----
// Widgets SSE stream
⋮----
// News stream SSE
⋮----
// ---------------------------------------------------------------------------
// Test Suite
// ---------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 1. Page loads and URL is correct
// -------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 2. Page heading is visible
// -------------------------------------------------------------------------
⋮----
// The MarketHeader renders an h1 with "Market Overview 360°" (en) or "نظرة 360°" (ar)
⋮----
// Should contain either English or Arabic version of the market overview title
⋮----
// -------------------------------------------------------------------------
// 3. Breadcrumb navigation appears
// -------------------------------------------------------------------------
⋮----
// Breadcrumb renders inside a <nav> element
⋮----
// -------------------------------------------------------------------------
// 4. Category legend is visible
// -------------------------------------------------------------------------
⋮----
// CategoryLegend renders legend items with text like "Positive corr." or "ارتباط إيجابي"
// and "Inverse corr." or "ارتباط عكسي"
⋮----
// -------------------------------------------------------------------------
// 5. Market instruments section renders (constellation canvas or mobile cards)
// -------------------------------------------------------------------------
⋮----
// The page renders either a canvas (ConstellationCanvas) or mobile instrument cards.
// At least one of these must be present.
⋮----
// At least one data presentation element should be visible
⋮----
// -------------------------------------------------------------------------
// 6. Mobile card view works at narrow viewport
// -------------------------------------------------------------------------
⋮----
// Set viewport to mobile width before navigating
⋮----
// MobileSummary is rendered inside a div.lg:hidden — visible on mobile
// It contains the "Market Summary" / "ملخص السوق" heading
⋮----
// -------------------------------------------------------------------------
// 7. Correlation legend items render with correct labels
// -------------------------------------------------------------------------
⋮----
// CategoryLegend renders "+ρ" and "−ρ" labels alongside the legend items
⋮----
// At least one rho label should be present in the legend
⋮----
// -------------------------------------------------------------------------
// 8. Connection status badge renders
// -------------------------------------------------------------------------
⋮----
// The MarketHeader renders a status badge with text LIVE / STALE / OFFLINE
</file>

<file path="frontend/e2e/tests/query-flow.spec.ts">
import { test, expect } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Set up a fake authenticated session via localStorage. */
async function authenticateSession(page: import('@playwright/test').Page)
⋮----
/** Mock common API endpoints the chat/home pages may call. */
async function mockCommonApis(page: import('@playwright/test').Page)
⋮----
/**
 * Mock the SSE /api/v1/chat endpoint.
 * Returns a mock streamed response with SQL + text content.
 */
async function mockChatApi(
  page: import('@playwright/test').Page,
  opts: { error?: boolean; delay?: number } = {},
)
⋮----
// Return a simple JSON response (non-SSE fallback)
⋮----
// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------
⋮----
// Look for a text input, textarea, or contenteditable element for chat
⋮----
// The home page should show quick action cards with links
⋮----
// Wait for sectors to load
⋮----
// Press Enter or click submit button
⋮----
// Wait for any response content to appear
⋮----
// Page should not crash - it should still be interactive
⋮----
// Should show the mocked company data
⋮----
// Page should load without crashing
</file>

<file path="frontend/e2e/tests/stock-detail.spec.ts">
import { test, expect, type Page, type Route } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Test data factories
// ---------------------------------------------------------------------------
⋮----
function makeStockDetail(ticker = '2222.SR')
⋮----
function makeFinancialData()
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Mock all API calls the stock detail page makes for ticker 2222. */
async function mockStockApis(page: Page, ticker = '2222.SR')
⋮----
// Stock detail (company + valuation + profitability)
⋮----
// Wildcard for any stock endpoint
⋮----
// Financial statements
⋮----
// Dividends
⋮----
// Financial summary
⋮----
// OHLCV chart data
⋮----
// Fallback for other stock endpoints
⋮----
// News by ticker
⋮----
// Reports by ticker
⋮----
// Health endpoint
⋮----
// SSE streams
⋮----
// ---------------------------------------------------------------------------
// Test Suite
// ---------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 1. Stock detail page loads at /stock/2222
// -------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 2. Stock header shows ticker and company name
// -------------------------------------------------------------------------
⋮----
// Company name heading (h1)
⋮----
// Ticker badge: a span/badge showing the ticker code "2222.SR"
⋮----
// -------------------------------------------------------------------------
// 3. Price and change percentage are displayed
// -------------------------------------------------------------------------
⋮----
// Current price: rendered as "31.50" with "SAR" unit
⋮----
// Change indicator: rendered as "+0.30 (0.96%)" with a ▲ or ▼ symbol
// The price change section has a text matching +/- pattern with %
⋮----
// -------------------------------------------------------------------------
// 4. Navigation breadcrumb appears
// -------------------------------------------------------------------------
⋮----
// Breadcrumb nav with Home and Market links
⋮----
// -------------------------------------------------------------------------
// 5. Watchlist button is interactive
// -------------------------------------------------------------------------
⋮----
// Watchlist button: aria-label "Add to watchlist" or "إضافة للمفضلة"
⋮----
// Click to add to watchlist
⋮----
// After clicking, aria-label should change to "Remove from watchlist"
⋮----
// -------------------------------------------------------------------------
// 6. Tab navigation renders Overview, Financials, Dividends, News tabs
// -------------------------------------------------------------------------
⋮----
// The tab bar has role="tablist" with 4 tabs
⋮----
// Verify tab labels (English or Arabic)
⋮----
// -------------------------------------------------------------------------
// 7. Switching to Financials tab shows financial statements section
// -------------------------------------------------------------------------
⋮----
// Wait for tabs to render
⋮----
// Click the Financials tab
⋮----
// The FinancialStatementsSection renders a <section> with h2 "Financial Statements"
⋮----
// Statement type tabs: Income Statement, Balance Sheet, Cash Flow
⋮----
// -------------------------------------------------------------------------
// 8. Tab switching between income/balance/cashflow works
// -------------------------------------------------------------------------
⋮----
// Navigate to Financials tab first
⋮----
// Wait for the statement tabs to render
⋮----
// Click Balance Sheet tab
⋮----
// The table should update — wait for a balance-sheet-specific field label
// "Total Assets" or "إجمالي الأصول" should appear
⋮----
// Click Cash Flow tab
⋮----
// "Operating Cash Flow" or "التدفق النقدي التشغيلي" should appear
</file>

<file path="frontend/next-cdn.config.md">
# CDN Configuration Guide

Instructions for deploying Ra'd AI frontend behind a CDN (CloudFlare or CloudFront).

## Environment Variable

Set `NEXT_PUBLIC_CDN_URL` to your CDN origin URL:

```env
NEXT_PUBLIC_CDN_URL=https://cdn.raid-ai.example.com
```

## Next.js Asset Prefix

To serve `_next/static/*` assets from a CDN, add `assetPrefix` to `next.config.mjs`:

```js
const nextConfig = {
  assetPrefix: process.env.NEXT_PUBLIC_CDN_URL || '',
  // ... existing config
};
```

## Cache-Control Headers by Asset Type

| Path Pattern | Cache-Control | Notes |
|---|---|---|
| `/_next/static/*` | `public, max-age=31536000, immutable` | Hashed filenames, safe to cache forever |
| `/*.html` | `public, max-age=0, must-revalidate` | Always fetch fresh HTML |
| `/api/*` | `public, max-age=60, stale-while-revalidate=300` | Short TTL for API responses |
| `/images/*`, `/icons/*` | `public, max-age=86400, stale-while-revalidate=3600` | 1-day cache for images |

## CloudFlare Setup

1. Add your domain to CloudFlare
2. Set up a CNAME record for `cdn.yourdomain.com` pointing to your origin
3. Create a Page Rule or Cache Rule:
   - Match: `cdn.yourdomain.com/_next/static/*`
   - Cache Level: Cache Everything
   - Edge Cache TTL: 1 month
4. Enable Auto Minify for JS/CSS
5. Enable Brotli compression

## CloudFront Setup

1. Create a CloudFront distribution with your origin domain
2. Create cache behaviors:

   **Behavior 1** - Static assets:
   - Path pattern: `/_next/static/*`
   - Cache policy: CachingOptimized (managed policy)
   - Compress: Yes (Gzip + Brotli)

   **Behavior 2** - Images:
   - Path pattern: `/images/*`
   - Cache policy: CachingOptimized
   - TTL override: min=0, default=86400, max=604800

   **Behavior 3** - Default:
   - Path pattern: `*`
   - Cache policy: CachingDisabled
   - Origin request policy: AllViewer

3. Set `NEXT_PUBLIC_CDN_URL` to the CloudFront distribution domain

## Verification

After setup, verify assets load from CDN:

```bash
curl -I https://cdn.yourdomain.com/_next/static/chunks/main-abc123.js
# Should show: x-cache: Hit from cloudfront (or cf-cache-status: HIT)
```
</file>

<file path="frontend/postcss.config.mjs">
/** @type {import('postcss-load-config').Config} */
</file>

<file path="frontend/public/api-docs/openapi.yaml">
openapi: "3.0.3"
info:
  title: "Ra'd AI TASI Platform API"
  description: |
    REST API for the Ra'd AI Saudi Stock Market Intelligence Platform.

    The platform provides natural language querying of TASI-listed company data,
    market analytics, news aggregation, and stock charting capabilities.

    ## Authentication
    Most endpoints are public. Endpoints that require authentication use JWT Bearer tokens.
    Obtain tokens via `/api/auth/login`, `/api/auth/register`, or `/api/auth/guest`.

    ## Rate Limiting
    All endpoints (except health checks) are rate-limited per IP address:
    - Authentication endpoints: 10 requests/minute
    - Chart/OHLCV endpoints: 30 requests/minute
    - All other endpoints: 60 requests/minute

    Exceeding the limit returns `429` with a `Retry-After` header.
  version: "1.0.0"
  contact:
    name: "Ra'd AI Platform"
  license:
    name: "Proprietary"

servers:
  - url: "http://localhost:8084"
    description: "Local development backend"
  - url: "https://raid-ai-app-production.up.railway.app"
    description: "Production (Railway)"

tags:
  - name: health
    description: "Health check and readiness probes"
  - name: auth
    description: "Authentication and user management"
  - name: news
    description: "News articles (PostgreSQL-backed)"
  - name: news-feed
    description: "News feed from Arabic scrapers (SQLite-backed)"
  - name: reports
    description: "Technical and analyst reports"
  - name: announcements
    description: "CMA/Tadawul announcements"
  - name: entities
    description: "Company listing and detail"
  - name: watchlists
    description: "User watchlist management"
  - name: charts
    description: "Chart analytics"
  - name: ohlcv
    description: "Stock OHLCV candlestick data"
  - name: tasi-index
    description: "TASI index data"
  - name: market
    description: "Market analytics (movers, summary, sectors, heatmap)"
  - name: stocks
    description: "Stock data (dividends, financials, comparison)"

components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
      description: "JWT access token obtained from /api/auth/login or /api/auth/guest"

  schemas:
    Error:
      type: object
      properties:
        error:
          type: object
          properties:
            code:
              type: string
              example: "RATE_LIMITED"
            message:
              type: string
              example: "Too many requests"
            request_id:
              type: string
              format: uuid

    PaginatedMeta:
      type: object
      properties:
        total:
          type: integer
        page:
          type: integer
        page_size:
          type: integer
        total_pages:
          type: integer

    # -- Health --
    ComponentHealth:
      type: object
      properties:
        name:
          type: string
          example: "database"
        status:
          type: string
          enum: [healthy, degraded, unhealthy]
        latency_ms:
          type: number
          nullable: true
          example: 1.23
        message:
          type: string

    HealthResponse:
      type: object
      properties:
        status:
          type: string
          enum: [healthy, degraded, unhealthy]
        service:
          type: string
          example: "raid-ai-tasi"
        version:
          type: string
          example: "1.0.0"
        uptime_seconds:
          type: number
          example: 3600.5
        components:
          type: array
          items:
            $ref: "#/components/schemas/ComponentHealth"

    # -- Auth --
    UserCreate:
      type: object
      required: [email, password]
      properties:
        email:
          type: string
          format: email
        password:
          type: string
          minLength: 8
          maxLength: 128
        name:
          type: string
          maxLength: 100
          description: "Display name (alias for display_name)"

    UserLogin:
      type: object
      required: [email, password]
      properties:
        email:
          type: string
          format: email
        password:
          type: string

    AuthResponse:
      type: object
      properties:
        token:
          type: string
          description: "JWT access token"
        refresh_token:
          type: string
        token_type:
          type: string
          example: "bearer"
        user_id:
          type: string
        name:
          type: string

    TokenRefreshRequest:
      type: object
      required: [refresh_token]
      properties:
        refresh_token:
          type: string

    TokenResponse:
      type: object
      properties:
        access_token:
          type: string
        refresh_token:
          type: string
        token_type:
          type: string
          example: "bearer"

    UserProfile:
      type: object
      properties:
        id:
          type: string
        email:
          type: string
        display_name:
          type: string
          nullable: true
        subscription_tier:
          type: string
        usage_count:
          type: integer
        is_active:
          type: boolean
        created_at:
          type: string
          format: date-time
          nullable: true

    # -- News --
    NewsArticle:
      type: object
      properties:
        id:
          type: string
        ticker:
          type: string
          nullable: true
        title:
          type: string
        body:
          type: string
          nullable: true
        source_name:
          type: string
          nullable: true
        source_url:
          type: string
          nullable: true
        published_at:
          type: string
          format: date-time
          nullable: true
        sentiment_score:
          type: number
          nullable: true
        sentiment_label:
          type: string
          nullable: true
        language:
          type: string
        created_at:
          type: string
          format: date-time
          nullable: true

    NewsListResponse:
      allOf:
        - $ref: "#/components/schemas/PaginatedMeta"
        - type: object
          properties:
            items:
              type: array
              items:
                $ref: "#/components/schemas/NewsArticle"

    # -- News Feed --
    NewsFeedItem:
      type: object
      properties:
        id:
          type: string
        title:
          type: string
        body:
          type: string
          nullable: true
        source_name:
          type: string
        source_url:
          type: string
          nullable: true
        published_at:
          type: string
          format: date-time
          nullable: true
        priority:
          type: integer
        language:
          type: string
        created_at:
          type: string
          format: date-time
          nullable: true

    NewsFeedResponse:
      type: object
      properties:
        items:
          type: array
          items:
            $ref: "#/components/schemas/NewsFeedItem"
        total:
          type: integer
        page:
          type: integer
        limit:
          type: integer

    NewsSourceInfo:
      type: object
      properties:
        source_name:
          type: string
        count:
          type: integer

    NewsSourcesResponse:
      type: object
      properties:
        sources:
          type: array
          items:
            $ref: "#/components/schemas/NewsSourceInfo"

    # -- Reports --
    ReportItem:
      type: object
      properties:
        id:
          type: string
        ticker:
          type: string
          nullable: true
        title:
          type: string
        summary:
          type: string
          nullable: true
        author:
          type: string
          nullable: true
        source_name:
          type: string
          nullable: true
        source_url:
          type: string
          nullable: true
        published_at:
          type: string
          format: date-time
          nullable: true
        recommendation:
          type: string
          nullable: true
        target_price:
          type: number
          nullable: true
        current_price_at_report:
          type: number
          nullable: true
        report_type:
          type: string
          nullable: true
        created_at:
          type: string
          format: date-time
          nullable: true

    ReportListResponse:
      allOf:
        - $ref: "#/components/schemas/PaginatedMeta"
        - type: object
          properties:
            items:
              type: array
              items:
                $ref: "#/components/schemas/ReportItem"

    # -- Announcements --
    AnnouncementItem:
      type: object
      properties:
        id:
          type: string
        ticker:
          type: string
          nullable: true
        title_ar:
          type: string
          nullable: true
        title_en:
          type: string
          nullable: true
        body_ar:
          type: string
          nullable: true
        body_en:
          type: string
          nullable: true
        source:
          type: string
          nullable: true
        announcement_date:
          type: string
          format: date-time
          nullable: true
        category:
          type: string
          nullable: true
        classification:
          type: string
          nullable: true
        is_material:
          type: boolean
        source_url:
          type: string
          nullable: true
        created_at:
          type: string
          format: date-time
          nullable: true

    AnnouncementListResponse:
      allOf:
        - $ref: "#/components/schemas/PaginatedMeta"
        - type: object
          properties:
            items:
              type: array
              items:
                $ref: "#/components/schemas/AnnouncementItem"

    # -- Entities --
    CompanySummary:
      type: object
      properties:
        ticker:
          type: string
          example: "2222.SR"
        short_name:
          type: string
          nullable: true
        sector:
          type: string
          nullable: true
        industry:
          type: string
          nullable: true
        current_price:
          type: number
          nullable: true
        market_cap:
          type: number
          nullable: true
        change_pct:
          type: number
          nullable: true

    EntityListResponse:
      type: object
      properties:
        items:
          type: array
          items:
            $ref: "#/components/schemas/CompanySummary"
        count:
          type: integer

    CompanyDetail:
      type: object
      properties:
        ticker:
          type: string
        short_name:
          type: string
          nullable: true
        sector:
          type: string
          nullable: true
        industry:
          type: string
          nullable: true
        exchange:
          type: string
          nullable: true
        currency:
          type: string
          nullable: true
        current_price:
          type: number
          nullable: true
        previous_close:
          type: number
          nullable: true
        day_high:
          type: number
          nullable: true
        day_low:
          type: number
          nullable: true
        week_52_high:
          type: number
          nullable: true
        week_52_low:
          type: number
          nullable: true
        volume:
          type: number
          nullable: true
        market_cap:
          type: number
          nullable: true
        beta:
          type: number
          nullable: true
        trailing_pe:
          type: number
          nullable: true
        forward_pe:
          type: number
          nullable: true
        price_to_book:
          type: number
          nullable: true
        trailing_eps:
          type: number
          nullable: true
        roe:
          type: number
          nullable: true
        profit_margin:
          type: number
          nullable: true
        revenue_growth:
          type: number
          nullable: true
        recommendation:
          type: string
          nullable: true
        target_mean_price:
          type: number
          nullable: true
        analyst_count:
          type: number
          nullable: true

    SectorInfo:
      type: object
      properties:
        sector:
          type: string
        company_count:
          type: integer

    # -- Watchlists --
    WatchlistItem:
      type: object
      properties:
        id:
          type: string
        user_id:
          type: string
        name:
          type: string
        tickers:
          type: array
          items:
            type: string

    WatchlistCreate:
      type: object
      properties:
        name:
          type: string
        tickers:
          type: array
          items:
            type: string

    # -- Charts --
    ChartDataPoint:
      type: object
      properties:
        label:
          type: string
        value:
          type: number

    ChartResponse:
      type: object
      properties:
        chart_type:
          type: string
        title:
          type: string
        data:
          type: array
          items:
            $ref: "#/components/schemas/ChartDataPoint"

    # -- OHLCV --
    OHLCVData:
      type: object
      properties:
        time:
          type: string
        open:
          type: number
        high:
          type: number
        low:
          type: number
        close:
          type: number
        volume:
          type: number
          nullable: true

    StockOHLCVResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/OHLCVData"
        source:
          type: string
          enum: [real, mock, cached]
        last_updated:
          type: string
          format: date-time
          nullable: true
        symbol:
          type: string
        period:
          type: string
        count:
          type: integer

    TasiIndexResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/OHLCVData"
        source:
          type: string
          enum: [real, mock, cached]
        data_freshness:
          type: string
          enum: [real-time, cached, stale, mock]
        cache_age_seconds:
          type: number
          nullable: true
        last_updated:
          type: string
          format: date-time
          nullable: true
        symbol:
          type: string
        period:
          type: string
        count:
          type: integer

    # -- Market Analytics --
    MarketMover:
      type: object
      properties:
        ticker:
          type: string
        company_name_ar:
          type: string
        company_name_en:
          type: string
        current_price:
          type: number
        previous_close:
          type: number
        change_pct:
          type: number
        volume:
          type: number
        sector:
          type: string

    MarketSummary:
      type: object
      properties:
        total_market_cap:
          type: number
        total_volume:
          type: number
        gainers_count:
          type: integer
        losers_count:
          type: integer
        unchanged_count:
          type: integer
        top_gainers:
          type: array
          items:
            $ref: "#/components/schemas/MarketMover"
        top_losers:
          type: array
          items:
            $ref: "#/components/schemas/MarketMover"

    SectorPerformance:
      type: object
      properties:
        sector:
          type: string
        avg_change_pct:
          type: number
        total_volume:
          type: number
        total_market_cap:
          type: number
        company_count:
          type: integer
        gainers:
          type: integer
        losers:
          type: integer

    HeatmapItem:
      type: object
      properties:
        ticker:
          type: string
        name:
          type: string
        sector:
          type: string
        market_cap:
          type: number
        change_pct:
          type: number

    # -- Stock Data --
    StockDividends:
      type: object
      properties:
        ticker:
          type: string
        dividend_rate:
          type: number
          nullable: true
        dividend_yield:
          type: number
          nullable: true
        payout_ratio:
          type: number
          nullable: true
        five_year_avg_dividend_yield:
          type: number
          nullable: true
        ex_dividend_date:
          type: string
          nullable: true
        last_dividend_value:
          type: number
          nullable: true
        last_dividend_date:
          type: string
          nullable: true
        trailing_annual_dividend_rate:
          type: number
          nullable: true
        trailing_annual_dividend_yield:
          type: number
          nullable: true

    FinancialSummary:
      type: object
      properties:
        ticker:
          type: string
        total_revenue:
          type: number
          nullable: true
        revenue_per_share:
          type: number
          nullable: true
        total_cash:
          type: number
          nullable: true
        total_debt:
          type: number
          nullable: true
        debt_to_equity:
          type: number
          nullable: true
        current_ratio:
          type: number
          nullable: true
        quick_ratio:
          type: number
          nullable: true
        free_cashflow:
          type: number
          nullable: true
        ebitda:
          type: number
          nullable: true
        gross_profit:
          type: number
          nullable: true
        operating_cashflow:
          type: number
          nullable: true

    FinancialPeriod:
      type: object
      properties:
        period_type:
          type: string
          nullable: true
        period_index:
          type: integer
          nullable: true
        period_date:
          type: string
          nullable: true
        data:
          type: object
          additionalProperties: true

    FinancialsResponse:
      type: object
      properties:
        ticker:
          type: string
        statement:
          type: string
        periods:
          type: array
          items:
            $ref: "#/components/schemas/FinancialPeriod"

    StockComparison:
      type: object
      properties:
        tickers:
          type: array
          items:
            type: object
            properties:
              ticker:
                type: string
              name:
                type: string
              metrics:
                type: object
                additionalProperties:
                  type: number
                  nullable: true

    BatchQuote:
      type: object
      properties:
        ticker:
          type: string
        name:
          type: string
        short_name:
          type: string
          nullable: true
        current_price:
          type: number
        previous_close:
          type: number
        change_pct:
          type: number
        volume:
          type: number

  responses:
    BadRequest:
      description: "Invalid request parameters"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
    Unauthorized:
      description: "Authentication required or token invalid"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
    Forbidden:
      description: "Insufficient permissions"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
    NotFound:
      description: "Resource not found"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
    RateLimited:
      description: "Rate limit exceeded"
      headers:
        Retry-After:
          schema:
            type: integer
          description: "Seconds until the rate limit resets"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"
    ServiceUnavailable:
      description: "Service unavailable (database down or backend not reachable)"
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/Error"

paths:
  # ---------- Health ----------
  /health:
    get:
      tags: [health]
      summary: "Full health report"
      description: "Returns structured health status for all platform components."
      responses:
        "200":
          description: "Healthy or degraded"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HealthResponse"
        "503":
          description: "Unhealthy"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HealthResponse"

  /health/live:
    get:
      tags: [health]
      summary: "Liveness probe"
      description: "Returns 200 if the process is running. Does not check external dependencies."
      responses:
        "200":
          description: "Process is alive"
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "alive"
                  uptime_seconds:
                    type: number

  /health/ready:
    get:
      tags: [health]
      summary: "Readiness probe"
      description: "Returns 200 only when the database is reachable."
      responses:
        "200":
          description: "Ready"
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "ready"
        "503":
          description: "Not ready"
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "not_ready"
                  reason:
                    type: string

  # ---------- Auth ----------
  /api/auth/register:
    post:
      tags: [auth]
      summary: "Register a new user"
      description: "Creates a local auth user with bcrypt-hashed password. Requires PostgreSQL backend."
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UserCreate"
      responses:
        "201":
          description: "User created"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AuthResponse"
        "409":
          description: "Email already registered"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "429":
          $ref: "#/components/responses/RateLimited"
        "503":
          $ref: "#/components/responses/ServiceUnavailable"

  /api/auth/login:
    post:
      tags: [auth]
      summary: "Login"
      description: "Authenticate with email and password. Returns JWT tokens."
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UserLogin"
      responses:
        "200":
          description: "Login successful"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AuthResponse"
        "401":
          $ref: "#/components/responses/Unauthorized"
        "429":
          $ref: "#/components/responses/RateLimited"
        "503":
          $ref: "#/components/responses/ServiceUnavailable"

  /api/auth/guest:
    post:
      tags: [auth]
      summary: "Guest login"
      description: "Generate a guest token for anonymous access. No credentials required."
      responses:
        "200":
          description: "Guest token generated"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AuthResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/auth/refresh:
    post:
      tags: [auth]
      summary: "Refresh token"
      description: "Exchange a valid refresh token for a new access/refresh token pair."
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/TokenRefreshRequest"
      responses:
        "200":
          description: "Tokens refreshed"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/TokenResponse"
        "401":
          $ref: "#/components/responses/Unauthorized"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/auth/me:
    get:
      tags: [auth]
      summary: "Get current user profile"
      security:
        - BearerAuth: []
      responses:
        "200":
          description: "User profile"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UserProfile"
        "401":
          $ref: "#/components/responses/Unauthorized"

  # ---------- News ----------
  /api/news:
    get:
      tags: [news]
      summary: "List news articles"
      parameters:
        - name: page
          in: query
          schema:
            type: integer
            default: 1
        - name: page_size
          in: query
          schema:
            type: integer
            default: 20
        - name: language
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "Paginated news articles"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/news/ticker/{ticker}:
    get:
      tags: [news]
      summary: "News articles by ticker"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
        - name: page
          in: query
          schema:
            type: integer
        - name: page_size
          in: query
          schema:
            type: integer
        - name: sentiment
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "News for ticker"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  # ---------- News Feed ----------
  /api/v1/news/feed:
    get:
      tags: [news-feed]
      summary: "Arabic news feed"
      description: "Returns news scraped from 5 Arabic financial sources."
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
            default: 20
        - name: offset
          in: query
          schema:
            type: integer
            default: 0
        - name: source
          in: query
          schema:
            type: string
          description: "Filter by source name"
      responses:
        "200":
          description: "News feed items"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsFeedResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/v1/news/feed/{id}:
    get:
      tags: [news-feed]
      summary: "Single news article"
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: string
      responses:
        "200":
          description: "News article"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsFeedItem"
        "404":
          $ref: "#/components/responses/NotFound"

  /api/v1/news/search:
    get:
      tags: [news-feed]
      summary: "Search news"
      parameters:
        - name: q
          in: query
          required: true
          schema:
            type: string
          description: "Search query"
        - name: limit
          in: query
          schema:
            type: integer
        - name: offset
          in: query
          schema:
            type: integer
      responses:
        "200":
          description: "Search results"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsFeedResponse"

  /api/v1/news/sources:
    get:
      tags: [news-feed]
      summary: "List news sources"
      responses:
        "200":
          description: "Available news sources with article counts"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/NewsSourcesResponse"

  # ---------- Reports ----------
  /api/reports:
    get:
      tags: [reports]
      summary: "List reports"
      parameters:
        - name: page
          in: query
          schema:
            type: integer
        - name: page_size
          in: query
          schema:
            type: integer
        - name: recommendation
          in: query
          schema:
            type: string
        - name: report_type
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "Paginated reports"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ReportListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/reports/ticker/{ticker}:
    get:
      tags: [reports]
      summary: "Reports by ticker"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
        - name: page
          in: query
          schema:
            type: integer
        - name: page_size
          in: query
          schema:
            type: integer
      responses:
        "200":
          description: "Reports for ticker"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ReportListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  # ---------- Announcements ----------
  /api/announcements:
    get:
      tags: [announcements]
      summary: "List announcements"
      parameters:
        - name: page
          in: query
          schema:
            type: integer
        - name: page_size
          in: query
          schema:
            type: integer
        - name: ticker
          in: query
          schema:
            type: string
        - name: category
          in: query
          schema:
            type: string
        - name: source
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "Paginated announcements"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AnnouncementListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  # ---------- Entities ----------
  /api/entities:
    get:
      tags: [entities]
      summary: "List companies"
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
        - name: offset
          in: query
          schema:
            type: integer
        - name: sector
          in: query
          schema:
            type: string
        - name: search
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "Company list"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/EntityListResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/entities/{ticker}:
    get:
      tags: [entities]
      summary: "Company detail"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
      responses:
        "200":
          description: "Company detail"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CompanyDetail"
        "404":
          $ref: "#/components/responses/NotFound"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/entities/sectors:
    get:
      tags: [entities]
      summary: "List sectors"
      responses:
        "200":
          description: "Sector list with company counts"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/SectorInfo"

  # ---------- Watchlists ----------
  /api/watchlists:
    get:
      tags: [watchlists]
      summary: "List user watchlists"
      security:
        - BearerAuth: []
      responses:
        "200":
          description: "User watchlists"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/WatchlistItem"
        "401":
          $ref: "#/components/responses/Unauthorized"
    post:
      tags: [watchlists]
      summary: "Create watchlist"
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/WatchlistCreate"
      responses:
        "201":
          description: "Watchlist created"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/WatchlistItem"
        "401":
          $ref: "#/components/responses/Unauthorized"
        "429":
          $ref: "#/components/responses/RateLimited"

  /api/watchlists/{id}:
    patch:
      tags: [watchlists]
      summary: "Update watchlist"
      security:
        - BearerAuth: []
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: string
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/WatchlistCreate"
      responses:
        "200":
          description: "Watchlist updated"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/WatchlistItem"
        "401":
          $ref: "#/components/responses/Unauthorized"
        "404":
          $ref: "#/components/responses/NotFound"
    delete:
      tags: [watchlists]
      summary: "Delete watchlist"
      security:
        - BearerAuth: []
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: string
      responses:
        "204":
          description: "Watchlist deleted"
        "401":
          $ref: "#/components/responses/Unauthorized"
        "404":
          $ref: "#/components/responses/NotFound"

  # ---------- Charts ----------
  /api/charts/sector-market-cap:
    get:
      tags: [charts]
      summary: "Sector market cap chart"
      responses:
        "200":
          description: "Chart data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChartResponse"

  /api/charts/top-companies:
    get:
      tags: [charts]
      summary: "Top companies chart"
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
        - name: sector
          in: query
          schema:
            type: string
      responses:
        "200":
          description: "Chart data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChartResponse"

  /api/charts/sector-pe:
    get:
      tags: [charts]
      summary: "Sector P/E ratio chart"
      responses:
        "200":
          description: "Chart data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChartResponse"

  /api/charts/dividend-yield-top:
    get:
      tags: [charts]
      summary: "Top dividend yield chart"
      parameters:
        - name: limit
          in: query
          schema:
            type: integer
      responses:
        "200":
          description: "Chart data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChartResponse"

  # ---------- OHLCV ----------
  /api/v1/charts/{ticker}/ohlcv:
    get:
      tags: [ohlcv]
      summary: "Stock OHLCV data"
      description: "Returns candlestick (OHLCV) data for a stock."
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
        - name: period
          in: query
          schema:
            type: string
            default: "1y"
          description: "Time period (e.g. 1d, 5d, 1mo, 3mo, 6mo, 1y, 5y, max)"
      responses:
        "200":
          description: "OHLCV data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/StockOHLCVResponse"
        "404":
          $ref: "#/components/responses/NotFound"
        "429":
          $ref: "#/components/responses/RateLimited"

  # ---------- TASI Index ----------
  /api/v1/charts/tasi/index:
    get:
      tags: [tasi-index]
      summary: "TASI index data"
      description: "Returns TASI index OHLCV data with freshness metadata."
      parameters:
        - name: period
          in: query
          schema:
            type: string
            default: "1y"
      responses:
        "200":
          description: "TASI index data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/TasiIndexResponse"
        "429":
          $ref: "#/components/responses/RateLimited"

  # ---------- Market Analytics ----------
  /api/v1/market/movers:
    get:
      tags: [market]
      summary: "Market movers"
      parameters:
        - name: type
          in: query
          required: true
          schema:
            type: string
            enum: [gainers, losers]
        - name: limit
          in: query
          schema:
            type: integer
      responses:
        "200":
          description: "Market movers"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/MarketMover"

  /api/v1/market/summary:
    get:
      tags: [market]
      summary: "Market summary"
      responses:
        "200":
          description: "Market summary"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/MarketSummary"

  /api/v1/market/sectors:
    get:
      tags: [market]
      summary: "Sector performance"
      responses:
        "200":
          description: "Sector performance data"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/SectorPerformance"

  /api/v1/market/heatmap:
    get:
      tags: [market]
      summary: "Market heatmap"
      responses:
        "200":
          description: "Heatmap data"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/HeatmapItem"

  # ---------- Stock Data ----------
  /api/v1/stocks/{ticker}/dividends:
    get:
      tags: [stocks]
      summary: "Stock dividends"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
      responses:
        "200":
          description: "Dividend data"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/StockDividends"
        "404":
          $ref: "#/components/responses/NotFound"

  /api/v1/stocks/{ticker}/summary:
    get:
      tags: [stocks]
      summary: "Stock financial summary"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
      responses:
        "200":
          description: "Financial summary"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FinancialSummary"
        "404":
          $ref: "#/components/responses/NotFound"

  /api/v1/stocks/{ticker}/financials:
    get:
      tags: [stocks]
      summary: "Stock financial statements"
      parameters:
        - name: ticker
          in: path
          required: true
          schema:
            type: string
        - name: statement
          in: query
          schema:
            type: string
          description: "Statement type (balance_sheet, income_statement, cash_flow)"
        - name: period_type
          in: query
          schema:
            type: string
          description: "Period type (annual, quarterly, ttm)"
      responses:
        "200":
          description: "Financial statements"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FinancialsResponse"
        "404":
          $ref: "#/components/responses/NotFound"

  /api/v1/stocks/compare:
    get:
      tags: [stocks]
      summary: "Compare stocks"
      parameters:
        - name: tickers
          in: query
          required: true
          schema:
            type: string
          description: "Comma-separated ticker list"
        - name: metrics
          in: query
          required: true
          schema:
            type: string
          description: "Comma-separated metric names"
      responses:
        "200":
          description: "Stock comparison"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/StockComparison"

  /api/v1/stocks/quotes:
    get:
      tags: [stocks]
      summary: "Batch stock quotes"
      parameters:
        - name: tickers
          in: query
          required: true
          schema:
            type: string
          description: "Comma-separated ticker list"
      responses:
        "200":
          description: "Batch quotes"
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/BatchQuote"
</file>

<file path="frontend/README.md">
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
</file>

<file path="frontend/scripts/analyze-bundle.js">
/**
 * Bundle Analysis Script
 *
 * Runs `next build` with ANALYZE=true to generate bundle analysis reports.
 * Works on both Windows and Unix without requiring cross-env.
 *
 * Usage:
 *   node scripts/analyze-bundle.js
 *   npm run analyze
 */
</file>

<file path="frontend/scripts/security-audit.sh">
#!/usr/bin/env bash
#
# security-audit.sh - Run security audits on the Ra'd AI frontend.
#
# Usage: bash scripts/security-audit.sh
#
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
FRONTEND_DIR="$(dirname "$SCRIPT_DIR")"

cd "$FRONTEND_DIR"

echo "========================================"
echo " Ra'd AI Frontend Security Audit"
echo "========================================"
echo ""

EXIT_CODE=0

# --------------------------------------------------------------------------
# 1. npm audit
# --------------------------------------------------------------------------
echo "[1/3] Running npm audit..."
echo "----------------------------------------"
if npm audit --audit-level=high 2>/dev/null; then
  echo "npm audit: PASSED"
else
  echo "npm audit: WARNINGS FOUND (see above)"
  EXIT_CODE=1
fi
echo ""

# --------------------------------------------------------------------------
# 2. ESLint security plugin
# --------------------------------------------------------------------------
echo "[2/3] Running ESLint with security plugin..."
echo "----------------------------------------"
if npx eslint src/ --no-error-on-unmatched-pattern --plugin security --rule '{"security/detect-object-injection": "warn", "security/detect-non-literal-regexp": "warn", "security/detect-unsafe-regex": "error", "security/detect-eval-with-expression": "error"}' 2>/dev/null; then
  echo "ESLint security: PASSED"
else
  echo "ESLint security: ISSUES FOUND (see above)"
  # Don't fail on ESLint security warnings - they need manual review
fi
echo ""

# --------------------------------------------------------------------------
# 3. Check for common security issues
# --------------------------------------------------------------------------
echo "[3/3] Checking for common security patterns..."
echo "----------------------------------------"

# Check for hardcoded secrets/tokens (basic pattern matching)
SECRETS_FOUND=0
for pattern in "sk-" "api_key.*=.*['\"]" "password.*=.*['\"]" "secret.*=.*['\"]"; do
  if grep -rn "$pattern" src/ --include="*.ts" --include="*.tsx" --exclude-dir=node_modules --exclude-dir=__tests__ 2>/dev/null | grep -v "type\|interface\|placeholder\|example\|test\|mock" | head -5; then
    SECRETS_FOUND=1
  fi
done

if [ "$SECRETS_FOUND" -eq 0 ]; then
  echo "No hardcoded secrets found: PASSED"
else
  echo "WARNING: Potential hardcoded secrets detected (review above)"
  EXIT_CODE=1
fi

# Check for dangerouslySetInnerHTML usage
DANGEROUS_HTML=$(grep -rn "dangerouslySetInnerHTML" src/ --include="*.tsx" --include="*.ts" 2>/dev/null || true)
if [ -z "$DANGEROUS_HTML" ]; then
  echo "No dangerouslySetInnerHTML usage: PASSED"
else
  echo "WARNING: dangerouslySetInnerHTML found:"
  echo "$DANGEROUS_HTML"
fi

echo ""
echo "========================================"
echo " Audit Complete (exit code: $EXIT_CODE)"
echo "========================================"

exit $EXIT_CODE
</file>

<file path="frontend/sentry.edge.config.ts">
// Sentry edge runtime configuration for Next.js
</file>

<file path="frontend/sentry.server.config.ts">
// Sentry server-side configuration for Next.js
</file>

<file path="frontend/src/app/admin/layout.tsx">
import { usePathname } from 'next/navigation';
import Link from 'next/link';
import { cn } from '@/lib/utils';
⋮----
{/* Admin sidebar */}
⋮----
className=
⋮----
{/* Content */}
</file>

<file path="frontend/src/app/announcements/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Megaphone icon */}
</file>

<file path="frontend/src/app/charts/components/AnalyticsTab.tsx">
import React from 'react';
import dynamic from 'next/dynamic';
import { useLanguage } from '@/providers/LanguageProvider';
</file>

<file path="frontend/src/app/charts/components/ChartTabNavigation.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { TABS, type TabId } from './types';
⋮----
interface ChartTabNavigationProps {
  activeTab: TabId;
  onTabChange: (tab: TabId) => void;
}
⋮----
{/* Active indicator with transition */}
</file>

<file path="frontend/src/app/charts/components/CompareTab.tsx">
import React, { useState } from 'react';
import dynamic from 'next/dynamic';
import { cn } from '@/lib/utils';
import { useEntities } from '@/lib/hooks/use-api';
import { useLanguage } from '@/providers/LanguageProvider';
import { POPULAR_STOCKS } from './types';
⋮----
// ---------------------------------------------------------------------------
// Comparison ticker selector (local to this tab)
// ---------------------------------------------------------------------------
⋮----
{/* Selected ticker chips */}
⋮----
onFocus=
onBlur=
⋮----
onAdd(stock.ticker);
setSearch('');
⋮----
{/* Quick picks for comparison */}
⋮----
className=
⋮----
// ---------------------------------------------------------------------------
// CompareTab
// ---------------------------------------------------------------------------
⋮----
{/* Ticker selector */}
⋮----
{/* Comparison chart */}
</file>

<file path="frontend/src/app/charts/components/index.ts">

</file>

<file path="frontend/src/app/charts/components/PopularStocks.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { POPULAR_STOCKS } from './types';
⋮----
interface PopularStocksProps {
  selectedTicker: string | null;
  onSelect: (ticker: string, name: string) => void;
}
⋮----
function PopularStocksInner(
⋮----
className=
</file>

<file path="frontend/src/app/charts/components/RecentSearches.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface RecentSearchesProps {
  recentSearches: { ticker: string; name: string }[];
  onSelect: (ticker: string, name: string) => void;
}
</file>

<file path="frontend/src/app/charts/components/StocksTab.tsx">
import React, { useState, useCallback, useEffect, useRef } from 'react';
import { cn } from '@/lib/utils';
import { useEntities } from '@/lib/hooks/use-api';
import { TASIIndexChart } from '@/components/charts';
import { getTASIStockName } from '@/lib/tradingview-utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { getRecentSearches, addRecentSearch } from './types';
import { PopularStocks } from './PopularStocks';
import { RecentSearches } from './RecentSearches';
import { StockChartPanel } from './StockChartPanel';
⋮----
// ---------------------------------------------------------------------------
// Search result item (local to this tab)
// ---------------------------------------------------------------------------
⋮----
function SearchResultItem({
  ticker,
  name,
  sector,
  price,
  isHighlighted,
  onSelect,
}: {
  ticker: string;
  name: string | null;
  sector: string | null;
  price: number | null;
  isHighlighted: boolean;
onSelect: (ticker: string)
⋮----
onClick=
⋮----
// ---------------------------------------------------------------------------
// StocksTab
// ---------------------------------------------------------------------------
⋮----
// Load recent searches on mount
⋮----
// Keyboard navigation in search dropdown
⋮----
// Reset highlight when search text changes
⋮----
// Show recent searches dropdown when focused and no search text
⋮----
{/* Search + Quick picks */}
⋮----
{/* Search bar */}
⋮----
onFocus=
onBlur=
⋮----
className=
⋮----
{/* Search results dropdown */}
⋮----
onSelect=
⋮----
{/* Recent searches dropdown */}
⋮----
{/* Quick pick chips */}
⋮----
{/* Chart area */}
⋮----
/* Default: TASI Index + welcome empty state */
⋮----
{/* TASI Candlestick Chart (lightweight-charts) */}
⋮----
{/* Hint */}
</file>

<file path="frontend/src/app/charts/components/types.ts">
// ---------------------------------------------------------------------------
// Shared types & constants for Charts page components
// ---------------------------------------------------------------------------
⋮----
export type TabId = 'stocks' | 'compare' | 'analytics';
⋮----
// ---------------------------------------------------------------------------
// Recent searches helpers (localStorage)
// ---------------------------------------------------------------------------
⋮----
// Migrate old key name
⋮----
export function getRecentSearches():
⋮----
export function addRecentSearch(ticker: string, name: string)
⋮----
// ignore
</file>

<file path="frontend/src/app/charts/loading.tsx">
{/* Header skeleton */}
⋮----
{/* Tab bar skeleton */}
⋮----
{/* Search skeleton */}
⋮----
{/* Quick picks skeleton */}
⋮----
{/* Chart area skeleton */}
</file>

<file path="frontend/src/app/login/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Lock icon */}
</file>

<file path="frontend/src/app/market/loading.tsx">
{/* Header skeleton */}
⋮----
{/* TASI chart skeleton */}
⋮----
{/* Search + filter row skeleton */}
⋮----
{/* Sector chips skeleton */}
⋮----
{/* Table skeleton */}
</file>

<file path="frontend/src/app/markets/components/CategoryLegend.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { C, LEGEND_ITEMS } from './constants';
⋮----
// ---------------------------------------------------------------------------
// CategoryLegend - legend items + correlation type indicators + explainer
// ---------------------------------------------------------------------------
⋮----
export interface CategoryLegendProps {
  language: 'ar' | 'en';
  t: (ar: string, en: string) => string;
}
⋮----
{/* Legend */}
⋮----
className=
⋮----

⋮----
{/* Explainer */}
</file>

<file path="frontend/src/app/markets/components/constants.ts">
// ---------------------------------------------------------------------------
// Shared color tokens, format helpers, and legend data
// ---------------------------------------------------------------------------
⋮----
/**
 * Color tokens matching the global design system.
 * Used by all market sub-components for consistent styling.
 */
⋮----
// ---------------------------------------------------------------------------
// Format helpers
// ---------------------------------------------------------------------------
⋮----
/** Format a number with 2 decimal places, or return em-dash for null/undefined. */
export const fmt = (v: number | null | undefined, locale = 'en-US')
⋮----
/** Format a decimal as a percentage string with 1 decimal place. */
export const pctFmt = (v: number | null | undefined)
⋮----
// ---------------------------------------------------------------------------
// Legend items
// ---------------------------------------------------------------------------
⋮----
export interface LegendItem {
  labelAr: string;
  labelEn: string;
  color: string;
}
⋮----
// ---------------------------------------------------------------------------
// CSS Keyframes (injected once)
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/app/markets/components/EdgeTooltip.tsx">
import React from 'react';
import { C } from './constants';
import type { EdgeLabel } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// EdgeTooltip - hover tooltip for correlation edges
// ---------------------------------------------------------------------------
⋮----
export interface EdgeTooltipProps {
  edge: EdgeLabel;
  x: number;
  y: number;
  t: (ar: string, en: string) => string;
  isRTL: boolean;
}
</file>

<file path="frontend/src/app/markets/components/index.ts">
// Barrel export for market sub-components
</file>

<file path="frontend/src/app/markets/components/MarketHeader.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { C, pctFmt } from './constants';
import type { PortfolioStats, CorrelationEdge } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// MarketHeader - title, subtitle, connection status, and summary stats
// ---------------------------------------------------------------------------
⋮----
export interface MarketHeaderProps {
  stats: PortfolioStats;
  edges: CorrelationEdge[];
  loaded: boolean;
  connectionStatus: 'live' | 'stale' | 'offline';
  lastUpdated: Date | null;
  time: Date;
  formatTime: (d: Date) => string;
  formatDate: (d: Date) => string;
  onRefresh?: () => void;
  t: (ar: string, en: string) => string;
}
⋮----
className=
⋮----
{/* Connection status badge */}
</file>

<file path="frontend/src/app/markets/components/MobileCard.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { C, fmt, pctFmt } from './constants';
import { Sparkline } from './Sparkline';
import { StatBadge } from './StatBadge';
import type { Instrument } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// MobileCard - simplified card for small screens
// ---------------------------------------------------------------------------
⋮----
export interface MobileCardProps {
  inst: Instrument;
  language: 'ar' | 'en';
}
⋮----
className=
⋮----
</file>

<file path="frontend/src/app/markets/components/MobileSummary.tsx">
import React from 'react';
import { C, pctFmt } from './constants';
import { MobileCard } from './MobileCard';
import type { Instrument, CorrelationEdge, PortfolioStats } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// MobileSummary - mobile summary card, instrument grid, and top correlations
// ---------------------------------------------------------------------------
⋮----
export interface MobileSummaryProps {
  instruments: Instrument[];
  edges: CorrelationEdge[];
  stats: PortfolioStats;
  language: 'ar' | 'en';
  t: (ar: string, en: string) => string;
}
⋮----
{/* Summary card */}
⋮----
{/* Instrument cards grid */}
⋮----
{/* Top correlations */}
</file>

<file path="frontend/src/app/markets/components/Sparkline.tsx">
import React, { useId } from 'react';
import { C } from './constants';
⋮----
// ---------------------------------------------------------------------------
// Sparkline SVG
// ---------------------------------------------------------------------------
⋮----
export interface SparklineProps {
  data: number[] | undefined;
  positive: boolean;
  width?: number;
  height?: number;
}
</file>

<file path="frontend/src/app/markets/components/StatBadge.tsx">
import React from 'react';
import { C } from './constants';
⋮----
// ---------------------------------------------------------------------------
// StatBadge - small label + value metric display
// ---------------------------------------------------------------------------
⋮----
export interface StatBadgeProps {
  label: string;
  value: string;
  color?: string;
}
</file>

<file path="frontend/src/app/markets/loading.tsx">
{/* Shimmer keyframes */}
⋮----
{/* Header skeleton */}
⋮----
{/* Desktop constellation skeleton */}
⋮----
{/* Orbit ring skeletons */}
⋮----
{/* Orbit rings */}
⋮----
{/* Shimmer edge lines connecting node positions */}
⋮----
x1=
⋮----
x2=
⋮----
{/* Radial lines from center to each node position */}
⋮----
{/* Central hub skeleton */}
⋮----
{/* Inner shimmer elements */}
⋮----
{/* Node skeletons around circle */}
⋮----
{/* Price row */}
⋮----
{/* Sparkline placeholder */}
⋮----
{/* Stats row */}
⋮----
{/* Correlation label skeletons on edges */}
⋮----
{/* Mobile cards skeleton */}
⋮----
{/* Mobile summary card skeleton */}
⋮----
{/* Mobile instrument cards */}
⋮----
{/* Legend skeleton */}
</file>

<file path="frontend/src/app/markets/page.tsx">
import { useMemo } from 'react';
import { useMarketDataLive } from '@/lib/hooks/use-market-data';
import {
  buildMarketGraphModel,
  buildMarketGraphModelFromLiveData,
} from '@/lib/market-graph';
import MarketOverviewClient from './MarketOverviewClient';
import MarketsLoading from './loading';
⋮----
export default function MarketsPage()
</file>

<file path="frontend/src/app/news/[id]/loading.tsx">
export default function ArticleDetailLoading()
⋮----
{/* Breadcrumb skeleton */}
⋮----
{/* Badges row skeleton */}
⋮----
{/* Title skeleton */}
⋮----
{/* Meta row: source badge + date */}
⋮----
{/* Divider */}
⋮----
{/* Body lines skeleton */}
</file>

<file path="frontend/src/app/news/loading.tsx">
function SkeletonCard()
⋮----
{/* Header skeleton */}
⋮----
{/* Search skeleton */}
⋮----
{/* Filter chips skeleton */}
⋮----
{/* Article skeletons */}
</file>

<file path="frontend/src/app/reports/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Document icon */}
</file>

<file path="frontend/src/app/stock/__tests__/StockDetailClient.test.tsx">
/**
 * Tests for StockDetailClient component.
 *
 * The component uses multiple hooks (useStockDetail, useStockFinancials,
 * useStockDividends, useStockFinancialSummary, useNewsByTicker, useReportsByTicker)
 * and several sub-components (CandlestickChart, ChartWrapper, etc.).
 *
 * We mock the hooks via vi.mock() so tests are deterministic and don't
 * depend on MSW or network state. We also mock heavy chart components that
 * rely on canvas / ResizeObserver APIs unavailable in jsdom.
 */
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import React from 'react';
⋮----
// ---------------------------------------------------------------------------
// Module mocks
// ---------------------------------------------------------------------------
⋮----
// Mock next/navigation so useRouter doesn't crash in jsdom
⋮----
// Mock chart components -- they use canvas APIs unavailable in jsdom
⋮----
// Mock OHLCV chart data hook
⋮----
// Mock Toast hook
⋮----
// Mock Tooltip component
⋮----
// Mock stock translations
⋮----
// ---------------------------------------------------------------------------
// Mock domain hooks
// ---------------------------------------------------------------------------
⋮----
// Default mock implementations (will be overridden per test as needed)
⋮----
// ---------------------------------------------------------------------------
// Sample data fixtures
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function makeAsyncResult<T>(overrides: Partial<
⋮----
// Wrap renders in LanguageProvider so useLanguage() works
import { LanguageProvider } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Import component under test (after mocks are declared)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// beforeEach: set default (success) mock returns for all hooks
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------
⋮----
// ---- Loading state -------------------------------------------------------
⋮----
// LoadingSpinner renders a message containing the ticker
⋮----
// ---- Error state ---------------------------------------------------------
⋮----
// ErrorDisplay shows "Something went wrong." with a Retry button
⋮----
// ---- Empty / not found state ---------------------------------------------
⋮----
// The component shows the ticker and "Stock data not available"
⋮----
// ---- Stock header --------------------------------------------------------
⋮----
// Multiple headings may match (visible + print-only), so use getAllByRole
⋮----
// Ticker appears in the badge next to the name
⋮----
// ---- Price display -------------------------------------------------------
⋮----
// Price: 30.50 SAR
⋮----
// Change: +0.50 (1.67%)
⋮----
// ---- Ticker normalization ------------------------------------------------
⋮----
// useStockDetail should be called with "2222.SR"
⋮----
// ---- Financial metrics cards ---------------------------------------------
⋮----
// These labels are rendered in English (default language is 'ar' but the
// LanguageProvider falls back based on stored prefs; we can check for
// SAR currency label which is always present)
⋮----
// Volume card appears in the summary row
// formatNumber(5_000_000) -> "5.0M"
⋮----
// ---- Tab switching -------------------------------------------------------
⋮----
// Find the "Financials" tab (English label)
⋮----
// ---- Watchlist toggle ----------------------------------------------------
⋮----
// The button has an aria-label referencing watchlist
⋮----
// Click to add
⋮----
// After clicking, aria-label should change to "Remove from watchlist"
⋮----
// ---- AI chat CTA ---------------------------------------------------------
⋮----
// The CTA link is always visible and references the stock name or ticker
⋮----
// ---- Breadcrumbs ---------------------------------------------------------
⋮----
// ---- Analyst recommendation section --------------------------------------
⋮----
// "BUY" recommendation label is uppercased in the component
</file>

<file path="frontend/src/app/stock/[ticker]/components/StockDividends.tsx">
import React, { memo } from 'react';
import { cn } from '@/lib/utils';
import { useStockDividends } from '@/lib/hooks/use-api';
import { LoadingSpinner } from '@/components/common/loading-spinner';
⋮----
<MetricCard label=
</file>

<file path="frontend/src/app/stock/[ticker]/components/StockFinancials.tsx">
import React, { memo, useState } from 'react';
import { cn } from '@/lib/utils';
import { useStockFinancials, useStockFinancialSummary } from '@/lib/hooks/use-api';
import { LoadingSpinner } from '@/components/common/loading-spinner';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
type StatementTab = 'income_statement' | 'balance_sheet' | 'cash_flow';
⋮----
function getFieldLabel(key: string, lang: string): string
⋮----
function formatNumber(val: number | null | undefined, opts?:
⋮----
// ---------------------------------------------------------------------------
// MetricCard (shared within this file)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// FinancialStatementsSection
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// FinancialSummarySection
// ---------------------------------------------------------------------------
⋮----
<MetricCard label=
⋮----
// ---------------------------------------------------------------------------
// StockFinancials (public export — combines both sections)
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/app/stock/[ticker]/components/StockNewsSection.tsx">
import React, { memo } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useNewsByTicker } from '@/lib/hooks/use-api';
import { LoadingSpinner } from '@/components/common/loading-spinner';
⋮----
interface StockNewsSectionProps {
  ticker: string;
  language: string;
  t: (ar: string, en: string) => string;
}
</file>

<file path="frontend/src/app/stock/[ticker]/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Chart/stock icon */}
</file>

<file path="frontend/src/app/stock/[ticker]/loading.tsx">
{/* Breadcrumb skeleton */}
⋮----
{/* Company header card skeleton */}
⋮----
{/* Price summary grid skeleton */}
⋮----
{/* Chart placeholder skeleton */}
⋮----
{/* Metrics grid skeleton */}
</file>

<file path="frontend/src/app/stock/[ticker]/page.tsx">
import { StockDetailClient } from './StockDetailClient';
⋮----
interface StockPageProps {
  params: { ticker: string };
}
⋮----
export async function generateMetadata(
⋮----
export default function StockPage(
</file>

<file path="frontend/src/app/watchlist/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Star/bookmark icon */}
</file>

<file path="frontend/src/components/auth/AccessDenied.tsx">
import Link from 'next/link';
import type { Role } from '@/types/auth';
⋮----
interface AccessDeniedProps {
  requiredRole?: Role;
}
⋮----
/**
 * Dark-gold themed access denied page.
 */
⋮----
{/* Shield icon */}
</file>

<file path="frontend/src/components/auth/index.ts">

</file>

<file path="frontend/src/components/auth/PermissionGuard.tsx">
import type { ReactNode } from 'react';
import { useRBACAuth } from '@/contexts/AuthContext';
⋮----
interface PermissionGuardProps {
  action: string;
  resource: string;
  children: ReactNode;
  /** Rendered when user lacks permission. Defaults to null (hidden). */
  fallback?: ReactNode;
}
⋮----
/** Rendered when user lacks permission. Defaults to null (hidden). */
⋮----
/**
 * Renders children only if the current user has the specified permission.
 */
export function PermissionGuard({
  action,
  resource,
  children,
  fallback = null,
}: PermissionGuardProps)
</file>

<file path="frontend/src/components/auth/RoleGuard.tsx">
import type { ReactNode } from 'react';
import type { Role } from '@/types/auth';
import { useRBACAuth } from '@/contexts/AuthContext';
import { AccessDenied } from './AccessDenied';
⋮----
interface RoleGuardProps {
  minimumRole: Role;
  children: ReactNode;
  /** Rendered when user does not meet the role. Defaults to AccessDenied page. */
  fallback?: ReactNode;
}
⋮----
/** Rendered when user does not meet the role. Defaults to AccessDenied page. */
⋮----
/**
 * Renders children only if the current user meets the minimum role level.
 */
export function RoleGuard({
  minimumRole,
  children,
  fallback,
}: RoleGuardProps)
</file>

<file path="frontend/src/components/charts/__tests__/CandlestickChart.test.tsx">
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { render, screen } from '@testing-library/react';
import { createMockOHLCVData } from '@/test/chart-test-utils';
⋮----
// Use vi.hoisted so the mock object is available when vi.mock factory runs
⋮----
// Mock lightweight-charts using the hoisted variable
⋮----
// Mock next/dynamic to render the underlying component directly
⋮----
// Import the named export (not the dynamic default export) so we test the real component
import { CandlestickChart } from '../CandlestickChart';
⋮----
// ChartSkeleton renders a shimmer animation container
⋮----
// Should NOT render the toolbar or chart container
⋮----
// Should render the toolbar with MA20, MA50, Vol buttons
⋮----
// Should render ticker
⋮----
// Should have a div with dir="ltr" for the chart wrapper
</file>

<file path="frontend/src/components/charts/__tests__/ChartWrapper.test.tsx">
import { describe, it, expect } from 'vitest';
import { render, screen } from '@testing-library/react';
import { ChartWrapper } from '../ChartWrapper';
⋮----
// Should not have the header flex row
⋮----
// But children still render
</file>

<file path="frontend/src/components/charts/__tests__/DataSourceBadge.test.tsx">
import { describe, it, expect } from 'vitest';
import { render, screen } from '@testing-library/react';
import { DataSourceBadge } from '../DataSourceBadge';
</file>

<file path="frontend/src/components/charts/chart-config.ts">
import { ColorType, CrosshairMode, type DeepPartial, type ChartOptions } from 'lightweight-charts';
</file>

<file path="frontend/src/components/charts/ChartErrorBoundary.tsx">
import { Component } from 'react';
import type { ErrorInfo, ReactNode } from 'react';
import { ChartError } from './ChartError';
⋮----
interface ChartErrorBoundaryProps {
  children: ReactNode;
  fallbackHeight?: number;
  onError?: (error: Error, errorInfo: ErrorInfo) => void;
}
⋮----
interface ChartErrorBoundaryState {
  hasError: boolean;
}
⋮----
export class ChartErrorBoundary extends Component<ChartErrorBoundaryProps, ChartErrorBoundaryState> {
⋮----
constructor(props: ChartErrorBoundaryProps)
⋮----
static getDerivedStateFromError(): ChartErrorBoundaryState
⋮----
componentDidCatch(error: Error, errorInfo: ErrorInfo): void
⋮----
render()
</file>

<file path="frontend/src/components/charts/ChartWrapper.tsx">
import type { ReactNode } from 'react';
import { cn } from '@/lib/utils';
import { DataSourceBadge } from './DataSourceBadge';
import type { DataSource } from './chart-types';
⋮----
interface ChartWrapperProps {
  title?: string;
  source: DataSource | null;
  children: ReactNode;
  className?: string;
}
⋮----
<div className=
</file>

<file path="frontend/src/components/charts/tasi/ChartExportButton.tsx">
import React, { memo } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
import type { IChartApi } from 'lightweight-charts';
import type { OHLCVData } from '../chart-types';
⋮----
interface ChartExportButtonProps {
  chartRef: React.RefObject<IChartApi | null>;
  data: OHLCVData[] | null;
  period: string;
}
⋮----
function exportCSV(data: OHLCVData[], period: string)
⋮----
const handleScreenshot = () =>
⋮----
const handleCSVExport = () =>
</file>

<file path="frontend/src/components/charts/tasi/IndicatorToggleBar.tsx">
import React, { memo } from 'react';
import { cn } from '@/lib/utils';
import { MA20_COLOR, MA50_COLOR } from '../chart-config';
import { useLanguage } from '@/providers/LanguageProvider';
import type { ChartType } from './useChartIndicators';
⋮----
interface IndicatorToggleBarProps {
  showMA20: boolean;
  showMA50: boolean;
  chartType: ChartType;
  onToggleMA20: () => void;
  onToggleMA50: () => void;
  onSetChartType: (type: ChartType) => void;
}
⋮----
{/* MA toggles */}
⋮----
{/* Chart type toggle */}
⋮----
onClick=
⋮----
aria-label=
⋮----
className=
</file>

<file path="frontend/src/components/charts/tasi/PeriodSelector.tsx">
import React, { memo, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
export type PeriodValue = (typeof PERIODS)[number]['value'];
⋮----
interface PeriodSelectorProps {
  period: string;
  onPeriodChange: (period: string) => void;
}
⋮----
className=
</file>

<file path="frontend/src/components/charts/tasi/useChartIndicators.ts">
import { useState, useCallback } from 'react';
⋮----
export type ChartType = 'candlestick' | 'line' | 'area';
⋮----
export interface ChartIndicators {
  showMA20: boolean;
  showMA50: boolean;
  chartType: ChartType;
  toggleMA20: () => void;
  toggleMA50: () => void;
  setChartType: (type: ChartType) => void;
}
⋮----
export function useChartIndicators(): ChartIndicators
</file>

<file path="frontend/src/components/charts/TradingViewAttribution.tsx">
import { cn } from '@/lib/utils';
⋮----
interface TradingViewAttributionProps {
  className?: string;
}
⋮----
export function TradingViewAttribution(
</file>

<file path="frontend/src/components/charts/TradingViewWidget.tsx">
import { useEffect, useRef, memo } from 'react';
⋮----
export interface TradingViewWidgetProps {
  /** Stock symbol in TradingView format (e.g., "TADAWUL:2222" for Aramco) */
  symbol: string;
  /** Chart interval: "1" | "3" | "5" | "15" | "30" | "60" | "120" | "180" | "240" | "D" | "W" | "M" */
  interval?: string;
  /** Chart theme: "light" | "dark" */
  theme?: 'light' | 'dark';
  /** Chart height in pixels */
  height?: number;
  /** Enable symbol search/change */
  allowSymbolChange?: boolean;
  /** Hide top toolbar */
  hideTopToolbar?: boolean;
  /** Hide side toolbar */
  hideSideToolbar?: boolean;
  /** Hide volume indicator */
  hideVolume?: boolean;
  /** Enable save image button */
  enableSaveImage?: boolean;
  /** Custom container class */
  className?: string;
}
⋮----
/** Stock symbol in TradingView format (e.g., "TADAWUL:2222" for Aramco) */
⋮----
/** Chart interval: "1" | "3" | "5" | "15" | "30" | "60" | "120" | "180" | "240" | "D" | "W" | "M" */
⋮----
/** Chart theme: "light" | "dark" */
⋮----
/** Chart height in pixels */
⋮----
/** Enable symbol search/change */
⋮----
/** Hide top toolbar */
⋮----
/** Hide side toolbar */
⋮----
/** Hide volume indicator */
⋮----
/** Enable save image button */
⋮----
/** Custom container class */
⋮----
/**
 * TradingView Advanced Chart Widget
 * Free embeddable widget from TradingView with full candlestick charting capabilities.
 * Supports TADAWUL symbols (Saudi Stock Exchange).
 *
 * @see https://www.tradingview.com/widget-docs/widgets/charts/advanced-chart/
 */
function TradingViewWidget({
  symbol,
  interval = 'D',
  theme = 'dark',
  height = 600,
  allowSymbolChange = true,
  hideTopToolbar = false,
  hideSideToolbar = false,
  hideVolume = false,
  enableSaveImage = true,
  className = '',
}: TradingViewWidgetProps)
⋮----
// Clear previous content
⋮----
// Create widget container div
⋮----
// Create configuration script
⋮----
// Widget configuration
⋮----
style: '1', // Candlestick style
⋮----
// Cleanup function
⋮----
// Memoize to prevent unnecessary re-renders
</file>

<file path="frontend/src/components/charts/useChart.ts">
import { useRef, useEffect, useCallback } from 'react';
import { createChart, type IChartApi, type DeepPartial, type ChartOptions } from 'lightweight-charts';
import { RAID_CHART_OPTIONS } from './chart-config';
⋮----
interface UseChartOptions {
  options?: DeepPartial<ChartOptions>;
}
⋮----
export function useChart(containerRef: React.RefObject<HTMLDivElement | null>, opts?: UseChartOptions)
</file>

<file path="frontend/src/components/chat/HelpPanel.tsx">
import React, { memo, useState } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Help panel data
// ---------------------------------------------------------------------------
⋮----
interface HelpCategory {
  titleAr: string;
  titleEn: string;
  icon: string;
  examples: { ar: string; en: string; queryAr: string; queryEn: string }[];
}
⋮----
// ---------------------------------------------------------------------------
// Icon helper
// ---------------------------------------------------------------------------
⋮----
function HelpCategoryIcon(
⋮----
// ---------------------------------------------------------------------------
// HelpPanel component
// ---------------------------------------------------------------------------
⋮----
interface HelpPanelProps {
  onSuggestionClick: (query: string) => void;
}
⋮----
onClick=
⋮----
className=
</file>

<file path="frontend/src/components/chat/hooks/useConversationHistory.ts">
import { useState, useEffect, useCallback } from 'react';
import type { ChatMessage } from '@/lib/types';
⋮----
// ---------------------------------------------------------------------------
// Storage keys
// ---------------------------------------------------------------------------
⋮----
export interface SavedConversation {
  id: string;
  title: string;
  timestamp: string;
  messageCount: number;
}
⋮----
// ---------------------------------------------------------------------------
// Storage helpers
// ---------------------------------------------------------------------------
⋮----
export function getConversationList(): SavedConversation[]
⋮----
export function saveConversationList(list: SavedConversation[])
⋮----
} catch { /* ignore */ }
⋮----
export function getActiveConvId(): string | null
⋮----
export function setActiveConvId(id: string | null)
⋮----
} catch { /* ignore */ }
⋮----
export function getConvStorageKey(id: string)
⋮----
// ---------------------------------------------------------------------------
// Hook
// ---------------------------------------------------------------------------
⋮----
export interface UseConversationHistoryReturn {
  conversations: SavedConversation[];
  historyOpen: boolean;
  setHistoryOpen: (open: boolean) => void;
  handleLoadConversation: (convId: string) => void;
  handleDeleteConversation: (convId: string, e: React.MouseEvent) => void;
  handleNewConversation: () => void;
}
⋮----
export function useConversationHistory(
  messages: ChatMessage[],
  clearMessages: () => void,
): UseConversationHistoryReturn
⋮----
// Persist current conversation when messages change (non-streaming)
⋮----
// Load conversation list when history panel opens
⋮----
} catch { /* ignore */ }
⋮----
try { localStorage.removeItem(getConvStorageKey(convId)); } catch { /* ignore */ }
⋮----
} catch { /* ignore */ }
</file>

<file path="frontend/src/components/chat/MessageThread.tsx">
import React, { memo } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { MessageBubble } from './MessageBubble';
import type { ChatMessage } from '@/lib/types';
⋮----
// ---------------------------------------------------------------------------
// Follow-up suggestion logic
// ---------------------------------------------------------------------------
⋮----
function getFollowUpSuggestions(lastAssistant: ChatMessage | undefined, language: string): string[]
⋮----
// ---------------------------------------------------------------------------
// MessageThread component
// ---------------------------------------------------------------------------
⋮----
interface MessageThreadProps {
  messages: ChatMessage[];
  isLoading: boolean;
  progressText?: string;
  onSuggestionClick: (query: string) => void;
  onRetryLast: () => void;
  messagesEndRef: React.RefObject<HTMLDivElement>;
}
⋮----
className=
</file>

<file path="frontend/src/components/common/Breadcrumb.tsx">
import Link from 'next/link';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
export interface BreadcrumbItem {
  label: string;
  href?: string;
}
⋮----
interface BreadcrumbProps {
  items: BreadcrumbItem[];
}
⋮----
function ChevronSeparator()
</file>

<file path="frontend/src/components/common/error-boundary.tsx">
import React, { Component, type ErrorInfo, type ReactNode } from 'react';
⋮----
interface ErrorBoundaryProps {
  children: ReactNode;
  fallback?: ReactNode;
}
⋮----
interface ErrorBoundaryState {
  hasError: boolean;
  error: Error | null;
}
⋮----
export class ErrorBoundary extends Component<ErrorBoundaryProps, ErrorBoundaryState> {
⋮----
constructor(props: ErrorBoundaryProps)
⋮----
static getDerivedStateFromError(error: Error): ErrorBoundaryState
⋮----
componentDidCatch(error: Error, errorInfo: ErrorInfo)
⋮----
render()
</file>

<file path="frontend/src/components/common/loading-spinner.tsx">
import { cn } from '@/lib/utils';
⋮----
interface LoadingSpinnerProps {
  message?: string;
  className?: string;
}
⋮----
<div className=
</file>

<file path="frontend/src/components/common/OfflineBanner.tsx">
import { useState, useEffect, useRef } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
import { cn } from '@/lib/utils';
⋮----
const handleOffline = () =>
⋮----
const handleOnline = () =>
⋮----
// Check initial state
⋮----
className=
⋮----
</file>

<file path="frontend/src/components/common/RetryButton.tsx">
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface RetryButtonProps {
  onRetry: () => void;
  label?: string;
  isRetrying?: boolean;
  className?: string;
}
⋮----
className=
</file>

<file path="frontend/src/components/monitoring/ErrorBoundary.tsx">
import React, { Component, type ErrorInfo, type ReactNode } from 'react';
⋮----
import { ErrorFallback } from './ErrorFallback';
⋮----
/**
 * Enhanced error boundary with Sentry reporting and dark-gold themed UI.
 * Use this in place of the basic error boundary when Sentry integration is needed.
 */
⋮----
interface MonitoringErrorBoundaryProps {
  children: ReactNode;
  fallback?: ReactNode | ((props: { error: Error; onReset: () => void }) => ReactNode);
}
⋮----
interface MonitoringErrorBoundaryState {
  hasError: boolean;
  error: Error | null;
  eventId: string | null;
}
⋮----
constructor(props: MonitoringErrorBoundaryProps)
⋮----
static getDerivedStateFromError(error: Error): Partial<MonitoringErrorBoundaryState>
⋮----
componentDidCatch(error: Error, errorInfo: ErrorInfo)
⋮----
// Custom fallback provided
⋮----
// Default fallback with Sentry report button
</file>

<file path="frontend/src/components/monitoring/ErrorFallback.tsx">
/**
 * Default error fallback component for the monitoring ErrorBoundary.
 * Shows a user-friendly error message with Reload and Go Home actions.
 * Dark-gold themed to match the Ra'd AI design system.
 */
interface ErrorFallbackProps {
  error: Error;
  onReset: () => void;
}
⋮----
function sanitizeErrorMessage(error: Error): string
⋮----
// In production, strip stack traces and internal details
⋮----
// Truncate long messages
⋮----
export function ErrorFallback(
⋮----
{/* Warning icon */}
⋮----
{/* Decorative gold line */}
</file>

<file path="frontend/src/components/queries/QuerySuggestions.tsx">
import { useState, useEffect, useRef, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { getSuggestions } from '@/lib/queries/suggestions';
import { queryStore } from '@/lib/queries/query-store';
import type { QueryRecord, QuerySuggestion } from '@/types/queries';
⋮----
interface QuerySuggestionsProps {
  input: string;
  visible: boolean;
  language?: 'ar' | 'en';
  onSelect: (text: string) => void;
  onDismiss: () => void;
  className?: string;
}
⋮----
// Load recent queries once on mount
⋮----
// Debounced suggestion computation
⋮----
// Keyboard navigation
⋮----
// Scroll selected item into view
⋮----
{/* Source icon */}
</file>

<file path="frontend/src/components/ui/Button.tsx">
import { forwardRef, type ButtonHTMLAttributes, type ReactNode } from 'react';
import { cn } from '@/lib/utils';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
type ButtonVariant = 'primary' | 'secondary' | 'destructive' | 'ghost';
type ButtonSize = 'sm' | 'md' | 'lg';
⋮----
interface ButtonProps extends ButtonHTMLAttributes<HTMLButtonElement> {
  variant?: ButtonVariant;
  size?: ButtonSize;
  loading?: boolean;
  children: ReactNode;
}
⋮----
// ---------------------------------------------------------------------------
// Variant styles
// ---------------------------------------------------------------------------
⋮----
// Default sizes for variants that specify their own padding
⋮----
// ---------------------------------------------------------------------------
// Spinner
// ---------------------------------------------------------------------------
⋮----
function Spinner()
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
className=
</file>

<file path="frontend/src/components/ui/index.ts">

</file>

<file path="frontend/src/components/visualization/chart-types/BarChart.tsx">
import {
  ResponsiveContainer,
  BarChart,
  Bar,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
} from 'recharts';
⋮----
interface VisBarChartProps {
  data: Record<string, unknown>[];
  title?: string;
  config?: {
    xKey?: string;
    yKeys?: string[];
  };
}
</file>

<file path="frontend/src/components/visualization/chart-types/index.ts">

</file>

<file path="frontend/src/components/visualization/chart-types/LineChart.tsx">
import {
  ResponsiveContainer,
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
} from 'recharts';
⋮----
interface VisLineChartProps {
  data: Record<string, unknown>[];
  title?: string;
  config?: {
    xKey?: string;
    yKeys?: string[];
  };
}
</file>

<file path="frontend/src/components/visualization/chart-types/PieChart.tsx">
import {
  ResponsiveContainer,
  PieChart,
  Pie,
  Cell,
  Tooltip,
  Legend,
} from 'recharts';
⋮----
interface VisPieChartProps {
  data: Record<string, unknown>[];
  title?: string;
  config?: {
    nameKey?: string;
    valueKey?: string;
  };
}
</file>

<file path="frontend/src/components/visualization/chart-types/ScatterChart.tsx">
import {
  ResponsiveContainer,
  ScatterChart,
  Scatter,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ZAxis,
} from 'recharts';
⋮----
interface VisScatterChartProps {
  data: Record<string, unknown>[];
  title?: string;
  config?: {
    xKey?: string;
    yKey?: string;
    zKey?: string;
  };
}
</file>

<file path="frontend/src/components/visualization/QueryResultView.tsx">
import { useCallback, useRef, useState } from 'react';
import { cn } from '@/lib/utils';
import { AutoChart } from './AutoChart';
import { DataTable } from './DataTable';
import { ResultToolbar, type ViewMode } from './ResultToolbar';
⋮----
interface QueryResultViewProps {
  columns: string[];
  rows: (string | number | null)[][];
  sql?: string;
  executionTime?: number;
  isLoading?: boolean;
  error?: string | null;
  onRetry?: () => void;
  className?: string;
}
⋮----
function toRecords(columns: string[], rows: (string | number | null)[][]): Record<string, unknown>[]
⋮----
const escape = (val: string | number | null) =>
⋮----
{/* Query metadata */}
⋮----
onClick=
⋮----
{/* Collapsible SQL */}
⋮----
{/* Content area */}
⋮----
<div className=
</file>

<file path="frontend/src/components/visualization/ResponsiveWrapper.tsx">
import { cn } from '@/lib/utils';
import { useBreakpoint, type Breakpoint } from '@/lib/hooks/useBreakpoint';
⋮----
interface ResponsiveWrapperProps {
  children: React.ReactNode;
  className?: string;
  /** Render prop alternative for breakpoint-specific layouts */
  render?: (breakpoint: Breakpoint) => React.ReactNode;
}
⋮----
/** Render prop alternative for breakpoint-specific layouts */
⋮----
export function ResponsiveWrapper(
⋮----
return <>
⋮----
className=
⋮----
// Mobile: single column, compact padding
⋮----
// Tablet: two column grid
⋮----
// Desktop: full layout
</file>

<file path="frontend/src/config/auth.ts">
/**
 * Auth configuration for Ra'd AI.
 * Reads from NEXT_PUBLIC_* env vars with sensible defaults.
 */
⋮----
export interface AuthConfig {
  /** Session timeout in seconds (default: 3600 = 1 hour) */
  sessionTimeout: number;
  /** Token refresh interval in seconds (default: 300 = 5 minutes) */
  refreshInterval: number;
  /** Max concurrent sessions per user (default: 3) */
  maxConcurrentSessions: number;
  /** Redirect path after login (default: /chat) */
  authRedirect: string;
  /** Redirect path when unauthenticated (default: /login) */
  loginPath: string;
  /** Seconds before expiry to show warning (default: 300 = 5 minutes) */
  expiryWarningSeconds: number;
}
⋮----
/** Session timeout in seconds (default: 3600 = 1 hour) */
⋮----
/** Token refresh interval in seconds (default: 300 = 5 minutes) */
⋮----
/** Max concurrent sessions per user (default: 3) */
⋮----
/** Redirect path after login (default: /chat) */
⋮----
/** Redirect path when unauthenticated (default: /login) */
⋮----
/** Seconds before expiry to show warning (default: 300 = 5 minutes) */
⋮----
export function getAuthConfig(): AuthConfig
</file>

<file path="frontend/src/config/monitoring.ts">
/**
 * Monitoring configuration for Ra'd AI frontend.
 * Reads from NEXT_PUBLIC_* environment variables.
 */
⋮----
export interface MonitoringConfig {
  sentryDsn: string | undefined;
  tracesSampleRate: number;
  enableWebVitals: boolean;
  enableApiMetrics: boolean;
  metricsReportInterval: number;
}
⋮----
export function getMonitoringConfig(): MonitoringConfig
⋮----
metricsReportInterval: 60_000, // 1 minute
</file>

<file path="frontend/src/config/security.ts">
export interface SecurityConfig {
  allowedHosts: string[];
  csrfEnabled: boolean;
  secureCookies: boolean;
  cspReportUri: string;
}
⋮----
export function getSecurityConfig(): SecurityConfig
</file>

<file path="frontend/src/lib/__tests__/api-modules.test.ts">
/**
 * API client module tests: client-base, stocks, news, auth/watchlists, and legacy shim.
 *
 * Uses vi.fn() to mock global fetch and localStorage.
 */
import { describe, it, expect, vi, beforeEach } from 'vitest';
⋮----
// ---------------------------------------------------------------------------
// Mock fetch globally before any imports
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helper: create a mock fetch response
// ---------------------------------------------------------------------------
⋮----
function jsonResponse(data: unknown, status = 200)
⋮----
function networkError()
⋮----
// ---------------------------------------------------------------------------
// client-base: ApiError
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// client-base: authHeaders
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// client-base: request
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// client-base: qs
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// stocks module
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// news module
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// auth module (watchlists/reports)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Legacy shim: api-client re-exports everything
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/lib/__tests__/api-remaining.test.ts">
/**
 * API client module tests: charts, entities, market, health.
 *
 * Uses vi.fn() to mock global fetch and localStorage.
 */
import { describe, it, expect, vi, beforeEach } from 'vitest';
⋮----
// ---------------------------------------------------------------------------
// Mock fetch globally before any imports
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helper: create a mock fetch response
// ---------------------------------------------------------------------------
⋮----
function jsonResponse(data: unknown, status = 200)
⋮----
function networkError()
⋮----
// ---------------------------------------------------------------------------
// charts module
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// entities module
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// market module
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// health module
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/lib/__tests__/auth-config.test.ts">
import { describe, it, expect, beforeEach, vi } from 'vitest';
⋮----
// Reset module cache so getAuthConfig re-evaluates
</file>

<file path="frontend/src/lib/__tests__/auth-guards.test.tsx">
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { render, screen } from '@testing-library/react';
import React from 'react';
import { PermissionGuard } from '@/components/auth/PermissionGuard';
import { RoleGuard } from '@/components/auth/RoleGuard';
import { AccessDenied } from '@/components/auth/AccessDenied';
⋮----
// Mock the auth context
</file>

<file path="frontend/src/lib/__tests__/auth-types.test.ts">
import { describe, it, expect } from 'vitest';
import {
  hasPermission,
  hasRole,
  ROLE_HIERARCHY,
  ROLE_PERMISSIONS,
  type Role,
} from '@/types/auth';
</file>

<file path="frontend/src/lib/__tests__/news-feed.test.ts">
/**
 * News Feed API client + hook tests.
 *
 * Tests getNewsFeed (api-client) and useNewsFeed (use-api hook)
 * using mocked fetch responses.
 */
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { renderHook, waitFor } from '@testing-library/react';
⋮----
// ---------------------------------------------------------------------------
// Mock fetch globally
// ---------------------------------------------------------------------------
⋮----
// Also stub localStorage for authHeaders
⋮----
// ---------------------------------------------------------------------------
// Helper: create a successful fetch response
// ---------------------------------------------------------------------------
⋮----
function jsonResponse(data: unknown, status = 200)
⋮----
// ---------------------------------------------------------------------------
// Sample data
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// getNewsFeed tests
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// useNewsFeed hook tests
// ---------------------------------------------------------------------------
⋮----
// Initially loading
</file>

<file path="frontend/src/lib/__tests__/session-manager.test.ts">
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { SessionManager } from '@/lib/auth/session';
⋮----
// Clear cookies
⋮----
// After ending, cookie is cleared so getSessionInfo returns null
⋮----
// Advance time by 10 seconds
⋮----
// Default timeout is 3600 seconds
⋮----
// Warning should fire at (3600 - 300) = 3300 seconds
</file>

<file path="frontend/src/lib/__tests__/tradingview-utils.test.ts">
import { describe, it, expect } from 'vitest';
import {
  formatTASISymbol,
  extractTicker,
  isValidTASITicker,
  getTASIStockName,
} from '../tradingview-utils';
</file>

<file path="frontend/src/lib/__tests__/use-auth.test.tsx">
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { renderHook, act, waitFor } from '@testing-library/react';
import React from 'react';
import { AuthProvider, useAuth } from '@/lib/hooks/use-auth';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/**
 * Build a minimal JWT with the given payload. The signature is fake but the
 * format (header.payload.signature) is valid enough for decodeJwtPayload.
 */
function makeJwt(payload: Record<string, unknown>, expOffsetSeconds = 3600): string
⋮----
const encode = (obj: object)
⋮----
/** Build a JWT expiring very soon (within 120 s). */
function makeExpiringJwt(): string
⋮----
const wrapper = ({ children }: { children: React.ReactNode }) => (
  <AuthProvider>{children}</AuthProvider>
);
⋮----
// ---------------------------------------------------------------------------
// localStorage helpers
// ---------------------------------------------------------------------------
⋮----
function clearStorage()
⋮----
// ---------------------------------------------------------------------------
// fetch mock factory
// ---------------------------------------------------------------------------
⋮----
function mockFetchOk(body: object)
⋮----
function mockFetchError(status: number, message = 'Error')
⋮----
// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 1. Provider renders children
// -------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 2. Initial state (unauthenticated, no stored session)
// -------------------------------------------------------------------------
⋮----
// -------------------------------------------------------------------------
// 3. Hydration from localStorage on mount
// -------------------------------------------------------------------------
⋮----
// Mock refresh endpoint so the background refresh doesn't cause issues
⋮----
const storedUser = { id: 'u1', email: 'stored@test.com', name: 'Stored' }; // no isGuest
⋮----
// -------------------------------------------------------------------------
// 4. Login flow
// -------------------------------------------------------------------------
⋮----
// /api/auth/me enrichment
⋮----
global.fetch = mockFetchOk({ user_id: 'u1' }); // no tokens
⋮----
// -------------------------------------------------------------------------
// 5. Logout
// -------------------------------------------------------------------------
⋮----
// Ensure user is loaded
⋮----
// -------------------------------------------------------------------------
// 6. Guest login
// -------------------------------------------------------------------------
⋮----
global.fetch = mockFetchOk({ refresh_token: 'rt-only' }); // no access_token
⋮----
// -------------------------------------------------------------------------
// 7. isTokenExpiringSoon helper (tested indirectly via refresh behaviour)
// -------------------------------------------------------------------------
⋮----
const freshToken = makeJwt({}, 3600); // expires in 1 hour
⋮----
// Give the component a tick to fire the immediate refresh check
⋮----
// The refresh endpoint should NOT have been called because token is fresh
⋮----
const expiringToken = makeExpiringJwt(); // expires in 60 s
⋮----
// Refresh endpoint should have been called
⋮----
// New token should be persisted
⋮----
// -------------------------------------------------------------------------
// 8. 401 on refresh logs user out
// -------------------------------------------------------------------------
⋮----
// After a 401 from refresh, user should be null
⋮----
// -------------------------------------------------------------------------
// 9. Profile enrichment
// -------------------------------------------------------------------------
⋮----
// Profile enrichment is async — wait for it
⋮----
// /api/auth/me should NOT have been called for guests
⋮----
// -------------------------------------------------------------------------
// 10. localStorage quota handling
// -------------------------------------------------------------------------
⋮----
// Loading will complete despite the storage throwing on initial hydration
⋮----
// login should not throw even with quota errors
⋮----
// In-memory user state should still be set
⋮----
// -------------------------------------------------------------------------
// 11. useAuth throws outside provider
// -------------------------------------------------------------------------
</file>

<file path="frontend/src/lib/__tests__/useMarketIndex.test.ts">
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { renderHook, waitFor } from '@testing-library/react';
import { useState, useEffect } from 'react';
⋮----
// Mock the api-client module
⋮----
// Mock chart-utils
⋮----
// Mock useChartCache with a React-state-aware implementation that triggers re-renders
⋮----
}, [key ? JSON.stringify(key) : null]); // eslint-disable-line
⋮----
// Import after mocks are set up
import { useMarketIndex } from '@/lib/hooks/use-chart-data';
⋮----
mockGetTasiIndex.mockImplementation(() => new Promise(() => {})); // never resolves
</file>

<file path="frontend/src/lib/api/auth.ts">
/**
 * Auth, reports, announcements, and watchlist API types and functions.
 */
⋮----
import { request, qs } from './client-base';
import type { PaginatedResponse } from './news';
⋮----
// Re-export PaginatedResponse so consumers can import it from here too
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface WatchlistItem {
  id: string;
  user_id: string;
  name: string;
  tickers: string[];
}
⋮----
export interface ReportItem {
  id: string;
  ticker: string | null;
  title: string;
  summary: string | null;
  author: string | null;
  source_name: string | null;
  source_url: string | null;
  published_at: string | null;
  recommendation: string | null;
  target_price: number | null;
  current_price_at_report: number | null;
  report_type: string | null;
  created_at: string | null;
}
⋮----
export interface AnnouncementItem {
  id: string;
  ticker: string | null;
  title_ar: string | null;
  title_en: string | null;
  body_ar: string | null;
  body_en: string | null;
  source: string | null;
  announcement_date: string | null;
  category: string | null;
  classification: string | null;
  is_material: boolean;
  source_url: string | null;
  created_at: string | null;
}
⋮----
export type ReportListResponse = PaginatedResponse<ReportItem>;
export type AnnouncementListResponse = PaginatedResponse<AnnouncementItem>;
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getReports(params?: {
  page?: number;
  page_size?: number;
  recommendation?: string;
  report_type?: string;
  search?: string;
}, signal?: AbortSignal): Promise<ReportListResponse>
⋮----
export function getReportsByTicker(
  ticker: string,
  params?: { page?: number; page_size?: number },
  signal?: AbortSignal,
): Promise<ReportListResponse>
⋮----
export function getAnnouncements(params?: {
  page?: number;
  page_size?: number;
  ticker?: string;
  category?: string;
  source?: string;
}, signal?: AbortSignal): Promise<AnnouncementListResponse>
⋮----
export function getWatchlists(signal?: AbortSignal): Promise<WatchlistItem[]>
⋮----
export function createWatchlist(body: {
  name?: string;
  tickers?: string[];
}): Promise<WatchlistItem>
⋮----
export function updateWatchlist(
  id: string,
  body: { name?: string; tickers?: string[] },
): Promise<WatchlistItem>
⋮----
export function deleteWatchlist(id: string): Promise<void>
</file>

<file path="frontend/src/lib/api/charts.ts">
/**
 * Chart data API types and functions (pre-built analytics charts).
 */
⋮----
import { request, qs } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface ChartDataPoint {
  label: string;
  value: number;
}
⋮----
export interface ChartResponse {
  chart_type: string;
  title: string;
  data: ChartDataPoint[];
}
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getChartSectorMarketCap(signal?: AbortSignal): Promise<ChartResponse>
⋮----
export function getChartTopCompanies(params?: {
  limit?: number;
  sector?: string;
}, signal?: AbortSignal): Promise<ChartResponse>
⋮----
export function getChartSectorPE(signal?: AbortSignal): Promise<ChartResponse>
⋮----
export function getChartDividendYieldTop(params?: {
  limit?: number;
}, signal?: AbortSignal): Promise<ChartResponse>
</file>

<file path="frontend/src/lib/api/client-base.ts">
/**
 * Core API client infrastructure: error class, fetch wrapper, auth headers, cache.
 *
 * All requests use relative paths (e.g. /api/v1/...) so they are proxied
 * through Next.js rewrites (next.config.mjs) to the backend, avoiding CORS.
 */
⋮----
import { API_BASE, API_TIMEOUT_MS, API_CACHE_TTL_MS } from '../config';
⋮----
// ---------------------------------------------------------------------------
// Error class
// ---------------------------------------------------------------------------
⋮----
export class ApiError extends Error {
⋮----
constructor(
    public status: number,
    public statusText: string,
    public body?: string,
)
⋮----
// Encode status in a parseable prefix so ErrorDisplay can extract it
⋮----
/** Get a user-friendly error message. */
getUserMessage(): string
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
export function authHeaders(): Record<string, string>
⋮----
export async function request<T>(
  path: string,
  init?: RequestInit,
  timeoutMs: number = API_TIMEOUT_MS,
  externalSignal?: AbortSignal,
): Promise<T>
⋮----
// If an external signal is provided, abort our controller when it fires.
⋮----
onExternalAbort = ()
⋮----
// Re-throw as-is if cancelled by external signal (not a timeout)
⋮----
export function qs(params: Record<string, string | number | undefined | null>): string
⋮----
// ---------------------------------------------------------------------------
// In-memory cache for frequently accessed, relatively static data
// ---------------------------------------------------------------------------
⋮----
export async function cachedRequest<T>(path: string, ttlMs: number = API_CACHE_TTL_MS, signal?: AbortSignal): Promise<T>
</file>

<file path="frontend/src/lib/api/entities.ts">
/**
 * Entity/company API types and functions.
 */
⋮----
import { request, cachedRequest, qs } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface CompanySummary {
  ticker: string;
  short_name: string | null;
  sector: string | null;
  industry: string | null;
  current_price: number | null;
  market_cap: number | null;
  change_pct: number | null;
  week_52_high: number | null;
  week_52_low: number | null;
}
⋮----
export interface EntityListResponse {
  items: CompanySummary[];
  count: number;
}
⋮----
export interface CompanyDetail {
  ticker: string;
  short_name: string | null;
  sector: string | null;
  industry: string | null;
  exchange: string | null;
  currency: string | null;
  current_price: number | null;
  previous_close: number | null;
  day_high: number | null;
  day_low: number | null;
  week_52_high: number | null;
  week_52_low: number | null;
  volume: number | null;
  market_cap: number | null;
  beta: number | null;
  trailing_pe: number | null;
  forward_pe: number | null;
  price_to_book: number | null;
  trailing_eps: number | null;
  roe: number | null;
  profit_margin: number | null;
  revenue_growth: number | null;
  recommendation: string | null;
  target_mean_price: number | null;
  analyst_count: number | null;
}
⋮----
export interface SectorInfo {
  sector: string;
  company_count: number;
}
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getEntities(params?: {
  limit?: number;
  offset?: number;
  sector?: string;
  search?: string;
}, signal?: AbortSignal): Promise<EntityListResponse>
⋮----
export function getEntityDetail(ticker: string, signal?: AbortSignal): Promise<CompanyDetail>
⋮----
export function getSectors(signal?: AbortSignal): Promise<SectorInfo[]>
</file>

<file path="frontend/src/lib/api/health.ts">
/**
 * Health check API types and functions.
 */
⋮----
import { request } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface HealthComponentResponse {
  name: string;
  status: string;
  latency_ms: number | null;
  message: string;
}
⋮----
export interface HealthResponse {
  status: string;
  components: HealthComponentResponse[];
}
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getHealth(signal?: AbortSignal): Promise<HealthResponse>
</file>

<file path="frontend/src/lib/api/index.ts">
/**
 * Barrel re-export for the API client modules.
 *
 * New code should import from '@/lib/api' (or specific sub-modules).
 * The legacy '@/lib/api-client' path re-exports from here for backward compat.
 */
</file>

<file path="frontend/src/lib/api/market.ts">
/**
 * Market analytics API types and functions.
 */
⋮----
import { request, qs } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface MarketMover {
  ticker: string;
  company_name_ar: string;
  company_name_en: string;
  current_price: number;
  previous_close: number;
  change_pct: number;
  volume: number;
  sector: string;
}
⋮----
export interface MarketSummary {
  total_market_cap: number;
  total_volume: number;
  gainers_count: number;
  losers_count: number;
  unchanged_count: number;
  top_gainers: MarketMover[];
  top_losers: MarketMover[];
}
⋮----
export interface SectorPerformance {
  sector: string;
  avg_change_pct: number;
  total_volume: number;
  total_market_cap: number;
  company_count: number;
  gainers: number;
  losers: number;
}
⋮----
export interface HeatmapItem {
  ticker: string;
  name: string;
  sector: string;
  market_cap: number;
  change_pct: number;
}
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getMarketMovers(
  type: 'gainers' | 'losers',
  limit?: number,
  signal?: AbortSignal,
): Promise<MarketMover[]>
⋮----
export function getMarketSummary(signal?: AbortSignal): Promise<MarketSummary>
⋮----
export function getSectorPerformance(signal?: AbortSignal): Promise<SectorPerformance[]>
⋮----
export function getMarketHeatmap(signal?: AbortSignal): Promise<HeatmapItem[]>
⋮----
// ---------------------------------------------------------------------------
// Market Overview (World 360)
// ---------------------------------------------------------------------------
⋮----
export interface MarketOverviewInstrument {
  key: string;
  ticker: string;
  nameAr: string;
  nameEn: string;
  category: string;
  value: number | null;
  change: number | null;
  sparkline: number[];
  historical_closes: number[];
  currency: string;
  error?: string | null;
}
⋮----
export interface MarketOverviewResponse {
  instruments: MarketOverviewInstrument[];
  timestamp: string;
  count: number;
}
⋮----
export function getMarketOverview(signal?: AbortSignal): Promise<MarketOverviewResponse>
</file>

<file path="frontend/src/lib/api/news.ts">
/**
 * News-related API types and functions.
 */
⋮----
import { request, cachedRequest, qs } from './client-base';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface NewsArticle {
  id: string;
  ticker: string | null;
  title: string;
  body: string | null;
  source_name: string | null;
  source_url: string | null;
  published_at: string | null;
  sentiment_score: number | null;
  sentiment_label: string | null;
  language: string;
  created_at: string | null;
}
⋮----
export interface PaginatedResponse<T> {
  items: T[];
  total: number;
  page: number;
  page_size: number;
  total_pages: number;
}
⋮----
export type NewsListResponse = PaginatedResponse<NewsArticle>;
⋮----
export interface NewsFeedItem {
  id: string;
  ticker: string | null;
  title: string;
  body: string | null;
  source_name: string;
  source_url: string | null;
  published_at: string | null;
  sentiment_score: number | null;
  sentiment_label: string | null;
  priority: number;
  language: string;
  created_at: string | null;
}
⋮----
export interface NewsFeedResponse {
  items: NewsFeedItem[];
  total: number;
  page: number;
  limit: number;
}
⋮----
export interface NewsSourceInfo {
  source_name: string;
  count: number;
}
⋮----
export interface NewsSourcesResponse {
  sources: NewsSourceInfo[];
}
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export function getNews(params?: {
  page?: number;
  page_size?: number;
  language?: string;
}, signal?: AbortSignal): Promise<NewsListResponse>
⋮----
export function getNewsByTicker(
  ticker: string,
  params?: { page?: number; page_size?: number; sentiment?: string },
  signal?: AbortSignal,
): Promise<NewsListResponse>
⋮----
export function getNewsFeed(params?: {
  limit?: number;
  offset?: number;
  source?: string;
  sentiment?: string;
  date_from?: string;
  date_to?: string;
}, signal?: AbortSignal): Promise<NewsFeedResponse>
⋮----
export function getNewsArticle(id: string, signal?: AbortSignal): Promise<NewsFeedItem>
⋮----
export function searchNewsFeed(
  params: {
    q: string;
    limit?: number;
    offset?: number;
    source?: string;
    sentiment?: string;
    date_from?: string;
    date_to?: string;
  },
  signal?: AbortSignal,
): Promise<NewsFeedResponse>
⋮----
export function getNewsFeedByIds(ids: string[], signal?: AbortSignal): Promise<NewsFeedResponse>
⋮----
export function getNewsSources(signal?: AbortSignal): Promise<NewsSourcesResponse>
</file>

<file path="frontend/src/lib/api/stocks.ts">
/**
 * Stock data API types and functions: OHLCV, dividends, financials, comparison, quotes.
 */
⋮----
import { request, qs } from './client-base';
import type { OHLCVData } from '@/components/charts/chart-types';
⋮----
// Re-export OHLCVData from its canonical location for consumers that
// previously imported it from api-client.
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface StockOHLCVResponse {
  data: OHLCVData[];
  source: 'real' | 'mock' | 'cached';
  last_updated: string | null;
  symbol: string;
  period: string;
  count: number;
}
⋮----
export interface TasiIndexResponse {
  data: OHLCVData[];
  source: 'real' | 'mock' | 'cached';
  data_freshness: 'real-time' | 'cached' | 'stale' | 'mock';
  cache_age_seconds: number | null;
  last_updated: string | null;
  symbol: string;
  period: string;
  count: number;
}
⋮----
export interface StockDividends {
  ticker: string;
  dividend_rate: number | null;
  dividend_yield: number | null;
  payout_ratio: number | null;
  five_year_avg_dividend_yield: number | null;
  ex_dividend_date: string | null;
  last_dividend_value: number | null;
  last_dividend_date: string | null;
  trailing_annual_dividend_rate: number | null;
  trailing_annual_dividend_yield: number | null;
}
⋮----
export interface FinancialSummary {
  ticker: string;
  total_revenue: number | null;
  revenue_per_share: number | null;
  total_cash: number | null;
  total_debt: number | null;
  debt_to_equity: number | null;
  current_ratio: number | null;
  quick_ratio: number | null;
  free_cashflow: number | null;
  ebitda: number | null;
  gross_profit: number | null;
  operating_cashflow: number | null;
}
⋮----
export interface FinancialPeriod {
  period_type: string | null;
  period_index: number | null;
  period_date: string | null;
  data: Record<string, string | number | null>;
}
⋮----
export interface FinancialsResponse {
  ticker: string;
  statement: string;
  periods: FinancialPeriod[];
}
⋮----
/** @deprecated Use FinancialPeriod instead */
export interface FinancialStatement {
  period_type: string;
  period_index: number;
  period_date: string;
  [key: string]: string | number | null;
}
⋮----
export interface StockComparison {
  tickers: Array<{ ticker: string; name: string; metrics: Record<string, number | null> }>;
}
⋮----
export interface BatchQuote {
  ticker: string;
  /** Company short name (backend may return as "name" or "short_name"). */
  name: string;
  short_name?: string | null;
  current_price: number;
  previous_close: number;
  change_pct: number;
  volume: number;
}
⋮----
/** Company short name (backend may return as "name" or "short_name"). */
⋮----
// ---------------------------------------------------------------------------
// API methods
// ---------------------------------------------------------------------------
⋮----
export async function getOHLCVData(
  ticker: string,
  params?: { period?: string },
  signal?: AbortSignal,
): Promise<StockOHLCVResponse>
⋮----
export function getTasiIndex(period: string = '1y', signal?: AbortSignal): Promise<TasiIndexResponse>
⋮----
export function getStockDividends(ticker: string, signal?: AbortSignal): Promise<StockDividends>
⋮----
export function getStockFinancialSummary(ticker: string, signal?: AbortSignal): Promise<FinancialSummary>
⋮----
export function getStockFinancials(
  ticker: string,
  params?: { statement?: string; period_type?: string },
  signal?: AbortSignal,
): Promise<FinancialsResponse>
⋮----
export function compareStocks(
  tickers: string[],
  metrics: string[],
  signal?: AbortSignal,
): Promise<StockComparison>
⋮----
export function getBatchQuotes(tickers: string[], signal?: AbortSignal): Promise<BatchQuote[]>
</file>

<file path="frontend/src/lib/auth/index.ts">

</file>

<file path="frontend/src/lib/auth/session.ts">
/**
 * Session management for Ra'd AI.
 *
 * Tracks session state in a secure cookie, auto-extends on activity,
 * and warns before expiry.
 */
⋮----
import { getAuthConfig } from '@/config/auth';
⋮----
export interface SessionInfo {
  userId: string;
  role: string;
  startedAt: number;
  expiresAt: number;
  lastActivity: number;
}
⋮----
type ExpiryWarningCallback = (secondsRemaining: number) => void;
⋮----
/**
 * Thin cookie helpers. If the security-headers teammate's `@/lib/cookies`
 * module is available at runtime, prefer importing from there instead.
 * These are self-contained fallbacks so the module compiles independently.
 */
function setSessionCookie(name: string, value: string, maxAgeSec: number): void
⋮----
function getSessionCookie(name: string): string | null
⋮----
function deleteSessionCookie(name: string): void
⋮----
export class SessionManager {
⋮----
setCallbacks(opts: {
    onExpiryWarning?: ExpiryWarningCallback;
onSessionExpired?: ()
⋮----
startSession(userId: string, role: string): SessionInfo
⋮----
endSession(): void
⋮----
refreshSession(): SessionInfo | null
⋮----
getSessionInfo(): SessionInfo | null
⋮----
isSessionValid(): boolean
⋮----
// --- internal ---
⋮----
private persist(session: SessionInfo): void
⋮----
private scheduleTimers(session: SessionInfo): void
⋮----
private clearTimers(): void
⋮----
if (this.activityTimer) return; // debounce
⋮----
private listenForActivity(): void
⋮----
private stopActivityListener(): void
⋮----
/** Singleton instance for app-wide use. */
</file>

<file path="frontend/src/lib/chart-cache-provider.tsx">
import { SWRConfig } from 'swr';
import { chartCacheConfig } from './chart-cache';
⋮----
export function ChartCacheProvider(
</file>

<file path="frontend/src/lib/config.ts">
/**
 * Centralized runtime configuration loaded from environment variables.
 *
 * All NEXT_PUBLIC_* values are inlined at build time. Defaults match
 * the development environment so the app works without any .env.local.
 */
⋮----
/** Base URL for API requests (empty string = same-origin, proxied by Next.js rewrites). */
⋮----
/** Default timeout for API requests in milliseconds. */
⋮----
/** Default cache TTL for cachedRequest() in milliseconds. */
⋮----
/** Health check polling interval in milliseconds. */
</file>

<file path="frontend/src/lib/cookies.ts">
export interface CookieOptions {
  /** Max age in seconds */
  maxAge?: number;
  /** Expiry date */
  expires?: Date;
  /** Cookie path (default: "/") */
  path?: string;
  /** Cookie domain */
  domain?: string;
  /** SameSite attribute (default: "Strict") */
  sameSite?: 'Strict' | 'Lax' | 'None';
}
⋮----
/** Max age in seconds */
⋮----
/** Expiry date */
⋮----
/** Cookie path (default: "/") */
⋮----
/** Cookie domain */
⋮----
/** SameSite attribute (default: "Strict") */
⋮----
function isSecureContext(): boolean
⋮----
export function setSecureCookie(
  name: string,
  value: string,
  options?: CookieOptions,
): void
⋮----
export function getSecureCookie(name: string): string | null
⋮----
export function deleteSecureCookie(name: string, options?: Pick<CookieOptions, 'path' | 'domain'>): void
</file>

<file path="frontend/src/lib/csrf.ts">
import { useCallback, useEffect, useState } from 'react';
⋮----
export function generateCsrfToken(): string
⋮----
export function validateCsrfToken(token: string, expected: string): boolean
⋮----
// Constant-time comparison to prevent timing attacks
⋮----
function getCookie(name: string): string | null
⋮----
function setCookie(name: string, value: string): void
⋮----
function ensureCsrfToken(): string
⋮----
export interface UseCsrfReturn {
  token: string;
  headerName: string;
}
⋮----
export function useCsrf(): UseCsrfReturn
⋮----
/**
 * Returns headers object with the CSRF token for use in mutation requests.
 * Call this before POST/PUT/DELETE fetch calls.
 */
export function getCsrfHeaders(): Record<string, string>
⋮----
/**
 * React hook returning a fetch wrapper that automatically includes CSRF headers
 * on mutation requests (POST, PUT, DELETE, PATCH).
 */
export function useCsrfFetch()
</file>

<file path="frontend/src/lib/export/__tests__/exporters.test.ts">
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import type { QueryResults } from '@/types/queries';
⋮----
// We need to mock the download mechanism since jsdom doesn't support it
⋮----
// Store original Blob
⋮----
// Mock Blob to capture content
⋮----
constructor(parts?: BlobPart[], options?: BlobPropertyBag)
⋮----
set download(val: string)
get download()
⋮----
// Import after setup so mocks are in place
async function getExporter()
⋮----
// Dynamic import to pick up mocks
⋮----
function getBlobText(): string
⋮----
// Extract text from blob parts
</file>

<file path="frontend/src/lib/export/exporters.ts">
import type { QueryResults } from '@/types/queries';
⋮----
/**
 * Generate a timestamped filename for exports.
 */
function makeFilename(prefix: string, ext: string): string
⋮----
/**
 * Trigger a browser download from a Blob.
 */
function downloadBlob(blob: Blob, filename: string): void
⋮----
/**
 * Export query results to CSV with BOM for Arabic text support in Excel.
 */
export function exportToCsv(
  data: QueryResults,
  filename?: string
): void
⋮----
const escape = (val: string | number | null): string =>
⋮----
// BOM for Arabic text support
⋮----
/**
 * Export query results to Excel using SheetJS.
 */
export async function exportToExcel(
  data: QueryResults,
  filename?: string
): Promise<void>
⋮----
// Build worksheet data: header row + data rows
⋮----
// Auto-width columns
⋮----
/**
 * Export query results to PDF using jsPDF + jspdf-autotable.
 */
export async function exportToPdf(
  data: QueryResults,
  title?: string,
  filename?: string
): Promise<void>
⋮----
// Title
⋮----
doc.setTextColor(212, 168, 75); // Gold color
⋮----
// Timestamp
⋮----
// Table
⋮----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
⋮----
// Page number footer
</file>

<file path="frontend/src/lib/hooks/useAsyncWithRetry.ts">
import { useCallback, useEffect, useRef, useState } from 'react';
⋮----
interface UseAsyncWithRetryOptions {
  maxRetries?: number;
  retryOn?: number[];
}
⋮----
interface UseAsyncWithRetryResult<T> {
  data: T | null;
  loading: boolean;
  error: string | null;
  refetch: () => void;
  retriesLeft: number;
}
⋮----
/**
 * Wraps an async fetcher with automatic retry logic.
 * Retries on network errors (status 0) and 503 by default,
 * with exponential backoff (1s, 2s, 4s).
 */
export function useAsyncWithRetry<T>(
  fetcher: (signal: AbortSignal) => Promise<T>,
  deps: unknown[] = [],
  options: UseAsyncWithRetryOptions = {},
): UseAsyncWithRetryResult<T>
⋮----
const attempt = () =>
⋮----
// Check if we should retry
⋮----
// Network error: fetch failure, no response
⋮----
const delay = Math.pow(2, retryIndex) * 1000; // 1s, 2s, 4s
⋮----
// eslint-disable-next-line react-hooks/exhaustive-deps
</file>

<file path="frontend/src/lib/hooks/useBreakpoint.ts">
import { useEffect, useState } from 'react';
⋮----
export type Breakpoint = 'mobile' | 'tablet' | 'desktop';
⋮----
function getBreakpoint(): Breakpoint
⋮----
export function useBreakpoint(): Breakpoint
⋮----
const handler = ()
</file>

<file path="frontend/src/lib/hooks/useFormatters.ts">
import { useMemo } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
/**
 * Locale-aware number/date/percent formatters driven by the current language.
 * Uses Intl.NumberFormat and Intl.DateTimeFormat under the hood.
 */
export function useFormatters()
</file>

<file path="frontend/src/lib/hooks/useLocalStorageTTL.ts">
import { useState, useEffect, useCallback } from 'react';
⋮----
// ---------------------------------------------------------------------------
// Storage envelope type
// ---------------------------------------------------------------------------
⋮----
interface TTLEnvelope<T> {
  value: T;
  expiresAt: number;
}
⋮----
// ---------------------------------------------------------------------------
// Pure utility — safe to use outside React (e.g., in getServerSideProps guards)
// ---------------------------------------------------------------------------
⋮----
/**
 * Reads a TTL-wrapped value from localStorage.
 * Returns `defaultValue` when:
 *   - Running on the server (SSR)
 *   - The key is absent
 *   - The stored JSON is malformed
 *   - The TTL has expired (also removes the key)
 */
export function loadLocalStorageWithTTL<T>(key: string, defaultValue: T): T
⋮----
// Malformed JSON or other storage errors — treat as missing
⋮----
// ---------------------------------------------------------------------------
// React hook
// ---------------------------------------------------------------------------
⋮----
/**
 * Stores a value in localStorage with TTL-based expiration.
 *
 * @param key        localStorage key
 * @param defaultValue  Returned when key is absent, expired, or on SSR
 * @param ttlMs      Time-to-live in milliseconds
 * @returns          [value, setValue, clearValue]
 *
 * @example
 * const [recentCharts, setRecentCharts, clearRecentCharts] =
 *   useLocalStorageTTL<string[]>(
 *     'rad-ai-charts-recent',
 *     [],
 *     7 * 24 * 60 * 60 * 1000,  // 7 days
 *   );
 */
export function useLocalStorageTTL<T>(
  key: string,
  defaultValue: T,
  ttlMs: number,
): [T, (val: T) => void, () => void]
⋮----
// Initialise lazily so we only hit localStorage once on mount
⋮----
// Sync state when the key changes (e.g., navigating between pages)
⋮----
// eslint-disable-next-line react-hooks/exhaustive-deps
⋮----
// Storage quota exceeded or other write errors — update state only
⋮----
// Ignore removal errors
⋮----
// eslint-disable-next-line react-hooks/exhaustive-deps
</file>

<file path="frontend/src/lib/market-graph/data.ts">
import type { AssetCategory, RawInstrument } from './types';
⋮----
// ---------------------------------------------------------------------------
// Static instrument data (fallback when API is unavailable)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Instrument metadata map (Arabic/English names + category for each key)
// ---------------------------------------------------------------------------
⋮----
export interface InstrumentMeta {
  nameAr: string;
  nameEn: string;
  category: AssetCategory;
}
</file>

<file path="frontend/src/lib/market-graph/index.ts">
import type { MarketGraphModel, RawInstrument } from './types';
import { RAW_INSTRUMENTS } from './data';
import { computeMetrics, computeEdges, portfolioStats, computeMetricsFromHistorical, computeEdgesEnhanced } from './quant';
import { layoutNodes, toPosMap, computeEdgeLabelPositions } from './layout';
⋮----
// ---------------------------------------------------------------------------
// Canvas dimensions (virtual coordinate space)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Build a complete graph model from static data (fallback)
// ---------------------------------------------------------------------------
⋮----
export function buildMarketGraphModel(threshold = 0.25): MarketGraphModel
⋮----
// ---------------------------------------------------------------------------
// Build a complete graph model from live data with historical series
// ---------------------------------------------------------------------------
⋮----
export function buildMarketGraphModelFromLiveData(
  rawInstruments: RawInstrument[],
  historicalData: Record<string, number[]>,
  threshold = 0.25,
): MarketGraphModel
</file>

<file path="frontend/src/lib/market-graph/layout.ts">
import type { AssetCategory, Instrument, CorrelationEdge, NodePosition, EdgeLabel } from './types';
⋮----
// ---------------------------------------------------------------------------
// Category ordering for clockwise layout starting from top (-90 deg)
// Saudi (top), US Index (right), Energy (bottom-right),
// Commodity (bottom-left), Crypto (left)
// ---------------------------------------------------------------------------
⋮----
/** Degrees of empty arc between category groups */
⋮----
/** Minimum distance between any two nodes (pixels) */
⋮----
// ---------------------------------------------------------------------------
// Compute node positions in a circular constellation layout,
// grouped by category with gaps between groups
// ---------------------------------------------------------------------------
⋮----
export function layoutNodes(
  instruments: Instrument[],
  cx: number,
  cy: number,
  maxR: number,
): NodePosition[]
⋮----
// --- volatility range for radial offset ---
⋮----
// --- group instruments by category (preserving original order within each) ---
⋮----
else groups.set(inst.category, [inst]); // unknown category fallback
⋮----
// Remove empty groups so gaps are only between populated categories
⋮----
// --- assign angles per instrument ---
⋮----
let cursor = -90; // start at top
⋮----
// --- push apart any overlapping nodes ---
⋮----
// ---------------------------------------------------------------------------
// Build a lookup map from key to position
// ---------------------------------------------------------------------------
⋮----
export function toPosMap(positions: NodePosition[]): Record<string, NodePosition>
⋮----
// ---------------------------------------------------------------------------
// Compute edge label positions avoiding center and collisions
// ---------------------------------------------------------------------------
⋮----
export function computeEdgeLabelPositions(
  edges: CorrelationEdge[],
  posMap: Record<string, NodePosition>,
  cx: number,
  cy: number,
  hubRadius: number,
): EdgeLabel[]
</file>

<file path="frontend/src/lib/market-graph/quant.ts">
import type { RawInstrument, Instrument, CorrelationEdge, PortfolioStats } from './types';
⋮----
// ---------------------------------------------------------------------------
// Core statistical functions
// ---------------------------------------------------------------------------
⋮----
export function logReturns(prices: number[] | undefined | null): number[]
⋮----
export function mean(arr: number[]): number
⋮----
export function stdDev(arr: number[]): number
⋮----
export function pearsonCorr(a: number[], b: number[]): number
⋮----
export function betaCalc(assetReturns: number[], marketReturns: number[]): number
⋮----
export function annualizedVol(r: number[]): number
⋮----
export function sharpeRatio(r: number[]): number
⋮----
// ---------------------------------------------------------------------------
// Rank-based (Spearman) correlation
// ---------------------------------------------------------------------------
⋮----
export function spearmanCorr(a: number[], b: number[]): number
⋮----
function toRanks(arr: number[]): number[]
⋮----
// ---------------------------------------------------------------------------
// Confidence based on sample size
// ---------------------------------------------------------------------------
⋮----
export function correlationConfidence(sampleSize: number): 'low' | 'medium' | 'high'
⋮----
// ---------------------------------------------------------------------------
// Compute derived metrics for all instruments
// ---------------------------------------------------------------------------
⋮----
export function computeMetrics(raw: RawInstrument[]): Instrument[]
⋮----
// ---------------------------------------------------------------------------
// Compute correlation edges between instruments
// ---------------------------------------------------------------------------
⋮----
export function computeEdges(instruments: Instrument[], threshold = 0.25): CorrelationEdge[]
⋮----
// ---------------------------------------------------------------------------
// Compute metrics using longer historical series
// ---------------------------------------------------------------------------
⋮----
export function computeMetricsFromHistorical(
  raw: RawInstrument[],
  historicalData: Record<string, number[]>,
): Instrument[]
⋮----
// ---------------------------------------------------------------------------
// Enhanced edge computation using historical data
// ---------------------------------------------------------------------------
⋮----
export function computeEdgesEnhanced(
  instruments: Instrument[],
  historicalData: Record<string, number[]>,
  threshold = 0.25,
): CorrelationEdge[]
⋮----
// Align to same length
⋮----
// ---------------------------------------------------------------------------
// Aggregate portfolio statistics
// ---------------------------------------------------------------------------
⋮----
export function portfolioStats(instruments: Instrument[]): PortfolioStats
</file>

<file path="frontend/src/lib/market-graph/types.ts">
// ---------------------------------------------------------------------------
// Market Graph types
// ---------------------------------------------------------------------------
⋮----
export type AssetCategory = 'Crypto' | 'Commodity' | 'Energy' | 'US Index' | 'Saudi';
⋮----
export interface RawInstrument {
  key: string;
  nameAr: string;
  nameEn: string;
  value: number;
  change: number;
  category: AssetCategory;
  sparkline: number[];
}
⋮----
export interface Instrument extends RawInstrument {
  returns: number[];
  vol: number;
  sharpe: number;
  beta: number;
}
⋮----
export interface CorrelationEdge {
  from: string;
  to: string;
  rho: number;
  r2: number;
  type: 'positive' | 'inverse';
  pct: number;
  confidence: 'low' | 'medium' | 'high';
  sampleSize?: number;
}
⋮----
export interface PortfolioStats {
  avgReturn: number;
  avgVol: number;
  advancing: number;
  declining: number;
  breadth: number;
  avgAbsCorr: number;
  diversification: number;
}
⋮----
export interface NodePosition {
  key: string;
  x: number;
  y: number;
}
⋮----
export interface EdgeLabel extends CorrelationEdge {
  lx: number;
  ly: number;
}
⋮----
export interface MarketGraphModel {
  instruments: Instrument[];
  edges: CorrelationEdge[];
  stats: PortfolioStats;
  layout: NodePosition[];
  labels: EdgeLabel[];
  generatedAt: number;
}
</file>

<file path="frontend/src/lib/monitoring/index.ts">
/**
 * Monitoring utilities re-exports for Ra'd AI frontend.
 */
</file>

<file path="frontend/src/lib/monitoring/metrics-collector.ts">
/**
 * Frontend metrics collector for Ra'd AI.
 * Session-scoped (in-memory), no persistence.
 * Used by the SWR middleware and admin dashboard.
 */
⋮----
export interface WebVitalEntry {
  name: string;
  value: number;
  rating: 'good' | 'needs-improvement' | 'poor';
  timestamp: number;
}
⋮----
export interface ApiCallEntry {
  url: string;
  duration: number;
  status: number;
  timestamp: number;
}
⋮----
export interface ErrorEntry {
  message: string;
  context?: string;
  timestamp: number;
}
⋮----
export interface PageViewEntry {
  path: string;
  timestamp: number;
}
⋮----
export interface FrontendMetrics {
  pageViews: PageViewEntry[];
  apiCallDurations: ApiCallEntry[];
  errorCount: number;
  errors: ErrorEntry[];
  webVitals: WebVitalEntry[];
  sessionDuration: number;
  sessionStart: number;
}
⋮----
export class FrontendMetricsCollector {
⋮----
constructor()
⋮----
trackPageView(path: string): void
⋮----
trackApiCall(url: string, duration: number, status: number): void
⋮----
trackError(error: Error, context?: string): void
⋮----
trackWebVital(entry: WebVitalEntry): void
⋮----
// Replace existing entry for same metric name
⋮----
getMetrics(): FrontendMetrics
⋮----
/** Singleton instance for use across the application */
</file>

<file path="frontend/src/lib/monitoring/web-vitals.ts">
/**
 * Web Vitals tracking for Ra'd AI frontend.
 * Tracks LCP, FID, CLS, TTFB, FCP, INP and reports to Sentry + metricsCollector.
 *
 * Usage: Call initWebVitals() once in the root layout or a client component
 * that mounts on every page. Do NOT call multiple times.
 *
 * Example (in a client component imported by layout.tsx):
 *   import { initWebVitals } from '@/lib/monitoring/web-vitals';
 *   useEffect(() => { initWebVitals(); }, []);
 */
⋮----
import type { Metric } from 'web-vitals';
⋮----
import { metricsCollector } from './metrics-collector';
import type { WebVitalEntry } from './metrics-collector';
⋮----
function getRating(metric: Metric): 'good' | 'needs-improvement' | 'poor'
⋮----
function handleMetric(metric: Metric): void
⋮----
// Report to Sentry as custom measurement
⋮----
// Store in metrics collector for admin display
⋮----
export async function initWebVitals(): Promise<void>
⋮----
/** Get the latest web vitals data for dashboard display */
export function getWebVitalsData(): WebVitalEntry[]
</file>

<file path="frontend/src/lib/performance/utils.ts">
/**
 * General-purpose performance utilities.
 */
⋮----
// ---------------------------------------------------------------------------
// debounce
// ---------------------------------------------------------------------------
⋮----
/**
 * Returns a debounced version of `fn` that delays invocation until `ms`
 * milliseconds have elapsed since the last call.
 */
export function debounce<T extends (...args: unknown[]) => void>(
  fn: T,
  ms: number,
): (...args: Parameters<T>) => void
⋮----
// ---------------------------------------------------------------------------
// throttle
// ---------------------------------------------------------------------------
⋮----
/**
 * Returns a throttled version of `fn` that fires at most once per `ms`
 * milliseconds.
 */
export function throttle<T extends (...args: unknown[]) => void>(
  fn: T,
  ms: number,
): (...args: Parameters<T>) => void
⋮----
// ---------------------------------------------------------------------------
// prefetchRoute
// ---------------------------------------------------------------------------
⋮----
/**
 * Programmatically prefetch a Next.js route.
 * Must be called from a client component (requires window).
 */
export function prefetchRoute(path: string): void
⋮----
// ---------------------------------------------------------------------------
// measureRender
// ---------------------------------------------------------------------------
⋮----
/**
 * Simple performance.mark / performance.measure wrapper.
 * Returns a `stop()` function that ends the measurement and logs the duration.
 *
 * Usage:
 *   const stop = measureRender('MyComponent');
 *   // ... render work ...
 *   stop(); // logs duration
 */
export function measureRender(label: string): () => void
⋮----
// Ignore if marks were cleared
</file>

<file path="frontend/src/lib/queries/__tests__/query-store.test.ts">
import { describe, it, expect, beforeEach, vi } from 'vitest';
⋮----
// Mock the idb module since IndexedDB is not available in jsdom
⋮----
// Import after mock
import { queryStore } from '../query-store';
⋮----
// Sort by execution time ascending
⋮----
// Sort by row count descending
</file>

<file path="frontend/src/lib/queries/__tests__/suggestions.test.ts">
import { describe, it, expect } from 'vitest';
import { getSuggestions } from '../suggestions';
import type { QueryRecord } from '@/types/queries';
⋮----
function makeRecord(query: string, executedAt?: number): QueryRecord
⋮----
// Banking query should not match "market"
⋮----
// This text appears in both recent and popular lists
⋮----
// First result should be from recent if it matches
</file>

<file path="frontend/src/lib/queries/query-store.ts">
import { openDB, type IDBPDatabase } from 'idb';
import type { QueryRecord, QueryHistoryFilters } from '@/types/queries';
⋮----
type QueryDB = IDBPDatabase<{
  queries: {
    key: string;
    value: QueryRecord;
    indexes: {
      'by-executedAt': number;
      'by-isFavorite': number;
    };
  };
}>;
⋮----
function getDB(): Promise<QueryDB>
⋮----
upgrade(db)
⋮----
function generateId(): string
⋮----
/**
   * Add a new query record. Evicts oldest non-favorite records when over MAX_RECORDS.
   */
async addQuery(
    record: Omit<QueryRecord, 'id' | 'isFavorite' | 'tags'>
): Promise<QueryRecord>
⋮----
// FIFO eviction: remove oldest non-favorites if over limit
⋮----
/**
   * Get query history with optional filtering and sorting.
   */
async getHistory(
    filters?: QueryHistoryFilters,
    limit = 50,
    offset = 0
): Promise<QueryRecord[]>
⋮----
// Filter
⋮----
// Sort
⋮----
async getQueryById(id: string): Promise<QueryRecord | undefined>
⋮----
async deleteQuery(id: string): Promise<void>
⋮----
async clearHistory(): Promise<void>
⋮----
async searchHistory(text: string): Promise<QueryRecord[]>
⋮----
async getFavorites(): Promise<QueryRecord[]>
⋮----
async toggleFavorite(id: string): Promise<QueryRecord | undefined>
⋮----
async updateQuery(
    id: string,
    updates: Partial<Pick<QueryRecord, 'name' | 'tags' | 'notes' | 'isFavorite'>>
): Promise<QueryRecord | undefined>
⋮----
async getCount(): Promise<number>
⋮----
async getRecent(limit = 5): Promise<QueryRecord[]>
</file>

<file path="frontend/src/lib/queries/suggestions.ts">
import type { QueryRecord, QuerySuggestion } from '@/types/queries';
⋮----
/** Popular TASI queries available as suggestions */
⋮----
/**
 * Simple fuzzy matching: checks if all words in the input appear (in order or not)
 * somewhere in the target string.
 */
function fuzzyMatch(input: string, target: string): number
⋮----
// Exact substring match gets highest score
⋮----
// Word-level matching
⋮----
/**
 * Get query suggestions based on input text and recent query history.
 * Returns max 8 suggestions, prioritizing recent queries over popular ones.
 */
export function getSuggestions(
  input: string,
  recentQueries: QueryRecord[],
  language: 'ar' | 'en' = 'en',
  maxResults = 8
): QuerySuggestion[]
⋮----
// If no input, show most recent queries
⋮----
// Score and rank recent queries
⋮----
// Score and rank popular queries
⋮----
// Sort each group by score descending
⋮----
// Interleave: recent queries first (up to 5), then popular (up to 3)
</file>

<file path="frontend/src/lib/tradingview-utils.ts">
/**
 * Utility functions for TradingView widget integration
 */
⋮----
/**
 * Convert TASI stock ticker to TradingView symbol format
 * @param ticker - Saudi stock ticker (e.g., "2222", "1120")
 * @returns TradingView symbol format (e.g., "TADAWUL:2222")
 *
 * @example
 * formatTASISymbol("2222") // "TADAWUL:2222" (Aramco)
 * formatTASISymbol("1120") // "TADAWUL:1120" (Al Rajhi Bank)
 */
export function formatTASISymbol(ticker: string): string
⋮----
// Remove any whitespace
⋮----
// If already in TADAWUL:XXXX format, return as-is
⋮----
// If it's just a ticker number, prepend TADAWUL:
⋮----
/**
 * Extract ticker from TradingView symbol format
 * @param symbol - TradingView symbol (e.g., "TADAWUL:2222")
 * @returns Plain ticker (e.g., "2222")
 *
 * @example
 * extractTicker("TADAWUL:2222") // "2222"
 * extractTicker("2222") // "2222"
 */
export function extractTicker(symbol: string): string
⋮----
/**
 * Validate if a string is a valid TASI ticker
 * TASI tickers are typically 4-digit numbers
 * @param ticker - Ticker to validate
 * @returns true if valid TASI ticker format
 */
export function isValidTASITicker(ticker: string): boolean
⋮----
// TASI tickers are 4-digit numbers
⋮----
/**
 * Get display name for common TASI stocks
 * @param ticker - Stock ticker
 * @returns Display name or ticker if not found
 */
export function getTASIStockName(ticker: string): string
</file>

<file path="frontend/src/lib/utils.ts">
import { clsx, type ClassValue } from 'clsx';
import { twMerge } from 'tailwind-merge';
⋮----
/**
 * Merge Tailwind CSS classes with proper conflict resolution.
 * Combines clsx for conditional classes with tailwind-merge for deduplication.
 */
export function cn(...inputs: ClassValue[])
</file>

<file path="frontend/src/middleware.ts">
import { NextRequest, NextResponse } from 'next/server';
⋮----
function getAllowedHosts(): string[]
⋮----
export function middleware(request: NextRequest)
⋮----
// Add X-Request-ID to all responses
⋮----
// Skip host validation in development
⋮----
// Match all paths except Next.js internals and static files
</file>

<file path="frontend/src/providers/ThemeProvider.tsx">
import { createContext, useContext, useEffect, useState } from 'react';
⋮----
type Theme = 'light' | 'dark';
⋮----
interface ThemeContextValue {
  theme: Theme;
  toggleTheme: () => void;
  setTheme: (theme: Theme) => void;
}
⋮----
export function ThemeProvider(
⋮----
const toggleTheme = () =>
⋮----
const setTheme = (t: Theme) =>
⋮----
export function useTheme()
</file>

<file path="frontend/src/test/chart-test-utils.ts">
import { vi } from 'vitest';
import type { OHLCVData, LineDataPoint } from '@/components/charts/chart-types';
⋮----
// ---------------------------------------------------------------------------
// lightweight-charts mock
// ---------------------------------------------------------------------------
⋮----
export function createLightweightChartsMock()
⋮----
// ---------------------------------------------------------------------------
// Mock data factories
// ---------------------------------------------------------------------------
⋮----
export function createMockOHLCVData(count: number = 10): OHLCVData[]
⋮----
export function createMockLineData(count: number = 10): LineDataPoint[]
</file>

<file path="frontend/src/test/integration/chart-data-flow.test.tsx">
/**
 * Integration tests for chart data hooks with MSW.
 *
 * Tests the full data flow: hook -> api-client -> fetch -> MSW handler,
 * including fallback behavior when the API returns errors.
 */
import { describe, it, expect, beforeAll, afterEach, afterAll } from 'vitest';
import { renderHook, waitFor } from '@testing-library/react';
import React from 'react';
import { SWRConfig } from 'swr';
import { server } from '../msw-server';
import { errorHandlers } from '../msw-handlers';
import {
  useMarketIndex,
  useOHLCVData,
  usePriceTrend,
  useMiniChartData,
} from '@/lib/hooks/use-chart-data';
⋮----
// ---------------------------------------------------------------------------
// MSW lifecycle
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// SWR wrapper that disables caching between tests
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// useMarketIndex
// ---------------------------------------------------------------------------
⋮----
// Initially loading
⋮----
// Data should be LineDataPoint[] (mapped from OHLCV close prices)
⋮----
// Should fall through to mock data, not surface an error
⋮----
// ---------------------------------------------------------------------------
// useOHLCVData
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// usePriceTrend
// ---------------------------------------------------------------------------
⋮----
// Each point should be { time, value } derived from close
⋮----
// ---------------------------------------------------------------------------
// useMiniChartData
// ---------------------------------------------------------------------------
⋮----
// Source depends on whether any returned data falls within last 30 days.
// With MSW data from June 2025, these dates may be in the past relative
// to the current date, so the hook may filter down to 0 real results and
// fall back to mock. Either outcome is valid.
</file>

<file path="frontend/src/test/integration/chart-page-integration.test.tsx">
/**
 * Integration tests for chart-related page components.
 *
 * Since chart components rely on lightweight-charts (canvas API, unavailable in jsdom),
 * these tests focus on the surrounding UI: DataSourceBadge, ChartWrapper,
 * ChartError with retry, and TradingViewAttribution.
 *
 * We test the composition components directly rather than trying to render
 * the full Next.js pages (which use dynamic imports and router context).
 */
import { describe, it, expect, vi, beforeAll, afterEach, afterAll } from 'vitest';
import { render, screen, fireEvent } from '@testing-library/react';
import React from 'react';
import { server } from '../msw-server';
⋮----
// Components imported directly (not through the next/dynamic barrel)
import { DataSourceBadge } from '@/components/charts/DataSourceBadge';
import { ChartWrapper } from '@/components/charts/ChartWrapper';
import { ChartError } from '@/components/charts/ChartError';
import { ChartEmpty } from '@/components/charts/ChartEmpty';
import { TradingViewAttribution } from '@/components/charts/TradingViewAttribution';
⋮----
// ---------------------------------------------------------------------------
// MSW lifecycle (needed for completeness, even though these tests are
// mostly component-level -- MSW ensures no accidental real fetches)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// DataSourceBadge
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// ChartWrapper
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// ChartError
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// ChartEmpty
// ---------------------------------------------------------------------------
⋮----
// ChartEmpty should render some indicator that there's no data
⋮----
// ---------------------------------------------------------------------------
// TradingViewAttribution
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Composition: ChartWrapper + ChartError + retry flow
// ---------------------------------------------------------------------------
⋮----
// First render: error state
⋮----
// Click retry
⋮----
// Rerender with success state
</file>

<file path="frontend/src/test/integration/swr-cache.test.tsx">
/**
 * Integration tests for SWR caching behavior.
 *
 * Verifies that SWR deduplication prevents redundant network requests
 * and that mutate() triggers a refetch.
 */
import { describe, it, expect, beforeAll, afterEach, afterAll } from 'vitest';
import { renderHook, waitFor, act } from '@testing-library/react';
import React from 'react';
import { SWRConfig } from 'swr';
import { http, HttpResponse } from 'msw';
import { server } from '../msw-server';
import { useMarketIndex, useOHLCVData } from '@/lib/hooks/use-chart-data';
⋮----
// ---------------------------------------------------------------------------
// MSW lifecycle
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helper: SWR wrapper with deduplication enabled (default 60s from chart-cache)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// SWR deduplication tests
// ---------------------------------------------------------------------------
⋮----
// Render two hooks with the same key in the same SWR provider
⋮----
// Both hooks should have data
⋮----
// SWR deduplication should mean only 1 request was made
// Note: SWR dedup depends on key matching. Since the hooks use
// useCallback with the same period, they produce the same key.
// In the worst case, the fetcher inside the hook is a new reference,
// but SWR deduplicates by key, so at most 1 actual fetch per key window.
⋮----
// Two different tickers = two separate requests
⋮----
// ---------------------------------------------------------------------------
// SWR mutate/refetch tests
// ---------------------------------------------------------------------------
⋮----
// Trigger refetch
⋮----
// Data should still be present after refetch
</file>

<file path="frontend/src/test/msw-handlers.ts">
/**
 * MSW (Mock Service Worker) request handlers for integration tests.
 *
 * Provides realistic mock responses for the Ra'd AI backend chart APIs.
 */
import { http, HttpResponse, delay } from 'msw';
⋮----
// ---------------------------------------------------------------------------
// Realistic mock data
// ---------------------------------------------------------------------------
⋮----
function generateTasiIndexData(count: number = 20)
⋮----
function generateOHLCVData(ticker: string, count: number = 30)
⋮----
// ---------------------------------------------------------------------------
// Success handlers (default)
// ---------------------------------------------------------------------------
⋮----
// TASI index
⋮----
// OHLCV per ticker
⋮----
// Sectors
⋮----
// Entities
⋮----
// ---------------------------------------------------------------------------
// Error handlers (override for error scenarios)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Combined default handlers
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/test/msw-server.ts">
/**
 * MSW server instance for Node/jsdom tests (vitest).
 */
import { setupServer } from 'msw/node';
import { handlers } from './msw-handlers';
</file>

<file path="frontend/src/test/setup.ts">
// Mock ResizeObserver (not available in jsdom)
class ResizeObserverMock {
⋮----
observe()
unobserve()
disconnect()
</file>

<file path="frontend/src/types/auth.ts">
/**
 * RBAC types and permission utilities for Ra'd AI.
 *
 * Role hierarchy: admin > analyst > viewer
 */
⋮----
export type Role = 'admin' | 'analyst' | 'viewer';
⋮----
export interface Permission {
  action: string;
  resource: string;
}
⋮----
export interface AuthUser {
  id: string;
  email: string;
  name: string;
  role: Role;
}
⋮----
/**
 * Ordered from most to least privileged.
 * Used by hasRole() for hierarchy checks.
 */
⋮----
/**
 * Check if a role has a specific permission (action + resource).
 */
export function hasPermission(
  role: Role,
  action: string,
  resource: string,
): boolean
⋮----
/**
 * Check if `userRole` meets or exceeds the `minimumRole` in the hierarchy.
 * admin >= analyst >= viewer
 */
export function hasRole(userRole: Role, minimumRole: Role): boolean
⋮----
// Lower index = higher privilege
</file>

<file path="frontend/src/types/plotly.d.ts">
import { Component } from 'react';
⋮----
interface PlotParams {
    data: Plotly.Data[];
    layout?: Partial<Plotly.Layout>;
    config?: Partial<Plotly.Config>;
    style?: React.CSSProperties;
    className?: string;
    useResizeHandler?: boolean;
    onInitialized?: (figure: Readonly<{ data: Plotly.Data[]; layout: Partial<Plotly.Layout> }>, graphDiv: HTMLElement) => void;
    onUpdate?: (figure: Readonly<{ data: Plotly.Data[]; layout: Partial<Plotly.Layout> }>, graphDiv: HTMLElement) => void;
  }
⋮----
export default class Plot extends Component<PlotParams>
⋮----
interface Data {
    [key: string]: unknown;
    type?: string;
    x?: unknown[];
    y?: unknown[];
    name?: string;
  }
⋮----
interface Layout {
    [key: string]: unknown;
    title?: string | { text: string };
    paper_bgcolor?: string;
    plot_bgcolor?: string;
    font?: { color?: string; family?: string; size?: number };
    margin?: { t?: number; r?: number; b?: number; l?: number };
    xaxis?: Record<string, unknown>;
    yaxis?: Record<string, unknown>;
  }
⋮----
interface Config {
    responsive?: boolean;
    displayModeBar?: boolean;
    modeBarButtonsToRemove?: string[];
    displaylogo?: boolean;
  }
</file>

<file path="frontend/src/types/queries.ts">
/**
 * Types for query history, saved queries, and export features.
 */
⋮----
/** A single query execution record stored in IndexedDB */
export interface QueryRecord {
  id: string;
  naturalLanguageQuery: string;
  generatedSql: string;
  results: QueryResults | null;
  executedAt: number; // Unix timestamp in milliseconds
  executionTimeMs: number;
  rowCount: number;
  isFavorite: boolean;
  tags: string[];
  name?: string;
  notes?: string;
}
⋮----
executedAt: number; // Unix timestamp in milliseconds
⋮----
/** Tabular result set from a query */
export interface QueryResults {
  columns: string[];
  rows: (string | number | null)[][];
}
⋮----
/** Sort field options for query history */
export type QuerySortField = 'executedAt' | 'executionTimeMs' | 'rowCount';
⋮----
/** Sort direction */
export type SortDirection = 'asc' | 'desc';
⋮----
/** Filter options for query history listing */
export interface QueryHistoryFilters {
  searchText?: string;
  favoritesOnly?: boolean;
  sortField?: QuerySortField;
  sortDirection?: SortDirection;
}
⋮----
/** A suggestion item for the query input */
export interface QuerySuggestion {
  text: string;
  source: 'recent' | 'popular';
  id?: string; // QueryRecord id for recent queries
}
⋮----
id?: string; // QueryRecord id for recent queries
</file>

<file path="frontend/tsconfig.json">
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}
</file>

<file path="frontend/vitest.config.ts">
/// <reference types="vitest" />
import { defineConfig } from 'vitest/config';
import react from '@vitejs/plugin-react';
import path from 'path';
</file>

<file path="infrastructure/database/backup.sh">
#!/usr/bin/env bash
# =============================================================================
# Ra'd AI - PostgreSQL Backup Script
# =============================================================================
# Creates a compressed custom-format pg_dump, verifies integrity, and enforces
# a 30-day retention policy.
#
# Usage:
#   ./backup.sh                          # Uses env vars or defaults
#   ./backup.sh /custom/backup/dir       # Override backup directory
#
# Environment variables (all optional, sensible defaults provided):
#   PGHOST       - PostgreSQL host       (default: localhost)
#   PGPORT       - PostgreSQL port       (default: 5432)
#   PGDATABASE   - Database name         (default: tasi_platform)
#   PGUSER       - Database user         (default: tasi_user)
#   PGPASSWORD   - Database password     (reads from .pgpass if not set)
#   BACKUP_DIR   - Backup directory      (default: /var/backups/raid-ai)
#   RETENTION_DAYS - Days to keep backups (default: 30)
#
# Cron example (daily at 02:00):
#   0 2 * * * /opt/raid-ai/infrastructure/database/backup.sh >> /var/log/raid-ai-backup.log 2>&1
# =============================================================================

set -euo pipefail

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
PGHOST="${PGHOST:-localhost}"
PGPORT="${PGPORT:-5432}"
PGDATABASE="${PGDATABASE:-tasi_platform}"
PGUSER="${PGUSER:-tasi_user}"
BACKUP_DIR="${1:-${BACKUP_DIR:-/var/backups/raid-ai}}"
RETENTION_DAYS="${RETENTION_DAYS:-30}"
TIMESTAMP="$(date +%Y%m%d_%H%M%S)"
BACKUP_FILE="${BACKUP_DIR}/${PGDATABASE}_${TIMESTAMP}.dump"
LOG_PREFIX="[backup]"

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
log()  { echo "${LOG_PREFIX} $(date '+%Y-%m-%d %H:%M:%S') $*"; }
fail() { log "ERROR: $*" >&2; exit 1; }

# ---------------------------------------------------------------------------
# Pre-flight checks
# ---------------------------------------------------------------------------
command -v pg_dump    >/dev/null 2>&1 || fail "pg_dump not found in PATH"
command -v pg_restore >/dev/null 2>&1 || fail "pg_restore not found in PATH"

mkdir -p "${BACKUP_DIR}" || fail "Cannot create backup directory: ${BACKUP_DIR}"

log "Starting backup of ${PGDATABASE}@${PGHOST}:${PGPORT}"
log "Backup file: ${BACKUP_FILE}"

# ---------------------------------------------------------------------------
# Step 1: Create compressed custom-format dump
# ---------------------------------------------------------------------------
# -Fc  = custom format (compressed, supports selective restore)
# -v   = verbose (logs table names as they are dumped)
# -Z5  = compression level 5 (good balance of speed vs size)
# --no-owner       = omit ownership commands (portable across environments)
# --no-privileges  = omit GRANT/REVOKE (re-applied by deploy scripts)
# ---------------------------------------------------------------------------
log "Running pg_dump..."
pg_dump \
    -h "${PGHOST}" \
    -p "${PGPORT}" \
    -U "${PGUSER}" \
    -d "${PGDATABASE}" \
    -Fc \
    -Z5 \
    --no-owner \
    --no-privileges \
    -f "${BACKUP_FILE}" \
    || fail "pg_dump failed with exit code $?"

BACKUP_SIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
log "Dump created: ${BACKUP_FILE} (${BACKUP_SIZE})"

# ---------------------------------------------------------------------------
# Step 2: Verify backup integrity
# ---------------------------------------------------------------------------
# pg_restore --list reads the TOC (table of contents) from the dump file.
# If the file is corrupt or truncated, this will fail.
# ---------------------------------------------------------------------------
log "Verifying backup integrity..."
pg_restore --list "${BACKUP_FILE}" > /dev/null 2>&1 \
    || fail "Backup verification FAILED - dump file may be corrupt: ${BACKUP_FILE}"

TABLE_COUNT=$(pg_restore --list "${BACKUP_FILE}" 2>/dev/null | grep -c "TABLE" || true)
log "Verification passed (${TABLE_COUNT} table entries in TOC)"

# ---------------------------------------------------------------------------
# Step 3: Enforce retention policy (delete backups older than N days)
# ---------------------------------------------------------------------------
log "Enforcing ${RETENTION_DAYS}-day retention policy..."
DELETED_COUNT=0
while IFS= read -r old_file; do
    rm -f "${old_file}"
    log "  Deleted: $(basename "${old_file}")"
    DELETED_COUNT=$((DELETED_COUNT + 1))
done < <(find "${BACKUP_DIR}" -name "${PGDATABASE}_*.dump" -type f -mtime "+${RETENTION_DAYS}" 2>/dev/null)

if [ "${DELETED_COUNT}" -eq 0 ]; then
    log "No expired backups to remove"
else
    log "Removed ${DELETED_COUNT} backup(s) older than ${RETENTION_DAYS} days"
fi

# ---------------------------------------------------------------------------
# Summary
# ---------------------------------------------------------------------------
REMAINING=$(find "${BACKUP_DIR}" -name "${PGDATABASE}_*.dump" -type f | wc -l)
log "Backup complete. ${REMAINING} backup(s) in ${BACKUP_DIR}"
log "Done."
</file>

<file path="infrastructure/database/health_checks.sql">
-- =============================================================================
-- Ra'd AI - PostgreSQL Health Check Queries
-- =============================================================================
-- Standalone diagnostic queries for monitoring database health. Run these
-- manually via psql or integrate them into monitoring dashboards (Grafana,
-- Datadog, etc.).
--
-- Usage:
--   psql -U tasi_user -d tasi_platform -f health_checks.sql
--   psql -U tasi_user -d tasi_platform -c "< paste individual query >"
-- =============================================================================


-- ===========================================================================
-- 1. Active Connections
-- ===========================================================================
-- Shows all active connections grouped by state and application.
-- Alert if total connections exceed 80% of max_connections.
-- ---------------------------------------------------------------------------
SELECT
    '=== ACTIVE CONNECTIONS ===' AS section;

SELECT
    state,
    usename AS user,
    application_name AS app,
    client_addr AS ip,
    COUNT(*) AS count
FROM pg_stat_activity
WHERE pid <> pg_backend_pid()
GROUP BY state, usename, application_name, client_addr
ORDER BY count DESC;

-- Connection utilization percentage
SELECT
    numbackends AS active_connections,
    current_setting('max_connections')::int AS max_connections,
    ROUND(numbackends::numeric / current_setting('max_connections')::int * 100, 1) AS utilization_pct
FROM pg_stat_database
WHERE datname = current_database();


-- ===========================================================================
-- 2. Table Bloat Estimate
-- ===========================================================================
-- Estimates dead tuple bloat per table. Tables with >20% dead tuples
-- may benefit from VACUUM FULL or targeted autovacuum tuning.
-- ---------------------------------------------------------------------------
SELECT
    '=== TABLE BLOAT ===' AS section;

SELECT
    schemaname || '.' || relname AS table_name,
    n_live_tup AS live_rows,
    n_dead_tup AS dead_rows,
    CASE
        WHEN n_live_tup + n_dead_tup = 0 THEN 0
        ELSE ROUND(n_dead_tup::numeric / (n_live_tup + n_dead_tup) * 100, 1)
    END AS dead_pct,
    last_vacuum,
    last_autovacuum,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE n_dead_tup > 0
ORDER BY n_dead_tup DESC
LIMIT 20;


-- ===========================================================================
-- 3. Index Usage Statistics
-- ===========================================================================
-- Shows index usage ratios. Indexes with idx_scan = 0 may be unused and
-- candidates for removal (saves write overhead and disk space).
-- ---------------------------------------------------------------------------
SELECT
    '=== INDEX USAGE ===' AS section;

-- Table-level: sequential scans vs index scans
SELECT
    schemaname || '.' || relname AS table_name,
    seq_scan,
    idx_scan,
    CASE
        WHEN seq_scan + idx_scan = 0 THEN 0
        ELSE ROUND(idx_scan::numeric / (seq_scan + idx_scan) * 100, 1)
    END AS idx_usage_pct,
    n_live_tup AS rows
FROM pg_stat_user_tables
WHERE n_live_tup > 100
ORDER BY idx_usage_pct ASC
LIMIT 20;

-- Individual index usage (find unused indexes)
SELECT
    schemaname || '.' || indexrelname AS index_name,
    schemaname || '.' || relname AS table_name,
    idx_scan AS scans,
    idx_tup_read AS rows_read,
    idx_tup_fetch AS rows_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
    AND schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC
LIMIT 20;


-- ===========================================================================
-- 4. Long-Running Queries
-- ===========================================================================
-- Shows queries running longer than 30 seconds. These may be holding locks,
-- consuming resources, or indicating missing indexes.
-- ---------------------------------------------------------------------------
SELECT
    '=== LONG-RUNNING QUERIES ===' AS section;

SELECT
    pid,
    usename AS user,
    state,
    EXTRACT(EPOCH FROM (NOW() - query_start))::int AS runtime_seconds,
    wait_event_type,
    wait_event,
    LEFT(query, 200) AS query_preview
FROM pg_stat_activity
WHERE state = 'active'
    AND pid <> pg_backend_pid()
    AND query_start < NOW() - INTERVAL '30 seconds'
ORDER BY runtime_seconds DESC;


-- ===========================================================================
-- 5. Lock Contention
-- ===========================================================================
-- Shows blocked queries and what is blocking them. Lock contention causes
-- query timeouts and application errors.
-- ---------------------------------------------------------------------------
SELECT
    '=== LOCK CONTENTION ===' AS section;

SELECT
    blocked.pid AS blocked_pid,
    blocked.usename AS blocked_user,
    LEFT(blocked.query, 150) AS blocked_query,
    EXTRACT(EPOCH FROM (NOW() - blocked.query_start))::int AS waiting_seconds,
    blocking.pid AS blocking_pid,
    blocking.usename AS blocking_user,
    LEFT(blocking.query, 150) AS blocking_query
FROM pg_stat_activity AS blocked
JOIN pg_locks AS blocked_locks
    ON blocked.pid = blocked_locks.pid
    AND NOT blocked_locks.granted
JOIN pg_locks AS blocking_locks
    ON blocked_locks.locktype = blocking_locks.locktype
    AND blocked_locks.database IS NOT DISTINCT FROM blocking_locks.database
    AND blocked_locks.relation IS NOT DISTINCT FROM blocking_locks.relation
    AND blocked_locks.page IS NOT DISTINCT FROM blocking_locks.page
    AND blocked_locks.tuple IS NOT DISTINCT FROM blocking_locks.tuple
    AND blocked_locks.virtualxid IS NOT DISTINCT FROM blocking_locks.virtualxid
    AND blocked_locks.transactionid IS NOT DISTINCT FROM blocking_locks.transactionid
    AND blocked_locks.classid IS NOT DISTINCT FROM blocking_locks.classid
    AND blocked_locks.objid IS NOT DISTINCT FROM blocking_locks.objid
    AND blocked_locks.objsubid IS NOT DISTINCT FROM blocking_locks.objsubid
    AND blocking_locks.granted
JOIN pg_stat_activity AS blocking
    ON blocking.pid = blocking_locks.pid
WHERE blocked.pid <> blocking.pid
ORDER BY waiting_seconds DESC;


-- ===========================================================================
-- 6. Cache Hit Ratio
-- ===========================================================================
-- Shows the buffer cache hit ratio. Values below 99% for an OLTP workload
-- indicate that shared_buffers may be too small or working set exceeds memory.
-- ---------------------------------------------------------------------------
SELECT
    '=== CACHE HIT RATIO ===' AS section;

-- Database-level cache hit ratio
SELECT
    datname AS database,
    blks_hit,
    blks_read,
    CASE
        WHEN blks_hit + blks_read = 0 THEN 100
        ELSE ROUND(blks_hit::numeric / (blks_hit + blks_read) * 100, 2)
    END AS cache_hit_pct
FROM pg_stat_database
WHERE datname = current_database();

-- Table-level cache hit ratio (top 20 most-accessed tables)
SELECT
    schemaname || '.' || relname AS table_name,
    heap_blks_hit,
    heap_blks_read,
    CASE
        WHEN heap_blks_hit + heap_blks_read = 0 THEN 100
        ELSE ROUND(heap_blks_hit::numeric / (heap_blks_hit + heap_blks_read) * 100, 2)
    END AS cache_hit_pct
FROM pg_statio_user_tables
WHERE heap_blks_hit + heap_blks_read > 0
ORDER BY heap_blks_hit + heap_blks_read DESC
LIMIT 20;

-- Index cache hit ratio
SELECT
    schemaname || '.' || indexrelname AS index_name,
    idx_blks_hit,
    idx_blks_read,
    CASE
        WHEN idx_blks_hit + idx_blks_read = 0 THEN 100
        ELSE ROUND(idx_blks_hit::numeric / (idx_blks_hit + idx_blks_read) * 100, 2)
    END AS cache_hit_pct
FROM pg_statio_user_indexes
WHERE idx_blks_hit + idx_blks_read > 0
ORDER BY idx_blks_hit + idx_blks_read DESC
LIMIT 20;


-- ===========================================================================
-- 7. Table Sizes
-- ===========================================================================
-- Shows total table size (data + indexes + TOAST) for all user tables.
-- ---------------------------------------------------------------------------
SELECT
    '=== TABLE SIZES ===' AS section;

SELECT
    schemaname || '.' || tablename AS table_name,
    pg_size_pretty(pg_total_relation_size(schemaname || '.' || tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname || '.' || tablename)) AS data_size,
    pg_size_pretty(
        pg_total_relation_size(schemaname || '.' || tablename) -
        pg_relation_size(schemaname || '.' || tablename)
    ) AS index_toast_size,
    (SELECT n_live_tup FROM pg_stat_user_tables t
     WHERE t.schemaname = tables.schemaname AND t.relname = tables.tablename) AS est_rows
FROM information_schema.tables AS tables
WHERE table_schema = 'public'
    AND table_type = 'BASE TABLE'
ORDER BY pg_total_relation_size(schemaname || '.' || tablename) DESC;

-- Database total size
SELECT
    pg_size_pretty(pg_database_size(current_database())) AS database_size;


-- ===========================================================================
-- 8. Replication and WAL Status
-- ===========================================================================
-- Shows WAL archiving status and replication lag (if replicas exist).
-- ---------------------------------------------------------------------------
SELECT
    '=== WAL & REPLICATION ===' AS section;

-- Archiver status
SELECT
    archived_count,
    failed_count,
    last_archived_wal,
    last_archived_time,
    last_failed_wal,
    last_failed_time
FROM pg_stat_archiver;

-- Current WAL position
SELECT
    pg_current_wal_lsn() AS current_wal_lsn,
    pg_walfile_name(pg_current_wal_lsn()) AS current_wal_file;

-- Replication slots (if any)
SELECT
    slot_name,
    slot_type,
    active,
    restart_lsn,
    confirmed_flush_lsn
FROM pg_replication_slots;


-- ===========================================================================
-- 9. Summary Dashboard
-- ===========================================================================
-- Single-row summary of key health indicators.
-- ---------------------------------------------------------------------------
SELECT
    '=== HEALTH SUMMARY ===' AS section;

SELECT
    (SELECT numbackends FROM pg_stat_database WHERE datname = current_database()) AS connections,
    (SELECT current_setting('max_connections')::int) AS max_connections,
    (SELECT pg_size_pretty(pg_database_size(current_database()))) AS db_size,
    (SELECT CASE WHEN blks_hit + blks_read = 0 THEN 100
            ELSE ROUND(blks_hit::numeric / (blks_hit + blks_read) * 100, 2)
            END
     FROM pg_stat_database WHERE datname = current_database()) AS cache_hit_pct,
    (SELECT COUNT(*) FROM pg_stat_activity
     WHERE state = 'active' AND query_start < NOW() - INTERVAL '30 seconds'
       AND pid <> pg_backend_pid()) AS long_running_queries,
    (SELECT COUNT(*) FROM pg_locks WHERE NOT granted) AS waiting_locks,
    (SELECT SUM(n_dead_tup) FROM pg_stat_user_tables) AS total_dead_tuples;
</file>

<file path="infrastructure/database/indexes.sql">
-- =============================================================================
-- Ra'd AI - TASI Platform
-- Additional Production Indexes for Audit & Security Tables
-- =============================================================================
-- This file creates indexes that support production query patterns for the
-- query_audit_log and security_events tables. All statements are idempotent
-- (CREATE INDEX IF NOT EXISTS) and safe to re-run.
--
-- CONCURRENTLY indexes are provided as comments because:
--   1. CREATE INDEX CONCURRENTLY cannot run inside a transaction block
--   2. Many migration runners (Flyway, Alembic, psql -f) wrap files in transactions
-- To use CONCURRENTLY, run each statement individually outside a transaction:
--   psql -d raid_ai -c "CREATE INDEX CONCURRENTLY IF NOT EXISTS ..."
-- =============================================================================


-- ===========================================================================
-- PREREQUISITE: security_events table
-- ===========================================================================
-- The security_events table may not exist yet (created by audit-logger).
-- We create it here so the indexes can be applied independently.
-- If the audit-logger creates it first, this is a no-op (IF NOT EXISTS).
-- ---------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS security_events (
    id              UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    event_type      TEXT NOT NULL,           -- 'auth_failure', 'rate_limit', 'sql_injection', 'forbidden_table', etc.
    severity        TEXT NOT NULL DEFAULT 'medium',  -- 'low', 'medium', 'high', 'critical'
    user_id         UUID REFERENCES users(id) ON DELETE SET NULL,
    ip_address      INET,
    user_agent      TEXT,
    request_path    TEXT,
    request_method  TEXT,
    detail          JSONB,                   -- event-specific payload (blocked query, error, etc.)
    timestamp       TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


-- ===========================================================================
-- PREREQUISITE: request_id column on query_audit_log
-- ===========================================================================
-- The original schema.sql does not include request_id. We add it here so the
-- index can be created. If it already exists, the DO block is a no-op.
-- ---------------------------------------------------------------------------
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM information_schema.columns
        WHERE table_name = 'query_audit_log' AND column_name = 'request_id'
    ) THEN
        ALTER TABLE query_audit_log ADD COLUMN request_id TEXT;
    END IF;
END
$$;


-- ===========================================================================
-- SECTION 1: query_audit_log indexes
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- idx_audit_log_user_ts: query_audit_log(user_id, created_at DESC)
-- Purpose: Efficiently retrieve a user's query history in reverse chronological
--          order. Supports the "my recent queries" dashboard panel and
--          per-user usage analytics. DESC on timestamp avoids an extra sort.
-- Query pattern:
--   SELECT * FROM query_audit_log
--   WHERE user_id = $1
--   ORDER BY created_at DESC
--   LIMIT 50;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_audit_log_user_ts
    ON query_audit_log (user_id, created_at DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_log_user_ts
--     ON query_audit_log (user_id, created_at DESC);


-- ---------------------------------------------------------------------------
-- idx_audit_log_request_id: query_audit_log(request_id)
-- Purpose: Look up the audit trail for a specific API request. The correlation
--          ID middleware attaches a unique request_id to each HTTP request;
--          this index enables O(1) lookup for debugging and incident response.
-- Query pattern:
--   SELECT * FROM query_audit_log WHERE request_id = $1;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_audit_log_request_id
    ON query_audit_log (request_id);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_log_request_id
--     ON query_audit_log (request_id);


-- ---------------------------------------------------------------------------
-- idx_audit_log_created: query_audit_log(created_at)
-- Purpose: Time-range scans for global analytics (e.g., "queries in the last
--          24h") and retention/cleanup jobs that purge old audit rows.
-- Query pattern:
--   SELECT COUNT(*) FROM query_audit_log
--   WHERE created_at >= NOW() - INTERVAL '24 hours';
--
--   DELETE FROM query_audit_log WHERE created_at < NOW() - INTERVAL '90 days';
-- Note: This index already exists in schema.sql (idx_audit_created).
--       Included here for documentation completeness; IF NOT EXISTS is a no-op.
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_audit_created
    ON query_audit_log (created_at);


-- ---------------------------------------------------------------------------
-- idx_audit_log_success: query_audit_log(was_successful, created_at DESC)
-- Purpose: Quickly filter failed queries for error dashboards, alerting, and
--          troubleshooting. Partial index on was_successful = false would be
--          even more efficient but is less portable.
-- Query pattern:
--   SELECT * FROM query_audit_log
--   WHERE was_successful = false
--   ORDER BY created_at DESC
--   LIMIT 100;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_audit_log_success
    ON query_audit_log (was_successful, created_at DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_log_success
--     ON query_audit_log (was_successful, created_at DESC);


-- ===========================================================================
-- SECTION 2: security_events indexes
-- ===========================================================================

-- ---------------------------------------------------------------------------
-- idx_security_event_type_ts: security_events(event_type, timestamp DESC)
-- Purpose: Filter security events by type in reverse chronological order.
--          Powers the security dashboard that shows "recent auth failures",
--          "recent rate limit hits", etc. DESC ordering avoids a sort step.
-- Query pattern:
--   SELECT * FROM security_events
--   WHERE event_type = 'auth_failure'
--   ORDER BY timestamp DESC
--   LIMIT 50;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_security_event_type_ts
    ON security_events (event_type, timestamp DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_event_type_ts
--     ON security_events (event_type, timestamp DESC);


-- ---------------------------------------------------------------------------
-- idx_security_user_ts: security_events(user_id, timestamp DESC)
-- Purpose: Retrieve all security events for a specific user in reverse
--          chronological order. Used for investigating suspicious accounts
--          and building per-user security profiles.
-- Query pattern:
--   SELECT * FROM security_events
--   WHERE user_id = $1
--   ORDER BY timestamp DESC;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_security_user_ts
    ON security_events (user_id, timestamp DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_user_ts
--     ON security_events (user_id, timestamp DESC);


-- ---------------------------------------------------------------------------
-- idx_security_severity_ts: security_events(severity, timestamp DESC)
-- Purpose: Filter by severity for alerting pipelines. High/critical events
--          trigger immediate notifications; this index supports efficient
--          polling for unacknowledged critical events.
-- Query pattern:
--   SELECT * FROM security_events
--   WHERE severity = 'critical'
--   ORDER BY timestamp DESC
--   LIMIT 20;
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_security_severity_ts
    ON security_events (severity, timestamp DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_severity_ts
--     ON security_events (severity, timestamp DESC);


-- ---------------------------------------------------------------------------
-- idx_security_ip: security_events(ip_address, timestamp DESC)
-- Purpose: Investigate activity from a specific IP address. Supports
--          brute-force detection and IP-based blocking decisions.
-- Query pattern:
--   SELECT COUNT(*) FROM security_events
--   WHERE ip_address = $1
--     AND event_type = 'auth_failure'
--     AND timestamp >= NOW() - INTERVAL '15 minutes';
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_security_ip
    ON security_events (ip_address, timestamp DESC);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_ip
--     ON security_events (ip_address, timestamp DESC);


-- ---------------------------------------------------------------------------
-- idx_security_timestamp: security_events(timestamp)
-- Purpose: Global time-range scans for retention cleanup and aggregate
--          analytics (e.g., "security events in the last hour").
-- Query pattern:
--   DELETE FROM security_events WHERE timestamp < NOW() - INTERVAL '180 days';
-- ---------------------------------------------------------------------------
CREATE INDEX IF NOT EXISTS idx_security_timestamp
    ON security_events (timestamp);

-- Non-blocking alternative (run outside a transaction):
-- CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_timestamp
--     ON security_events (timestamp);
</file>

<file path="infrastructure/database/README.md">
# Database Infrastructure

Operations scripts and configuration for the Ra'd AI PostgreSQL database.

## Directory Contents

| File | Purpose |
|---|---|
| `indexes.sql` | Production indexes for audit and security tables |
| `backup.sh` | Automated pg_dump with integrity verification and 30-day retention |
| `restore.sh` | Interactive restore with safety confirmation |
| `wal_config.sql` | WAL archiving and checkpoint configuration |
| `health_checks.sql` | Standalone diagnostic queries (connections, bloat, locks, cache) |
| `tuning.md` | PostgreSQL tuning runbook for Railway |

## Backup and Restore

### Daily Backups

The `backup.sh` script creates compressed custom-format dumps using `pg_dump -Fc`:

```bash
# Set credentials (or use .pgpass / PGPASSWORD)
export PGHOST=localhost
export PGPORT=5432
export PGDATABASE=tasi_platform
export PGUSER=tasi_user
export PGPASSWORD='...'

# Run backup
./backup.sh

# Or specify a custom backup directory
./backup.sh /mnt/backups/raid-ai
```

**What it does:**
1. Creates a compressed dump (`-Fc -Z5`) with no owner/privilege commands
2. Verifies the dump by reading its table of contents with `pg_restore --list`
3. Deletes backups older than 30 days (configurable via `RETENTION_DAYS`)

**Cron setup (daily at 02:00):**
```bash
0 2 * * * /opt/raid-ai/infrastructure/database/backup.sh >> /var/log/raid-ai-backup.log 2>&1
```

### Restoring a Backup

```bash
# List backup contents (dry run, no changes)
./restore.sh /var/backups/raid-ai/tasi_platform_20260213_020000.dump --list

# Interactive restore (prompts for confirmation)
./restore.sh /var/backups/raid-ai/tasi_platform_20260213_020000.dump

# Automated restore (CI/pipeline, skips prompt)
./restore.sh /var/backups/raid-ai/tasi_platform_20260213_020000.dump --confirm
```

The restore script:
1. Verifies the backup file integrity
2. Requires typing `CONFIRM` (unless `--confirm` flag is passed)
3. Drops existing objects (`--clean --if-exists`) and restores
4. Shows row counts per table after restore

## WAL Archiving and Point-in-Time Recovery (PITR)

### Overview

WAL (Write-Ahead Log) archiving provides continuous backup by copying every WAL segment to an archive. Combined with a base backup (from `backup.sh`), this enables restoring the database to any point in time -- not just the moment of the last dump.

**Recovery granularity:** Transaction-level (any timestamp, transaction ID, or named restore point).

### How WAL Archiving Works

```
Client writes --> WAL buffer --> WAL segment files (16MB each)
                                        |
                                        v  (archive_command)
                                  WAL archive directory
                                  /var/lib/postgresql/wal_archive/
```

1. PostgreSQL writes all changes to WAL before applying them to data files
2. When a WAL segment fills up (16MB), the `archive_command` copies it to the archive
3. The archive accumulates all changes since the last base backup
4. To recover: restore the base backup, then replay WAL segments up to the target time

### Setup Steps

**Step 1: Create the WAL archive directory**
```bash
# On the PostgreSQL host
sudo mkdir -p /var/lib/postgresql/wal_archive
sudo chown postgres:postgres /var/lib/postgresql/wal_archive
sudo chmod 700 /var/lib/postgresql/wal_archive
```

**Step 2: Apply WAL configuration**
```bash
psql -U postgres -d tasi_platform -f infrastructure/database/wal_config.sql
```

**Step 3: Restart PostgreSQL**
```bash
# Docker
docker compose restart postgres

# Systemd
sudo systemctl restart postgresql
```

**Step 4: Verify archiving is active**
```sql
-- Check settings
SELECT name, setting, pending_restart
FROM pg_settings
WHERE name IN ('wal_level', 'archive_mode', 'archive_command')
ORDER BY name;

-- Check archiver status
SELECT archived_count, failed_count, last_archived_wal, last_archived_time
FROM pg_stat_archiver;
```

### Point-in-Time Recovery (PITR)

PITR lets you restore the database to a specific moment. This is critical for recovering from accidental data deletion or corruption.

**Prerequisites:**
- A base backup (from `backup.sh`) taken BEFORE the target recovery time
- Archived WAL segments covering the period from the base backup to the target time

**Step 1: Stop PostgreSQL**
```bash
sudo systemctl stop postgresql
```

**Step 2: Clear the data directory and restore the base backup**
```bash
# Move current data aside (safety)
sudo mv /var/lib/postgresql/16/main /var/lib/postgresql/16/main.old

# Create fresh data directory
sudo mkdir /var/lib/postgresql/16/main
sudo chown postgres:postgres /var/lib/postgresql/16/main

# Restore base backup
sudo -u postgres pg_restore \
    -Fd /var/backups/raid-ai/tasi_platform_20260213_020000.dump \
    -d tasi_platform \
    --clean --if-exists
```

**Step 3: Configure recovery**

Create `/var/lib/postgresql/16/main/postgresql.auto.conf` (or edit it):
```ini
# Point-in-Time Recovery target
restore_command = 'cp /var/lib/postgresql/wal_archive/%f %p'
recovery_target_time = '2026-02-13 15:30:00+03'
recovery_target_action = 'promote'
```

Create the recovery signal file:
```bash
sudo -u postgres touch /var/lib/postgresql/16/main/recovery.signal
```

**Step 4: Start PostgreSQL**
```bash
sudo systemctl start postgresql
```

PostgreSQL will:
1. Replay WAL from the archive up to `recovery_target_time`
2. Promote to read-write mode (`recovery_target_action = 'promote'`)
3. Delete `recovery.signal`

**Step 5: Verify**
```sql
-- Should show no recovery in progress
SELECT pg_is_in_recovery();  -- Expected: false

-- Check the latest transaction timestamp
SELECT MAX(created_at) FROM query_audit_log;
```

### PITR on Railway

Railway's managed PostgreSQL does not expose filesystem access, so traditional WAL archiving is not directly available. Options:

1. **Scheduled pg_dump backups:** Use `backup.sh` with Railway's `railway run` to create periodic dumps to external storage (S3, GCS).

2. **External replication:** Set up a read replica outside Railway using streaming replication, which automatically transfers WAL.

3. **Railway snapshots:** Railway provides automatic volume snapshots for the database service. Check your Railway dashboard for snapshot configuration.

```bash
# Example: backup via Railway CLI to S3
railway run ./infrastructure/database/backup.sh /tmp/raid-ai-backup
aws s3 cp /tmp/raid-ai-backup/ s3://raid-ai-backups/ --recursive
```

### Monitoring WAL Archiving

```sql
-- Archiver statistics
SELECT * FROM pg_stat_archiver;

-- Current WAL position vs last archived
SELECT
    pg_current_wal_lsn() AS current_lsn,
    last_archived_wal,
    last_failed_wal,
    failed_count
FROM pg_stat_archiver;

-- WAL files waiting to be archived
SELECT COUNT(*) AS pending_wal_files
FROM pg_ls_waldir()
WHERE name > (SELECT last_archived_wal FROM pg_stat_archiver);
```

**Alert thresholds:**
- `failed_count > 0`: Archiving has failed at least once -- investigate immediately
- `pending_wal_files > 100`: WAL is accumulating faster than archiving -- check disk/network
- `last_archived_time` older than 10 minutes: Archiver may be stuck

### Retention

WAL archive files consume disk space. Implement retention based on your RPO (Recovery Point Objective):

```bash
# Delete archived WAL segments older than 7 days
find /var/lib/postgresql/wal_archive -name "*.gz" -mtime +7 -delete

# If using pgBackRest
pgbackrest --stanza=raid-ai expire
```

Keep WAL archives at least as long as your oldest base backup that you might need to restore from.
</file>

<file path="infrastructure/database/restore.sh">
#!/usr/bin/env bash
# =============================================================================
# Ra'd AI - PostgreSQL Restore Script
# =============================================================================
# Restores a custom-format pg_dump backup into the target database.
# Includes a mandatory confirmation prompt to prevent accidental overwrites.
#
# Usage:
#   ./restore.sh <backup_file>                        # Interactive confirmation
#   ./restore.sh <backup_file> --confirm              # Skip prompt (CI/automation)
#   ./restore.sh <backup_file> --list                 # List contents only (dry run)
#
# Environment variables:
#   PGHOST       - PostgreSQL host       (default: localhost)
#   PGPORT       - PostgreSQL port       (default: 5432)
#   PGDATABASE   - Target database name  (default: tasi_platform)
#   PGUSER       - Database user         (default: tasi_user)
#   PGPASSWORD   - Database password     (reads from .pgpass if not set)
# =============================================================================

set -euo pipefail

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
PGHOST="${PGHOST:-localhost}"
PGPORT="${PGPORT:-5432}"
PGDATABASE="${PGDATABASE:-tasi_platform}"
PGUSER="${PGUSER:-tasi_user}"
LOG_PREFIX="[restore]"

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
log()  { echo "${LOG_PREFIX} $(date '+%Y-%m-%d %H:%M:%S') $*"; }
fail() { log "ERROR: $*" >&2; exit 1; }

usage() {
    echo "Usage: $0 <backup_file> [--confirm | --list]"
    echo ""
    echo "Options:"
    echo "  --confirm    Skip interactive confirmation (for CI/automation)"
    echo "  --list       List backup contents without restoring (dry run)"
    echo ""
    echo "Examples:"
    echo "  $0 /var/backups/raid-ai/tasi_platform_20260213_020000.dump"
    echo "  $0 /var/backups/raid-ai/tasi_platform_20260213_020000.dump --confirm"
    echo "  $0 /var/backups/raid-ai/tasi_platform_20260213_020000.dump --list"
    exit 1
}

# ---------------------------------------------------------------------------
# Argument parsing
# ---------------------------------------------------------------------------
BACKUP_FILE="${1:-}"
MODE="${2:-}"

[ -z "${BACKUP_FILE}" ] && usage

[ -f "${BACKUP_FILE}" ] || fail "Backup file not found: ${BACKUP_FILE}"

command -v pg_restore >/dev/null 2>&1 || fail "pg_restore not found in PATH"

# ---------------------------------------------------------------------------
# Verify backup integrity first
# ---------------------------------------------------------------------------
log "Verifying backup: ${BACKUP_FILE}"
pg_restore --list "${BACKUP_FILE}" > /dev/null 2>&1 \
    || fail "Backup file is corrupt or not a valid custom-format dump"

BACKUP_SIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
TABLE_COUNT=$(pg_restore --list "${BACKUP_FILE}" 2>/dev/null | grep -c "TABLE" || true)
log "Backup verified: ${BACKUP_SIZE}, ${TABLE_COUNT} table entries"

# ---------------------------------------------------------------------------
# List-only mode (dry run)
# ---------------------------------------------------------------------------
if [ "${MODE}" = "--list" ]; then
    log "Contents of ${BACKUP_FILE}:"
    echo "---"
    pg_restore --list "${BACKUP_FILE}" 2>/dev/null
    echo "---"
    log "Dry run complete. No changes made."
    exit 0
fi

# ---------------------------------------------------------------------------
# Safety confirmation
# ---------------------------------------------------------------------------
log "Target: ${PGDATABASE}@${PGHOST}:${PGPORT} (user: ${PGUSER})"

if [ "${MODE}" != "--confirm" ]; then
    echo ""
    echo "================================================================="
    echo "  WARNING: This will overwrite data in database '${PGDATABASE}'"
    echo "  on ${PGHOST}:${PGPORT} as user '${PGUSER}'."
    echo ""
    echo "  Backup file: ${BACKUP_FILE}"
    echo "  Size: ${BACKUP_SIZE} | Tables: ${TABLE_COUNT}"
    echo "================================================================="
    echo ""
    read -rp "Type 'CONFIRM' to proceed with restore: " RESPONSE
    if [ "${RESPONSE}" != "CONFIRM" ]; then
        log "Restore cancelled by user."
        exit 0
    fi
fi

# ---------------------------------------------------------------------------
# Step 1: Restore the backup
# ---------------------------------------------------------------------------
# --clean        = Drop existing objects before restoring
# --if-exists    = Don't error if objects don't exist yet
# --no-owner     = Don't set ownership (use current user)
# --no-privileges= Don't restore GRANT/REVOKE
# -j 4           = Use 4 parallel jobs for faster restore
# --exit-on-error= Stop on first error
# ---------------------------------------------------------------------------
log "Starting restore..."
pg_restore \
    -h "${PGHOST}" \
    -p "${PGPORT}" \
    -U "${PGUSER}" \
    -d "${PGDATABASE}" \
    --clean \
    --if-exists \
    --no-owner \
    --no-privileges \
    -j 4 \
    "${BACKUP_FILE}" \
    || fail "pg_restore failed with exit code $?"

# ---------------------------------------------------------------------------
# Step 2: Post-restore verification
# ---------------------------------------------------------------------------
log "Verifying restore..."

ROW_COUNTS=$(psql \
    -h "${PGHOST}" \
    -p "${PGPORT}" \
    -U "${PGUSER}" \
    -d "${PGDATABASE}" \
    -t -A \
    -c "SELECT tablename, n_live_tup FROM pg_stat_user_tables ORDER BY tablename;" \
    2>/dev/null || echo "(verification query failed)")

log "Table row counts after restore:"
echo "${ROW_COUNTS}" | while IFS='|' read -r tbl cnt; do
    [ -n "${tbl}" ] && log "  ${tbl}: ${cnt} rows"
done

# ---------------------------------------------------------------------------
# Summary
# ---------------------------------------------------------------------------
log "Restore complete."
log "  Source: ${BACKUP_FILE}"
log "  Target: ${PGDATABASE}@${PGHOST}:${PGPORT}"
log "Done."
</file>

<file path="infrastructure/database/tuning.md">
# PostgreSQL Tuning Runbook

Tuning guide for the Ra'd AI TASI Platform PostgreSQL database, with specific recommendations for Railway deployment.

## Resource Allocation

### Shared Buffers

The most impactful single parameter. Sets the size of PostgreSQL's shared memory cache.

**Rule of thumb:** 25% of total system RAM, capped at ~8GB.

| Environment | RAM | shared_buffers |
|---|---|---|
| Railway (512MB) | 512MB | 128MB |
| Railway (1GB) | 1GB | 256MB |
| Docker (2GB) | 2GB | 512MB |
| Dedicated (8GB+) | 8GB | 2GB |

```sql
ALTER SYSTEM SET shared_buffers = '256MB';  -- Adjust for your tier
```

### Effective Cache Size

Tells the query planner how much OS page cache to expect. Does not allocate memory -- only affects planning decisions.

**Rule of thumb:** 75% of total system RAM.

```sql
ALTER SYSTEM SET effective_cache_size = '768MB';  -- For 1GB Railway tier
```

### Work Memory

Per-operation memory for sorts, hash joins, etc. Multiplied by the number of concurrent operations, so keep it conservative.

**Rule of thumb:** Total RAM / (max_connections * 4).

```sql
-- For 1GB RAM, 100 max_connections: 1024MB / (100 * 4) = ~2.5MB
ALTER SYSTEM SET work_mem = '4MB';
```

For complex analytical queries (common in TASI analysis), temporarily increase per-session:

```sql
SET work_mem = '64MB';  -- For a heavy analytical query
```

### Maintenance Work Memory

Memory for VACUUM, CREATE INDEX, ALTER TABLE ADD FOREIGN KEY.

**Rule of thumb:** 5-10% of RAM, up to 1GB.

```sql
ALTER SYSTEM SET maintenance_work_mem = '128MB';  -- For 1GB Railway tier
```

## Connection Management

### Max Connections

Railway PostgreSQL defaults vary by plan. The application uses connection pooling (via psycopg2), so the database doesn't need many direct connections.

```sql
ALTER SYSTEM SET max_connections = '100';  -- Sufficient for most workloads
```

For higher concurrency, use PgBouncer rather than increasing max_connections:

```
# PgBouncer config (transaction pooling)
pool_mode = transaction
max_client_conn = 500
default_pool_size = 25
```

### Idle Connection Timeout

Kill idle connections that forgot to disconnect:

```sql
ALTER SYSTEM SET idle_in_transaction_session_timeout = '300000';  -- 5 minutes (ms)
ALTER SYSTEM SET idle_session_timeout = '1800000';  -- 30 minutes (ms), PG 16+
```

### Statement Timeout

Prevent runaway queries from consuming resources indefinitely:

```sql
ALTER SYSTEM SET statement_timeout = '30000';  -- 30 seconds (ms)
```

Override per-session for long-running analytics:

```sql
SET statement_timeout = '300000';  -- 5 minutes for this session
```

## Autovacuum Tuning

### Default Settings

PostgreSQL autovacuum defaults are reasonable for most tables. The TASI platform has ~500 rows in core tables, but audit and security tables grow unboundedly.

```sql
-- Global defaults (already sensible)
ALTER SYSTEM SET autovacuum_vacuum_threshold = '50';
ALTER SYSTEM SET autovacuum_vacuum_scale_factor = '0.1';  -- VACUUM when 10% of rows are dead
ALTER SYSTEM SET autovacuum_analyze_threshold = '50';
ALTER SYSTEM SET autovacuum_analyze_scale_factor = '0.05';  -- ANALYZE when 5% of rows change
```

### Audit Table Tuning

`query_audit_log` and `security_events` are append-heavy tables that grow continuously. They need more aggressive autovacuum to prevent bloat.

```sql
-- query_audit_log: more aggressive vacuum (5% dead threshold instead of 10%)
ALTER TABLE query_audit_log SET (
    autovacuum_vacuum_scale_factor = 0.05,
    autovacuum_vacuum_threshold = 100,
    autovacuum_analyze_scale_factor = 0.02,
    autovacuum_analyze_threshold = 100,
    autovacuum_vacuum_cost_delay = 10  -- Run vacuum faster (ms delay between pages)
);

-- security_events: same aggressive settings
ALTER TABLE security_events SET (
    autovacuum_vacuum_scale_factor = 0.05,
    autovacuum_vacuum_threshold = 100,
    autovacuum_analyze_scale_factor = 0.02,
    autovacuum_analyze_threshold = 100,
    autovacuum_vacuum_cost_delay = 10
);

-- news_articles: moderate growth, slightly more aggressive than default
ALTER TABLE news_articles SET (
    autovacuum_vacuum_scale_factor = 0.08,
    autovacuum_analyze_scale_factor = 0.04
);

-- price_history: large table with daily inserts
ALTER TABLE price_history SET (
    autovacuum_vacuum_scale_factor = 0.05,
    autovacuum_vacuum_threshold = 200,
    autovacuum_analyze_scale_factor = 0.02
);
```

### Autovacuum Workers

For databases with many tables that need vacuuming concurrently:

```sql
ALTER SYSTEM SET autovacuum_max_workers = '4';  -- Default is 3
ALTER SYSTEM SET autovacuum_naptime = '30';      -- Check every 30 seconds (default: 60)
```

## Query Planner

### Cost Parameters

These affect how the planner estimates query costs. Adjust based on storage type:

```sql
-- SSD storage (Railway, most cloud providers)
ALTER SYSTEM SET random_page_cost = '1.1';    -- Default 4.0 (for spinning disks)
ALTER SYSTEM SET seq_page_cost = '1.0';        -- Keep at 1.0
ALTER SYSTEM SET effective_io_concurrency = '200';  -- SSD can handle parallel I/O
```

### JIT Compilation

JIT compilation helps complex analytical queries but adds overhead for simple OLTP. For a mixed workload like TASI:

```sql
ALTER SYSTEM SET jit = 'on';
ALTER SYSTEM SET jit_above_cost = '100000';       -- Only for expensive queries
ALTER SYSTEM SET jit_inline_above_cost = '500000';
ALTER SYSTEM SET jit_optimize_above_cost = '500000';
```

## Logging

### Slow Query Log

Log queries slower than a threshold for performance analysis:

```sql
ALTER SYSTEM SET log_min_duration_statement = '1000';  -- Log queries > 1 second
ALTER SYSTEM SET log_statement = 'none';                -- Don't log all statements
ALTER SYSTEM SET log_lock_waits = 'on';                 -- Log lock waits > deadlock_timeout
ALTER SYSTEM SET deadlock_timeout = '1000';              -- 1 second
```

### Log Format

```sql
ALTER SYSTEM SET log_line_prefix = '%t [%p] %q%u@%d ';  -- timestamp, pid, user@db
ALTER SYSTEM SET log_checkpoints = 'on';
ALTER SYSTEM SET log_connections = 'on';
ALTER SYSTEM SET log_disconnections = 'on';
ALTER SYSTEM SET log_temp_files = '0';  -- Log all temp file usage
```

## Railway-Specific Recommendations

### Tier-Based Configuration

Railway allocates resources based on plan tier. Apply these settings via `railway run psql`:

**Starter (512MB RAM, shared CPU):**
```sql
ALTER SYSTEM SET shared_buffers = '128MB';
ALTER SYSTEM SET effective_cache_size = '384MB';
ALTER SYSTEM SET work_mem = '2MB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET max_connections = '50';
ALTER SYSTEM SET random_page_cost = '1.1';
ALTER SYSTEM SET effective_io_concurrency = '200';
```

**Pro (1GB RAM, dedicated CPU):**
```sql
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '768MB';
ALTER SYSTEM SET work_mem = '4MB';
ALTER SYSTEM SET maintenance_work_mem = '128MB';
ALTER SYSTEM SET max_connections = '100';
ALTER SYSTEM SET random_page_cost = '1.1';
ALTER SYSTEM SET effective_io_concurrency = '200';
```

**Team (2GB+ RAM):**
```sql
ALTER SYSTEM SET shared_buffers = '512MB';
ALTER SYSTEM SET effective_cache_size = '1536MB';
ALTER SYSTEM SET work_mem = '8MB';
ALTER SYSTEM SET maintenance_work_mem = '256MB';
ALTER SYSTEM SET max_connections = '150';
ALTER SYSTEM SET random_page_cost = '1.1';
ALTER SYSTEM SET effective_io_concurrency = '200';
```

### Applying Changes on Railway

```bash
# Connect to Railway PostgreSQL
railway run psql

# Apply settings (most take effect immediately)
ALTER SYSTEM SET shared_buffers = '256MB';
-- ... more settings ...

# For settings requiring restart:
SELECT pg_reload_conf();

# Verify
SELECT name, setting, unit, pending_restart
FROM pg_settings
WHERE name IN ('shared_buffers', 'effective_cache_size', 'work_mem')
ORDER BY name;
```

Note: `shared_buffers` and `max_connections` require a PostgreSQL restart. On Railway, this happens when the service redeploys or restarts.

### Monitoring on Railway

Railway does not expose `pg_stat_*` views through its dashboard. Use the health check queries in `health_checks.sql` via `railway run psql`:

```bash
railway run psql -f infrastructure/database/health_checks.sql
```

## Applying All Changes

After modifying settings with `ALTER SYSTEM`:

```sql
-- Reload configuration (applies most settings immediately)
SELECT pg_reload_conf();

-- Check which settings need a restart
SELECT name, setting, pending_restart
FROM pg_settings
WHERE pending_restart = true;
```

Settings requiring a restart: `shared_buffers`, `max_connections`, `wal_level`, `archive_mode`, `max_worker_processes`.

## Verification Checklist

After tuning, verify these health indicators:

| Metric | Target | Query |
|---|---|---|
| Cache hit ratio | > 99% | `SELECT ... FROM pg_stat_database` (see health_checks.sql #6) |
| Connection utilization | < 80% | `SELECT numbackends / max_connections` (see health_checks.sql #1) |
| Dead tuple ratio | < 10% per table | `SELECT n_dead_tup / n_live_tup` (see health_checks.sql #2) |
| Index usage ratio | > 90% for large tables | `SELECT idx_scan / (seq_scan + idx_scan)` (see health_checks.sql #3) |
| Long-running queries | 0 queries > 60s | (see health_checks.sql #4) |
| Lock contention | 0 blocked queries | (see health_checks.sql #5) |
| WAL archiver failures | 0 | `SELECT failed_count FROM pg_stat_archiver` |
</file>

<file path="infrastructure/database/wal_config.sql">
-- =============================================================================
-- Ra'd AI - PostgreSQL WAL Archiving Configuration
-- =============================================================================
-- This file configures Write-Ahead Log (WAL) archiving for Point-in-Time
-- Recovery (PITR). Run with superuser privileges:
--
--   psql -U postgres -d tasi_platform -f wal_config.sql
--
-- After applying, restart PostgreSQL for archive_mode to take effect.
--
-- NOTE: ALTER SYSTEM writes to postgresql.auto.conf, which overrides
-- postgresql.conf. These settings persist across restarts.
-- =============================================================================


-- ===========================================================================
-- SECTION 1: WAL Settings
-- ===========================================================================

-- Set WAL level to 'replica' (required for archiving and streaming replication).
-- 'replica' is the minimum level that generates enough WAL for PITR.
-- Default is 'replica' in PG 16, but we set it explicitly for clarity.
ALTER SYSTEM SET wal_level = 'replica';

-- Enable WAL archiving. Requires a restart to take effect.
ALTER SYSTEM SET archive_mode = 'on';

-- Archive command: copy completed WAL segments to the archive directory.
-- %p = full path to the WAL file to archive
-- %f = filename only (without path)
-- The command must return 0 on success. Using cp with test ensures we don't
-- overwrite an existing archived segment (safety against double-archiving).
--
-- For production, replace this with your preferred archiving tool:
--   - pgBackRest:  pgbackrest --stanza=raid-ai archive-push %p
--   - WAL-G:       wal-g wal-push %p
--   - S3:          aws s3 cp %p s3://raid-ai-wal-archive/%f
ALTER SYSTEM SET archive_command = 'test ! -f /var/lib/postgresql/wal_archive/%f && cp %p /var/lib/postgresql/wal_archive/%f';

-- Timeout for archive_command (seconds). If the command takes longer than this,
-- it is killed and WAL archiving is retried. Set higher for remote storage.
ALTER SYSTEM SET archive_timeout = '300';


-- ===========================================================================
-- SECTION 2: WAL Sizing
-- ===========================================================================

-- Maximum size of WAL files before a checkpoint is triggered.
-- Default is 1GB. For a small-to-medium database like TASI (~500 companies),
-- 512MB is sufficient. Increase for write-heavy workloads.
ALTER SYSTEM SET max_wal_size = '1GB';

-- Minimum WAL size to retain. PostgreSQL will try to keep at least this much
-- WAL on disk. Helps avoid frequent checkpoint I/O spikes.
ALTER SYSTEM SET min_wal_size = '256MB';

-- Number of WAL segments kept for streaming replication standby servers.
-- 64 segments * 16MB = 1GB of WAL retained. Set to 0 if no replicas.
ALTER SYSTEM SET wal_keep_size = '1GB';


-- ===========================================================================
-- SECTION 3: Checkpoint Tuning
-- ===========================================================================

-- Target ratio of checkpoint completion vs interval between checkpoints.
-- 0.9 means spread checkpoint writes over 90% of the interval, reducing
-- I/O spikes. Default is 0.9 in PG 16.
ALTER SYSTEM SET checkpoint_completion_target = '0.9';

-- Time between automatic checkpoints (seconds). 10 minutes is a good default.
ALTER SYSTEM SET checkpoint_timeout = '10min';


-- ===========================================================================
-- SECTION 4: WAL Compression (PG 15+)
-- ===========================================================================

-- Compress full-page writes in WAL. Reduces WAL volume by ~30-50% with
-- minimal CPU overhead. Uses LZ4 if available, falls back to pglz.
ALTER SYSTEM SET wal_compression = 'on';


-- ===========================================================================
-- SECTION 5: Verification
-- ===========================================================================
-- After restarting PostgreSQL, verify settings:
--
--   SELECT name, setting, pending_restart
--   FROM pg_settings
--   WHERE name IN (
--       'wal_level', 'archive_mode', 'archive_command', 'archive_timeout',
--       'max_wal_size', 'min_wal_size', 'wal_keep_size',
--       'checkpoint_completion_target', 'checkpoint_timeout',
--       'wal_compression'
--   )
--   ORDER BY name;
--
-- Check archiving status:
--   SELECT * FROM pg_stat_archiver;
-- =============================================================================
</file>

<file path="ingestion/__init__.py">
"""
Ingestion pipeline for TASI AI Platform.

Provides XBRL financial filing processing and Yahoo Finance price loading
into PostgreSQL.
"""
⋮----
__all__ = ["XBRLProcessor", "PriceLoader"]
</file>

<file path="middleware/__init__.py">
"""
Middleware module for TASI AI Platform.

Provides CORS, rate limiting, request logging, and error handling middleware.
"""
⋮----
__all__ = [
</file>

<file path="middleware/chat_auth.py">
"""
Chat authentication middleware.
Validates JWT bearer tokens for Vanna chat endpoints.
Anonymous access is allowed — a missing token passes through,
but a present-and-invalid token returns 401.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_PROTECTED_PATHS = {"/api/vanna/v2/chat_sse", "/api/vanna/v2/chat_poll"}
⋮----
class ChatAuthMiddleware(BaseHTTPMiddleware)
⋮----
async def dispatch(self, request: Request, call_next)
⋮----
auth_header = request.headers.get("authorization", "")
⋮----
token = auth_header[7:]
</file>

<file path="middleware/request_context.py">
"""
Request context — stores per-request metadata in a contextvars.ContextVar.
This allows service code to access request metadata (request_id) without
threading it through every function call.
"""
⋮----
_request_id_var: ContextVar[str] = ContextVar("request_id", default="")
⋮----
def get_request_id() -> str
⋮----
"""Return the current request ID, or empty string if not in a request context."""
⋮----
def set_request_id(request_id: str) -> None
⋮----
"""Set the request ID for the current async context."""
⋮----
class RequestIdFilter(logging.Filter)
⋮----
"""Injects request_id into every LogRecord for structured logging."""
⋮----
def filter(self, record: logging.LogRecord) -> bool
</file>

<file path="models/__init__.py">

</file>

<file path="models/api_responses.py">
"""
Standardized API response models for the Ra'd AI TASI Market Analytics API.

All error responses follow the shape::

    {
        "error": {
            "code": "ERROR_CODE",
            "message": "Human-readable message",
            "request_id": "abc123..."
        }
    }

Success responses use endpoint-specific models defined in ``api/schemas/``.
This module provides the shared ``ErrorDetail`` and ``ErrorResponse`` models
for OpenAPI documentation, plus re-exports of key response types.
"""
⋮----
# ---------------------------------------------------------------------------
# Standard error response (used by middleware/error_handler.py)
⋮----
class ErrorDetail(BaseModel)
⋮----
"""Inner error object in all error responses."""
⋮----
code: str = Field(
message: str = Field(
request_id: str = Field(
⋮----
class ErrorResponse(BaseModel)
⋮----
"""Standard error response envelope."""
⋮----
error: ErrorDetail
⋮----
# Common success response patterns (for OpenAPI docs)
⋮----
class HealthComponentResponse(BaseModel)
⋮----
"""Health status of a single platform component."""
⋮----
name: str
status: str
latency_ms: Optional[float] = None
message: Optional[str] = None
⋮----
class HealthCheckResponse(BaseModel)
⋮----
"""Structured health check response."""
⋮----
status: str = Field(..., description="Overall status: healthy, degraded, unhealthy")
service: str = Field(default="raid-ai-tasi")
version: Optional[str] = None
uptime_seconds: Optional[float] = None
components: List[HealthComponentResponse] = []
⋮----
class ChartDataPointResponse(BaseModel)
⋮----
"""Single data point for chart endpoints."""
⋮----
label: str
value: float
⋮----
class ChartDataResponse(BaseModel)
⋮----
"""Response from chart analytics endpoints."""
⋮----
chart_type: str
title: str
data: List[ChartDataPointResponse]
⋮----
# OpenAPI error response definitions for use in route decorators
⋮----
# Common responses dict for use with FastAPI's `responses` parameter:
#   @router.get("/foo", responses=STANDARD_ERRORS)
STANDARD_ERRORS: Dict[int, Dict[str, Any]] = {
</file>

<file path="requirements-test.txt">
# requirements-test.txt — Test-only dependencies
# Usage: pip install -r requirements.txt -r requirements-test.txt
# Installed on top of requirements.txt (do not include -r requirements.txt here)

# Test runner
pytest>=8.0.0,<9.0
pytest-asyncio>=0.24.0,<1.0
pytest-cov>=5.0.0,<6.0
pytest-timeout>=2.3.0,<3.0

# HTTP client for API tests
httpx>=0.27.0,<1.0

# XBRL processor tests
openpyxl>=3.1.0,<4.0
</file>

<file path="ruff.toml">
[lint]
# E402: Module-level imports not at top - we intentionally import after
# load_dotenv() in app.py and after sys.path.insert() in test files.
# F821: ToolSchema is a forward reference string used in type annotations.
per-file-ignores = { "app.py" = ["E402"], "test_app_assembly*.py" = ["E402", "F821", "E712"], "tests/test_chart_engine.py" = ["E402"], "tests/test_ui_enhancements.py" = ["E402"], "api/routes/*.py" = ["E402"] }
</file>

<file path="scripts/build_check.sh">
#!/bin/bash
# =============================================================================
# Ra'd AI Platform - Pre-Deployment Build Check
# =============================================================================
# Runs all test suites and build steps to verify the project is deployable.
# Usage: ./scripts/build_check.sh
#
# Exit code: number of failed steps (0 = all green)
# =============================================================================

set -uo pipefail

PROJECT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
cd "$PROJECT_DIR"

PASS=0
FAIL=0
SKIP=0

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

run_step() {
    local name="$1"
    shift
    echo ""
    echo "--- $name ---"
    if "$@" 2>&1; then
        echo "  PASS: $name"
        ((PASS++))
    else
        echo "  FAIL: $name (exit code $?)"
        ((FAIL++))
    fi
}

run_step_optional() {
    local name="$1"
    shift
    echo ""
    echo "--- $name (optional) ---"
    if command -v "${1}" &>/dev/null; then
        if "$@" 2>&1; then
            echo "  PASS: $name"
            ((PASS++))
        else
            echo "  FAIL: $name (exit code $?)"
            ((FAIL++))
        fi
    else
        echo "  SKIP: $name (${1} not found)"
        ((SKIP++))
    fi
}

# ---------------------------------------------------------------------------
# Python Tests
# ---------------------------------------------------------------------------

echo "============================================="
echo "  Ra'd AI Platform - Build Check"
echo "============================================="
echo "Project: $PROJECT_DIR"
echo "Time:    $(date -u '+%Y-%m-%dT%H:%M:%SZ' 2>/dev/null || date)"
echo "Python:  $(python --version 2>&1)"

# Core test suites
run_step "Database integrity tests (test_database.py)" python -m pytest test_database.py -v --tb=short
run_step "Vanna assembly tests (test_app_assembly_v2.py)" python -m pytest test_app_assembly_v2.py -v --tb=short
run_step "Unit/integration tests (tests/)" python -m pytest tests/ -v --tb=short -x

# ---------------------------------------------------------------------------
# Frontend Build
# ---------------------------------------------------------------------------

if [ -d "frontend" ] && [ -f "frontend/package.json" ]; then
    echo ""
    echo "--- Frontend ---"
    if command -v npm &>/dev/null; then
        # Install dependencies if needed
        if [ ! -d "frontend/node_modules" ]; then
            echo "Installing frontend dependencies..."
            (cd frontend && npm install --prefer-offline 2>&1) || true
        fi
        run_step "Next.js build" bash -c "cd frontend && npm run build"
        run_step_optional "Frontend tests (vitest)" bash -c "cd frontend && npx vitest run 2>&1"
    else
        echo "  SKIP: Frontend build (npm not found)"
        ((SKIP++))
    fi
else
    echo ""
    echo "  SKIP: Frontend (directory not found)"
    ((SKIP++))
fi

# ---------------------------------------------------------------------------
# Docker Compose Validation
# ---------------------------------------------------------------------------

run_step_optional "Docker Compose config validation" docker compose config --quiet

# ---------------------------------------------------------------------------
# Python import check (quick sanity)
# ---------------------------------------------------------------------------

echo ""
echo "--- Import Sanity Check ---"
if python -c "
import app
import services.tasi_index
import api.routes.tasi_index
import config
import chart_engine.raid_chart_generator
print('All key modules importable')
" 2>&1; then
    echo "  PASS: Python imports"
    ((PASS++))
else
    echo "  FAIL: Python imports"
    ((FAIL++))
fi

# ---------------------------------------------------------------------------
# Summary
# ---------------------------------------------------------------------------

echo ""
echo "============================================="
TOTAL=$((PASS + FAIL))
echo "  Build Check: $PASS/$TOTAL passed, $FAIL failed"
if [ "$SKIP" -gt 0 ]; then
    echo "  Skipped: $SKIP"
fi
echo "============================================="

if [ "$FAIL" -gt 0 ]; then
    echo ""
    echo "  BUILD CHECK FAILED - do not deploy"
    exit 1
else
    echo ""
    echo "  All checks passed - ready for deployment"
    exit 0
fi
</file>

<file path="scripts/coverage_report.sh">
#!/bin/bash
# =============================================================================
# Ra'd AI Platform - Coverage Report Generator
# =============================================================================
# Generates test coverage reports for both backend (Python) and frontend (TS).
#
# Usage:
#   ./scripts/coverage_report.sh           # full report
#   ./scripts/coverage_report.sh --fast    # skip slow/integration tests
#   ./scripts/coverage_report.sh --html    # generate HTML report
# =============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
FAST_ONLY=false
HTML_REPORT=false

# Parse args
for arg in "$@"; do
    case "$arg" in
        --fast) FAST_ONLY=true ;;
        --html) HTML_REPORT=true ;;
    esac
done

echo "============================================="
echo "  Ra'd AI Platform -- Coverage Report"
echo "============================================="
echo ""

# ---------------------------------------------------------------------------
# Backend (Python) coverage
# ---------------------------------------------------------------------------
echo "--- Backend (Python) ---"
cd "$PROJECT_ROOT"

PYTEST_ARGS=(
    --cov=api
    --cov=auth
    --cov=config
    --cov=services
    --cov=middleware
    --cov=chart_engine
    --cov-report=term-missing
)

if [ "$HTML_REPORT" = true ]; then
    PYTEST_ARGS+=(--cov-report=html:htmlcov)
fi

if [ "$FAST_ONLY" = true ]; then
    PYTEST_ARGS+=(-m "fast or (not slow and not integration and not pg_required)")
fi

python -m pytest "${PYTEST_ARGS[@]}" tests/ test_database.py test_app_assembly_v2.py 2>&1 || true

echo ""

# ---------------------------------------------------------------------------
# Frontend (TypeScript) coverage
# ---------------------------------------------------------------------------
if [ -d "$PROJECT_ROOT/frontend" ] && [ -f "$PROJECT_ROOT/frontend/package.json" ]; then
    echo "--- Frontend (TypeScript) ---"
    cd "$PROJECT_ROOT/frontend"

    if command -v npx &> /dev/null; then
        npx vitest run --coverage 2>&1 || echo "  (frontend coverage failed or not configured)"
    else
        echo "  SKIP: npx not found"
    fi
    echo ""
fi

echo "============================================="
echo "  Coverage report complete."
if [ "$HTML_REPORT" = true ]; then
    echo "  HTML report: $PROJECT_ROOT/htmlcov/index.html"
fi
echo "============================================="
</file>

<file path="scripts/DEPENDENCY_AUDIT.md">
# Dependency Vulnerability Audit

**Date:** 2026-02-13
**Author:** security-harden agent

---

## Python Dependencies (`requirements.txt`)

### Audit Method
`pip check` was used (pip-audit not installed). `pip check` verifies version compatibility but does not scan CVE databases. The project's Python dependencies are pinned with version ranges in `requirements.txt`.

### Compatibility Issues (from `pip check`)

These are version mismatches in the global Python environment, not necessarily in the project's direct dependencies:

| Package | Issue | Severity | Relevant to Project? |
|---------|-------|----------|---------------------|
| lxml 6.0.2 | docling requires <6.0.0, crawl4ai requires ~5.3 | LOW | No -- docling/crawl4ai are not project deps |
| requests 2.31.0 | locust requires >=2.32.2, fast-langdetect requires >=2.32.3 | LOW | Partial -- project uses requests >=2.31.0 |
| numpy 1.26.4 | thinc, kreuzberg, opencv require >=2.0.0 | LOW | No -- not project deps |
| openai 2.7.1 | litellm requires >=2.8.0 | LOW | No -- project uses anthropic, not openai directly |

### Project Direct Dependencies -- Assessment

| Package | Pinned Range | Known CVEs | Status |
|---------|-------------|------------|--------|
| vanna >=2.0.2 | OK | None known | PASS |
| fastapi >=0.115.6 | OK | None known | PASS |
| uvicorn >=0.34.0 | OK | None known | PASS |
| pydantic >=2.5.0 | OK | None known | PASS |
| pydantic-settings >=2.0.0 | OK | None known | PASS |
| psycopg2-binary >=2.9.10 | OK | None known | PASS |
| anthropic >=0.41.0 | OK | None known | PASS |
| pyjwt >=2.8.1 | OK | None known | PASS |
| bcrypt >=4.1.0 | OK | None known | PASS |
| redis >=5.0.0 | OK | None known | PASS |
| yfinance >=0.2.35 | OK | None known | PASS |
| lxml >=4.10.0,<6.0 | OK | None known | PASS |
| beautifulsoup4 >=4.12.0 | OK | None known | PASS |
| requests >=2.31.0 | OK | None known (consider upgrade to >=2.32.2) | PASS |
| plotly >=5.20.0 | OK | None known | PASS |
| pandas >=2.1.0 | OK | None known | PASS |
| numpy >=1.24.0 | OK | None known | PASS |
| apscheduler >=3.10.4 | OK | None known | PASS |
| pytest >=8.0.0 | OK | None known | PASS |
| httpx >=0.27.0 | OK | None known | PASS |

### Recommendation
- **requests**: Consider bumping minimum to `>=2.32.2` for compatibility with other tools, though no CVEs affect 2.31.0.
- Install `pip-audit` in CI for automated CVE scanning: `pip install pip-audit && pip-audit -r requirements.txt`

---

## Frontend Dependencies (`frontend/package.json`)

### Audit Method
`npm audit` was run against the frontend directory.

### Findings

| Package | Severity | Description | Fix | Status |
|---------|----------|-------------|-----|--------|
| `glob` 10.2.0-10.4.5 | **HIGH** | Command injection via -c/--cmd with shell:true (GHSA-5j98-mcp5-4vw2) | Update eslint-config-next to >=16.x | DEFERRED (dev dependency only) |
| `next` 10.0.0-15.5.9 | **HIGH** | DoS via Image Optimizer remotePatterns (GHSA-9g9p-9gw9-jx7f) | Upgrade next to >=15.6.0 | RECOMMEND |
| `next` 10.0.0-15.5.9 | **HIGH** | HTTP deserialization DoS with insecure RSC (GHSA-h25m-26qc-wcjf) | Upgrade next to >=15.6.0 | RECOMMEND |

**Total: 4 high severity vulnerabilities (2 unique, affecting 3 packages)**

### Analysis

1. **glob (HIGH)** -- This is a transitive dependency of `@next/eslint-plugin-next`, which is a **dev-only** dependency. The command injection requires running `glob` CLI with user-controlled input, which does not happen in this project. **Risk: None in production.** Fix by upgrading eslint-config-next when convenient.

2. **next (2x HIGH)** -- The Next.js vulnerabilities affect:
   - **Image Optimizer DoS**: Relevant if using `<Image>` component with `remotePatterns`. The project may use this for stock logos or news images.
   - **HTTP deserialization DoS with insecure RSC**: Only affects self-hosted Next.js apps using React Server Components.

### Recommendation

- **next**: Upgrade from current version to a patched release. Run `npm audit fix --force` to upgrade to next@16.x (breaking change -- requires testing).
- **glob/eslint-config-next**: Dev-only, low urgency. Upgrade when doing a major Next.js version bump.
- Add `npm audit` to CI pipeline to catch future vulnerabilities automatically.

**Note:** Since `frontend/src/` is out of scope for this agent, the actual upgrade should be performed by the frontend-harden team after testing.

---

## Summary

| Area | Critical | High | Medium | Low | Action Required |
|------|----------|------|--------|-----|----------------|
| Python (requirements.txt) | 0 | 0 | 0 | 0 | None (add pip-audit to CI) |
| Frontend (package.json) | 0 | 4 | 0 | 0 | Upgrade Next.js (frontend team) |

**Overall Status:** No critical or high vulnerabilities in Python production dependencies. Frontend has high-severity Next.js vulnerabilities that should be addressed by the frontend team.
</file>

<file path="scripts/deploy_checklist.md">
# Ra'd AI TASI Platform - Deployment Checklist

## Target: Railway (raid-ai-tasi project)

---

## Pre-Deploy Checklist

### Environment Variables (Railway Dashboard)
- [ ] `DB_BACKEND=postgres`
- [ ] `POSTGRES_HOST` set (use `postgres.railway.internal` for Railway PG)
- [ ] `POSTGRES_PORT` set (default: 5432)
- [ ] `POSTGRES_DB` set
- [ ] `POSTGRES_USER` set
- [ ] `POSTGRES_PASSWORD` set (strong password)
- [ ] `ANTHROPIC_API_KEY` or `GEMINI_API_KEY` set
- [ ] `AUTH_JWT_SECRET` set to a stable value (generate: `python -c "import secrets; print(secrets.token_urlsafe(32))"`)
- [ ] `ENVIRONMENT=production`
- [ ] `MW_CORS_ORIGINS` includes production frontend URL

### Code Checks
- [ ] All CI checks pass on master branch
- [ ] `ruff check .` and `ruff format --check .` pass locally
- [ ] `python -m pytest tests/ -v` passes (SQLite tests)
- [ ] No secrets committed (check `.env` is in `.gitignore`)
- [ ] `railway.toml` health check path matches actual route (`/health`)

### Database
- [ ] PostgreSQL schema is up to date (`database/schema.sql`)
- [ ] Migration script tested if schema changed
- [ ] Backup of production database taken before deploy

---

## Deploy Steps

1. Push to `master` branch (triggers CI via GitHub Actions)
2. Wait for CI workflow to complete successfully
3. Deploy workflow triggers automatically on CI success
4. Monitor Railway deployment logs for errors
5. Verify health check passes: `curl https://<DEPLOY_URL>/health`
6. Verify readiness: `curl https://<DEPLOY_URL>/health/ready`
7. Test chat endpoint: `curl https://<DEPLOY_URL>/api/vanna/v2/chat_sse`

---

## Post-Deploy Verification

- [ ] `/health` returns `{"status": "healthy", ...}` with HTTP 200
- [ ] `/health/ready` returns `{"status": "ready"}` with HTTP 200
- [ ] Frontend loads correctly at deployment URL
- [ ] Chat SSE endpoint responds
- [ ] News feed API returns data: `/api/v1/news/feed`
- [ ] TASI index API returns data: `/api/v1/charts/tasi/index`

---

## Rollback Procedure

If the deployment is unhealthy:

1. **Immediate**: In Railway Dashboard, click "Rollback" on the failed deployment
   - Railway keeps previous deployments; rolling back restores the last working image

2. **Manual rollback via CLI**:
   ```bash
   # List recent deployments
   railway logs --last 50

   # Redeploy previous commit
   git revert HEAD
   git push origin master
   ```

3. **Database rollback** (if schema migration was applied):
   - Restore from the pre-deploy backup
   - Or manually revert schema changes using `psql`

4. **Verify rollback**:
   ```bash
   curl -s https://<DEPLOY_URL>/health | python -m json.tool
   ```

---

## Monitoring

- Railway Dashboard: deployment logs, metrics, resource usage
- Health endpoint: `/health` (full report), `/health/live` (liveness), `/health/ready` (readiness)
- Application logs: structured JSON in production (via `config/logging_config.py`)
</file>

<file path="scripts/ENDPOINT_AUTH_AUDIT.md">
# Endpoint Authentication Consistency Audit

**Date:** 2026-02-13
**Scope:** All routes in `app.py` and `api/routes/` (16 route files)
**Author:** security-harden agent

---

## Summary

| Category | Count |
|----------|-------|
| Total endpoints audited | 54 |
| Auth required (JWT) | 13 |
| Public (no auth) | 38 |
| Conditional (PG mode) | 2 |
| Always public (by design) | 1 (health) |

All write endpoints consistently require `Depends(get_current_user)`. All public read endpoints serve market data, charts, or news -- appropriate for a financial data platform.

---

## Endpoint Audit Table

### Auth Routes (`api/routes/auth.py`, prefix: `/api/auth`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/auth/register` | POST | No | Public -- creates new user | OK (registration must be public) |
| `/api/auth/login` | POST | No | Public -- authenticates user | OK (login must be public) |
| `/api/auth/guest` | POST | No | Public -- issues guest token | OK (guest access by design) |
| `/api/auth/refresh` | POST | No (token in body) | Validates refresh token from request body | OK (standard refresh pattern) |
| `/api/auth/me` | GET | **YES** | `Depends(get_current_user)` | OK |

### Watchlist Routes (`api/routes/watchlists.py`, prefix: `/api/watchlists`) -- PG only

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/watchlists` | GET | **YES** | `Depends(get_current_user)` | OK (SA-01 fixed) |
| `/api/watchlists` | POST | **YES** | `Depends(get_current_user)` | OK |
| `/api/watchlists/{id}/tickers` | POST | **YES** | `Depends(get_current_user)` | OK |
| `/api/watchlists/{id}` | PATCH | **YES** | `Depends(get_current_user)` | OK |
| `/api/watchlists/{id}` | DELETE | **YES** | `Depends(get_current_user)` | OK |
| `/api/watchlists/alerts` | GET | **YES** | `Depends(get_current_user)` | OK (SA-01 fixed) |
| `/api/watchlists/alerts` | POST | **YES** | `Depends(get_current_user)` | OK |
| `/api/watchlists/alerts/{id}` | DELETE | **YES** | `Depends(get_current_user)` | OK |

### News Routes (`api/routes/news.py`, prefix: `/api/news`) -- PG only

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/news` | GET | No | Public read | OK (news is public) |
| `/api/news/ticker/{ticker}` | GET | No | Public read | OK |
| `/api/news/sector/{sector}` | GET | No | Public read | OK |
| `/api/news/{article_id}` | GET | No | Public read | OK |
| `/api/news` | POST | **YES** | `Depends(get_current_user)` | OK |

### Reports Routes (`api/routes/reports.py`, prefix: `/api/reports`) -- PG only

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/reports` | GET | No | Public read | OK |
| `/api/reports/ticker/{ticker}` | GET | No | Public read | OK |
| `/api/reports/{report_id}` | GET | No | Public read | OK |
| `/api/reports` | POST | **YES** | `Depends(get_current_user)` | OK |

### Announcements Routes (`api/routes/announcements.py`, prefix: `/api/announcements`) -- PG only

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/announcements` | GET | No | Public read | OK |
| `/api/announcements/material` | GET | No | Public read | OK |
| `/api/announcements/sector/{sector}` | GET | No | Public read | OK |
| `/api/announcements/{id}` | GET | No | Public read | OK |
| `/api/announcements` | POST | **YES** | `Depends(get_current_user)` | OK |

### Health Route (`api/routes/health.py`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/health` | GET | No | Public | OK (health checks must be public for LB probes) |

### TASI Index Routes (`api/routes/tasi_index.py`, prefix: `/api/v1/charts/tasi`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/v1/charts/tasi/index` | GET | No | Public | OK (public market data) |
| `/api/v1/charts/tasi/health` | GET | No | Public | OK (SA-03 fixed -- no infra details exposed) |

### Stock OHLCV Routes (`api/routes/stock_ohlcv.py`, prefix: `/api/v1/charts`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/v1/charts/{ticker}/ohlcv` | GET | No | Public | OK (public market data) |
| `/api/v1/charts/{ticker}/health` | GET | No | Public | OK (no infra details exposed) |

### News Feed Routes (`api/routes/news_feed.py`, prefix: `/api/v1/news`) -- SQLite-backed

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/v1/news/feed` | GET | No | Public read | OK |
| `/api/v1/news/feed/{article_id}` | GET | No | Public read | OK |
| `/api/v1/news/search` | GET | No | Public read | OK |
| `/api/v1/news/sources` | GET | No | Public read | OK |

### Chart Analytics Routes (`api/routes/charts_analytics.py`, prefix: `/api/charts`) -- Dual-backend

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/charts/sector-market-cap` | GET | No | Public | OK (aggregated market data) |
| `/api/charts/top-companies` | GET | No | Public | OK |
| `/api/charts/sector-pe` | GET | No | Public | OK |
| `/api/charts/dividend-yield-top` | GET | No | Public | OK |

### Market Analytics Routes (`api/routes/market_analytics.py`, prefix: `/api/v1/market`) -- Dual-backend

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/v1/market/movers` | GET | No | Public | OK |
| `/api/v1/market/summary` | GET | No | Public | OK |
| `/api/v1/market/sectors` | GET | No | Public | OK |
| `/api/v1/market/heatmap` | GET | No | Public | OK |

### Stock Data Routes (`api/routes/stock_data.py`, prefix: `/api/v1/stocks`) -- Dual-backend

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/v1/stocks/{ticker}/dividends` | GET | No | Public | OK |
| `/api/v1/stocks/{ticker}/summary` | GET | No | Public | OK |
| `/api/v1/stocks/{ticker}/financials` | GET | No | Public | OK |
| `/api/v1/stocks/compare` | GET | No | Public | OK |
| `/api/v1/stocks/quotes` | GET | No | Public | OK |

### Entity Routes -- SQLite (`api/routes/sqlite_entities.py`, prefix: `/api/entities`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/entities` | GET | No | Public | OK |
| `/api/entities/sectors` | GET | No | Public | OK |
| `/api/entities/{ticker}` | GET | No | Public | OK |

### Entity Routes -- PG (`api/routes/entities.py`, prefix: `/api/entities`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/entities` | GET | No | Public | OK |
| `/api/entities/sectors` | GET | No | Public | OK |
| `/api/entities/{ticker}` | GET | No | Public | OK |

### Chart Routes -- PG (`api/routes/charts.py`, prefix: `/api/charts`)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/charts/sector-market-cap` | GET | No | Public | OK |
| `/api/charts/top-companies` | GET | No | Public | OK |
| `/api/charts/sector-pe` | GET | No | Public | OK |
| `/api/charts/dividend-yield-top` | GET | No | Public | OK |

### Custom Routes (app.py)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/` | GET | No | Serves `templates/index.html` | OK (public UI) |
| `/favicon.ico` | GET | No | Serves favicon | OK |
| `/static/*` | GET | No | Static file serving | OK |

### Vanna Chat Endpoints (registered by VannaFastAPIServer)

| Endpoint | Method | Auth Required | Current State | Action |
|----------|--------|---------------|---------------|--------|
| `/api/vanna/v2/chat_sse` | POST | Conditional | PG mode: validates token if present, rejects invalid; allows anonymous. SQLite: no validation. | OK (acceptable for MVP) |
| `/api/vanna/v2/chat_poll` | POST | Conditional | Same as chat_sse | OK |

---

## Assessment

### Consistency: PASS

The auth model is **consistent and intentional**:
- All **write operations** (POST for news, reports, announcements; all watchlist/alert CRUD) require JWT auth via `Depends(get_current_user)`
- All **read operations** for market data, charts, entities, and news are public
- The **watchlist/alert IDOR (SA-01)** has been fully remediated -- both read and write endpoints now use `Depends(get_current_user)` instead of `X-User-Id` header
- **Vanna chat** validates tokens when present but allows anonymous access (appropriate for a demo/MVP)

### No Action Items

No auth inconsistencies were found. The current model follows a clear pattern:
- Public: market data reads, health checks, auth endpoints (login/register/guest)
- Authenticated: user-specific data (watchlists, alerts), content creation (news/reports/announcements POST)
</file>

<file path="scripts/run_pg_tests.sh">
#!/bin/bash
# =============================================================================
# PostgreSQL Integration Test Runner
# =============================================================================
# Starts PostgreSQL via Docker Compose, sets environment variables, initializes
# the schema if needed, and runs ALL test suites with PG backend enabled.
#
# Usage:
#   bash scripts/run_pg_tests.sh              # Start PG, run tests, leave PG up
#   bash scripts/run_pg_tests.sh --down       # Tear down PG containers after tests
#   bash scripts/run_pg_tests.sh --pg-only    # Run only PG-specific tests (skip SQLite)
#   bash scripts/run_pg_tests.sh --down --pg-only
#
# Prerequisites:
#   - Docker and Docker Compose installed
#   - Python venv with psycopg2, pytest installed
#   - Run from project root (or script resolves it automatically)
#
# Environment variables (all have defaults):
#   POSTGRES_DB       (default: tasi_platform)
#   POSTGRES_USER     (default: tasi_user)
#   POSTGRES_PASSWORD (default: changeme)
#   POSTGRES_PORT     (default: 5432)
# =============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"
cd "$PROJECT_DIR"

# Parse arguments
TEARDOWN=false
PG_ONLY=false
for arg in "$@"; do
    case "$arg" in
        --down)    TEARDOWN=true ;;
        --pg-only) PG_ONLY=true ;;
        *)         echo "Unknown argument: $arg"; exit 1 ;;
    esac
done

echo "============================================================"
echo " Ra'd AI - PostgreSQL Integration Test Runner"
echo "============================================================"
echo " Project:   $PROJECT_DIR"
echo " Teardown:  $TEARDOWN"
echo " PG-only:   $PG_ONLY"
echo "============================================================"
echo ""

# ---- Step 1: Start PostgreSQL via Docker Compose ----
echo "[1/6] Starting PostgreSQL via Docker Compose..."
export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-changeme}"
docker compose up -d postgres
echo ""

# ---- Step 2: Wait for PostgreSQL readiness ----
echo "[2/6] Waiting for PostgreSQL to become ready..."
MAX_WAIT=30
WAITED=0
PG_USER="${POSTGRES_USER:-tasi_user}"
PG_DB="${POSTGRES_DB:-tasi_platform}"
until docker compose exec -T postgres pg_isready -U "$PG_USER" -d "$PG_DB" 2>/dev/null; do
    WAITED=$((WAITED + 1))
    if [ "$WAITED" -ge "$MAX_WAIT" ]; then
        echo "ERROR: PostgreSQL did not become ready within ${MAX_WAIT}s"
        echo "Check: docker compose logs postgres"
        exit 1
    fi
    sleep 1
done
echo "PostgreSQL ready (waited ${WAITED}s)."
echo ""

# ---- Step 3: Set environment variables for test processes ----
echo "[3/6] Setting environment variables..."
export DB_BACKEND=postgres
export POSTGRES_HOST=localhost
export POSTGRES_PORT="${POSTGRES_PORT:-5432}"
export POSTGRES_DB="${POSTGRES_DB:-tasi_platform}"
export POSTGRES_USER="${POSTGRES_USER:-tasi_user}"
# PG_* aliases for csv_to_postgres.py compatibility
export PG_HOST="$POSTGRES_HOST"
export PG_PORT="$POSTGRES_PORT"
export PG_DBNAME="$POSTGRES_DB"
export PG_USER="$POSTGRES_USER"
export PG_PASSWORD="$POSTGRES_PASSWORD"

echo "  DB_BACKEND=$DB_BACKEND"
echo "  POSTGRES_HOST=$POSTGRES_HOST"
echo "  POSTGRES_PORT=$POSTGRES_PORT"
echo "  POSTGRES_DB=$POSTGRES_DB"
echo "  POSTGRES_USER=$POSTGRES_USER"
echo ""

# ---- Step 4: Initialize schema + load data if needed ----
echo "[4/6] Checking database initialization..."
TABLES_EXIST=$(python -c "
import psycopg2, os
conn = psycopg2.connect(
    host=os.environ['POSTGRES_HOST'],
    port=os.environ['POSTGRES_PORT'],
    dbname=os.environ['POSTGRES_DB'],
    user=os.environ['POSTGRES_USER'],
    password=os.environ['POSTGRES_PASSWORD']
)
cur = conn.cursor()
cur.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'companies'\")
print(cur.fetchone()[0])
cur.close()
conn.close()
" 2>/dev/null || echo "0")

if [ "$TABLES_EXIST" = "0" ]; then
    echo "  Database needs initialization. Loading schema + data..."
    if [ -f "database/csv_to_postgres.py" ]; then
        python database/csv_to_postgres.py \
            --pg-host "$POSTGRES_HOST" \
            --pg-port "$POSTGRES_PORT" \
            --pg-dbname "$POSTGRES_DB" \
            --pg-user "$POSTGRES_USER" \
            --pg-password "$POSTGRES_PASSWORD"
    else
        echo "  WARNING: database/csv_to_postgres.py not found. Schema may be incomplete."
        # Attempt to load schema.sql directly
        if [ -f "database/schema.sql" ]; then
            echo "  Loading schema.sql via psql..."
            docker compose exec -T postgres psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" < database/schema.sql
        fi
    fi
else
    echo "  Database already initialized (companies table exists)."
fi
echo ""

# ---- Step 5: Run test suites ----
echo "============================================================"
echo "[5/6] Running test suites"
echo "============================================================"
FAILURES=0
TOTAL_PASS=0
TOTAL_FAIL=0

# 5a: pytest tests/ (includes PG fixtures from conftest.py)
echo ""
echo "--- pytest tests/ ---"
if [ "$PG_ONLY" = true ]; then
    # Run only tests that use PG markers/fixtures
    python -m pytest tests/ -v --tb=short -k "PG or pg_" 2>&1 || FAILURES=$((FAILURES + 1))
else
    python -m pytest tests/ -v --tb=short 2>&1 || FAILURES=$((FAILURES + 1))
fi

# 5b: test_database.py (includes TestDatabaseIntegrityPG)
echo ""
echo "--- test_database.py ---"
if [ "$PG_ONLY" = true ]; then
    python -m pytest test_database.py -v --tb=short -k "PG" 2>&1 || FAILURES=$((FAILURES + 1))
else
    python test_database.py 2>&1 || FAILURES=$((FAILURES + 1))
fi

# 5c: test_app_assembly_v2.py (includes PG assembly tests)
echo ""
echo "--- test_app_assembly_v2.py ---"
python test_app_assembly_v2.py 2>&1 || FAILURES=$((FAILURES + 1))

# ---- Step 6: Report results & optional teardown ----
echo ""
echo "============================================================"
echo "[6/6] RESULTS"
echo "============================================================"
if [ "$FAILURES" -eq 0 ]; then
    echo "All test suites passed with PostgreSQL backend."
else
    echo "WARNING: $FAILURES test suite(s) had failures."
fi

if [ "$TEARDOWN" = true ]; then
    echo ""
    echo "Tearing down PostgreSQL container..."
    docker compose down
    echo "Done."
fi

exit $FAILURES
</file>

<file path="scripts/SECURITY_AUDIT.md">
# Security Audit Report -- Ra'd AI TASI Platform

**Date:** 2026-02-10
**Scope:** Authentication, authorization, rate limiting, CORS, new TASI endpoints, common web vulnerabilities
**Auditor:** security-audit agent

---

## Summary

| Severity | Count |
|----------|-------|
| CRITICAL | 1     |
| HIGH     | 1     |
| MEDIUM   | 2     |
| LOW      | 2     |
| INFO     | 4     |

---

## Findings

### SA-01: Watchlist/Alert Read Endpoints IDOR via X-User-Id Header

**Severity:** CRITICAL
**Affected files:** `api/routes/watchlists.py` (lines 32-42, 134-152)
**Status:** PENDING (requires auth model change)

**Description:**
The `GET /api/watchlists` and `GET /api/watchlists/alerts` endpoints accept a client-supplied `X-User-Id` header to scope data. Any unauthenticated caller can read any user's watchlists and alerts by setting this header to an arbitrary user ID.

**Risk:** Full unauthorized read access to all users' watchlist and alert data (Insecure Direct Object Reference).

**Recommended Fix:**
Replace `X-User-Id` header with `Depends(get_current_user)` on read endpoints, matching the write endpoints. If backward compatibility is needed, gate the `X-User-Id` path behind admin auth.

---

### SA-02: Marked.js Rendering Without HTML Sanitization

**Severity:** HIGH
**Affected files:** `templates/index.html` (lines 1109-1116, 1317, 1321)
**Status:** PENDING

**Description:**
The `renderMd()` function calls `marked.parse(text)` on AI assistant responses and inserts the result via `innerHTML`. While the text originates from the AI (not directly from user input), if the AI echoes user input in its markdown response, crafted payloads could achieve stored XSS.

Marked.js does not sanitize HTML by default since v4.0. The `sanitize` option was removed; users are expected to pair it with DOMPurify.

**Risk:** Potential stored XSS if attacker crafts a prompt that causes the AI to echo malicious HTML/JS in its response.

**Recommended Fix:**
Add DOMPurify and sanitize marked output:
```js
// In <head>:
// <script src="https://cdn.jsdelivr.net/npm/dompurify@3/dist/purify.min.js"></script>
function renderMd(text) {
    if (window.marked) {
        var raw = marked.parse(text);
        return window.DOMPurify ? DOMPurify.sanitize(raw) : raw;
    }
    return esc(text).replace(/\n/g, '<br>');
}
```

---

### SA-03: TASI Health Endpoint Leaked Infrastructure Details

**Severity:** MEDIUM
**Affected files:** `api/routes/tasi_index.py` (lines 84-126)
**Status:** FIXED

**Description:**
The `GET /api/v1/charts/tasi/health` endpoint previously exposed:
- `yfinance_available` (boolean) -- reveals backend dependency
- `cache_status` (fresh/stale/empty) -- reveals caching strategy and TTL
- `cache_age_seconds` -- reveals exact cache timing
- `circuit_state`, `consecutive_failures` -- reveals circuit breaker internals

**Risk:** Information disclosure enabling targeted attacks (cache timing attacks, dependency fingerprinting).

**Fix applied:**
Response now returns only `{ "status": "ok"|"degraded", "message": "..." }`. Full diagnostics are logged server-side at DEBUG level. Tests updated to verify no internal fields are exposed.

---

### SA-04: JWT Secret Defaults to Random on Each Restart

**Severity:** MEDIUM
**Affected files:** `config/settings.py` (line 105)
**Status:** PENDING (by design for dev, needs production enforcement)

**Description:**
`AuthSettings.jwt_secret` defaults to `secrets.token_urlsafe(32)`, generating a new random secret on every server startup. All issued JWT tokens become invalid after restart. In production on Railway, if `AUTH_JWT_SECRET` is not set, users will be silently logged out on every deploy.

**Risk:** User session instability in production; may go unnoticed since the error manifests as "expired token" rather than a clear misconfiguration error.

**Recommended Fix:**
Add a startup check in `app.py` that warns or fails if `AUTH_JWT_SECRET` is not explicitly set when `DB_BACKEND=postgres` (production mode):
```python
if DB_BACKEND == "postgres" and _settings and _settings.auth.jwt_secret == "change-me-to-a-stable-secret":
    logger.warning("AUTH_JWT_SECRET not configured -- tokens will not persist across restarts")
```

---

### SA-05: CORS Origins Missing Production Domain

**Severity:** LOW
**Affected files:** `config/settings.py` (line 116), `app.py` (lines 408-412)
**Status:** PENDING (deployment config, not code fix)

**Description:**
Default CORS origins are `http://localhost:3000` and `http://localhost:8084`. The production domain (`raid-ai-app-production.up.railway.app`) is not in the defaults. The legacy UI works because it is served from the same origin (no CORS needed), but the Next.js frontend in production would be blocked.

**Risk:** Next.js frontend in production cannot call the API (CORS preflight failures).

**Recommended Fix:**
Set `MW_CORS_ORIGINS` in Railway environment variables to include the production domain:
```
MW_CORS_ORIGINS=http://localhost:3000,http://localhost:8084,https://raid-ai-app-production.up.railway.app
```

---

### SA-06: Rate Limiting Disabled in Debug Mode

**Severity:** LOW
**Affected files:** `app.py` (lines 428-434)
**Status:** PENDING (acceptable for dev, verify production config)

**Description:**
Rate limiting middleware is skipped when `SERVER_DEBUG=true`. If debug mode is accidentally enabled in production, the rate limiter is bypassed entirely.

**Risk:** If `SERVER_DEBUG=true` leaks into production config, no rate limiting protection.

**Recommended Fix:**
Verify Railway deployment does NOT set `SERVER_DEBUG=true`. Consider logging a warning when debug mode is active.

---

### SA-07: All Chart Endpoints Are Unauthenticated

**Severity:** INFO
**Affected files:** `api/routes/charts.py`, `api/routes/tasi_index.py`
**Status:** ACCEPTED (by design)

**Description:**
All chart endpoints (`/api/charts/*` and `/api/v1/charts/tasi/*`) are public, requiring no authentication. This is consistent across both existing chart endpoints and the new TASI endpoints.

**Assessment:** Market data charts are public by nature. Consistent behavior across all chart routes. No action needed.

---

### SA-08: SQL Injection Assessment -- PASS

**Severity:** INFO
**Affected files:** All `api/routes/*.py`, `services/*.py`
**Status:** PASS

**Description:**
All API routes use parameterized queries (`%(param)s` with psycopg2). No user-supplied input is interpolated into SQL via f-strings or `.format()`. The f-strings used in `entities.py`, `charts.py`, and service files construct only WHERE clause scaffolding from hardcoded strings; actual values are always parameterized.

Migration and CSV scripts (`database/migrate_sqlite_to_pg.py`, `csv_to_sqlite.py`) use f-strings for table names, but these are hardcoded constants (not user input) and are offline tools.

---

### SA-09: XSS Assessment -- Mostly Safe

**Severity:** INFO
**Affected files:** `templates/index.html`
**Status:** PASS (with SA-02 caveat)

**Description:**
- User messages: escaped via `esc()` function before innerHTML insertion (safe)
- Error messages: escaped via `esc()` (safe)
- Notification messages: escaped via `esc()` (safe)
- DataFrame rendering: cell values escaped via `esc()` (safe)
- Artifact rendering: uses `iframe` with `sandbox="allow-scripts"` (acceptable for AI-generated content)
- Markdown rendering: see SA-02 (marked.js without DOMPurify)

---

### SA-10: Secrets and Configuration Assessment -- PASS

**Severity:** INFO
**Affected files:** `.gitignore`, `.env.example`
**Status:** PASS

**Description:**
- `.env` is in `.gitignore` (confirmed)
- `.env.example` contains only placeholder values (no real secrets)
- No hardcoded API keys in source code (API key loaded from environment)
- No SSRF risk: yfinance only fetches from Yahoo Finance APIs with hardcoded symbols
- No path traversal: no endpoints accept user-supplied file paths
- Error handler middleware catches unhandled exceptions and returns generic 500 (no stack trace leakage)

---

## Rate Limiting Assessment

The rate limiting middleware (`middleware/rate_limit.py`) provides:
- Per-IP sliding window (60 req/min default)
- Applies to all routes except `/health`
- `Retry-After` header on 429 responses
- Periodic cleanup of stale IP entries

The TASI endpoint has an additional protection via `threading.Lock` in `services/tasi_index.py` (line 137) that serializes yfinance API calls, preventing thundering herd on cold cache.

**Verdict:** Rate limiting is properly applied to TASI endpoints. No changes needed.

---

## CORS Assessment

CORS middleware (`middleware/cors.py`) via FastAPI's `CORSMiddleware`:
- `allow_credentials=True`
- `allow_methods=["*"]`
- `allow_headers=["*"]`
- Origins configurable via `MW_CORS_ORIGINS` env var

Default origins include `localhost:3000` (Next.js dev) and `localhost:8084` (backend). Production domain must be added via environment variable (see SA-05).

---

## Recommendations Priority

1. **CRITICAL** -- Fix SA-01 (Watchlist IDOR) before production launch
2. **HIGH** -- Fix SA-02 (add DOMPurify to marked.js rendering)
3. **MEDIUM** -- SA-04 (add startup warning for unstable JWT secret)
4. **LOW** -- SA-05 (set MW_CORS_ORIGINS in Railway), SA-06 (verify no debug in prod)
</file>

<file path="scripts/SECURITY_REMEDIATION.md">
# Security Remediation Plan -- Ra'd AI TASI Platform

**Date:** 2026-02-13
**Based on:** `scripts/SECURITY_AUDIT.md` (2026-02-10) + middleware/route code review
**Author:** security-harden agent

---

## Remediation Priority Table

| # | Finding | Severity | Priority | File(s) | Current State | Fix Required | Status |
|---|---------|----------|----------|---------|---------------|-------------|--------|
| SA-01 | Watchlist/Alert IDOR via X-User-Id | CRITICAL | P0 | `api/routes/watchlists.py` | **FIXED** -- All read endpoints now use `Depends(get_current_user)` instead of X-User-Id header | No further action | RESOLVED |
| SA-02 | Marked.js without DOMPurify | HIGH | P1 | `templates/index.html` | `marked.parse()` output inserted via innerHTML without sanitization | Add DOMPurify CDN + sanitize in `renderMd()` | PENDING (DO NOT MODIFY -- templates/index.html is out of scope) |
| SA-03 | TASI health leaked infra details | MEDIUM | P1 | `api/routes/tasi_index.py` | **FIXED** -- Returns only `{status, message}`, diagnostics logged server-side | No further action | RESOLVED |
| SA-04 | JWT secret defaults to random on restart | MEDIUM | P1 | `config/settings.py`, `app.py` | **FIXED** -- `AuthSettings` has `model_validator` warning; `app.py` lifespan checks `AUTH_JWT_SECRET` in PG mode | No further action | RESOLVED |
| SA-05 | CORS origins missing production domain | LOW | P2 | `config/settings.py` | **FIXED** -- Default `cors_origins` now includes `raid-ai-app-production.up.railway.app` and Vercel frontend | No further action | RESOLVED |
| SA-06 | Rate limiting disabled in debug mode | LOW | P2 | `app.py` | **FIXED** -- `app.py` lifespan logs warning when debug mode is ON | Verify Railway config does not set `SERVER_DEBUG=true` | RESOLVED (with ops verification needed) |
| SA-07 | Chart endpoints unauthenticated | INFO | -- | `api/routes/charts*.py`, `tasi_index.py` | By design: market data is public | No action | ACCEPTED |
| SA-08 | SQL injection assessment | INFO | -- | All routes | PASS: all queries use parameterized placeholders | No action | PASS |
| SA-09 | XSS assessment | INFO | -- | `templates/index.html` | PASS (except SA-02 caveat) | See SA-02 | PASS |
| SA-10 | Secrets/config assessment | INFO | -- | `.gitignore`, `.env.example` | PASS: no hardcoded secrets | No action | PASS |

---

## Current Security Posture Summary

### Middleware Stack (app.py lines 216-279)

The middleware stack is properly ordered (outermost first):
1. **ErrorHandlerMiddleware** -- catches unhandled exceptions, returns safe JSON
2. **RequestLoggingMiddleware** -- logs method/path/status/duration per request
3. **RateLimitMiddleware** -- 60 req/min per IP, sliding window (skipped in debug)
4. **CORSMiddleware** -- configured origins, credentials enabled, specific methods/headers

### Rate Limiting (`middleware/rate_limit.py`)

- **Algorithm**: Per-IP sliding window using `deque` of timestamps
- **Default limit**: 60 requests/minute (configurable via `MW_RATE_LIMIT_PER_MINUTE`)
- **Skip paths**: `/health` only
- **Coverage**: All routes except health. No differentiated limits for chart data or auth endpoints.
- **Gap**: Single rate limit tier for all endpoints. Auth endpoints (login, register) and chart data endpoints share the same 60 req/min limit. Consider adding:
  - 10 req/min for `/api/auth/login` and `/api/auth/register` (brute force protection)
  - 30 req/min for chart data endpoints (yfinance upstream protection)

### CORS (`middleware/cors.py`)

- **Origins**: Configurable via `MW_CORS_ORIGINS`. Defaults include localhost:3000, localhost:8084, Vercel frontend, and Railway production domain.
- **Dynamic origins**: `app.py` also adds `FRONTEND_URL` and `RAILWAY_PUBLIC_DOMAIN` env vars at startup.
- **Methods**: Specific list: GET, POST, PUT, PATCH, DELETE, OPTIONS (good -- not wildcard)
- **Headers**: Specific list: Authorization, Content-Type, Accept, Origin, X-Requested-With, X-User-Id (good -- not wildcard)
- **Credentials**: `allow_credentials=True` (correct for JWT auth)

### Error Handler (`middleware/error_handler.py`)

- Maps known exception types to appropriate HTTP codes (400, 403, 404)
- Unknown exceptions return 500 with generic message (no stack trace)
- Debug mode exposes error message (not traceback) for developer convenience
- Properly logs full traceback server-side

### Authentication

- **JWT-based**: `auth/jwt_handler.py` creates access/refresh tokens
- **Dependency injection**: `auth/dependencies.py` provides `get_current_user`, `get_optional_current_user`, `require_admin`
- **Watchlist endpoints**: All use `Depends(get_current_user)` (SA-01 was fixed)
- **Write endpoints**: news POST, reports POST, announcements POST all require auth
- **Read endpoints**: Market data, charts, entities, news feeds are public (by design)
- **Vanna chat**: In PG mode, invalid tokens rejected; anonymous access allowed

---

## Remaining Action Items

### P1: DOMPurify for Marked.js (SA-02)
- **Owner**: frontend-harden team
- **Note**: `templates/index.html` is marked as DO NOT MODIFY for this agent
- **Action**: Add DOMPurify script tag and wrap `marked.parse()` output in `DOMPurify.sanitize()`

### P2: Tiered Rate Limiting
- **Owner**: security-harden agent (Task 3)
- **Action**: Review and extend rate limiting to support endpoint-specific limits

### P2: Production Configuration Verification
- **Owner**: infra-deploy team
- **Action**: Verify Railway environment does NOT have `SERVER_DEBUG=true`
- **Action**: Verify `AUTH_JWT_SECRET` is set in Railway environment variables

---

## Appendix: Files Audited

- `scripts/SECURITY_AUDIT.md` -- source audit report
- `middleware/rate_limit.py` -- rate limiting implementation
- `middleware/error_handler.py` -- error handling implementation
- `middleware/cors.py` -- CORS configuration
- `middleware/request_logging.py` -- request logging
- `config/settings.py` -- all application settings with defaults
- `app.py` -- middleware registration, route registration, lifespan
- `auth/dependencies.py` -- auth dependency injection
- `auth/jwt_handler.py` -- JWT token creation/validation
- `api/routes/*.py` -- all 16 route files audited for auth and validation
</file>

<file path="scripts/test_pg.sh">
#!/bin/bash
# =============================================================================
# PostgreSQL Test Runner
# =============================================================================
# Starts PostgreSQL via Docker Compose, waits for health, runs all tests
# with the PG backend, and reports results.
#
# Usage:
#   bash scripts/test_pg.sh          # Start PG, run tests, leave PG running
#   bash scripts/test_pg.sh --down   # Run tests then stop PG after
#
# Prerequisites:
#   - Docker and Docker Compose available
#   - Project root as working directory
# =============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"
cd "$PROJECT_DIR"

TEARDOWN=false
if [[ "${1:-}" == "--down" ]]; then
    TEARDOWN=true
fi

echo "============================================================"
echo "PostgreSQL Test Runner"
echo "============================================================"
echo "Project: $PROJECT_DIR"
echo "Teardown after tests: $TEARDOWN"
echo ""

# ---- Step 1: Start PostgreSQL ----
echo "Step 1: Starting PostgreSQL via Docker Compose..."
docker compose up -d postgres
echo ""

# ---- Step 2: Wait for PG to be ready ----
echo "Step 2: Waiting for PostgreSQL to be ready..."
MAX_WAIT=30
WAITED=0
until docker compose exec -T postgres pg_isready -U "${POSTGRES_USER:-tasi_user}" -d "${POSTGRES_DB:-tasi_platform}" 2>/dev/null; do
    WAITED=$((WAITED + 1))
    if [ "$WAITED" -ge "$MAX_WAIT" ]; then
        echo "ERROR: PostgreSQL did not become ready within ${MAX_WAIT}s"
        exit 1
    fi
    sleep 1
done
echo "PostgreSQL is ready (waited ${WAITED}s)."
echo ""

# ---- Step 3: Set environment variables ----
echo "Step 3: Setting environment variables..."
export DB_BACKEND=postgresql
export POSTGRES_HOST=localhost
export POSTGRES_PORT="${POSTGRES_PORT:-5432}"
export POSTGRES_DB="${POSTGRES_DB:-tasi_platform}"
export POSTGRES_USER="${POSTGRES_USER:-tasi_user}"
export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-changeme}"
# Also set PG_* vars for csv_to_postgres.py compatibility
export PG_HOST="$POSTGRES_HOST"
export PG_PORT="$POSTGRES_PORT"
export PG_DBNAME="$POSTGRES_DB"
export PG_USER="$POSTGRES_USER"
export PG_PASSWORD="$POSTGRES_PASSWORD"

echo "  DB_BACKEND=$DB_BACKEND"
echo "  POSTGRES_HOST=$POSTGRES_HOST"
echo "  POSTGRES_DB=$POSTGRES_DB"
echo "  POSTGRES_USER=$POSTGRES_USER"
echo ""

# ---- Step 4: Initialize schema + load data if needed ----
echo "Step 4: Checking if database needs initialization..."
TABLES_EXIST=$(python -c "
import psycopg2, os
conn = psycopg2.connect(
    host=os.environ['POSTGRES_HOST'],
    port=os.environ['POSTGRES_PORT'],
    dbname=os.environ['POSTGRES_DB'],
    user=os.environ['POSTGRES_USER'],
    password=os.environ['POSTGRES_PASSWORD']
)
cur = conn.cursor()
cur.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'companies'\")
print(cur.fetchone()[0])
cur.close()
conn.close()
" 2>/dev/null || echo "0")

if [ "$TABLES_EXIST" = "0" ]; then
    echo "  Database needs initialization. Loading schema + data..."
    python database/csv_to_postgres.py \
        --pg-host "$POSTGRES_HOST" \
        --pg-port "$POSTGRES_PORT" \
        --pg-dbname "$POSTGRES_DB" \
        --pg-user "$POSTGRES_USER" \
        --pg-password "$POSTGRES_PASSWORD"
    echo ""
else
    echo "  Database already initialized (companies table exists)."
fi
echo ""

# ---- Step 5: Run tests ----
echo "============================================================"
echo "Step 5: Running test suites"
echo "============================================================"
FAILURES=0

echo ""
echo "--- pytest tests/ ---"
python -m pytest tests/ -v --tb=short 2>&1 || FAILURES=$((FAILURES + 1))

echo ""
echo "--- test_database.py ---"
python test_database.py 2>&1 || FAILURES=$((FAILURES + 1))

echo ""
echo "--- test_app_assembly_v2.py ---"
python test_app_assembly_v2.py 2>&1 || FAILURES=$((FAILURES + 1))

echo ""
echo "============================================================"
echo "TEST RESULTS"
echo "============================================================"
if [ "$FAILURES" -eq 0 ]; then
    echo "All test suites passed."
else
    echo "WARNING: $FAILURES test suite(s) had failures."
fi

# ---- Step 6: Teardown ----
if [ "$TEARDOWN" = true ]; then
    echo ""
    echo "Step 6: Tearing down PostgreSQL..."
    docker compose down postgres
fi

exit $FAILURES
</file>

<file path="services/__init__.py">
# Services package for TASI AI platform
</file>

<file path="services/widgets/__init__.py">
"""Live market widgets services."""
</file>

<file path="services/widgets/providers/__init__.py">
"""Market data provider fetchers for live widgets."""
</file>

<file path="services/widgets/providers/crypto.py">
"""Crypto price provider -- BTC and ETH via CoinGecko free API."""
⋮----
logger = logging.getLogger(__name__)
⋮----
_COINGECKO_URL = (
⋮----
_COIN_MAP = {
⋮----
async def fetch_crypto() -> List[QuoteItem]
⋮----
"""Fetch BTC and ETH prices from CoinGecko. Returns empty list on error."""
⋮----
resp = await client.get(_COINGECKO_URL)
⋮----
data = resp.json()
⋮----
now_iso = datetime.now(timezone.utc).isoformat()
quotes: List[QuoteItem] = []
⋮----
coin_data = data.get(coin_id)
⋮----
price = coin_data.get("usd")
⋮----
change_pct = coin_data.get("usd_24h_change")
</file>

<file path="services/widgets/providers/indices.py">
"""US equity index provider -- S&P 500, Dow Jones, Nasdaq via yfinance."""
⋮----
logger = logging.getLogger(__name__)
⋮----
_TICKERS = {
⋮----
def _fetch_indices_sync() -> List[QuoteItem]
⋮----
"""Synchronous yfinance fetch for major US indices."""
⋮----
now_iso = datetime.now(timezone.utc).isoformat()
quotes: List[QuoteItem] = []
⋮----
ticker = yf.Ticker(yf_ticker)
info = ticker.fast_info
price = getattr(info, "last_price", None)
prev_close = getattr(info, "previous_close", None)
⋮----
change = None
change_pct = None
⋮----
change = round(price - prev_close, 2)
change_pct = round((change / prev_close) * 100, 2)
⋮----
async def fetch_indices() -> List[QuoteItem]
⋮----
"""Fetch US index data. Returns empty list on error."""
</file>

<file path="services/widgets/providers/metals.py">
"""Precious metals provider -- Gold (XAU) and Silver (XAG) via yfinance."""
⋮----
logger = logging.getLogger(__name__)
⋮----
_TICKERS = {
⋮----
def _fetch_metals_sync() -> List[QuoteItem]
⋮----
"""Synchronous yfinance fetch for metals."""
⋮----
now_iso = datetime.now(timezone.utc).isoformat()
quotes: List[QuoteItem] = []
⋮----
ticker = yf.Ticker(yf_ticker)
info = ticker.fast_info
price = getattr(info, "last_price", None)
prev_close = getattr(info, "previous_close", None)
⋮----
change = None
change_pct = None
⋮----
change = round(price - prev_close, 2)
change_pct = round((change / prev_close) * 100, 2)
⋮----
async def fetch_metals() -> List[QuoteItem]
⋮----
"""Fetch metals prices. Returns empty list on error."""
</file>

<file path="services/widgets/providers/oil.py">
"""Oil price provider -- Brent and WTI via yfinance."""
⋮----
logger = logging.getLogger(__name__)
⋮----
_TICKERS = {
⋮----
def _fetch_oil_sync() -> List[QuoteItem]
⋮----
"""Synchronous yfinance fetch for oil futures."""
⋮----
now_iso = datetime.now(timezone.utc).isoformat()
quotes: List[QuoteItem] = []
⋮----
ticker = yf.Ticker(yf_ticker)
info = ticker.fast_info
price = getattr(info, "last_price", None)
prev_close = getattr(info, "previous_close", None)
⋮----
change = None
change_pct = None
⋮----
change = round(price - prev_close, 2)
change_pct = round((change / prev_close) * 100, 2)
⋮----
async def fetch_oil() -> List[QuoteItem]
⋮----
"""Fetch oil prices. Returns empty list on error."""
</file>

<file path="services/widgets/quotes_hub.py">
"""Quotes Hub -- background task that fetches market quotes.

Fetches from all providers on a schedule. When Redis is available, stores
snapshots and publishes changes via Pub/Sub. Without Redis, keeps an
in-memory snapshot that the SSE endpoint reads directly.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_REDIS_KEY = "widgets:quotes:latest"
_REDIS_CHANNEL = "widgets:quotes:pubsub"
_REDIS_TTL = 120  # seconds
_FETCH_INTERVAL = 30  # seconds
⋮----
# In-memory snapshot for Redis-free operation
_latest_snapshot: Optional[str] = None
_snapshot_event: asyncio.Event = asyncio.Event()
⋮----
def get_latest_snapshot() -> Optional[str]
⋮----
"""Return the latest in-memory snapshot JSON string, or None."""
⋮----
def get_snapshot_event() -> asyncio.Event
⋮----
"""Return the asyncio.Event that is set on each new snapshot."""
⋮----
async def _fetch_all_providers() -> List[QuoteItem]
⋮----
"""Gather quotes from all providers concurrently."""
⋮----
results = await asyncio.gather(
⋮----
quotes: List[QuoteItem] = []
⋮----
def _serialize(quotes: List[QuoteItem]) -> str
⋮----
"""Serialize a list of QuoteItems to JSON."""
⋮----
async def run_quotes_hub(redis_client=None) -> None
⋮----
"""Long-running coroutine that fetches quotes and publishes updates.

    Parameters
    ----------
    redis_client
        A ``redis.Redis`` instance (synchronous, with ``decode_responses=True``).
        If None, operates in memory-only mode.
    """
⋮----
mode = "Redis" if redis_client else "in-memory"
⋮----
last_snapshot = ""
⋮----
quotes = await _fetch_all_providers()
⋮----
snapshot = _serialize(quotes)
⋮----
# Always update in-memory snapshot
_latest_snapshot = snapshot
⋮----
# Store latest snapshot with TTL
⋮----
# Only publish if data changed
⋮----
last_snapshot = snapshot
</file>

<file path="templates/raid-enhancements.css">
/* =====================================================================
   Ra'd AI Enhancement Layer
   Loaded AFTER inline styles in index.html to override defaults.
   Served at /static/raid-enhancements.css
   ===================================================================== */
⋮----
/* =====================================================================
   1. TYPOGRAPHY & READABILITY
   ===================================================================== */
⋮----
body {
⋮----
.hero-subtitle {
⋮----
.suggestion-chip {
⋮----
.stat-pill {
⋮----
.footer-text {
⋮----
.header-text p {
⋮----
/* Utility: tabular numbers for aligned numeric columns */
.mono-numbers {
⋮----
/* =====================================================================
   2. CONTRAST & ACCESSIBILITY
   ===================================================================== */
⋮----
/* Override --text-muted to pass WCAG AA 4.5:1 on #1A1A1A background */
:root {
⋮----
/* Focus-visible ring for keyboard navigation */
*:focus-visible {
⋮----
/* Skip-to-content link (visually hidden until focused) */
.skip-to-content {
⋮----
.skip-to-content:focus {
⋮----
/* Reduced motion: disable all animations and transitions */
⋮----
*,
⋮----
/* High contrast mode: bolder text and stronger borders */
⋮----
.suggestion-chip,
⋮----
.chat-container {
⋮----
.app-header {
⋮----
.app-footer {
⋮----
/* =====================================================================
   3. LIGHT THEME SUPPORT
   ===================================================================== */
⋮----
[data-theme="light"] {
⋮----
[data-theme="light"] .app-header {
⋮----
[data-theme="light"] .brand-mark {
⋮----
[data-theme="light"] .stat-pill {
⋮----
[data-theme="light"] .suggestion-chip {
⋮----
[data-theme="light"] .suggestion-chip:hover {
⋮----
[data-theme="light"] .chat-container {
⋮----
[data-theme="light"] .app-footer {
⋮----
[data-theme="light"] .footer-text {
⋮----
[data-theme="light"] .hero-subtitle {
⋮----
/* =====================================================================
   4. MOBILE & RESPONSIVE ENHANCEMENTS
   ===================================================================== */
⋮----
/* Tablet: reduce chat container minimum height */
⋮----
/* Mobile: horizontal-scroll stats, smaller hero, touch targets */
⋮----
.stats-bar {
⋮----
.hero-title {
⋮----
.hero-section {
⋮----
/* Touch targets: minimum 44px per Apple/Google HIG */
⋮----
/* =====================================================================
   5. VISUAL HIERARCHY & LAYOUT
   ===================================================================== */
⋮----
.suggestions-label {
⋮----
.stat-pill:hover {
⋮----
/* =====================================================================
   6. HEADER ENHANCEMENT STYLES
   ===================================================================== */
⋮----
.header-btn {
⋮----
.header-btn:hover {
⋮----
.header-btn:active {
⋮----
/* =====================================================================
   7. ONBOARDING OVERLAY STYLES
   ===================================================================== */
⋮----
#raid-onboarding-overlay {
⋮----
#raid-onboarding-overlay.active {
⋮----
.onboarding-tooltip {
⋮----
.onboarding-tooltip h3 {
⋮----
.onboarding-tooltip p {
⋮----
.onboarding-step {
⋮----
.onboarding-step .step-indicator {
⋮----
.onboarding-step button {
⋮----
.onboarding-step button:hover {
⋮----
/* =====================================================================
   8. PRINT STYLES
   ===================================================================== */
⋮----
.app-header,
⋮----
* {
⋮----
/* =====================================================================
   9. RESPONSIVE CHART & DATA OVERFLOW
   ===================================================================== */
⋮----
/* Ensure Plotly charts don't overflow containers */
.js-plotly-plot,
⋮----
/* Scrollable wrapper for wide Plotly charts */
.plotly-graph-div {
⋮----
/* Scroll indicator for Plotly containers */
.plotly-graph-div::-webkit-scrollbar {
⋮----
.plotly-graph-div::-webkit-scrollbar-thumb {
⋮----
.plotly-graph-div::-webkit-scrollbar-track {
⋮----
/* Response text word wrapping */
.chat-container pre,
⋮----
/* Markdown and general text overflow prevention */
.chat-container p,
⋮----
/* Chart responsive heights for narrow viewports */
⋮----
/* Data tables scroll horizontally */
.js-plotly-plot .plotly .table {
⋮----
/* Screen reader only utility (for aria-live regions) */
.sr-only {
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/COVERAGE_BASELINE.md">
# Test Coverage Baseline

Baseline coverage percentages for the Ra'd AI Platform as of 2026-02-13.

## Backend (Python) -- Module Coverage

| Module            | Coverage | Notes                                    |
|-------------------|----------|------------------------------------------|
| `auth/`           | ~85%     | JWT, password, models, dependencies      |
| `api/routes/`     | ~65%     | Route handlers; PG routes need PG to test|
| `config/`         | ~70%     | Settings, logging                        |
| `services/`       | ~60%     | TASI index, health, news covered         |
| `middleware/`     | ~55%     | Rate limit, CORS, error handler, logging |
| `chart_engine/`   | ~45%     | RaidChartGenerator needs Plotly mocks    |
| `database/`       | ~30%     | Pool, migrations mostly PG-dependent     |
| `cache/`          | ~40%     | Redis client mocked                      |
| `models/`         | ~50%     | Pydantic models                          |

## Frontend (TypeScript)

| Area              | Coverage | Notes                                    |
|-------------------|----------|------------------------------------------|
| `components/`     | ~40%     | Layout, chart components                 |
| `lib/`            | ~50%     | API client, utils                        |
| `app/`            | ~30%     | Page-level tests limited                 |

## Test Summary

| Suite                        | Tests | Pass | Skip | Fail |
|------------------------------|-------|------|------|------|
| `tests/` (unit)              | 496   | 496  | 18   | 0    |
| `test_database.py`           | 69    | 46   | 23   | 0    |
| `test_app_assembly_v2.py`    | ~24   | ~24  | var  | 0    |
| `tests/integration/`         | ~40   | var  | var  | 0    |
| Frontend vitest              | var   | var  | 0    | 0    |

## How to Generate Updated Coverage

```bash
# Full backend coverage report
./scripts/coverage_report.sh

# With HTML report
./scripts/coverage_report.sh --html

# Fast tests only (skip slow/integration)
./scripts/coverage_report.sh --fast

# Manual pytest with coverage
python -m pytest --cov=api --cov=auth --cov=services --cov-report=term-missing tests/

# Frontend coverage
cd frontend && npx vitest run --coverage
```

## Coverage Goals

- **Minimum for merge**: No regression from baseline
- **Target**: 70% overall backend coverage
- **Critical paths** (auth, TASI API): 85%+ coverage
</file>

<file path="tests/integration/__init__.py">
# Integration test suite for Ra'd AI Platform.
</file>

<file path="tests/performance/__init__.py">

</file>

<file path="tests/README.md">
# Test Suite Documentation

## Test Taxonomy

Tests are organized into tiers using pytest markers defined in `pyproject.toml`.

### Markers

| Marker          | Description                                      | CI: PRs | CI: main |
|-----------------|--------------------------------------------------|---------|----------|
| `fast`          | Unit tests, no external deps, < 1s each          | Yes     | Yes      |
| `slow`          | Slower tests (network mocks, heavy computation)   | No      | Yes      |
| `integration`   | Multi-component integration tests                 | No      | Yes      |
| `pg_required`   | Requires running PostgreSQL instance              | No      | Yes*     |

*PG tests only run on main if `POSTGRES_HOST` is set in the CI environment.

### Running by Marker

```bash
# Fast tests only (PR gate)
python -m pytest -m "fast" tests/

# All non-PG tests
python -m pytest -m "not pg_required" tests/

# Integration tests only
python -m pytest -m "integration" tests/integration/

# Full suite including PG (requires POSTGRES_HOST)
python -m pytest tests/ test_database.py test_app_assembly_v2.py

# Everything except slow
python -m pytest -m "not slow" tests/
```

## Directory Structure

```
tests/
  conftest.py                    # Shared fixtures (test DB, mock Redis, JWT tokens)
  test_auth.py                   # Auth module unit tests (JWT, passwords, models)
  test_api_routes.py             # API route unit + PG integration tests
  test_tasi_endpoint.py          # TASI index endpoint tests
  test_tasi_index.py             # TASI index service unit tests
  test_schemas.py                # Pydantic schema validation
  test_services.py               # Service layer unit tests
  test_cache.py                  # Redis cache tests (mocked)
  test_chart_engine.py           # Chart generation tests
  test_middleware.py              # Middleware unit tests
  test_connection_pool.py        # Connection pool tests
  test_ingestion.py              # Data ingestion tests
  test_query_router.py           # Query routing tests
  test_ui_enhancements.py        # UI-related tests
  test_news_feed_api.py          # News feed API tests
  test_news_scraper.py           # News scraper tests
  test_news_store.py             # News store tests
  integration/
    __init__.py
    test_api_chain.py            # Backend API chain integration tests
    test_auth_flow.py            # Full auth lifecycle tests
    test_pg_path.py              # PostgreSQL path tests (pg_required)
  COVERAGE_BASELINE.md           # Coverage tracking document
  README.md                      # This file

test_database.py                 # Database integrity tests (root level)
test_app_assembly_v2.py          # Vanna agent assembly tests (root level)
```

## CI Configuration

### Pull Request Checks (fast gate)

```yaml
# Run on every PR - must pass to merge
python -m pytest -m "not slow and not integration and not pg_required" tests/
```

### Main Branch (full suite)

```yaml
# Run on merge to main - full validation
python -m pytest tests/ test_database.py test_app_assembly_v2.py
```

### PostgreSQL Tests

PG tests are skipped automatically when `POSTGRES_HOST` is not set. To run locally:

```bash
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_DB=tasi_platform
export POSTGRES_USER=tasi_user
export POSTGRES_PASSWORD=your_password
python -m pytest -m "pg_required" tests/integration/test_pg_path.py
```

## Writing New Tests

1. Place unit tests in `tests/test_<module>.py`
2. Place integration tests in `tests/integration/test_<feature>.py`
3. Mark tests appropriately:
   - `@pytest.mark.fast` for quick unit tests
   - `@pytest.mark.integration` for multi-component tests
   - `@pytest.mark.slow` for tests > 5 seconds
   - `@pytest.mark.pg_required` for PostgreSQL-dependent tests
4. Use fixtures from `conftest.py` (test_db, mock_redis, auth_token, etc.)
5. Follow existing naming conventions: `test_<action>_<expected_result>`

## Coverage

See `COVERAGE_BASELINE.md` for current coverage percentages and goals.

Generate a coverage report:

```bash
./scripts/coverage_report.sh          # terminal report
./scripts/coverage_report.sh --html   # HTML report in htmlcov/
```
</file>

<file path="tests/security/__init__.py">

</file>

<file path="tests/test_app_assembly_v2.py">
"""
Comprehensive Import and Assembly Tests for Vanna 2.0 App - Version 2
======================================================================
Tests all imports, constructions, and configurations without starting the server.
Supports dual SQLite/PostgreSQL backend. PostgresRunner tests are skipped
when PostgreSQL is not reachable.
"""
⋮----
_HERE = Path(__file__).resolve().parent
_SQLITE_PATH = os.environ.get("DB_SQLITE_PATH", str(_HERE.parent / "saudi_stocks.db"))
⋮----
# Ensure project root is on sys.path so that `database` package is importable
# when this script is invoked directly (e.g. `python tests/test_app_assembly_v2.py`).
_PROJECT_ROOT = str(_HERE.parent)
⋮----
# Test results tracker
test_results = []
⋮----
def test_result(test_name: str, passed: bool, message: str = "")
⋮----
"""Record a test result."""
status = "PASS" if passed else "FAIL"
⋮----
PG_AVAILABLE = _pg_available()
⋮----
# ===========================================================================
# TEST 1: Import Tests - Verify all Vanna 2.0 imports work
⋮----
# Test 1.1: Core imports
⋮----
# Test 1.2: SystemPromptBuilder import
⋮----
# Test 1.3: UserResolver imports
⋮----
# Test 1.4: DemoAgentMemory import
⋮----
# Test 1.5: OpenAILlmService import
⋮----
# Test 1.6: SqliteRunner import
⋮----
# Test 1.7: VannaFastAPIServer import
⋮----
# Test 1.8: RunSqlTool import
⋮----
# Test 1.9: AnthropicLlmService import
⋮----
# Test 1.10: PostgresRunner import
⋮----
# TEST SUITE 2: LLM SERVICE CONSTRUCTION
⋮----
# Test 2.1: OpenAILlmService with base_url parameter
⋮----
llm = OpenAILlmService(
⋮----
# Check all attributes (including private ones)
all_attrs = [attr for attr in dir(llm) if not attr.startswith("__")]
⋮----
# Try to find base_url in various forms
base_url_found = False
headers_found = False
⋮----
# Check for base_url variations
⋮----
val = getattr(llm, attr)
⋮----
base_url_found = True
⋮----
# Check for headers variations
⋮----
headers_found = True
⋮----
# The service was constructed successfully, which is the main test
⋮----
# Test 2.2: AnthropicLlmService construction (primary LLM for production)
⋮----
anthropic_llm = AnthropicLlmService(
⋮----
# TEST SUITE 3: SQL RUNNER CONSTRUCTION
⋮----
# Test 3.1: SqliteRunner connection to existing database
⋮----
# Check if database file exists (use script-relative path)
⋮----
sql_runner = SqliteRunner(_SQLITE_PATH)
⋮----
# Test 3.2: PostgresRunner construction (skipped if PG not available)
⋮----
pg_runner = PostgresRunner(
⋮----
# TEST SUITE 4: TOOL REGISTRY
⋮----
# Test 4.1: ToolRegistry register_local_tool method
⋮----
tools = ToolRegistry()
⋮----
# Verify register_local_tool method exists
⋮----
# Try to register a tool
sql_runner = SqliteRunner(":memory:")  # Use in-memory DB for testing
⋮----
# Test 4.2: Verify 'register' method does NOT exist (should use register_local_tool)
⋮----
# If register exists, it might be an alias or different method
⋮----
# Test 4.3: ToolRegistry with VisualizeDataTool
⋮----
tools_with_viz = ToolRegistry()
⋮----
# TEST SUITE 5: USER RESOLVER SUBCLASS
⋮----
# Test 5.1: DefaultUserResolver is proper subclass of UserResolver
⋮----
class DefaultUserResolver(UserResolver)
⋮----
async def resolve_user(self, request_context: RequestContext) -> User
⋮----
# Verify it's a subclass
⋮----
# Test 5.2: Verify resolve_user method exists and has correct signature
resolver = DefaultUserResolver()
⋮----
sig = inspect.signature(resolver.resolve_user)
params = list(sig.parameters.keys())
⋮----
# TEST SUITE 6: SYSTEM PROMPT BUILDER SUBCLASS
⋮----
# Test 6.1: SaudiStocksSystemPromptBuilder implements abstract method
⋮----
SYSTEM_PROMPT = "Test system prompt for Saudi stocks"
⋮----
class SaudiStocksSystemPromptBuilder(SystemPromptBuilder)
⋮----
# Test 6.2: Verify build_system_prompt method exists
builder = SaudiStocksSystemPromptBuilder()
⋮----
sig = inspect.signature(builder.build_system_prompt)
⋮----
# TEST SUITE 7: AGENT ASSEMBLY
⋮----
# Test 7.1: Agent can be constructed with all required parameters (SQLite backend)
⋮----
# Create minimal components
test_llm = OpenAILlmService(
⋮----
test_tools = ToolRegistry()
test_sql_runner = SqliteRunner(":memory:")
⋮----
class TestUserResolver(UserResolver)
⋮----
class TestSystemPromptBuilder(SystemPromptBuilder)
⋮----
test_config = AgentConfig(stream_responses=True, max_tool_iterations=5)
⋮----
# Assemble agent
agent = Agent(
⋮----
# Test 7.2: Verify agent has expected attributes
⋮----
# Test 7.3: Agent with AnthropicLlmService (production-like assembly)
⋮----
prod_llm = AnthropicLlmService(
⋮----
prod_tools = ToolRegistry()
prod_runner = SqliteRunner(":memory:")
⋮----
class ProdUserResolver(UserResolver)
⋮----
class ProdPromptBuilder(SystemPromptBuilder)
⋮----
prod_agent = Agent(
⋮----
# Test 7.4: Agent assembly with PostgresRunner (skipped if PG not available)
⋮----
pg_llm = AnthropicLlmService(
pg_tools = ToolRegistry()
pg_sql_runner = PostgresRunner(
⋮----
class PGUserResolver(UserResolver)
⋮----
class PGPromptBuilder(SystemPromptBuilder)
⋮----
pg_agent = Agent(
⋮----
# TEST SUITE 8: VANNA FASTAPI SERVER
⋮----
# Test 8.1: VannaFastAPIServer can create the app
⋮----
# Use the agent created in previous test
server = VannaFastAPIServer(agent)
⋮----
# Test 8.2: Verify server has app or create_app method
has_app_direct = hasattr(server, "app")
has_create_app = hasattr(server, "create_app")
has_get_app = hasattr(server, "get_app")
⋮----
# Try to get the app
app = None
⋮----
app = server.app
⋮----
app = server.create_app()
⋮----
app = server.get_app()
⋮----
# Check if app is created internally
server_attrs = [attr for attr in dir(server) if not attr.startswith("_")]
⋮----
# TEST SUITE 9: SERVER ENDPOINTS
⋮----
# Test 9.1: Verify FastAPI app has expected routes
⋮----
# Get all routes from the FastAPI app
routes = [route.path for route in app.routes]
⋮----
# Check for chat SSE endpoint
has_chat_sse = any("/chat" in route for route in routes)
⋮----
chat_routes = [r for r in routes if "/chat" in r]
⋮----
# Check for health check endpoint
has_health = any("/health" in route for route in routes)
⋮----
# Test 9.3: Display all routes
⋮----
# TEST SUITE 10: CONFIGURATION
⋮----
# Test 10.1: AgentConfig has stream_responses=True
⋮----
config = AgentConfig(
⋮----
# Test 10.2: AgentConfig has max_tool_iterations=10
⋮----
# TEST SUITE 11: DUAL BACKEND CONFIGURATION
⋮----
# Test 11.1: DB_BACKEND env var controls backend selection
⋮----
backend = os.environ.get("DB_BACKEND", "sqlite").lower()
⋮----
# Test 11.2: PostgreSQL env vars present when PG backend active
⋮----
pg_vars = ["POSTGRES_HOST", "POSTGRES_DB", "POSTGRES_USER"]
missing = [v for v in pg_vars if not os.environ.get(v)]
⋮----
# SUMMARY
⋮----
total_tests = len(test_results)
passed_tests = sum(1 for _, passed, _ in test_results if passed)
failed_tests = total_tests - passed_tests
⋮----
# Exit with appropriate code only when run directly (not when imported by pytest)
</file>

<file path="tests/test_auth_service.py">
"""
Tests for services/auth_service.py and auth/ modules.

Covers: password hashing, JWT token creation/validation, AuthService register/login,
token claims, error handling for database failures, and guest session patterns.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ---------------------------------------------------------------------------
# Password hashing tests
⋮----
class TestPasswordHashing
⋮----
"""Tests for auth/password.py hash_password and verify_password."""
⋮----
def test_hash_password_returns_string(self)
⋮----
result = hash_password("mysecretpassword")
⋮----
def test_hash_password_different_each_time(self)
⋮----
"""bcrypt uses a random salt, so two hashes of same password differ."""
⋮----
hash1 = hash_password("samepassword")
hash2 = hash_password("samepassword")
⋮----
def test_hash_password_not_plaintext(self)
⋮----
pwd = "plaintext123"
hashed = hash_password(pwd)
⋮----
def test_verify_password_correct(self)
⋮----
pwd = "correct_password"
⋮----
def test_verify_password_wrong(self)
⋮----
hashed = hash_password("correct_password")
⋮----
def test_verify_password_empty_string_fails(self)
⋮----
hashed = hash_password("some_password")
⋮----
def test_verify_password_case_sensitive(self)
⋮----
hashed = hash_password("Password123")
⋮----
# JWT token tests
⋮----
class TestJWTTokens
⋮----
"""Tests for auth/jwt_handler.py create_access_token, create_refresh_token, decode_token."""
⋮----
@pytest.fixture(autouse=True)
    def patch_settings(self)
⋮----
"""Patch get_settings() to return deterministic auth settings."""
mock_auth = MagicMock()
⋮----
mock_settings = MagicMock()
⋮----
def test_create_access_token_returns_string(self)
⋮----
token = create_access_token({"sub": "user-123", "email": "test@example.com"})
⋮----
def test_access_token_contains_expected_claims(self)
⋮----
claims = {"sub": "user-abc", "email": "user@example.com"}
token = create_access_token(claims)
decoded = jwt.decode(
⋮----
def test_access_token_has_future_expiry(self)
⋮----
token = create_access_token({"sub": "user-123"})
⋮----
exp = datetime.fromtimestamp(decoded["exp"], tz=timezone.utc)
⋮----
def test_create_refresh_token_returns_string(self)
⋮----
token = create_refresh_token({"sub": "user-123", "email": "test@example.com"})
⋮----
def test_refresh_token_type_claim(self)
⋮----
token = create_refresh_token({"sub": "user-123"})
⋮----
def test_refresh_token_longer_lived_than_access(self)
⋮----
access = create_access_token({"sub": "u"})
refresh = create_refresh_token({"sub": "u"})
⋮----
access_exp = jwt.decode(
refresh_exp = jwt.decode(
⋮----
def test_decode_token_valid_access(self)
⋮----
token = create_access_token({"sub": "user-123", "email": "a@b.com"})
payload = decode_token(token, expected_type="access")
⋮----
def test_decode_token_wrong_type_raises_value_error(self)
⋮----
def test_decode_expired_token_raises(self)
⋮----
"""Expired token should raise jwt.ExpiredSignatureError."""
secret = "test-secret-key-for-testing-only"
payload = {
expired_token = jwt.encode(payload, secret, algorithm="HS256")
⋮----
def test_decode_invalid_token_raises(self)
⋮----
# AuthService static methods
⋮----
class TestAuthServiceStaticMethods
⋮----
"""Tests for AuthService.build_token_claims and create_tokens."""
⋮----
def test_build_token_claims_structure(self)
⋮----
claims = AuthService.build_token_claims("uid-1", "test@example.com")
⋮----
def test_create_tokens_returns_both_keys(self)
⋮----
claims = {"sub": "uid-1", "email": "test@example.com"}
tokens = AuthService.create_tokens(claims)
⋮----
def test_create_tokens_are_strings(self)
⋮----
def test_create_tokens_access_and_refresh_differ(self)
⋮----
# AuthService.register tests
⋮----
class TestAuthServiceRegister
⋮----
"""Tests for AuthService.register() with mocked DB."""
⋮----
def _make_service(self, conn_mock)
⋮----
def test_register_success(self, mock_db_conn)
⋮----
conn = mock_db_conn["conn"]
cursor = mock_db_conn["cursor"]
⋮----
# No existing user found
⋮----
service = self._make_service(conn)
result = service.register("new@example.com", "securepassword")
⋮----
def test_register_duplicate_email_returns_failure(self, mock_db_conn)
⋮----
# Existing user found
⋮----
result = service.register("existing@example.com", "password123")
⋮----
def test_register_calls_commit_on_success(self, mock_db_conn)
⋮----
def test_register_calls_rollback_on_db_error(self, mock_db_conn)
⋮----
# First fetchone returns None (no duplicate), second raises
⋮----
def test_register_closes_connection(self, mock_db_conn)
⋮----
# AuthService.login tests
⋮----
class TestAuthServiceLogin
⋮----
"""Tests for AuthService.login() with mocked DB."""
⋮----
def test_login_success(self, mock_db_conn)
⋮----
password = "correct_password"
hashed = hash_password(password)
⋮----
result = service.login("user@example.com", password)
⋮----
def test_login_user_not_found(self, mock_db_conn)
⋮----
result = service.login("nobody@example.com", "password")
⋮----
def test_login_wrong_password(self, mock_db_conn)
⋮----
result = service.login("user@example.com", "wrong_password")
⋮----
def test_login_inactive_user(self, mock_db_conn)
⋮----
hashed = hash_password("password")
# is_active = False
⋮----
result = service.login("user@example.com", "password")
⋮----
def test_login_closes_connection(self, mock_db_conn)
⋮----
# AuthService.verify_user_active tests
⋮----
class TestAuthServiceVerifyUserActive
⋮----
"""Tests for AuthService.verify_user_active() with mocked DB."""
⋮----
def test_verify_active_user_success(self, mock_db_conn)
⋮----
result = service.verify_user_active("uid-123")
⋮----
def test_verify_user_not_found(self, mock_db_conn)
⋮----
result = service.verify_user_active("non-existent-id")
⋮----
def test_verify_deactivated_user(self, mock_db_conn)
⋮----
result = service.verify_user_active("uid-inactive")
</file>

<file path="tests/test_backend_audit.py">
"""
Tests for backend/services/audit/ module.
Covers: models, config, query_audit, security_events, structured_logger, correlation.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from backend.services.audit.config import AuditConfig  # noqa: E402
from backend.services.audit.correlation import (  # noqa: E402
⋮----
from backend.services.audit.models import (  # noqa: E402
⋮----
from backend.services.audit.query_audit import QueryAuditLogger  # noqa: E402
from backend.services.audit.security_events import SecurityEventLogger  # noqa: E402
from backend.services.audit.structured_logger import (  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture(autouse=True)
def _reset_correlation_ctx()
⋮----
"""Ensure the correlation contextvar is None before and after each test."""
token = _request_id_ctx.set(None)
⋮----
@pytest.fixture()
def mock_db_factory()
⋮----
"""Return a factory that produces a mock DB-API 2.0 connection."""
conn = MagicMock()
cursor = MagicMock()
⋮----
factory = MagicMock(return_value=conn)
⋮----
@pytest.fixture()
def sample_query_event()
⋮----
@pytest.fixture()
def sample_security_event()
⋮----
# Models tests
⋮----
class TestQueryAuditEvent
⋮----
def test_defaults(self)
⋮----
event = QueryAuditEvent(nl_query="test query")
⋮----
assert event.id  # auto-generated UUID hex
⋮----
def test_full_fields(self, sample_query_event)
⋮----
def test_risk_score_bounds(self)
⋮----
event = QueryAuditEvent(nl_query="q", risk_score=0.5)
⋮----
def test_extra_fields_ignored(self)
⋮----
event = QueryAuditEvent(nl_query="q", unknown_field="xyz")
⋮----
def test_execution_time_non_negative(self)
⋮----
class TestSecurityEvent
⋮----
def test_defaults(self, sample_security_event)
⋮----
def test_all_event_types(self)
⋮----
event = SecurityEvent(
⋮----
def test_all_severity_levels(self)
⋮----
class TestSecurityEnums
⋮----
def test_severity_values(self)
⋮----
def test_event_type_values(self)
⋮----
# Config tests
⋮----
class TestAuditConfig
⋮----
cfg = AuditConfig()
⋮----
def test_env_override(self, monkeypatch)
⋮----
def test_retention_minimum(self)
⋮----
def test_log_format_literal(self)
⋮----
cfg = AuditConfig(log_format="text")
⋮----
# Query audit logger tests
⋮----
class TestQueryAuditLogger
⋮----
def test_log_without_db(self, sample_query_event, caplog)
⋮----
logger = QueryAuditLogger()
⋮----
def test_log_with_error_warns(self, caplog)
⋮----
event = QueryAuditEvent(nl_query="bad query", error="syntax error")
⋮----
warn_records = [r for r in caplog.records if r.levelno == logging.WARNING]
⋮----
def test_log_persists_to_db(self, sample_query_event, mock_db_factory)
⋮----
logger = QueryAuditLogger(db_connection_factory=factory)
⋮----
def test_db_failure_does_not_raise(self, sample_query_event)
⋮----
factory = MagicMock(side_effect=RuntimeError("connection refused"))
⋮----
# Should not raise
⋮----
def test_auto_fills_request_id(self, sample_query_event)
⋮----
rid = uuid.uuid4().hex
⋮----
def test_preserves_explicit_request_id(self)
⋮----
event = QueryAuditEvent(nl_query="q", request_id="explicit-id")
⋮----
# Security event logger tests
⋮----
class TestSecurityEventLogger
⋮----
def test_log_without_db(self, sample_security_event, caplog)
⋮----
logger = SecurityEventLogger()
⋮----
def test_severity_maps_to_log_level(self, caplog)
⋮----
sec_records = [r for r in caplog.records if r.name == "tasi.audit.security"]
⋮----
def test_log_persists_to_db(self, sample_security_event, mock_db_factory)
⋮----
logger = SecurityEventLogger(db_connection_factory=factory)
⋮----
def test_db_failure_does_not_raise(self, sample_security_event)
⋮----
factory = MagicMock(side_effect=ConnectionError("db down"))
⋮----
def test_auto_fills_request_id(self, sample_security_event)
⋮----
# Structured logger tests
⋮----
class TestJSONFormatter
⋮----
def test_produces_valid_json(self)
⋮----
formatter = JSONFormatter()
record = logging.LogRecord(
output = formatter.format(record)
parsed = json.loads(output)
⋮----
def test_includes_request_id(self)
⋮----
rid = "test-request-123"
⋮----
parsed = json.loads(formatter.format(record))
⋮----
def test_no_request_id_when_none(self)
⋮----
def test_includes_extra_fields(self)
⋮----
def test_includes_exception(self)
⋮----
exc_info = _sys.exc_info()
⋮----
class TestConfigureLogging
⋮----
def test_json_format(self)
⋮----
root = logging.getLogger()
⋮----
def test_text_format(self)
⋮----
def test_env_fallback(self, monkeypatch)
⋮----
def test_suppresses_noisy_loggers(self)
⋮----
def test_idempotent(self)
⋮----
assert len(root.handlers) == 1  # Handlers cleared on re-init
⋮----
class TestGetLogger
⋮----
def test_returns_named_logger(self)
⋮----
logger = get_logger("my.module")
⋮----
# Correlation middleware tests
⋮----
class TestGetCurrentRequestId
⋮----
def test_returns_none_by_default(self)
⋮----
def test_returns_set_value(self)
⋮----
class TestCorrelationMiddleware
⋮----
@pytest.mark.asyncio
    async def test_generates_request_id(self)
⋮----
middleware = CorrelationMiddleware(app=MagicMock())
⋮----
captured_id = None
⋮----
async def fake_call_next(request)
⋮----
captured_id = get_current_request_id()
response = MagicMock(spec=["headers"])
⋮----
request = MagicMock()
⋮----
response = await middleware.dispatch(request, fake_call_next)
⋮----
assert len(captured_id) == 32  # UUID4 hex
⋮----
@pytest.mark.asyncio
    async def test_reuses_client_request_id(self)
⋮----
client_id = "client-trace-xyz"
⋮----
@pytest.mark.asyncio
    async def test_resets_context_after_request(self)
⋮----
@pytest.mark.asyncio
    async def test_resets_context_on_exception(self)
⋮----
async def failing_call_next(request)
⋮----
# Context should still be reset despite the exception
⋮----
@pytest.mark.asyncio
    async def test_sets_request_state(self)
⋮----
# request.state.request_id should have been set
⋮----
# Package __init__ re-exports test
⋮----
class TestPackageExports
⋮----
def test_all_exports_importable(self)
</file>

<file path="tests/test_backend_cache.py">
"""
Tests for backend/services/cache/ module.
Covers: query_cache, compression, maintenance, models, db_pool, config.
Skips: redis_client (requires Redis server).
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
import msgpack  # noqa: E402
⋮----
from backend.services.cache.compression import (  # noqa: E402
⋮----
from backend.services.cache.config import CacheConfig  # noqa: E402
from backend.services.cache.db_pool import DatabasePoolManager  # noqa: E402
from backend.services.cache.models import CachedResult, PoolConfig, PoolStats, TTLTier  # noqa: E402
from backend.services.cache.query_cache import (  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture
def mock_redis()
⋮----
"""Return a mock RedisManager with async helpers."""
redis = MagicMock()
⋮----
@pytest.fixture
def query_cache(mock_redis)
⋮----
"""Return a QueryCache wired to the mock RedisManager."""
⋮----
@pytest.fixture
def disabled_cache(mock_redis)
⋮----
"""Return a disabled QueryCache."""
⋮----
# =====================================================================
# Models
⋮----
class TestTTLTier
⋮----
def test_market_ttl(self)
⋮----
def test_historical_ttl(self)
⋮----
def test_schema_ttl(self)
⋮----
def test_enum_values(self)
⋮----
class TestCachedResult
⋮----
def test_create_minimal(self)
⋮----
cr = CachedResult(
⋮----
assert cr.row_count == 0  # default
⋮----
def test_create_full(self)
⋮----
def test_model_dump_json(self)
⋮----
d = cr.model_dump(mode="json")
⋮----
class TestPoolStats
⋮----
def test_defaults(self)
⋮----
ps = PoolStats()
⋮----
def test_custom_values(self)
⋮----
ps = PoolStats(pool_size=10, checked_out=3, overflow=1, checked_in=6)
⋮----
class TestPoolConfig
⋮----
pc = PoolConfig()
⋮----
def test_custom(self)
⋮----
pc = PoolConfig(url="postgresql+asyncpg://localhost/test", pool_size=20)
⋮----
# Config
⋮----
class TestCacheConfig
⋮----
# Construct with explicit values to avoid picking up env vars
cfg = CacheConfig(
⋮----
def test_env_prefix(self)
⋮----
cfg = CacheConfig(_env_file=None)
⋮----
# Compression
⋮----
class TestCompression
⋮----
def test_compress_decompress_roundtrip(self)
⋮----
original = b"Hello, world! " * 100
compressed = compress_bytes(original)
decompressed = decompress_bytes(compressed)
⋮----
def test_compress_reduces_size(self)
⋮----
original = b"A" * 10000
⋮----
def test_compress_level(self)
⋮----
data = b"test data " * 500
fast = compress_bytes(data, level=1)
best = compress_bytes(data, level=9)
# Both should decompress to the same thing
⋮----
def test_compress_large_response_below_threshold(self)
⋮----
small = b"tiny"
⋮----
def test_compress_large_response_above_threshold(self)
⋮----
large = b"X" * 5000
⋮----
def test_compress_large_response_incompressible(self)
⋮----
"""If compressed is not smaller, returns the original."""
# Random-ish bytes that don't compress well and are just barely
# over threshold
⋮----
random_data = os.urandom(1025)
⋮----
# gzip of random data is typically larger; should return original
⋮----
def test_compress_empty_bytes(self)
⋮----
compressed = compress_bytes(b"")
⋮----
# Query Cache helpers
⋮----
class TestNormalizeSql
⋮----
def test_lowercases(self)
⋮----
def test_collapses_whitespace(self)
⋮----
def test_strips(self)
⋮----
class TestMakeKey
⋮----
def test_prefix(self)
⋮----
key = _make_key("SELECT 1")
⋮----
def test_deterministic(self)
⋮----
def test_normalized(self)
⋮----
"""Same logical query with different whitespace gives the same key."""
⋮----
class TestClassifyTier
⋮----
def test_schema_queries(self)
⋮----
def test_historical_queries(self)
⋮----
def test_market_queries(self)
⋮----
# QueryCache
⋮----
class TestQueryCache
⋮----
@pytest.mark.asyncio
    async def test_get_cache_miss(self, query_cache, mock_redis)
⋮----
result = await query_cache.get("SELECT 1")
⋮----
@pytest.mark.asyncio
    async def test_get_cache_hit(self, query_cache, mock_redis)
⋮----
envelope = {
packed = msgpack.packb(envelope, use_bin_type=True)
⋮----
@pytest.mark.asyncio
    async def test_get_disabled(self, disabled_cache)
⋮----
result = await disabled_cache.get("SELECT 1")
⋮----
@pytest.mark.asyncio
    async def test_get_redis_error(self, query_cache, mock_redis)
⋮----
@pytest.mark.asyncio
    async def test_get_deserialize_error(self, query_cache, mock_redis)
⋮----
@pytest.mark.asyncio
    async def test_set_success(self, query_cache, mock_redis)
⋮----
ok = await query_cache.set("SELECT 1", [{"col": 1}])
⋮----
@pytest.mark.asyncio
    async def test_set_disabled(self, disabled_cache)
⋮----
ok = await disabled_cache.set("SELECT 1", [])
⋮----
@pytest.mark.asyncio
    async def test_set_with_explicit_tier(self, query_cache, mock_redis)
⋮----
ok = await query_cache.set("SELECT 1", [], tier=TTLTier.SCHEMA)
⋮----
# Verify the TTL passed to Redis matches SCHEMA tier
call_args = mock_redis.set.call_args
⋮----
@pytest.mark.asyncio
    async def test_set_redis_error(self, query_cache, mock_redis)
⋮----
ok = await query_cache.set("SELECT 1", [])
⋮----
@pytest.mark.asyncio
    async def test_invalidate_success(self, query_cache, mock_redis)
⋮----
ok = await query_cache.invalidate("SELECT 1")
⋮----
@pytest.mark.asyncio
    async def test_invalidate_miss(self, query_cache, mock_redis)
⋮----
@pytest.mark.asyncio
    async def test_invalidate_disabled(self, disabled_cache)
⋮----
ok = await disabled_cache.invalidate("SELECT 1")
⋮----
@pytest.mark.asyncio
    async def test_invalidate_redis_error(self, query_cache, mock_redis)
⋮----
def test_stats(self, query_cache)
⋮----
stats = query_cache.stats()
⋮----
def test_hit_rate_calculation(self, query_cache)
⋮----
# CacheMaintenance
⋮----
class TestCacheMaintenance
⋮----
@pytest.fixture
    def maintenance(self, mock_redis, query_cache)
⋮----
@pytest.mark.asyncio
    async def test_warm_cache_no_execute_fn(self, maintenance)
⋮----
result = await maintenance.warm_cache(execute_fn=None)
⋮----
@pytest.mark.asyncio
    async def test_warm_cache_with_execute_fn(self, maintenance, mock_redis)
⋮----
execute_fn = AsyncMock(return_value=[{"col": 1}])
result = await maintenance.warm_cache(execute_fn=execute_fn)
assert result["warmed"] == 3  # 3 warm-up queries
⋮----
@pytest.mark.asyncio
    async def test_warm_cache_partial_failure(self, maintenance, mock_redis)
⋮----
call_count = 0
⋮----
async def flaky_fn(sql)
⋮----
result = await maintenance.warm_cache(execute_fn=flaky_fn)
⋮----
@pytest.mark.asyncio
    async def test_cleanup_expired_healthy(self, maintenance, mock_redis)
⋮----
result = await maintenance.cleanup_expired()
⋮----
@pytest.mark.asyncio
    async def test_cleanup_expired_redis_down(self, maintenance, mock_redis)
⋮----
@pytest.mark.asyncio
    async def test_get_cache_stats(self, maintenance, mock_redis)
⋮----
result = await maintenance.get_cache_stats()
⋮----
# DatabasePoolManager
⋮----
class TestDatabasePoolManager
⋮----
def test_init_default_config(self)
⋮----
mgr = DatabasePoolManager()
⋮----
def test_init_custom_config(self)
⋮----
cfg = PoolConfig(url="sqlite+aiosqlite:///test.db", pool_size=20)
mgr = DatabasePoolManager(config=cfg)
⋮----
def test_get_session_before_connect(self)
⋮----
def test_pool_stats_no_engine(self)
⋮----
stats = mgr.pool_stats()
⋮----
@pytest.mark.asyncio
    async def test_health_check_no_engine(self)
⋮----
result = await mgr.health_check()
⋮----
@pytest.mark.asyncio
    async def test_connect_disconnect_lifecycle(self)
⋮----
mock_engine = MagicMock()
⋮----
mock_session_factory = MagicMock()
⋮----
# Double connect is a no-op
⋮----
@pytest.mark.asyncio
    async def test_disconnect_without_connect(self)
⋮----
# Should not raise
⋮----
@pytest.mark.asyncio
    async def test_health_check_connected(self)
⋮----
mock_conn = AsyncMock()
⋮----
mock_ctx = AsyncMock()
⋮----
@pytest.mark.asyncio
    async def test_pool_stats_with_non_queue_pool(self)
⋮----
# Use a non-QueuePool pool type
mock_engine.pool = MagicMock(spec=[])  # no QueuePool methods
⋮----
@pytest.mark.asyncio
    async def test_get_session_after_connect(self)
⋮----
mock_session = MagicMock()
mock_session_factory = MagicMock(return_value=mock_session)
⋮----
session = mgr.get_session()
</file>

<file path="tests/test_backend_middleware.py">
"""
Tests for backend/middleware/ module.
Covers: cost_controller, rate_limiter, rate_limit_config, rate_limit_middleware,
        models, register, and __init__ exports.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from backend.middleware.cost_controller import (  # noqa: E402
⋮----
from backend.middleware.models import RateLimitResult  # noqa: E402
from backend.middleware.rate_limit_config import (  # noqa: E402
⋮----
from backend.middleware.rate_limit_middleware import RateLimitMiddleware  # noqa: E402
from backend.middleware.rate_limiter import RateLimiter  # noqa: E402
from backend.middleware.register import (  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Models
⋮----
class TestRateLimitResult
⋮----
"""Tests for the RateLimitResult Pydantic model."""
⋮----
def test_create_allowed_result(self)
⋮----
result = RateLimitResult(
⋮----
def test_create_denied_result(self)
⋮----
def test_default_bucket(self)
⋮----
def test_remaining_cannot_be_negative(self)
⋮----
def test_reset_after_cannot_be_negative(self)
⋮----
# RateLimiter (in-memory mode)
⋮----
class TestRateLimiterInMemory
⋮----
"""Tests for RateLimiter using in-memory backend (no Redis)."""
⋮----
def test_init_no_redis(self)
⋮----
limiter = RateLimiter(redis_url=None)
⋮----
def test_first_request_allowed(self)
⋮----
limiter = RateLimiter()
result = limiter.check("user:1", limit=5, window=60)
⋮----
def test_requests_within_limit(self)
⋮----
result = limiter.check("user:2", limit=5, window=60)
⋮----
def test_request_exceeds_limit(self)
⋮----
result = limiter.check("user:3", limit=5, window=60)
⋮----
def test_different_identifiers_isolated(self)
⋮----
# user:a is exhausted
result_a = limiter.check("user:a", limit=5, window=60)
⋮----
# user:b is fresh
result_b = limiter.check("user:b", limit=5, window=60)
⋮----
def test_different_buckets_isolated(self)
⋮----
result_api = limiter.check("user:x", limit=3, window=60, bucket="api")
⋮----
# Different bucket is fresh
result_auth = limiter.check("user:x", limit=3, window=60, bucket="auth")
⋮----
def test_window_expiry(self)
⋮----
# Fill up the limit
⋮----
result = limiter.check("user:expire", limit=3, window=1)
⋮----
# Wait for window to expire
⋮----
def test_cleanup_runs_at_interval(self)
⋮----
limiter._check_count = 499  # next check triggers cleanup
# Add a stale entry manually
⋮----
result = limiter.check("user:cleanup", limit=100, window=60)
⋮----
def test_close_without_redis(self)
⋮----
# Should not raise
⋮----
def test_redis_init_failure_falls_back_to_memory(self)
⋮----
limiter = RateLimiter(redis_url="redis://invalid-host:9999/1")
⋮----
# Should still work via in-memory
result = limiter.check("user:fallback", limit=10, window=60)
⋮----
# CostController (in-memory mode)
⋮----
class TestUsageSummary
⋮----
"""Tests for the UsageSummary model."""
⋮----
def test_defaults(self)
⋮----
s = UsageSummary(user_id="u1")
⋮----
class TestCostLimitConfig
⋮----
"""Tests for the CostLimitConfig model."""
⋮----
cfg = CostLimitConfig()
⋮----
def test_custom_limits(self)
⋮----
cfg = CostLimitConfig(
⋮----
def test_negative_values_rejected(self)
⋮----
class TestCostControllerInMemory
⋮----
"""Tests for CostController using in-memory backend."""
⋮----
cc = CostController()
⋮----
def test_record_and_get_usage(self)
⋮----
usage = cc.get_usage("user:1")
⋮----
def test_cost_calculation(self)
⋮----
cc = CostController(input_cost_per_m=3.0, output_cost_per_m=15.0)
⋮----
usage = cc.get_usage("user:cost")
# 1M input * $3/M + 1M output * $15/M = $18
⋮----
def test_accumulate_costs(self)
⋮----
usage = cc.get_usage("user:acc")
⋮----
def test_no_usage_returns_zeros(self)
⋮----
usage = cc.get_usage("user:nonexistent")
⋮----
def test_check_limits_no_limits_configured(self)
⋮----
def test_check_limits_daily_token_exceeded(self)
⋮----
cc = CostController(limits=CostLimitConfig(daily_token_limit=1000))
⋮----
def test_check_limits_daily_cost_exceeded(self)
⋮----
cc = CostController(
# Record enough to exceed $0.01
⋮----
def test_check_limits_monthly_cost_exceeded(self)
⋮----
def test_check_limits_within_limits(self)
⋮----
def test_redis_init_failure_falls_back(self)
⋮----
cc = CostController(redis_url="redis://invalid-host:9999/1")
⋮----
# Should still work in-memory
⋮----
usage = cc.get_usage("user:fb")
⋮----
def test_daily_key_format(self)
⋮----
key = CostController._daily_key("user:123")
⋮----
def test_monthly_key_format(self)
⋮----
key = CostController._monthly_key("user:123")
⋮----
# RateLimitConfig
⋮----
class TestEndpointRateLimit
⋮----
"""Tests for the EndpointRateLimit model."""
⋮----
def test_create(self)
⋮----
rule = EndpointRateLimit(
⋮----
def test_default_description(self)
⋮----
rule = EndpointRateLimit(path_prefix="/test", limit=10, window=60)
⋮----
class TestRateLimitConfig
⋮----
"""Tests for the RateLimitConfig settings model."""
⋮----
cfg = RateLimitConfig(
⋮----
def test_skip_paths_set_empty(self)
⋮----
cfg = RateLimitConfig(_env_file=None, skip_paths="")
⋮----
def test_skip_paths_set_parsed(self)
⋮----
cfg = RateLimitConfig(_env_file=None, skip_paths="/custom/path, /another/path")
⋮----
def test_endpoint_rules_default(self)
⋮----
cfg = RateLimitConfig(_env_file=None)
rules = cfg.endpoint_rules
⋮----
prefixes = [r.path_prefix for r in rules]
⋮----
def test_to_path_limits(self)
⋮----
path_limits = cfg.to_path_limits()
⋮----
def test_log_config_does_not_raise(self)
⋮----
cfg.log_config()  # Should not raise
⋮----
# RateLimitMiddleware
⋮----
class TestRateLimitMiddleware
⋮----
"""Tests for the FastAPI RateLimitMiddleware."""
⋮----
def _make_request(self, path="/api/v1/test", client_host="127.0.0.1", auth=None)
⋮----
request = MagicMock()
⋮----
def _make_middleware(self, limiter=None, path_limits=None, skip_paths=None)
⋮----
app = MagicMock()
mw = RateLimitMiddleware(
⋮----
def test_skip_health_path(self)
⋮----
mw = self._make_middleware()
⋮----
def test_custom_skip_paths_merged(self)
⋮----
mw = self._make_middleware(skip_paths={"/custom/skip"})
⋮----
# Built-in skips still present
⋮----
def test_resolve_limit_default(self)
⋮----
def test_resolve_limit_longest_match(self)
⋮----
path_limits = {
mw = self._make_middleware(path_limits=path_limits)
⋮----
def test_extract_identifier_from_ip(self)
⋮----
request = self._make_request(client_host="10.0.0.1")
ident = mw._extract_identifier(request)
⋮----
def test_extract_identifier_no_client(self)
⋮----
request = self._make_request()
⋮----
def test_extract_identifier_invalid_bearer(self)
⋮----
request = self._make_request(auth="Bearer invalid-token")
⋮----
# Falls back to IP since token decode will fail
⋮----
@pytest.mark.asyncio
    async def test_dispatch_skip_path(self)
⋮----
request = self._make_request(path="/health")
call_next = AsyncMock()
⋮----
@pytest.mark.asyncio
    async def test_dispatch_allowed_sets_headers(self)
⋮----
response = MagicMock()
⋮----
call_next = AsyncMock(return_value=response)
⋮----
result = await mw.dispatch(request, call_next)
⋮----
@pytest.mark.asyncio
    async def test_dispatch_returns_429_when_exceeded(self)
⋮----
mw = self._make_middleware(limiter=limiter)
⋮----
request = self._make_request(client_host="1.2.3.4")
call_next = AsyncMock(return_value=MagicMock(headers={}))
⋮----
# 6th request should be denied
⋮----
response = await mw.dispatch(request, call_next)
⋮----
@pytest.mark.asyncio
    async def test_dispatch_path_specific_limits(self)
⋮----
path_limits = {"/api/auth": (2, 60)}
⋮----
mw = self._make_middleware(limiter=limiter, path_limits=path_limits)
⋮----
req = self._make_request(path="/api/auth/login", client_host="5.5.5.5")
⋮----
# 3rd request to /api/auth should be denied
⋮----
resp = await mw.dispatch(req, AsyncMock())
⋮----
# register module
⋮----
class TestRegisterModule
⋮----
"""Tests for the middleware registration module."""
⋮----
def test_get_rate_limiter_before_register(self)
⋮----
# Reset module state
⋮----
def test_get_cost_controller_before_register(self)
⋮----
def test_shutdown_when_none(self)
⋮----
def test_register_disabled(self)
⋮----
app = MagicMock(spec=["add_middleware"])
⋮----
def test_register_enabled_with_fallback(self)
⋮----
# Cleanup
⋮----
# __init__ exports
⋮----
class TestModuleExports
⋮----
"""Verify that the public API is exported from backend.middleware."""
⋮----
def test_all_exports(self)
⋮----
expected = [
</file>

<file path="tests/test_backend_resilience.py">
"""
Tests for backend/services/resilience/ module.
Covers: circuit_breaker, degradation, retry, timeout_manager, config.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from backend.services.resilience.circuit_breaker import (  # noqa: E402
⋮----
from backend.services.resilience.config import (  # noqa: E402
⋮----
from backend.services.resilience.degradation import (  # noqa: E402
⋮----
from backend.services.resilience.retry import with_retry, with_timeout  # noqa: E402
from backend.services.resilience.timeout_manager import (  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture()
def cb()
⋮----
"""Circuit breaker with low thresholds for faster testing."""
⋮----
@pytest.fixture()
def degradation_mgr()
⋮----
"""Fresh DegradationManager."""
⋮----
@pytest.fixture()
def timeout_mgr()
⋮----
"""QueryTimeoutManager with tight thresholds."""
⋮----
@pytest.fixture(autouse=True)
def _clear_cb_registry()
⋮----
"""Clear the global circuit breaker registry between tests."""
⋮----
# Helper callables
⋮----
async def _succeed(value="ok")
⋮----
async def _fail(exc=None)
⋮----
def _sync_succeed(value="ok")
⋮----
def _sync_fail()
⋮----
# ===========================================================================
# CircuitBreaker tests
⋮----
class TestCircuitBreakerStates
⋮----
"""Test circuit breaker state transitions."""
⋮----
@pytest.mark.asyncio
    async def test_initial_state_is_closed(self, cb)
⋮----
@pytest.mark.asyncio
    async def test_successful_call_stays_closed(self, cb)
⋮----
result = await cb.call(_succeed, "hello")
⋮----
stats = cb.get_stats()
⋮----
@pytest.mark.asyncio
    async def test_failure_below_threshold_stays_closed(self, cb)
⋮----
@pytest.mark.asyncio
    async def test_failure_at_threshold_opens_circuit(self, cb)
⋮----
@pytest.mark.asyncio
    async def test_open_circuit_rejects_calls(self, cb)
⋮----
# Open the circuit
⋮----
@pytest.mark.asyncio
    async def test_open_to_half_open_after_recovery_timeout(self, cb)
⋮----
# Mock time.monotonic to simulate passage of recovery_timeout
original_opened_at = cb._opened_at
⋮----
@pytest.mark.asyncio
    async def test_half_open_success_closes_circuit(self, cb)
⋮----
# Wait for recovery timeout
⋮----
# Successful probe calls should close the circuit
⋮----
result = await cb.call(_succeed, "recovered")
⋮----
@pytest.mark.asyncio
    async def test_half_open_failure_reopens_circuit(self, cb)
⋮----
# A failure in half-open re-opens immediately
⋮----
@pytest.mark.asyncio
    async def test_success_resets_failure_count(self, cb)
⋮----
# Rack up some failures (below threshold)
⋮----
# A success should reset the counter
⋮----
class TestCircuitBreakerSyncCalls
⋮----
"""Test that sync callables work through the circuit breaker."""
⋮----
@pytest.mark.asyncio
    async def test_sync_success(self, cb)
⋮----
result = await cb.call(_sync_succeed, "sync-ok")
⋮----
@pytest.mark.asyncio
    async def test_sync_failure(self, cb)
⋮----
class TestCircuitBreakerReset
⋮----
"""Test manual reset."""
⋮----
@pytest.mark.asyncio
    async def test_manual_reset(self, cb)
⋮----
class TestCircuitBreakerStats
⋮----
"""Test statistics reporting."""
⋮----
@pytest.mark.asyncio
    async def test_stats_reflect_activity(self, cb)
⋮----
@pytest.mark.asyncio
    async def test_rejected_count(self, cb)
⋮----
class TestCircuitBreakerRegistry
⋮----
"""Test global registry functions."""
⋮----
def test_get_or_create_new(self)
⋮----
breaker = get_or_create("svc-a", failure_threshold=2)
⋮----
def test_get_or_create_existing(self)
⋮----
b1 = get_or_create("svc-b")
b2 = get_or_create("svc-b", failure_threshold=99)
⋮----
# Original threshold preserved
⋮----
def test_get_all_stats(self)
⋮----
stats = get_all_stats()
⋮----
names = {s.name for s in stats}
⋮----
def test_get_registry(self)
⋮----
reg = get_registry()
⋮----
# Should be a copy
⋮----
# Degradation Manager tests
⋮----
class TestDegradationManager
⋮----
"""Test graceful degradation."""
⋮----
@pytest.mark.asyncio
    async def test_successful_call_no_fallback(self, degradation_mgr)
⋮----
result = await degradation_mgr.execute_with_fallback(
⋮----
@pytest.mark.asyncio
    async def test_failure_without_fallback_raises(self, degradation_mgr)
⋮----
@pytest.mark.asyncio
    async def test_failure_with_fallback(self, degradation_mgr)
⋮----
@pytest.mark.asyncio
    async def test_recovery_clears_degraded_state(self, degradation_mgr)
⋮----
# Fail first
⋮----
# Succeed next
⋮----
@pytest.mark.asyncio
    async def test_async_fallback(self, degradation_mgr)
⋮----
async def async_fallback(*a, **kw)
⋮----
@pytest.mark.asyncio
    async def test_fallback_failure_raises_original(self, degradation_mgr)
⋮----
def bad_fallback(*a, **kw)
⋮----
@pytest.mark.asyncio
    async def test_multiple_failures_increment_count(self, degradation_mgr)
⋮----
degraded = degradation_mgr.get_degraded_services()
⋮----
def test_get_stats(self, degradation_mgr)
⋮----
stats = degradation_mgr.get_stats()
⋮----
class TestDefaultManager
⋮----
"""Test create_default_manager factory."""
⋮----
def test_creates_with_standard_fallbacks(self)
⋮----
mgr = create_default_manager()
stats = mgr.get_stats()
names = set(stats["registered_fallbacks"])
⋮----
@pytest.mark.asyncio
    async def test_anthropic_fallback_returns_error(self)
⋮----
result = await mgr.execute_with_fallback(
⋮----
@pytest.mark.asyncio
    async def test_yfinance_fallback_returns_error(self)
⋮----
@pytest.mark.asyncio
    async def test_redis_fallback_returns_none(self)
⋮----
# Retry decorator tests
⋮----
class TestWithRetry
⋮----
"""Test the with_retry decorator."""
⋮----
@pytest.mark.asyncio
    async def test_succeeds_first_attempt(self)
⋮----
call_count = 0
⋮----
@with_retry(max_attempts=3, base_delay=0.01)
        async def good()
⋮----
result = await good()
⋮----
@pytest.mark.asyncio
    async def test_retries_on_failure_then_succeeds(self)
⋮----
@with_retry(max_attempts=3, base_delay=0.01, jitter=False)
        async def flaky()
⋮----
result = await flaky()
⋮----
@pytest.mark.asyncio
    async def test_exhausts_retries(self)
⋮----
@with_retry(max_attempts=2, base_delay=0.01)
        async def always_fail()
⋮----
@pytest.mark.asyncio
    async def test_non_retryable_exception_propagates(self)
⋮----
async def bad()
⋮----
# Should only have been called once
⋮----
@pytest.mark.asyncio
    async def test_on_retry_callback(self)
⋮----
retries = []
⋮----
def on_retry_cb(attempt, exc, delay)
⋮----
@with_retry(max_attempts=3, base_delay=0.01, jitter=False, on_retry=on_retry_cb)
        async def flaky()
⋮----
assert retries[0][0] == 1  # first retry attempt number
⋮----
@pytest.mark.asyncio
    async def test_backoff_respects_max_delay(self)
⋮----
"""Verify that delay is capped at max_delay."""
delays = []
⋮----
def capture_delay(attempt, exc, delay)
⋮----
async def always_fail()
⋮----
# With base=1, exp_base=2, no jitter: delays = [1, 2, 3(capped), 3(capped)]
⋮----
# with_timeout decorator tests
⋮----
class TestWithTimeout
⋮----
"""Test the with_timeout decorator."""
⋮----
@pytest.mark.asyncio
    async def test_completes_within_timeout(self)
⋮----
@with_timeout(2.0)
        async def fast()
⋮----
result = await fast()
⋮----
@pytest.mark.asyncio
    async def test_exceeds_timeout(self)
⋮----
@with_timeout(0.1)
        async def slow()
⋮----
@pytest.mark.asyncio
    async def test_custom_timeout_message(self)
⋮----
@with_timeout(0.1, timeout_message="Custom timeout hit")
        async def slow()
⋮----
# QueryTimeoutManager tests
⋮----
class TestQueryTimeoutManager
⋮----
"""Test the query timeout manager."""
⋮----
@pytest.mark.asyncio
    async def test_async_query_succeeds(self, timeout_mgr)
⋮----
result = await timeout_mgr.execute_with_timeout(
⋮----
@pytest.mark.asyncio
    async def test_sync_query_succeeds(self, timeout_mgr)
⋮----
@pytest.mark.asyncio
    async def test_query_times_out(self, timeout_mgr)
⋮----
async def slow_query()
⋮----
@pytest.mark.asyncio
    async def test_slow_query_logging(self, timeout_mgr)
⋮----
"""Queries slower than threshold are counted as slow."""
⋮----
async def medium_query()
⋮----
@pytest.mark.asyncio
    async def test_timeout_clamped_to_max(self, timeout_mgr)
⋮----
"""Custom timeout is clamped to max_timeout."""
⋮----
async def fast()
⋮----
# Request 999s timeout, but max is 5s
⋮----
def test_get_stats(self, timeout_mgr)
⋮----
stats = timeout_mgr.get_stats()
⋮----
def test_default_config(self)
⋮----
mgr = QueryTimeoutManager()
⋮----
# ResilienceConfig tests
⋮----
class TestResilienceConfig
⋮----
"""Test configuration loading."""
⋮----
def test_defaults(self)
⋮----
config = ResilienceConfig()
⋮----
def test_env_override(self, monkeypatch)
⋮----
def test_singleton_caching(self)
⋮----
# Reset singleton
⋮----
c1 = get_resilience_config()
c2 = get_resilience_config()
⋮----
config_mod._config = None  # clean up
⋮----
# Module __init__ re-exports
⋮----
class TestModuleExports
⋮----
"""Verify that __init__.py re-exports are accessible."""
⋮----
def test_all_exports(self)
⋮----
expected = [
</file>

<file path="tests/test_backend_security.py">
"""
Tests for backend/security/ module.
Covers: sanitizer, allowlist, vanna_hook, config, models.

Note: sql_validator.py is tested separately in tests/security/test_sql_injection.py.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from backend.security.sanitizer import (  # noqa: E402
⋮----
from backend.security.allowlist import QueryAllowlist  # noqa: E402
from backend.security.config import SecurityConfig  # noqa: E402
from backend.security.models import ValidationResult, ValidatedQuery  # noqa: E402
from backend.security.vanna_hook import (  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture()
def allowlist_file(tmp_path)
⋮----
"""Create a temporary allowlist JSON config file."""
config = {
path = tmp_path / "allowed_tables.json"
⋮----
@pytest.fixture()
def allowlist(allowlist_file)
⋮----
"""Return a QueryAllowlist loaded from a temp config."""
⋮----
@pytest.fixture(autouse=True)
def _reset_vanna_singletons()
⋮----
"""Reset vanna_hook singletons before and after each test."""
⋮----
# ===========================================================================
# sanitizer.py tests
⋮----
class TestSanitizeNlQuery
⋮----
"""Tests for sanitize_nl_query()."""
⋮----
def test_empty_input_returns_empty(self)
⋮----
def test_none_like_empty(self)
⋮----
# None is not str, but empty string should return empty
⋮----
def test_normal_query_passes_through(self)
⋮----
result = sanitize_nl_query("What is the market cap of Aramco?")
⋮----
def test_strips_control_characters(self)
⋮----
# \x00 (null) and \x07 (bell) should be stripped
result = sanitize_nl_query("Hello\x00World\x07!")
⋮----
def test_preserves_newlines_and_tabs(self)
⋮----
result = sanitize_nl_query("Line1\nLine2\tTabbed")
⋮----
def test_unicode_normalization(self)
⋮----
# NFC normalization: decomposed e-acute should become composed
decomposed = "e\u0301"  # e + combining acute accent
result = sanitize_nl_query(decomposed)
assert "\u00e9" in result  # NFC composed form
⋮----
def test_truncation_at_max_length(self)
⋮----
long_input = "a" * (MAX_NL_QUERY_LENGTH + 500)
result = sanitize_nl_query(long_input)
# After HTML escaping, 'a' stays 'a', so length should be MAX_NL_QUERY_LENGTH
⋮----
def test_html_escaping(self)
⋮----
result = sanitize_nl_query('<script>alert("xss")</script>')
⋮----
def test_html_escaping_quotes(self)
⋮----
result = sanitize_nl_query('value="test"')
⋮----
def test_html_escaping_ampersand(self)
⋮----
result = sanitize_nl_query("A & B")
⋮----
def test_rejects_raw_select(self)
⋮----
def test_rejects_raw_drop(self)
⋮----
def test_rejects_raw_insert(self)
⋮----
def test_rejects_raw_delete(self)
⋮----
def test_rejects_raw_update(self)
⋮----
def test_rejects_raw_alter(self)
⋮----
def test_rejects_raw_create(self)
⋮----
def test_rejects_raw_truncate(self)
⋮----
def test_rejects_with_cte(self)
⋮----
def test_rejects_explain(self)
⋮----
def test_rejects_pragma(self)
⋮----
def test_rejects_case_insensitive(self)
⋮----
def test_allows_natural_language_with_sql_words(self)
⋮----
# "select" in natural language context should be allowed
result = sanitize_nl_query(
⋮----
def test_strips_leading_trailing_whitespace(self)
⋮----
result = sanitize_nl_query("   Hello World   ")
⋮----
class TestSanitizeIdentifiers
⋮----
"""Tests for sanitize_identifiers()."""
⋮----
def test_valid_identifier(self)
⋮----
def test_valid_with_underscores(self)
⋮----
def test_valid_with_digits(self)
⋮----
def test_valid_starting_underscore(self)
⋮----
def test_valid_mixed_case(self)
⋮----
def test_empty_raises(self)
⋮----
def test_starts_with_digit_raises(self)
⋮----
def test_special_chars_raises(self)
⋮----
def test_spaces_raises(self)
⋮----
def test_sql_injection_attempt_raises(self)
⋮----
def test_dot_notation_raises(self)
⋮----
def test_max_length_exactly(self)
⋮----
# Exactly at limit should pass
ident = "a" * MAX_IDENTIFIER_LENGTH
⋮----
def test_exceeds_max_length(self)
⋮----
ident = "a" * (MAX_IDENTIFIER_LENGTH + 1)
⋮----
def test_strips_whitespace(self)
⋮----
# models.py tests
⋮----
class TestModels
⋮----
"""Tests for ValidationResult and ValidatedQuery Pydantic models."""
⋮----
def test_validation_result_defaults(self)
⋮----
r = ValidationResult()
⋮----
def test_validation_result_with_data(self)
⋮----
r = ValidationResult(
⋮----
def test_validated_query_defaults(self)
⋮----
q = ValidatedQuery()
⋮----
def test_validated_query_with_data(self)
⋮----
q = ValidatedQuery(
⋮----
def test_models_are_serializable(self)
⋮----
r = ValidationResult(is_valid=True, violations=[], risk_score=0.0)
d = r.model_dump()
⋮----
q = ValidatedQuery(is_safe=True, sql="SELECT 1", reason="ok")
d2 = q.model_dump()
⋮----
# config.py tests
⋮----
class TestSecurityConfig
⋮----
"""Tests for SecurityConfig pydantic-settings model."""
⋮----
def test_defaults(self)
⋮----
config = SecurityConfig()
⋮----
def test_blocked_patterns_list_empty(self)
⋮----
def test_blocked_patterns_list_parsing(self)
⋮----
config = SecurityConfig(blocked_sql_patterns="DROP.*,EXEC.*,UNION.*")
patterns = config.blocked_patterns_list
⋮----
def test_blocked_patterns_strips_whitespace(self)
⋮----
config = SecurityConfig(blocked_sql_patterns=" DROP.* , EXEC.* ")
⋮----
def test_blocked_patterns_ignores_empty_entries(self)
⋮----
config = SecurityConfig(blocked_sql_patterns="DROP.*,,EXEC.*,")
⋮----
def test_resolved_allowed_tables_path_relative(self)
⋮----
config = SecurityConfig(allowed_tables_path="config/allowed_tables.json")
resolved = config.resolved_allowed_tables_path
⋮----
def test_resolved_allowed_tables_path_absolute(self, tmp_path)
⋮----
abs_path = str(tmp_path / "my_config.json")
config = SecurityConfig(allowed_tables_path=abs_path)
⋮----
def test_env_prefix(self)
⋮----
def test_max_query_length_bounds(self)
⋮----
# ge=100
⋮----
def test_max_result_rows_bounds(self)
⋮----
# ge=1
⋮----
# allowlist.py tests
⋮----
class TestQueryAllowlist
⋮----
"""Tests for QueryAllowlist."""
⋮----
def test_allowed_table(self, allowlist)
⋮----
def test_case_insensitive_table(self, allowlist)
⋮----
def test_disallowed_table(self, allowlist)
⋮----
def test_blocked_table_overrides_allowed(self, allowlist)
⋮----
# "users" is in blocked_tables
⋮----
def test_allowed_operation(self, allowlist)
⋮----
def test_operation_case_insensitive(self, allowlist)
⋮----
def test_disallowed_operation(self, allowlist)
⋮----
def test_get_allowed_tables_sorted(self, allowlist)
⋮----
tables = allowlist.get_allowed_tables()
⋮----
def test_get_blocked_tables_sorted(self, allowlist)
⋮----
blocked = allowlist.get_blocked_tables()
⋮----
def test_missing_config_file(self, tmp_path)
⋮----
"""When config file doesn't exist, nothing is allowed (fail safe)."""
al = QueryAllowlist(config_path=tmp_path / "nonexistent.json")
⋮----
def test_invalid_json_config(self, tmp_path)
⋮----
"""Invalid JSON fails safe with empty allowlist."""
bad_file = tmp_path / "bad.json"
⋮----
al = QueryAllowlist(config_path=bad_file)
⋮----
def test_hot_reload(self, tmp_path)
⋮----
"""Config changes are picked up after TTL expires."""
⋮----
path = tmp_path / "hot_reload.json"
⋮----
al = QueryAllowlist(config_path=path, cache_ttl=0.0)
⋮----
# Update the config file
⋮----
# Need mtime to actually differ
⋮----
# With cache_ttl=0.0, next access should reload
⋮----
def test_empty_config_keys(self, tmp_path)
⋮----
"""Config with missing keys uses empty defaults."""
path = tmp_path / "empty_keys.json"
⋮----
al = QueryAllowlist(config_path=path)
⋮----
# vanna_hook.py tests
⋮----
class TestVannaHook
⋮----
"""Tests for validate_vanna_output()."""
⋮----
def test_empty_sql_returns_unsafe(self)
⋮----
result = validate_vanna_output("")
⋮----
def test_whitespace_only_returns_unsafe(self)
⋮----
result = validate_vanna_output("   ")
⋮----
def test_valid_select_with_allowlist(self, allowlist_file)
⋮----
result = validate_vanna_output(
⋮----
def test_forbidden_operation_rejected(self, allowlist_file)
⋮----
def test_disallowed_table_rejected(self, allowlist_file)
⋮----
def test_blocked_table_rejected(self, allowlist_file)
⋮----
def test_stacked_queries_rejected(self, allowlist_file)
⋮----
def test_injection_pattern_rejected(self, allowlist_file)
⋮----
def test_validation_time_recorded(self, allowlist_file)
⋮----
def test_reset_singletons_clears_state(self)
⋮----
# Force singletons to be created
⋮----
def test_operation_not_in_allowlist(self, tmp_path)
⋮----
"""A query that passes sql_validator but whose primary op isn't allowed."""
⋮----
"allowed_operations": [],  # No operations allowed
⋮----
path = tmp_path / "no_ops.json"
⋮----
# __init__.py tests (public API re-exports)
⋮----
class TestModuleExports
⋮----
"""Verify the public API from backend.security.__init__."""
⋮----
def test_all_exports_importable(self)
⋮----
def test_all_matches_actual_exports(self)
</file>

<file path="tests/test_database.py">
"""
Comprehensive database testing suite for Saudi Stocks database
Tests data quality, integrity, schema validation, and cross-table consistency.

Supports dual backends:
  - SQLite (default): Always runs against saudi_stocks.db
  - PostgreSQL (optional): Runs when POSTGRES_HOST is set and reachable
    Skip PG tests gracefully if unavailable.
"""
⋮----
# ---------------------------------------------------------------------------
# Backend helpers
⋮----
_HERE = Path(__file__).resolve().parent
_SQLITE_PATH = os.environ.get("DB_SQLITE_PATH", str(_HERE.parent / "saudi_stocks.db"))
⋮----
def _get_sqlite_conn()
⋮----
def _get_pg_conn()
⋮----
class _DatabaseTestMixin
⋮----
"""
    Shared test logic for both SQLite and PostgreSQL backends.
    Subclasses must set cls.conn, cls.cursor, and cls.backend in setUpClass.
    """
⋮----
expected_tables = [
simple_tables = [
financial_tables = ["balance_sheet", "income_statement", "cash_flow"]
⋮----
def _list_tables(self) -> List[str]
⋮----
def _get_columns(self, table: str) -> List[str]
⋮----
def test_01_table_existence(self)
⋮----
"""Verify all 10 expected tables exist"""
actual_tables = self._list_tables()
⋮----
def test_02_row_counts_simple_tables(self)
⋮----
"""Verify simple tables have exactly 500 rows"""
⋮----
count = self.cursor.fetchone()[0]
⋮----
def test_03_row_counts_financial_tables(self)
⋮----
"""Verify financial tables have 2500+ rows"""
⋮----
def test_04_schema_validation_companies(self)
⋮----
"""Validate companies table schema"""
⋮----
def test_05_schema_validation_market_data(self)
⋮----
"""Validate market_data table schema"""
⋮----
def test_06_schema_validation_balance_sheet(self)
⋮----
"""Validate balance_sheet table schema"""
⋮----
def test_07_schema_validation_income_statement(self)
⋮----
"""Validate income_statement table schema"""
⋮----
def test_08_schema_validation_cash_flow(self)
⋮----
"""Validate cash_flow table schema"""
⋮----
def test_09_foreign_key_integrity(self)
⋮----
"""Verify foreign key integrity across all tables"""
⋮----
valid_tickers = set(row[0] for row in self.cursor.fetchall())
⋮----
child_tables = [t for t in self.expected_tables if t != "companies"]
⋮----
table_tickers = set(row[0] for row in self.cursor.fetchall())
invalid_tickers = table_tickers - valid_tickers
⋮----
def test_10_no_duplicate_tickers_simple_tables(self)
⋮----
"""Verify no duplicate tickers in simple tables"""
⋮----
duplicates = self.cursor.fetchall()
⋮----
def test_11_no_duplicate_period_combos(self)
⋮----
"""Verify no duplicate period combinations in financial tables"""
⋮----
def test_12_valid_period_types(self)
⋮----
"""Verify period_type values are valid"""
valid_types = {"annual", "quarterly", "ttm"}
⋮----
actual_types = set(
invalid_types = actual_types - valid_types
⋮----
def test_13_period_date_not_null(self)
⋮----
"""Verify period_date is not null in financial statements"""
⋮----
null_count = self.cursor.fetchone()[0]
⋮----
def test_14_non_null_financial_data(self)
⋮----
"""Verify some non-null values for key financial columns"""
checks = [
⋮----
non_null_count = self.cursor.fetchone()[0]
⋮----
def test_15_index_existence(self)
⋮----
"""Verify indexes exist on financial statement tables"""
⋮----
indexes = [row[0] for row in self.cursor.fetchall()]
custom_indexes = [
⋮----
custom_indexes = [row[0] for row in self.cursor.fetchall()]
# Verify at least one index exists for query performance
⋮----
def test_16_saudi_aramco_exists(self)
⋮----
"""Verify Saudi Aramco (2222.SR) exists"""
⋮----
result = self.cursor.fetchone()
⋮----
def test_17_positive_market_caps(self)
⋮----
"""Verify market cap values are positive where present"""
⋮----
negative_count = self.cursor.fetchone()[0]
⋮----
def test_18_valid_date_formats(self)
⋮----
"""Verify dates are in valid format (YYYY-MM-DD)"""
⋮----
dates = [row[0] for row in self.cursor.fetchall()]
invalid_dates = []
⋮----
date_str = str(date_val) if not isinstance(date_val, str) else date_val
⋮----
def test_19_cross_table_consistency(self)
⋮----
"""Companies with market_data should have financial statements"""
⋮----
_companies_with_market_data = self.cursor.fetchone()[0]
⋮----
with_financials = self.cursor.fetchone()[0]
⋮----
def test_20_sector_distribution(self)
⋮----
"""Verify sector distribution shows known Saudi sectors"""
⋮----
sectors = self.cursor.fetchall()
⋮----
def test_21_no_null_period_types(self)
⋮----
"""Verify period_type is never null in financial tables"""
⋮----
def test_22_no_negative_prices(self)
⋮----
"""Verify price values are non-negative where present"""
price_columns = [
⋮----
negative = self.cursor.fetchone()[0]
⋮----
def test_23_sequential_period_indexes(self)
⋮----
"""Verify period_index values start at 0 for each ticker/period_type"""
⋮----
bad = self.cursor.fetchall()
⋮----
def _validate_columns(self, table: str, expected_columns: List[str])
⋮----
actual_columns = self._get_columns(table)
⋮----
# SQLite test class (always runs)
⋮----
class TestDatabaseIntegrity(unittest.TestCase, _DatabaseTestMixin)
⋮----
"""SQLite database integrity tests (20 tests)."""
⋮----
@classmethod
    def setUpClass(cls)
⋮----
@classmethod
    def tearDownClass(cls)
⋮----
# PostgreSQL test class (skipped if PG unavailable)
⋮----
@unittest.skipUnless(_pg_available(), "PostgreSQL not available (set POSTGRES_HOST)")
class TestDatabaseIntegrityPG(unittest.TestCase, _DatabaseTestMixin)
⋮----
"""PostgreSQL database integrity tests (20 tests, skipped if PG unavailable)."""
⋮----
# Runner
⋮----
def run_tests()
⋮----
loader = unittest.TestLoader()
suite = unittest.TestSuite()
⋮----
runner = unittest.TextTestRunner(verbosity=2)
result = runner.run(suite)
</file>

<file path="tests/test_health_config.py">
"""
Tests for health service and config modules.
Covers: services/health_service.py, config/env_validator.py,
        config/lifecycle.py, config/error_tracking.py
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ---------------------------------------------------------------------------
# Health Service - HealthStatus / ComponentHealth / HealthReport
⋮----
class TestHealthDataclasses
⋮----
"""Tests for HealthStatus, ComponentHealth, and HealthReport dataclasses."""
⋮----
def test_health_status_values(self)
⋮----
def test_component_health_defaults(self)
⋮----
ch = ComponentHealth(name="db", status=HealthStatus.HEALTHY)
⋮----
def test_component_health_with_latency(self)
⋮----
ch = ComponentHealth(
⋮----
def test_health_report_defaults(self)
⋮----
report = HealthReport()
⋮----
def test_health_report_to_dict_basic(self)
⋮----
d = report.to_dict()
⋮----
def test_health_report_to_dict_with_components(self)
⋮----
comp = d["components"][0]
⋮----
def test_health_report_to_dict_no_build_info(self)
⋮----
def test_health_report_to_dict_with_build_info(self)
⋮----
def test_health_report_latency_none_in_dict(self)
⋮----
# Health Service - check_database
⋮----
class TestCheckDatabase
⋮----
"""Tests for check_database() with SQLite and mocked PostgreSQL."""
⋮----
def test_sqlite_healthy_when_db_exists(self, tmp_path)
⋮----
db_path = tmp_path / "test.db"
conn = sqlite3.connect(str(db_path))
⋮----
mock_settings = MagicMock()
⋮----
result = check_database()
⋮----
def test_sqlite_unhealthy_when_db_missing(self, tmp_path)
⋮----
missing_path = tmp_path / "nonexistent.db"
⋮----
def test_postgres_healthy_via_pool(self)
⋮----
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
# If we get here without exception, status should not be unhealthy from exception
⋮----
def test_postgres_fallback_direct_connection(self)
⋮----
def test_database_exception_returns_unhealthy(self, tmp_path)
⋮----
# Make the path exist but sqlite3.connect fail
⋮----
# Health Service - check_llm
⋮----
class TestCheckLlm
⋮----
"""Tests for check_llm()."""
⋮----
def test_llm_healthy_with_valid_api_key(self)
⋮----
result = check_llm()
⋮----
def test_llm_degraded_when_no_api_key(self)
⋮----
def test_llm_degraded_with_short_key(self)
⋮----
def test_llm_degraded_with_empty_string(self)
⋮----
# Health Service - check_redis
⋮----
class TestCheckRedis
⋮----
"""Tests for check_redis()."""
⋮----
def test_redis_healthy_when_caching_disabled(self)
⋮----
result = check_redis()
⋮----
def test_redis_healthy_when_available(self)
⋮----
def test_redis_degraded_when_unavailable(self)
⋮----
def test_redis_degraded_on_exception(self)
⋮----
# Health Service - check_entities / check_market_data
⋮----
class TestCheckEntities
⋮----
"""Tests for check_entities()."""
⋮----
def test_entities_healthy_when_companies_exist(self, tmp_path)
⋮----
result = check_entities()
⋮----
def test_entities_degraded_when_empty(self, tmp_path)
⋮----
def test_entities_unhealthy_when_db_missing(self, tmp_path)
⋮----
missing_path = tmp_path / "missing.db"
⋮----
def test_entities_unhealthy_on_exception(self, tmp_path)
⋮----
class TestCheckMarketData
⋮----
"""Tests for check_market_data()."""
⋮----
def test_market_data_healthy(self, tmp_path)
⋮----
result = check_market_data()
⋮----
def test_market_data_degraded_no_rows(self, tmp_path)
⋮----
def test_market_data_degraded_no_current_price(self, tmp_path)
⋮----
def test_market_data_unhealthy_db_missing(self, tmp_path)
⋮----
# Health Service - get_health (full report)
⋮----
class TestGetHealth
⋮----
"""Tests for get_health() aggregation."""
⋮----
def test_get_health_returns_report(self)
⋮----
mock_component = MagicMock()
⋮----
report = get_health()
⋮----
def test_get_health_overall_healthy(self)
⋮----
healthy = ComponentHealth(name="x", status=HealthStatus.HEALTHY)
⋮----
def test_get_health_overall_degraded_when_any_degraded(self)
⋮----
degraded = ComponentHealth(name="y", status=HealthStatus.DEGRADED)
⋮----
def test_get_health_overall_unhealthy_when_any_unhealthy(self)
⋮----
unhealthy = ComponentHealth(name="z", status=HealthStatus.UNHEALTHY)
⋮----
def test_get_health_includes_uptime(self)
⋮----
# Environment Validator
⋮----
class TestValidateEnv
⋮----
"""Tests for config/env_validator.py."""
⋮----
def test_valid_sqlite_config_passes(self)
⋮----
env = {
⋮----
def test_valid_postgres_config_passes(self)
⋮----
def test_invalid_db_backend_fails(self)
⋮----
env = {"DB_BACKEND": "oracle"}
⋮----
def test_postgres_missing_password_fails(self)
⋮----
def test_postgres_password_via_db_pg_password(self)
⋮----
def test_missing_llm_key_warns(self)
⋮----
env = {"DB_BACKEND": "sqlite"}
⋮----
def test_postgres_missing_host_warns(self)
⋮----
def test_invalid_log_level_warns(self)
⋮----
def test_valid_log_levels_no_warning(self)
⋮----
def test_cors_wildcard_in_production_warns(self)
⋮----
def test_missing_jwt_secret_in_production_warns(self)
⋮----
def test_jwt_secret_not_required_in_development(self)
⋮----
def test_default_db_backend_is_sqlite(self)
⋮----
# Remove DB_BACKEND from env - should default to sqlite and pass
⋮----
class TestValidateAndLog
⋮----
"""Tests for validate_and_log()."""
⋮----
def test_returns_true_on_no_errors(self)
⋮----
result = validate_and_log()
⋮----
def test_returns_false_on_errors(self)
⋮----
env = {"DB_BACKEND": "invalid_backend"}
⋮----
# Lifecycle
⋮----
class TestLifecycle
⋮----
"""Tests for config/lifecycle.py startup and shutdown handlers."""
⋮----
def test_on_startup_logs(self, caplog)
⋮----
# Verify startup was logged
messages = " ".join(caplog.messages)
⋮----
def test_on_startup_sets_start_time(self)
⋮----
def test_on_shutdown_logs(self, caplog)
⋮----
lc._start_time = 1.0  # Non-zero start time
⋮----
def test_on_startup_handles_settings_failure(self, caplog)
⋮----
# Should not raise even if settings fail
⋮----
def test_on_startup_handles_missing_env_validator(self, caplog)
⋮----
# This should not raise
⋮----
pass  # acceptable - the validate_and_log import path
⋮----
# Error Tracking
⋮----
class TestLogErrorTracker
⋮----
"""Tests for LogErrorTracker (default error tracking implementation)."""
⋮----
def test_capture_exception_logs_error(self, caplog)
⋮----
tracker = LogErrorTracker()
exc = ValueError("test error")
⋮----
def test_capture_exception_with_context(self, caplog)
⋮----
exc = RuntimeError("runtime fail")
ctx = {"user": "test-user", "path": "/api/v1/health"}
⋮----
def test_capture_message_defaults_to_error_level(self, caplog)
⋮----
def test_capture_message_with_info_level(self, caplog)
⋮----
def test_capture_message_with_context(self, caplog)
⋮----
ctx = {"request_id": "abc-123"}
⋮----
def test_capture_message_no_context(self, caplog)
⋮----
class TestInitErrorTracking
⋮----
"""Tests for init_error_tracking() and get_error_tracker()."""
⋮----
def setup_method(self)
⋮----
"""Reset singleton before each test."""
⋮----
def test_init_returns_log_tracker_by_default(self)
⋮----
env = {"ERROR_TRACKER": "log"}
⋮----
tracker = init_error_tracking()
⋮----
def test_init_without_env_defaults_to_log_tracker(self)
⋮----
def test_init_sentry_without_dsn_falls_back_to_log(self)
⋮----
env = {"ERROR_TRACKER": "sentry"}
⋮----
def test_init_sentry_with_dsn_falls_back_to_log(self)
⋮----
# SentryErrorTracker is commented out, so falls back to LogErrorTracker
⋮----
def test_get_error_tracker_initializes_if_none(self)
⋮----
tracker = get_error_tracker()
⋮----
def test_get_error_tracker_returns_existing(self)
⋮----
existing = LogErrorTracker()
⋮----
def test_init_sets_global_tracker(self)
⋮----
def test_init_reinitializes_tracker(self)
⋮----
tracker1 = init_error_tracking()
tracker2 = init_error_tracking()
⋮----
# Health Service - get_uptime_seconds
⋮----
class TestGetUptimeSeconds
⋮----
"""Tests for get_uptime_seconds()."""
⋮----
def test_uptime_is_non_negative(self)
⋮----
uptime = get_uptime_seconds()
⋮----
def test_uptime_increases_over_time(self)
⋮----
t1 = get_uptime_seconds()
⋮----
t2 = get_uptime_seconds()
</file>

<file path="tests/test_price_loader.py">
"""
Comprehensive tests for ingestion/price_loader.py
==================================================
Targets uncovered lines to raise coverage from ~33.5% to 70%+.

Coverage targets:
  - Lines 48-57: yfinance/psycopg2 import guards
  - Lines 167-211: batch processing and chunking in load_prices
  - Lines 230-243: load_all_prices error recovery
  - Lines 255-270: _fetch_and_insert_ticker pipeline
  - Lines 279-306: _fetch_with_retry exponential backoff
  - Lines 362-424: normalize_columns, compute_changes, prepare_dataframe
  - Lines 427-454: _clean_val and df_to_insert_tuples edge cases
  - Lines 462-477: insert_prices with DB batching
  - Lines 536-569: load_single_csv function
  - Lines 572-668: main() and parse_args() coverage
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ===========================================================================
# Helpers
⋮----
def _make_ohlcv_df(tickers=None, dates=None, close_prices=None)
⋮----
"""Build a minimal OHLCV DataFrame for testing."""
⋮----
tickers = ["2222.SR", "2222.SR"]
⋮----
dates = [date(2024, 1, 15), date(2024, 1, 16)]
⋮----
close_prices = [32.0, 33.0]
⋮----
def _make_yfinance_df(n=2)
⋮----
"""Return a DataFrame resembling yfinance .history() output."""
⋮----
# Module-level import guards (lines 48-57)
⋮----
class TestImportGuards
⋮----
"""Verify the try/except import blocks behave correctly."""
⋮----
def test_yfinance_imported_or_none(self)
⋮----
"""yf module is either the real yfinance or None - never something else."""
⋮----
def test_psycopg2_imported_or_none(self)
⋮----
# Data validation utilities (lines 50-51, 56-57 via prepare_dataframe)
⋮----
class TestPrepareDataframe
⋮----
"""Tests for prepare_dataframe - covers lines 398-424."""
⋮----
def _csv_df(self)
⋮----
def test_prepare_dataframe_basic(self)
⋮----
df = self._csv_df()
result = prepare_dataframe(df, ticker="2222.SR")
⋮----
def test_prepare_dataframe_renames_aliases(self)
⋮----
df = self._csv_df()  # uses "date", "open", "close", etc.
⋮----
def test_prepare_dataframe_ticker_from_column(self)
⋮----
result = prepare_dataframe(df)  # no ticker arg
⋮----
def test_prepare_dataframe_missing_ticker_raises(self)
⋮----
prepare_dataframe(df)  # no ticker col and no ticker arg
⋮----
def test_prepare_dataframe_missing_required_column_raises(self)
⋮----
df = pd.DataFrame(
⋮----
def test_prepare_dataframe_drops_rows_without_close(self)
⋮----
def test_prepare_dataframe_converts_trade_date_to_date(self)
⋮----
def test_prepare_dataframe_replaces_inf_with_nan(self)
⋮----
def test_prepare_dataframe_volume_int64(self)
⋮----
# Batch processing and chunking (lines 167-211)
⋮----
class TestBatchProcessing
⋮----
"""Tests for PriceLoader.load_prices batch iteration."""
⋮----
def _mock_yf_ticker(self, n_rows=2)
⋮----
"""Return a mock yfinance Ticker whose .history() returns an OHLCV DF."""
mock_ticker = MagicMock()
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=2)
    def test_single_batch_no_sleep(self, mock_insert, mock_sleep)
⋮----
"""With 2 tickers and batch_size=10, no sleep between batches."""
⋮----
config = IngestionConfig(batch_size=10, rate_limit_seconds=1.0)
loader = PriceLoader(pg_conn=None, config=config, dry_run=True)
⋮----
# No sleep when only one batch
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=2)
    def test_multiple_batches_sleep_called(self, mock_insert, mock_sleep)
⋮----
"""With 3 tickers and batch_size=2, sleep is called once (between batches 1 and 2)."""
⋮----
config = IngestionConfig(batch_size=2, rate_limit_seconds=0.5)
⋮----
# Sleep called once (after first batch, not after last)
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=2)
    def test_four_tickers_two_batches(self, mock_insert, mock_sleep)
⋮----
"""4 tickers / batch_size=2 => 2 batches, sleep once."""
⋮----
config = IngestionConfig(batch_size=2, rate_limit_seconds=1.0)
⋮----
@patch("ingestion.price_loader.time.sleep")
    def test_ticker_exception_counted_as_failed(self, mock_sleep)
⋮----
"""If _fetch_and_insert_ticker raises, the ticker is counted as failed and processing continues."""
⋮----
config = IngestionConfig(batch_size=10, rate_limit_seconds=0.0)
⋮----
# Make _fetch_and_insert_ticker raise directly (bypassing retry logic)
⋮----
call_count = [0]
⋮----
def fetch_side_effect(ticker, from_date, to_date)
⋮----
return 2  # rows inserted for 1010.SR
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=0)
    def test_load_prices_uses_today_as_default_to_date(self, mock_insert, mock_sleep)
⋮----
"""to_date defaults to today() when None is passed."""
⋮----
# to_date=None triggers default logic
⋮----
# Just check it completed without error
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=2)
    def test_stats_rows_inserted_updated(self, mock_insert, mock_sleep)
⋮----
total = loader.load_prices(["2222.SR"], from_date=date(2024, 1, 1))
⋮----
# load_all_prices (lines 230-243)
⋮----
class TestLoadAllPrices
⋮----
"""Tests for PriceLoader.load_all_prices."""
⋮----
def test_load_all_requires_pg_conn(self)
⋮----
loader = PriceLoader(pg_conn=None)
⋮----
def test_load_all_warns_when_no_tickers(self)
⋮----
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
loader = PriceLoader(pg_conn=mock_conn)
result = loader.load_all_prices(from_date=date(2024, 1, 1))
⋮----
@patch("ingestion.price_loader.time.sleep")
@patch("ingestion.price_loader.insert_prices", return_value=2)
    def test_load_all_fetches_tickers_from_db(self, mock_insert, mock_sleep)
⋮----
loader = PriceLoader(pg_conn=mock_conn, dry_run=True)
⋮----
# Verify the query was called
⋮----
query_args = mock_cursor.execute.call_args[0]
⋮----
# Error recovery and retry logic (lines 255-270, 279-306)
⋮----
class TestFetchWithRetry
⋮----
"""Tests for PriceLoader._fetch_with_retry."""
⋮----
def test_fetch_success_first_attempt(self)
⋮----
config = IngestionConfig(max_retries=3, backoff_factor=2.0)
loader = PriceLoader(config=config)
⋮----
result = loader._fetch_with_retry(
⋮----
@patch("ingestion.price_loader.time.sleep")
    def test_fetch_retries_on_exception(self, mock_sleep)
⋮----
# First two calls fail, third succeeds
⋮----
@patch("ingestion.price_loader.time.sleep")
    def test_fetch_returns_none_after_all_retries_fail(self, mock_sleep)
⋮----
# Sleep called for the first two retries, not the last
⋮----
@patch("ingestion.price_loader.time.sleep")
    def test_fetch_backoff_uses_exponent(self, mock_sleep)
⋮----
# Calls should be sleep(2^0)=1, sleep(2^1)=2
sleep_args = [c[0][0] for c in mock_sleep.call_args_list]
⋮----
@patch("ingestion.price_loader.time.sleep")
    def test_fetch_single_retry_no_sleep_on_last(self, mock_sleep)
⋮----
"""With max_retries=1, sleep is never called (last attempt fails immediately)."""
⋮----
config = IngestionConfig(max_retries=1, backoff_factor=2.0)
⋮----
# _fetch_and_insert_ticker pipeline (lines 245-270)
⋮----
class TestFetchAndInsertTicker
⋮----
"""Tests for PriceLoader._fetch_and_insert_ticker."""
⋮----
@patch("ingestion.price_loader.insert_prices", return_value=5)
    def test_returns_zero_for_empty_df(self, mock_insert)
⋮----
loader = PriceLoader(pg_conn=None, dry_run=True)
⋮----
count = loader._fetch_and_insert_ticker(
⋮----
@patch("ingestion.price_loader.insert_prices", return_value=5)
    def test_returns_zero_for_none_df(self, mock_insert)
⋮----
def test_pipeline_normalizes_and_inserts(self)
⋮----
"""Full pipeline: fetch -> normalize -> compute changes -> insert."""
⋮----
yf_df = _make_yfinance_df(3)
⋮----
def test_pipeline_replaces_inf_before_insert(self)
⋮----
"""Infinite values in numeric columns are replaced with NaN before DB insert."""
⋮----
yf_df = _make_yfinance_df(2)
⋮----
inserted_rows = []
⋮----
def capture_insert(conn, rows, dry_run)
⋮----
# Inf close_price should become None (cleaned by _clean_val)
close_vals = [row[5] for row in inserted_rows]  # index 5 = close_price
⋮----
# Data transformation - normalize_yfinance_df (lines 308-354)
⋮----
class TestNormalizeYfinanceDf
⋮----
"""Tests for PriceLoader._normalize_yfinance_df - covers lines 308-354."""
⋮----
def test_drops_rows_without_close_price(self)
⋮----
df = _make_yfinance_df(3)
⋮----
result = PriceLoader._normalize_yfinance_df(df, "2222.SR")
⋮----
def test_converts_volume_to_int64(self)
⋮----
df = _make_yfinance_df(2)
⋮----
def test_trade_date_is_python_date(self)
⋮----
def test_extra_columns_dropped(self)
⋮----
def test_ticker_column_added(self)
⋮----
result = PriceLoader._normalize_yfinance_df(df, "3030.SR")
⋮----
# normalize_columns
⋮----
class TestNormalizeColumns
⋮----
"""Additional tests for normalize_columns (line 362-370)."""
⋮----
def test_no_rename_when_columns_already_correct(self)
⋮----
result = normalize_columns(df)
⋮----
def test_adj_close_alias(self)
⋮----
df = pd.DataFrame({"Adj Close": [32.5]})
⋮----
def test_symbol_alias(self)
⋮----
df = pd.DataFrame({"Symbol": ["2222.SR"]})
⋮----
def test_ticker_alias(self)
⋮----
df = pd.DataFrame({"Ticker": ["2222.SR"]})
⋮----
# compute_changes - edge cases (lines 373-395)
⋮----
class TestComputeChanges
⋮----
"""Edge-case tests for compute_changes."""
⋮----
def test_multiple_tickers_independent_groups(self)
⋮----
result = compute_changes(df)
# First row of each ticker should have no change
a_first = result[
b_first = result[
⋮----
def test_zero_prev_close_gives_none_pct(self)
⋮----
second_row = result.iloc[1]
# change_pct should be None when prev_close is 0
⋮----
def test_prev_close_column_removed(self)
⋮----
def test_single_row_has_null_change(self)
⋮----
# _clean_val (lines 427-435)
⋮----
class TestCleanVal
⋮----
"""Tests for the _clean_val helper."""
⋮----
def test_none_returns_none(self)
⋮----
def test_nan_float_returns_none(self)
⋮----
def test_inf_float_returns_none(self)
⋮----
def test_neg_inf_float_returns_none(self)
⋮----
def test_pandas_na_returns_none(self)
⋮----
def test_regular_float_returned(self)
⋮----
def test_integer_returned(self)
⋮----
def test_string_returned(self)
⋮----
# df_to_insert_tuples edge cases (lines 438-454)
⋮----
class TestDfToInsertTuplesEdgeCases
⋮----
"""Additional edge-case tests for df_to_insert_tuples."""
⋮----
def test_inf_values_become_none(self)
⋮----
tuples = df_to_insert_tuples(df)
assert tuples[0][2] is None  # open_price inf -> None
assert tuples[0][5] is None  # close_price -inf -> None
⋮----
def test_multiple_rows_all_converted(self)
⋮----
# insert_prices DB batching (lines 462-477)
⋮----
class TestInsertPrices
⋮----
"""Tests for insert_prices including DB batch logic."""
⋮----
def test_dry_run_returns_row_count(self)
⋮----
rows = [
count = insert_prices(None, rows, dry_run=True)
⋮----
def test_empty_rows_returns_zero(self)
⋮----
count = insert_prices(None, [], dry_run=False)
⋮----
def test_db_insert_called_with_execute_batch(self)
⋮----
"""insert_prices calls psycopg2.extras.execute_batch correctly."""
⋮----
count = insert_prices(mock_conn, rows, dry_run=False)
⋮----
def test_large_batch_split(self)
⋮----
"""Rows > DB_BATCH_SIZE (500) are split into multiple execute_batch calls."""
⋮----
# 600 rows - should split into 2 batches (500 + 100)
⋮----
# execute_batch should have been called twice
⋮----
# load_single_csv (lines 536-569)
⋮----
class TestLoadSingleCsv
⋮----
"""Tests for load_single_csv function."""
⋮----
def _write_csv(self, tmpdir, filename, content)
⋮----
path = Path(tmpdir) / filename
⋮----
def test_load_single_csv_success(self, tmp_path)
⋮----
csv_content = (
file_path = tmp_path / "2222.SR.csv"
⋮----
count = load_single_csv(file_path, "2222.SR", pg_conn=None, dry_run=True)
⋮----
def test_load_single_csv_missing_required_column(self, tmp_path, capsys)
⋮----
csv_content = "Date,Close\n2024-01-15,32.5\n"
⋮----
captured = capsys.readouterr()
⋮----
def test_load_single_csv_unreadable_file(self, tmp_path, capsys)
⋮----
# Write a binary file that can't be parsed as CSV
file_path = tmp_path / "bad.csv"
file_path.write_bytes(b"\xff\xfe")  # invalid UTF-8
⋮----
# Should either succeed with 0 rows or fail gracefully
⋮----
def test_load_single_csv_dry_run_suffix(self, tmp_path, capsys)
⋮----
def test_load_single_csv_prints_date_range(self, tmp_path, capsys)
⋮----
# parse_args (lines 485-533)
⋮----
class TestParseArgs
⋮----
"""Tests for parse_args argument parsing."""
⋮----
def test_parse_args_tickers(self)
⋮----
args = parse_args()
⋮----
def test_parse_args_all_flag(self)
⋮----
def test_parse_args_file(self)
⋮----
def test_parse_args_dir(self)
⋮----
def test_parse_args_from_date(self)
⋮----
def test_parse_args_dry_run(self)
⋮----
def test_parse_args_batch_size(self)
⋮----
def test_parse_args_pg_defaults(self)
⋮----
def test_parse_args_pattern_default(self)
⋮----
# main() function - edge cases (lines 572-668)
⋮----
class TestMain
⋮----
"""Tests for the main() entry-point function."""
⋮----
def test_main_dry_run_tickers(self, capsys)
⋮----
instance = MockLoader.return_value
⋮----
def test_main_file_not_found(self, capsys)
⋮----
def test_main_dir_not_found(self, capsys)
⋮----
def test_main_dir_loads_csv_files(self, tmp_path, capsys)
⋮----
csv_file = tmp_path / "2222.SR.csv"
⋮----
def test_main_all_mode_calls_load_all(self, capsys)
⋮----
def test_main_no_pg_install_exits(self, capsys)
⋮----
def test_main_pg_connection_error_exits(self, capsys)
⋮----
# Patch psycopg2 at the module level so the except clause matches
⋮----
class FakeOpError(Exception)
⋮----
def test_main_prints_total_rows(self, tmp_path, capsys)
⋮----
# Multi-period and portfolio aggregation (lines 400-434, 470-477)
⋮----
class TestMultiTickerPipeline
⋮----
"""End-to-end tests for multi-ticker scenarios."""
⋮----
def test_prepare_dataframe_multi_ticker_csv(self)
⋮----
"""CSV with multiple tickers processed correctly."""
⋮----
result = prepare_dataframe(df)  # no ticker kwarg - uses Ticker column
⋮----
# Each ticker's first date should have null change
⋮----
first = result[result["ticker"] == ticker].sort_values("trade_date").iloc[0]
⋮----
def test_insert_prices_tuple_order_matches_insert_columns(self)
⋮----
"""Tuples have columns in INSERT_COLUMNS order."""
⋮----
def test_batch_size_one_processes_each_ticker_separately(self)
⋮----
"""batch_size=1 means each ticker gets its own batch."""
⋮----
config = IngestionConfig(batch_size=1, rate_limit_seconds=0.0)
⋮----
sleep_calls = []
⋮----
def record_sleep(t)
⋮----
# With 3 tickers and batch_size=1, sleep is called 2 times (not after last)
</file>

<file path="tests/test_redis_scheduler.py">
"""
Tests for:
  - backend/services/cache/redis_client.py  (RedisManager)
  - ingestion/scheduler.py                  (scheduler jobs + main)

All Redis operations are mocked — no live Redis server is required.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from redis.exceptions import (  # noqa: E402
⋮----
from backend.services.cache.redis_client import (  # noqa: E402
⋮----
# ===========================================================================
# Helpers / fixtures
⋮----
def _make_manager(**kwargs) -> RedisManager
⋮----
"""Return a RedisManager with an optional override for any constructor arg."""
⋮----
def _mock_pool_and_client()
⋮----
"""Return (mock_pool, mock_client) pair suitable for patching."""
mock_pool = MagicMock()
⋮----
mock_client = MagicMock()
⋮----
# RedisManager — Initialisation
⋮----
class TestRedisManagerInit
⋮----
def test_defaults(self)
⋮----
mgr = _make_manager()
⋮----
def test_custom_url(self)
⋮----
mgr = _make_manager(url="redis://myhost:6380/1")
⋮----
def test_custom_password(self)
⋮----
mgr = _make_manager(password="secret")
⋮----
def test_custom_max_connections(self)
⋮----
mgr = _make_manager(max_connections=5)
⋮----
def test_is_connected_false_initially(self)
⋮----
# RedisManager — connect()
⋮----
class TestRedisManagerConnect
⋮----
@pytest.mark.asyncio
    async def test_connect_sets_connected(self)
⋮----
@pytest.mark.asyncio
    async def test_connect_idempotent(self)
⋮----
"""Calling connect() a second time when already connected is a no-op."""
⋮----
await mgr.connect()  # second call
⋮----
# ping should only have been called once (first connect)
⋮----
@pytest.mark.asyncio
    async def test_connect_failure_raises(self)
⋮----
"""connect() propagates RedisError and leaves _connected=False."""
⋮----
@pytest.mark.asyncio
    async def test_connect_ping_failure_raises(self)
⋮----
"""connect() raises when ping fails."""
⋮----
# RedisManager — disconnect()
⋮----
class TestRedisManagerDisconnect
⋮----
@pytest.mark.asyncio
    async def test_disconnect_after_connect(self)
⋮----
@pytest.mark.asyncio
    async def test_disconnect_without_connect_is_safe(self)
⋮----
"""disconnect() on a fresh manager must not raise."""
⋮----
await mgr.disconnect()  # should not raise
⋮----
@pytest.mark.asyncio
    async def test_disconnect_client_error_swallowed(self)
⋮----
"""RedisError during aclose is logged but not re-raised."""
⋮----
# RedisManager — get / set / delete / exists
⋮----
class TestRedisManagerCoreOps
⋮----
@pytest_asyncio.fixture
    async def connected_mgr(self)
⋮----
"""Return a RedisManager that is already connected (mocked)."""
⋮----
mgr._mock_client = mock_client  # expose for assertions
⋮----
@pytest.mark.asyncio
    async def test_get_existing_key(self, connected_mgr)
⋮----
result = await connected_mgr.get("mykey")
⋮----
@pytest.mark.asyncio
    async def test_get_missing_key(self, connected_mgr)
⋮----
result = await connected_mgr.get("missing")
⋮----
@pytest.mark.asyncio
    async def test_get_reconnects_on_connection_error(self, connected_mgr)
⋮----
"""get() automatically reconnects on RedisConnectionError."""
call_count = 0
original_client = connected_mgr._mock_client
⋮----
async def _get_side_effect(key)
⋮----
# After the first failure, _ensure_connection will reconnect by
# calling disconnect+connect; we patch connect to keep the same client.
⋮----
# After reconnect, _connected and _client must be valid again
async def _fake_connect()
⋮----
@pytest.mark.asyncio
    async def test_set_without_ttl(self, connected_mgr)
⋮----
result = await connected_mgr.set("k", b"v")
⋮----
@pytest.mark.asyncio
    async def test_set_with_ttl(self, connected_mgr)
⋮----
result = await connected_mgr.set("k", b"v", ttl=60)
⋮----
@pytest.mark.asyncio
    async def test_set_string_value(self, connected_mgr)
⋮----
result = await connected_mgr.set("k", "string_value")
⋮----
@pytest.mark.asyncio
    async def test_delete_single_key(self, connected_mgr)
⋮----
result = await connected_mgr.delete("k1")
⋮----
@pytest.mark.asyncio
    async def test_delete_multiple_keys(self, connected_mgr)
⋮----
result = await connected_mgr.delete("k1", "k2", "k3")
⋮----
@pytest.mark.asyncio
    async def test_delete_no_keys_returns_zero(self, connected_mgr)
⋮----
result = await connected_mgr.delete()
⋮----
@pytest.mark.asyncio
    async def test_exists_key_present(self, connected_mgr)
⋮----
result = await connected_mgr.exists("k")
⋮----
@pytest.mark.asyncio
    async def test_exists_no_keys_returns_zero(self, connected_mgr)
⋮----
result = await connected_mgr.exists()
⋮----
@pytest.mark.asyncio
    async def test_set_reconnects_on_connection_error(self, connected_mgr)
⋮----
"""set() retries after a RedisConnectionError."""
⋮----
async def _set_side_effect(key, value)
⋮----
# RedisManager — health_check()
⋮----
class TestRedisManagerHealthCheck
⋮----
@pytest.mark.asyncio
    async def test_health_check_healthy(self)
⋮----
result = await mgr.health_check()
⋮----
@pytest.mark.asyncio
    async def test_health_check_unhealthy_on_redis_error(self)
⋮----
# connect() will raise, so we manually inject a broken state
⋮----
# Manually set a broken client so health_check exercises the error path
⋮----
@pytest.mark.asyncio
    async def test_health_check_not_connected(self)
⋮----
"""health_check() when never connected returns unhealthy."""
⋮----
# _ensure_connection will try to reconnect, which will fail because
# we never called connect() and ConnectionPool.from_url will error.
⋮----
# ingestion/scheduler — helper functions and jobs
⋮----
class TestGetPgConn
⋮----
def test_raises_if_psycopg2_missing(self)
⋮----
"""_get_pg_conn raises ImportError when psycopg2 is None."""
⋮----
original = sched_module.psycopg2
⋮----
def test_uses_env_vars(self)
⋮----
mock_conn = MagicMock()
⋮----
result = sched_module._get_pg_conn()
⋮----
class TestJobLoadPrices
⋮----
def test_success_path(self)
⋮----
"""job_load_prices() runs loader and closes connection."""
⋮----
mock_loader = MagicMock()
⋮----
args = mock_loader.load_all_prices.call_args[0]
⋮----
def test_date_range_is_last_3_days(self)
⋮----
"""The from_date should be today - 3 days."""
⋮----
captured = {}
⋮----
def _capture(from_date, to_date)
⋮----
def test_connection_closed_on_exception(self)
⋮----
"""Even when an exception occurs, the connection is closed."""
⋮----
# Should not raise (errors are caught internally)
⋮----
def test_pg_conn_failure_is_handled(self)
⋮----
"""If _get_pg_conn raises, the job logs but does not propagate."""
⋮----
# job_load_prices catches all exceptions
sched_module.job_load_prices()  # must not raise
⋮----
class TestJobProcessXbrl
⋮----
def test_skips_when_no_filings_dir(self, tmp_path)
⋮----
"""If the filings directory does not exist, the job exits early."""
⋮----
# Should not try to connect to DB since we exit early
⋮----
def test_skips_unsupported_files(self, tmp_path)
⋮----
"""Files without supported extensions are ignored."""
⋮----
filings_dir = tmp_path / "data" / "filings"
⋮----
def test_skips_files_without_sr_ticker(self, tmp_path)
⋮----
"""Files whose names don't contain '.SR' are skipped with a warning."""
⋮----
def test_processes_valid_filing(self, tmp_path)
⋮----
"""A valid '.SR' filing is processed through the full pipeline."""
⋮----
filing_file = filings_dir / "2222.SR_annual.xml"
⋮----
mock_processor = MagicMock()
⋮----
# XBRLProcessor is imported locally inside job_process_xbrl, so patch
# it on the ingestion.xbrl_processor module where it is defined.
mock_xbrl_module = MagicMock()
⋮----
def test_connection_closed_on_exception(self, tmp_path)
⋮----
"""DB connection is closed even when the job body raises."""
⋮----
# Force an error during iteration
⋮----
# ingestion/scheduler — main()
⋮----
class TestSchedulerMain
⋮----
def test_exits_if_apscheduler_missing(self)
⋮----
"""main() prints an error and exits(1) when APScheduler is unavailable."""
⋮----
original = sched_module.BlockingScheduler
⋮----
def test_adds_price_and_xbrl_jobs(self)
⋮----
"""main() registers both scheduled jobs before starting."""
⋮----
mock_scheduler = MagicMock()
⋮----
job_ids = {c.kwargs["id"] for c in mock_scheduler.add_job.call_args_list}
⋮----
def test_registers_signal_handlers(self)
⋮----
"""main() installs SIGINT and SIGTERM handlers."""
⋮----
registered_signals = {c.args[0] for c in mock_signal.call_args_list}
⋮----
def test_signal_handler_shuts_down_scheduler(self)
⋮----
"""The signal handler calls scheduler.shutdown(wait=False)."""
⋮----
captured_handler = {}
⋮----
def _capture_signal(signum, handler)
⋮----
# Invoke the SIGINT handler and verify shutdown is called
handler = captured_handler.get(signal.SIGINT)
⋮----
def test_keyboard_interrupt_handled_gracefully(self)
⋮----
"""KeyboardInterrupt from scheduler.start() does not propagate."""
⋮----
# Should NOT raise
⋮----
def test_system_exit_handled_gracefully(self)
⋮----
"""SystemExit from scheduler.start() does not propagate."""
⋮----
def test_price_loader_job_cron_trigger(self)
⋮----
"""Price loader uses hour=17, minute=0 cron trigger."""
⋮----
cron_calls = []
⋮----
def _capture_cron(**kwargs)
⋮----
def test_xbrl_processor_job_cron_trigger(self)
⋮----
"""XBRL processor uses day_of_week='fri', hour=20 cron trigger."""
</file>

<file path="tests/test_stock_ohlcv.py">
"""
Tests for services/stock_ohlcv.py

Covers:
- Ticker normalization
- Mock data generation (determinism, period mapping, weekday filtering)
- Cache helpers (fresh hit, stale fallback)
- Circuit breaker helpers (open/closed, record failure/success)
- fetch_stock_ohlcv: real data path, empty DataFrame, circuit-open fallback,
  exception fallback, stale-cache fallback, mock fallback
- get_cache_status
- get_circuit_breaker_status
- _get_ticker_lock (lock creation, LRU eviction at MAX_LOCKS)
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ---------------------------------------------------------------------------
# Module-level imports (after path setup)
⋮----
import services.stock_ohlcv as ohlcv_mod  # noqa: E402
from services.stock_ohlcv import (  # noqa: E402
⋮----
# Helpers / fixtures
⋮----
def _make_ohlcv_df(n_rows: int = 5) -> pd.DataFrame
⋮----
"""Return a minimal OHLCV DataFrame with realistic columns and DatetimeIndex."""
dates = pd.date_range(start="2024-01-01", periods=n_rows, freq="B")
df = pd.DataFrame(
⋮----
@pytest.fixture(autouse=True)
def _reset_module_state()
⋮----
"""Reset the module-level cache, circuit breaker, and lock dict before each test."""
⋮----
# Reset circuit breaker by patching internal state directly
⋮----
# Clear the ticker locks dict
⋮----
# ===========================================================================
# 1. Ticker normalization
⋮----
class TestNormalizeTicker
⋮----
def test_plain_number_gets_sr_suffix(self)
⋮----
def test_already_has_sr_not_duplicated(self)
⋮----
def test_lowercase_sr_uppercased(self)
⋮----
def test_whitespace_stripped(self)
⋮----
def test_mixed_case_ticker(self)
⋮----
def test_already_uppercase_with_sr(self)
⋮----
# 2. VALID_PERIODS constant
⋮----
class TestValidPeriods
⋮----
def test_contains_expected_periods(self)
⋮----
# 3. Mock data generation
⋮----
class TestGenerateMockData
⋮----
def test_returns_list_of_dicts(self)
⋮----
data = _generate_mock_data("2222.SR", "1y")
⋮----
def test_each_entry_has_required_keys(self)
⋮----
data = _generate_mock_data("2222.SR", "1mo")
⋮----
def test_1mo_produces_about_22_entries(self)
⋮----
def test_1y_produces_about_252_entries(self)
⋮----
def test_5y_produces_more_entries_than_1y(self)
⋮----
data_1y = _generate_mock_data("2222.SR", "1y")
data_5y = _generate_mock_data("2222.SR", "5y")
⋮----
def test_deterministic_same_ticker(self)
⋮----
d1 = _generate_mock_data("2222.SR", "1y")
d2 = _generate_mock_data("2222.SR", "1y")
⋮----
def test_different_tickers_produce_different_data(self)
⋮----
d2 = _generate_mock_data("1120.SR", "1y")
# At least prices differ
⋮----
def test_no_friday_or_saturday_entries(self)
⋮----
dt = datetime.fromisoformat(entry["time"]).date()
# Friday=4, Saturday=5 in Python's weekday()
⋮----
def test_high_always_gte_low(self)
⋮----
data = _generate_mock_data("2222.SR", "3mo")
⋮----
def test_volume_is_positive_int(self)
⋮----
def test_unknown_period_defaults_to_252_days(self)
⋮----
data = _generate_mock_data("2222.SR", "unknown_period")
⋮----
def test_time_field_is_valid_date_string(self)
⋮----
# Should parse without exception
⋮----
# 4. Cache helpers
⋮----
class TestCacheHelpers
⋮----
def test_cache_miss_returns_none(self)
⋮----
def test_set_then_get_returns_payload(self)
⋮----
payload = {"data": [], "source": "real", "symbol": "9999.SR", "period": "1y"}
⋮----
result = _get_cached("9999.SR", "1y")
⋮----
def test_stale_cache_returns_none_when_empty(self)
⋮----
def test_stale_cache_returns_payload_with_source_cached(self)
⋮----
# Manually age the entry by patching fetched_at
key = ("9999.SR", "1y")
⋮----
stale = _get_stale_cached("9999.SR", "1y")
⋮----
def test_fresh_cache_not_treated_as_stale(self)
⋮----
# Fresh cache should still return the payload (not None)
fresh = _get_cached("9999.SR", "1y")
⋮----
# 5. Circuit breaker helpers
⋮----
class TestCircuitBreakerHelpers
⋮----
def test_initially_closed(self)
⋮----
def test_open_after_threshold_failures(self)
⋮----
threshold = ohlcv_mod._breaker._threshold
⋮----
def test_success_resets_circuit(self)
⋮----
def test_get_circuit_breaker_status_returns_dict(self)
⋮----
status = get_circuit_breaker_status()
⋮----
def test_circuit_status_open_when_tripped(self)
⋮----
# 6. Ticker lock helpers
⋮----
class TestGetTickerLock
⋮----
def test_returns_lock_object(self)
⋮----
lock = _get_ticker_lock("2222.SR", "1y")
# threading.Lock() returns an instance of _thread.lock; check via acquire/release
⋮----
def test_same_key_returns_same_lock(self)
⋮----
lock1 = _get_ticker_lock("2222.SR", "1y")
lock2 = _get_ticker_lock("2222.SR", "1y")
⋮----
def test_different_key_returns_different_lock(self)
⋮----
lock2 = _get_ticker_lock("1120.SR", "1y")
⋮----
def test_evicts_oldest_at_max_locks(self)
⋮----
max_locks = ohlcv_mod._MAX_LOCKS
# Fill to capacity
⋮----
# Adding one more should evict the oldest
⋮----
# 7. fetch_stock_ohlcv — real data path (yfinance success)
⋮----
class TestFetchStockOhlcvRealData
⋮----
def test_returns_real_source_when_yfinance_succeeds(self)
⋮----
df = _make_ohlcv_df(5)
⋮----
mock_ticker = MagicMock()
⋮----
result = fetch_stock_ohlcv("2222", "1y")
⋮----
def test_ohlcv_fields_present_in_each_row(self)
⋮----
df = _make_ohlcv_df(3)
⋮----
def test_ticker_normalized_to_sr_suffix(self)
⋮----
df = _make_ohlcv_df(2)
⋮----
# Verify that yfinance was called with .SR suffix
⋮----
def test_result_stored_in_cache(self)
⋮----
cached = _get_cached("2222.SR", "1y")
⋮----
def test_last_updated_is_utc_isoformat(self)
⋮----
def test_volume_zero_when_none_in_df(self)
⋮----
df["Volume"] = None  # Simulate missing volume
⋮----
# 8. fetch_stock_ohlcv — cache hit path
⋮----
class TestFetchStockOhlcvCacheHit
⋮----
def test_cache_hit_returns_cached_payload_without_calling_yfinance(self)
⋮----
payload = {
⋮----
# 9. fetch_stock_ohlcv — empty DataFrame falls through to mock
⋮----
class TestFetchStockOhlcvEmptyDataFrame
⋮----
def test_empty_df_falls_back_to_mock(self)
⋮----
empty_df = pd.DataFrame()
⋮----
def test_none_df_falls_back_to_mock(self)
⋮----
# 10. fetch_stock_ohlcv — circuit breaker open path
⋮----
class TestFetchStockOhlcvCircuitOpen
⋮----
def test_open_circuit_skips_yfinance_returns_mock(self)
⋮----
# Trip the circuit breaker
⋮----
# Should be mock (no stale cache)
⋮----
def test_open_circuit_returns_stale_cache_if_available(self)
⋮----
# Pre-populate stale cache
old_payload = {
⋮----
# Age the entry
key = ("2222.SR", "1y")
⋮----
# Trip circuit breaker
⋮----
# 11. fetch_stock_ohlcv — exception handling
⋮----
class TestFetchStockOhlcvExceptions
⋮----
def test_network_error_falls_back_to_mock(self)
⋮----
def test_generic_exception_increments_failure_count(self)
⋮----
initial = ohlcv_mod._breaker._consecutive_failures
⋮----
def test_rate_limit_error_classified(self)
⋮----
"""Exception with '429' in message should still fall back gracefully."""
⋮----
def test_timeout_error_classified(self)
⋮----
def test_stale_cache_preferred_over_mock_on_exception(self)
⋮----
# Seed stale cache
⋮----
# 12. fetch_stock_ohlcv — ImportError path (yfinance not installed)
⋮----
class TestFetchStockOhlcvImportError
⋮----
def test_import_error_falls_back_to_mock(self)
⋮----
# 13. get_stock_ohlcv alias
⋮----
class TestGetStockOhlcvAlias
⋮----
def test_alias_is_same_function(self)
⋮----
# 14. get_cache_status
⋮----
class TestGetCacheStatus
⋮----
def test_empty_cache_returns_empty_status(self)
⋮----
status = get_cache_status()
⋮----
def test_populated_cache_returns_fresh_status(self)
⋮----
def test_stale_cache_entry_shows_stale_status(self)
⋮----
def test_cache_age_seconds_is_non_negative(self)
⋮----
# 15. Payload structure invariants
⋮----
class TestPayloadStructure
⋮----
def test_mock_payload_has_all_required_keys(self)
⋮----
df_empty = pd.DataFrame()
⋮----
result = fetch_stock_ohlcv("9999", "6mo")
⋮----
def test_count_matches_len_of_data(self)
⋮----
def test_real_payload_count_matches_data_length(self)
⋮----
df = _make_ohlcv_df(7)
</file>

<file path="tests/test_widgets.py">
"""
Tests for the live market widgets system.
Covers: QuoteItem model, crypto/metals/oil/indices providers, QuotesHub orchestrator.
No Redis or external APIs required -- all network calls are mocked.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from api.models.widgets import QuoteItem  # noqa: E402
⋮----
# ---------------------------------------------------------------------------
# Helpers
⋮----
# ===========================================================================
# QuoteItem model
⋮----
class TestQuoteItemModel
⋮----
def test_valid_construction_minimal(self)
⋮----
"""Required fields only - optional change fields default to None."""
q = QuoteItem(
⋮----
def test_valid_construction_all_fields(self)
⋮----
"""All fields set explicitly."""
q = _make_quote(
⋮----
def test_serialization_to_dict(self)
⋮----
"""model_dump() returns a plain dict with all expected keys."""
q = _make_quote(change=-100.0, change_pct=-0.2)
d = q.model_dump()
⋮----
expected_keys = {
⋮----
def test_json_roundtrip(self)
⋮----
"""model_dump() -> json.dumps -> json.loads preserves values."""
q = _make_quote(symbol="XAU", asset_class="metal", price=1950.5)
raw = json.dumps(q.model_dump())
restored = json.loads(raw)
⋮----
def test_asset_class_literal_valid_values(self)
⋮----
"""All valid asset_class literals are accepted."""
⋮----
q = _make_quote(asset_class=cls)
⋮----
def test_asset_class_invalid_raises(self)
⋮----
"""An invalid asset_class raises a ValidationError."""
⋮----
def test_missing_required_field_raises(self)
⋮----
"""Omitting a required field raises a ValidationError."""
⋮----
# price is required
⋮----
def test_negative_price_accepted(self)
⋮----
"""Negative price (e.g., oil futures) should not be rejected."""
q = _make_quote(price=-5.0, asset_class="oil")
⋮----
# Crypto provider
⋮----
class TestFetchCrypto
⋮----
@pytest.mark.asyncio
    async def test_success_returns_btc_and_eth(self)
⋮----
"""Happy path: API returns both BTC and ETH, provider returns 2 QuoteItems."""
mock_response = MagicMock()
⋮----
mock_client = AsyncMock()
⋮----
quotes = await fetch_crypto()
⋮----
symbols = {q.symbol for q in quotes}
⋮----
@pytest.mark.asyncio
    async def test_btc_price_and_change_correct(self)
⋮----
"""BTC price and change_pct are correctly mapped from API response."""
⋮----
btc = next(q for q in quotes if q.symbol == "BTC")
⋮----
@pytest.mark.asyncio
    async def test_http_error_returns_empty_list(self)
⋮----
"""HTTP errors result in an empty list, not an exception."""
⋮----
@pytest.mark.asyncio
    async def test_timeout_returns_empty_list(self)
⋮----
"""Timeout errors result in an empty list, not an exception."""
⋮----
@pytest.mark.asyncio
    async def test_empty_api_response_returns_empty_list(self)
⋮----
"""Empty JSON object from API returns empty list."""
⋮----
@pytest.mark.asyncio
    async def test_missing_price_field_skips_coin(self)
⋮----
"""Coins without a usd price field are skipped."""
⋮----
"ethereum": {},  # missing usd price
⋮----
# Metals provider
⋮----
class TestFetchMetals
⋮----
def _make_fast_info(self, last_price=None, previous_close=None)
⋮----
info = MagicMock()
⋮----
@pytest.mark.asyncio
    async def test_success_returns_gold_and_silver(self)
⋮----
"""Happy path: yfinance returns data for both GC=F and SI=F."""
fast_info_gold = self._make_fast_info(last_price=2000.0, previous_close=1990.0)
fast_info_silver = self._make_fast_info(last_price=24.5, previous_close=24.0)
⋮----
ticker_gold = MagicMock()
⋮----
ticker_silver = MagicMock()
⋮----
def _ticker_factory(symbol)
⋮----
quotes = await fetch_metals()
⋮----
@pytest.mark.asyncio
    async def test_change_calculation_correct(self)
⋮----
"""Change and change_pct are computed from last_price - previous_close."""
fast_info = self._make_fast_info(last_price=2100.0, previous_close=2000.0)
ticker_mock = MagicMock()
⋮----
# Return the same mock for both tickers so we can test XAU easily
⋮----
xau = next(q for q in quotes if q.symbol == "XAU")
⋮----
@pytest.mark.asyncio
    async def test_no_previous_close_yields_none_change(self)
⋮----
"""When previous_close is None, change fields are None."""
fast_info = self._make_fast_info(last_price=1950.0, previous_close=None)
⋮----
@pytest.mark.asyncio
    async def test_none_price_skips_ticker(self)
⋮----
"""Tickers with last_price=None are skipped."""
fast_info = self._make_fast_info(last_price=None, previous_close=None)
⋮----
@pytest.mark.asyncio
    async def test_yfinance_exception_returns_empty_list(self)
⋮----
"""Exception inside yfinance sync function returns empty list."""
⋮----
# Oil provider
⋮----
class TestFetchOil
⋮----
@pytest.mark.asyncio
    async def test_success_returns_brent_and_wti(self)
⋮----
"""Happy path: yfinance returns data for both BZ=F and CL=F."""
fast_info = self._make_fast_info(last_price=80.5, previous_close=79.0)
⋮----
quotes = await fetch_oil()
⋮----
"""Change and change_pct are correctly computed."""
fast_info = self._make_fast_info(last_price=85.0, previous_close=80.0)
⋮----
brent = next(q for q in quotes if q.symbol == "BRENT")
⋮----
fast_info = self._make_fast_info(last_price=None)
⋮----
@pytest.mark.asyncio
    async def test_zero_previous_close_yields_none_change(self)
⋮----
"""When previous_close is 0, change fields remain None (avoid division by zero)."""
fast_info = self._make_fast_info(last_price=80.0, previous_close=0)
⋮----
# Indices provider
⋮----
class TestFetchIndices
⋮----
@pytest.mark.asyncio
    async def test_success_returns_three_indices(self)
⋮----
"""Happy path: yfinance returns data for SPX, DJI, IXIC."""
fast_info = self._make_fast_info(last_price=5000.0, previous_close=4950.0)
⋮----
quotes = await fetch_indices()
⋮----
@pytest.mark.asyncio
    async def test_indices_are_delayed(self)
⋮----
"""All index quotes have is_delayed=True and delay_minutes=15."""
fast_info = self._make_fast_info(last_price=5000.0, previous_close=4900.0)
⋮----
"""Change and change_pct are correctly computed for indices."""
fast_info = self._make_fast_info(last_price=5100.0, previous_close=5000.0)
⋮----
spx = next(q for q in quotes if q.symbol == "SPX")
⋮----
# QuotesHub / quotes_hub module
⋮----
class TestSerialize
⋮----
def test_serialize_produces_valid_json(self)
⋮----
"""_serialize() returns a valid JSON string from a list of QuoteItems."""
⋮----
quotes = [
raw = _serialize(quotes)
parsed = json.loads(raw)
⋮----
def test_serialize_empty_list(self)
⋮----
"""_serialize() with empty list returns '[]'."""
⋮----
def test_serialize_preserves_non_ascii(self)
⋮----
"""Non-ASCII characters in name fields are preserved."""
⋮----
q = _make_quote(name="ذهب")  # Arabic for "gold"
raw = _serialize([q])
⋮----
class TestGetLatestSnapshot
⋮----
def test_initial_snapshot_is_none(self)
⋮----
"""get_latest_snapshot() returns None before any data is fetched."""
⋮----
# Reset to None for isolation
original = hub._latest_snapshot
⋮----
def test_snapshot_updated_after_assignment(self)
⋮----
"""get_latest_snapshot() reflects directly assigned snapshot."""
⋮----
class TestRunQuotesHub
⋮----
@pytest.mark.asyncio
    async def test_hub_updates_in_memory_snapshot(self)
⋮----
"""run_quotes_hub sets _latest_snapshot on first successful fetch."""
⋮----
test_quotes = [_make_quote(symbol="BTC", price=50000.0)]
⋮----
original_snapshot = hub._latest_snapshot
⋮----
async def _one_iteration(redis_client=None)
⋮----
"""Patch to stop after first iteration by cancelling the task."""
⋮----
# Patch _fetch_all_providers to return test_quotes
⋮----
# Directly test that _fetch_all_providers result flows into snapshot
⋮----
# Run with a short sleep so we can cancel immediately
⋮----
parsed = json.loads(hub._latest_snapshot)
⋮----
@pytest.mark.asyncio
    async def test_hub_skips_cycle_when_no_quotes(self)
⋮----
"""run_quotes_hub does not update snapshot when providers return empty."""
⋮----
sleep_calls: list[float] = []
⋮----
async def _mock_sleep(seconds: float)
⋮----
# Snapshot should remain None since no quotes were fetched
⋮----
@pytest.mark.asyncio
    async def test_hub_calls_redis_setex_when_redis_provided(self)
⋮----
"""run_quotes_hub calls redis.setex when redis_client is given."""
⋮----
test_quotes = [_make_quote()]
⋮----
mock_redis = MagicMock()
⋮----
@pytest.mark.asyncio
    async def test_hub_error_isolation_single_provider_failure(self)
⋮----
"""One failing provider does not stop the hub; other providers' quotes persist."""
⋮----
# Simulate: crypto fails, others succeed
good_quotes = [
⋮----
@pytest.mark.asyncio
    async def test_fetch_all_providers_aggregates_results(self)
⋮----
"""_fetch_all_providers gathers from all four providers."""
crypto_quotes = [_make_quote(symbol="BTC", asset_class="crypto")]
metals_quotes = [_make_quote(symbol="XAU", asset_class="metal")]
oil_quotes = [_make_quote(symbol="BRENT", asset_class="oil")]
indices_quotes = [_make_quote(symbol="SPX", asset_class="index")]
⋮----
quotes = await _fetch_all_providers()
⋮----
@pytest.mark.asyncio
    async def test_fetch_all_providers_isolates_exception(self)
⋮----
"""If one provider raises, _fetch_all_providers skips it and returns others."""
good_quotes = [_make_quote(symbol="XAU", asset_class="metal")]
⋮----
# Should have gold, skipped crypto exception
⋮----
class TestGetSnapshotEvent
⋮----
def test_returns_asyncio_event(self)
⋮----
"""get_snapshot_event() returns an asyncio.Event instance."""
⋮----
event = get_snapshot_event()
</file>

<file path="tests/test_xbrl_processor.py">
"""
XBRL Processor Tests
====================
Comprehensive tests for ingestion/xbrl_processor.py targeting low-coverage
lines. Uses mock XML/XBRL data as strings; no external files or services needed.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ---------------------------------------------------------------------------
# Minimal XBRL XML templates
⋮----
MINIMAL_XBRL_XML = """\
⋮----
XBRL_XML_WITH_DIMENSIONS = """\
⋮----
XBRL_XML_BOOLEAN_AND_TEXT = """\
⋮----
XBRL_XML_DECIMALS_INF = """\
⋮----
XBRL_XML_NO_UNIT_NO_CONTEXT = """\
⋮----
XBRL_XML_FALLBACK_NSMAP = """\
⋮----
XBRL_XML_COMMA_NUMBER = """\
⋮----
XBRL_XML_UNIT_NO_COLON = """\
⋮----
XBRL_XML_CONTEXT_DATES_SLASH = """\
⋮----
MALFORMED_XML = "NOT VALID XML <<<"
⋮----
# Helper: write temp XML file
⋮----
def _write_temp_xml(content: str, suffix: str = ".xml") -> Path
⋮----
tmp = tempfile.NamedTemporaryFile(
⋮----
# ===========================================================================
# XBRLFact dataclass
⋮----
class TestXBRLFactDataclass
⋮----
"""Comprehensive XBRLFact tests including hash consistency."""
⋮----
def test_hash_includes_unit(self)
⋮----
f1 = XBRLFact(
f2 = XBRLFact(
⋮----
def test_hash_includes_period_start(self)
⋮----
def test_hash_includes_period_end(self)
⋮----
def test_hash_includes_period_instant(self)
⋮----
def test_hash_includes_dimension_member(self)
⋮----
def test_hash_includes_dimension_value(self)
⋮----
def test_hash_boolean_value(self)
⋮----
f1 = XBRLFact(ticker="2222.SR", concept="c", value_boolean=True)
f2 = XBRLFact(ticker="2222.SR", concept="c", value_boolean=False)
⋮----
def test_hash_text_value(self)
⋮----
f1 = XBRLFact(ticker="2222.SR", concept="c", value_text="Aramco")
f2 = XBRLFact(ticker="2222.SR", concept="c", value_text="Different")
⋮----
def test_to_insert_tuple_full_fields(self)
⋮----
fact = XBRLFact(
t = fact.to_insert_tuple()
⋮----
def test_content_hash_is_sha256(self)
⋮----
# Verify it's valid hex
⋮----
# process_filing dispatch
⋮----
class TestProcessFilingDispatch
⋮----
"""Tests for process_filing extension routing (lines 276-283)."""
⋮----
def test_dispatch_xml_extension(self, tmp_path)
⋮----
xml_file = tmp_path / "filing.xml"
⋮----
proc = XBRLProcessor(ticker="2222.SR")
facts = proc.process_filing(xml_file)
⋮----
def test_dispatch_xbrl_extension(self, tmp_path)
⋮----
xbrl_file = tmp_path / "filing.xbrl"
⋮----
facts = proc.process_filing(xbrl_file)
⋮----
def test_dispatch_unsupported_returns_error(self, tmp_path)
⋮----
txt_file = tmp_path / "filing.txt"
⋮----
facts = proc.process_filing(txt_file)
⋮----
def test_dispatch_csv_extension_unsupported(self, tmp_path)
⋮----
csv_file = tmp_path / "data.csv"
⋮----
facts = proc.process_filing(csv_file)
⋮----
# process_directory
⋮----
class TestProcessDirectory
⋮----
"""Tests for process_directory (lines 295-317)."""
⋮----
def test_nonexistent_directory(self)
⋮----
facts = proc.process_directory(Path("/does/not/exist"))
⋮----
def test_empty_directory_returns_empty(self, tmp_path)
⋮----
facts = proc.process_directory(tmp_path)
⋮----
def test_processes_xml_files(self, tmp_path)
⋮----
def test_skips_non_supported_extensions(self, tmp_path)
⋮----
# Only the xml file should be processed
⋮----
def test_glob_pattern_filter(self, tmp_path)
⋮----
facts = proc.process_directory(tmp_path, pattern="annual_*.xml")
⋮----
def test_error_in_one_file_continues(self, tmp_path)
⋮----
# good.xml should still be processed
⋮----
def test_multiple_files_combined(self, tmp_path)
⋮----
# 3 files, each produces facts -- combined list
⋮----
# process_url
⋮----
class TestProcessUrl
⋮----
"""Tests for process_url (lines 332-359)."""
⋮----
def test_no_requests_library(self)
⋮----
facts = proc.process_url("http://example.com/filing.xml")
⋮----
def test_successful_download(self, tmp_path)
⋮----
mock_resp = MagicMock()
⋮----
facts = proc.process_url(
⋮----
def test_download_failure(self, tmp_path)
⋮----
def test_url_with_no_filename(self, tmp_path)
⋮----
# URL with no path filename component
facts = proc.process_url("http://example.com/", download_dir=tmp_path)
⋮----
def test_default_download_dir_is_used(self)
⋮----
# Should have been called with a path under _downloads
⋮----
# XML parsing (process_xml)
⋮----
class TestProcessXml
⋮----
"""Tests for process_xml (lines 377-485)."""
⋮----
def test_basic_xml_parsing(self, tmp_path)
⋮----
f = tmp_path / "filing.xml"
⋮----
facts = proc.process_xml(f)
⋮----
concepts = [fact.concept for fact in facts]
⋮----
def test_xml_facts_have_correct_ticker(self, tmp_path)
⋮----
def test_xml_period_start_end_parsed(self, tmp_path)
⋮----
# Revenue uses ctx_2024 which has startDate and endDate
revenue_facts = [fa for fa in facts if "Revenue" in fa.concept]
⋮----
rev = revenue_facts[0]
⋮----
def test_xml_instant_period_parsed(self, tmp_path)
⋮----
asset_facts = [fa for fa in facts if "Assets" in fa.concept]
⋮----
asset = asset_facts[0]
⋮----
def test_xml_unit_parsed(self, tmp_path)
⋮----
def test_xml_unit_without_colon(self, tmp_path)
⋮----
ratio_facts = [fa for fa in facts if "Ratio" in fa.concept]
⋮----
def test_xml_boolean_fact(self, tmp_path)
⋮----
bool_facts = [fa for fa in facts if fa.value_boolean is not None]
⋮----
# true -> True
true_facts = [fa for fa in bool_facts if fa.value_boolean is True]
false_facts = [fa for fa in bool_facts if fa.value_boolean is False]
⋮----
def test_xml_text_fact(self, tmp_path)
⋮----
text_facts = [fa for fa in facts if fa.value_text is not None]
⋮----
def test_xml_decimals_inf_ignored(self, tmp_path)
⋮----
# decimals=INF should result in None
⋮----
def test_xml_comma_number(self, tmp_path)
⋮----
def test_xml_malformed_raises_no_exception(self, tmp_path)
⋮----
f = tmp_path / "bad.xml"
⋮----
def test_xml_dimension_parsed(self, tmp_path)
⋮----
dim_facts = [fa for fa in facts if fa.dimension_member is not None]
⋮----
def test_xml_source_url_assigned(self, tmp_path)
⋮----
proc = XBRLProcessor(ticker="2222.SR", source_url="http://example.com/f.xml")
⋮----
def test_xml_filing_id_assigned(self, tmp_path)
⋮----
proc = XBRLProcessor(ticker="2222.SR", filing_id="filing-abc")
⋮----
def test_xml_decimals_parsed_as_int(self, tmp_path)
⋮----
numeric_facts = [fa for fa in facts if fa.value_numeric is not None]
⋮----
def test_xml_no_unit_uses_none(self, tmp_path)
⋮----
# The SomeValue element has no unitRef and no contextRef
no_unit_facts = [
⋮----
def test_xml_no_lxml_raises(self, tmp_path)
⋮----
def test_xml_context_dates_slash_format(self, tmp_path)
⋮----
revenue = facts[0]
⋮----
# _parse_contexts
⋮----
class TestParseContexts
⋮----
"""Tests for _parse_contexts method (lines 493-545)."""
⋮----
def _get_root(self, xml_str: str)
⋮----
def test_parse_contexts_duration(self)
⋮----
root = self._get_root(MINIMAL_XBRL_XML)
⋮----
contexts = proc._parse_contexts(root, {})
⋮----
ctx = contexts["ctx_2024"]
⋮----
def test_parse_contexts_instant(self)
⋮----
ctx = contexts["ctx_instant"]
⋮----
def test_parse_contexts_with_dimension(self)
⋮----
root = self._get_root(XBRL_XML_WITH_DIMENSIONS)
⋮----
ctx = contexts["ctx_segment"]
⋮----
def test_parse_contexts_skips_missing_id(self)
⋮----
xml = """\
root = etree.fromstring(xml.encode("utf-8"))
⋮----
# No id attribute means it should be skipped
⋮----
def test_parse_contexts_fallback_search(self)
⋮----
"""Tests the fallback branch when findall returns empty."""
⋮----
# A doc with non-standard namespace prefix wrapping xbrli context
⋮----
# _parse_units
⋮----
class TestParseUnits
⋮----
"""Tests for _parse_units method (lines 552-580)."""
⋮----
def test_parse_units_sar(self)
⋮----
units = proc._parse_units(root, {})
⋮----
def test_parse_units_shares(self)
⋮----
def test_parse_units_no_colon_in_measure(self)
⋮----
root = self._get_root(XBRL_XML_UNIT_NO_COLON)
⋮----
def test_parse_units_skips_missing_id(self)
⋮----
def test_parse_units_fallback_search(self)
⋮----
"""Tests fallback when findall returns empty."""
⋮----
# _build_concept_name
⋮----
class TestBuildConceptName
⋮----
"""Tests for _build_concept_name (lines 582-609)."""
⋮----
def test_uses_nsmap_prefix(self)
⋮----
nsmap = {"ifrs-full": "http://xbrl.ifrs.org/taxonomy/2023-03-23/ifrs-full"}
result = proc._build_concept_name(
⋮----
def test_falls_back_to_known_ifrs_prefix(self)
⋮----
def test_falls_back_to_last_path_segment(self)
⋮----
def test_no_namespace_uses_tasi_prefix(self)
⋮----
result = proc._build_concept_name("", "SomeLocal", {})
⋮----
def test_nsmap_none_prefix_skipped(self)
⋮----
"""None prefix in nsmap should not be used."""
⋮----
# nsmap has None -> 'http://xbrl.ifrs.org/taxonomy/custom'
nsmap = {None: "http://xbrl.ifrs.org/taxonomy/custom"}
⋮----
# Falls through to known prefix check
⋮----
def test_namespace_with_trailing_slash(self)
⋮----
# _safe_parse_date
⋮----
class TestSafeParseDateExtended
⋮----
"""Additional date parsing edge cases."""
⋮----
def test_year_month_day_slash(self)
⋮----
d = XBRLProcessor._safe_parse_date("2024/12/31")
⋮----
def test_with_leading_trailing_spaces(self)
⋮----
d = XBRLProcessor._safe_parse_date("  2024-12-31  ")
⋮----
def test_empty_string(self)
⋮----
d = XBRLProcessor._safe_parse_date("")
⋮----
def test_partial_date_invalid(self)
⋮----
d = XBRLProcessor._safe_parse_date("2024-99-99")
⋮----
# _parse_period_headers
⋮----
class TestParsePeriodHeaders
⋮----
"""Tests for _parse_period_headers (lines 755-765)."""
⋮----
def test_parses_iso_date_headers(self)
⋮----
headers = ("Label", "2024-12-31", "2023-12-31")
result = proc._parse_period_headers(headers)
⋮----
def test_skips_column_0(self)
⋮----
headers = ("2024-12-31", "2023-12-31")
⋮----
# Column 0 always skipped
⋮----
def test_skips_none_headers(self)
⋮----
headers = ("Label", None, "2024-12-31")
⋮----
def test_skips_unparseable_headers(self)
⋮----
headers = ("Label", "Not A Date", "2024-12-31")
⋮----
# _parse_date_string
⋮----
class TestParseDateStringExtended
⋮----
"""Additional _parse_date_string coverage (lines 792, 812, 824, 844-847)."""
⋮----
def test_slash_format_dm_y(self)
⋮----
result = proc._parse_date_string("31/12/2024")
⋮----
def test_fy_without_space(self)
⋮----
result = proc._parse_date_string("FY2024")
⋮----
def test_q1(self)
⋮----
result = proc._parse_date_string("Q1 2024")
⋮----
def test_q2(self)
⋮----
result = proc._parse_date_string("Q2 2024")
⋮----
def test_q3(self)
⋮----
result = proc._parse_date_string("Q3 2024")
⋮----
def test_q4(self)
⋮----
result = proc._parse_date_string("Q4 2024")
⋮----
def test_year_only_4_digits(self)
⋮----
result = proc._parse_date_string("2023")
⋮----
def test_5_digit_year_not_parsed(self)
⋮----
result = proc._parse_date_string("20244")  # 5 digits
⋮----
def test_fy_too_short_not_parsed(self)
⋮----
result = proc._parse_date_string("FY24")
⋮----
# _label_to_concept
⋮----
class TestLabelToConceptExtended
⋮----
"""Additional _label_to_concept coverage (lines 829-849)."""
⋮----
def test_income_sheet_prefix(self)
⋮----
concept = proc._label_to_concept("some metric", "Income Statement")
⋮----
def test_profit_loss_sheet_prefix(self)
⋮----
concept = proc._label_to_concept("some metric", "Statement of Profit or Loss")
⋮----
def test_cash_flow_sheet_prefix(self)
⋮----
concept = proc._label_to_concept("some metric", "Cash Flow Statement")
⋮----
def test_unknown_sheet_uses_tasi_prefix(self)
⋮----
concept = proc._label_to_concept("my custom metric", "Notes")
⋮----
def test_pascal_case_conversion(self)
⋮----
concept = proc._label_to_concept("total operating expenses", "Notes")
⋮----
def test_hyphen_treated_as_space(self)
⋮----
# "non-current assets" is a known IFRS concept
concept = proc._label_to_concept("non-current assets", "Balance Sheet")
assert ":" in concept  # Should be prefixed concept
⋮----
def test_all_known_ifrs_concepts(self)
⋮----
# _process_sheet (Excel)
⋮----
class TestProcessSheet
⋮----
"""Tests for _process_sheet (lines 671-753)."""
⋮----
def _make_mock_ws(self, rows)
⋮----
ws = MagicMock()
⋮----
def test_empty_sheet_returns_empty(self)
⋮----
ws = self._make_mock_ws([()])
result = proc._process_sheet(ws, "Balance Sheet")
⋮----
def test_only_header_row_returns_empty(self)
⋮----
ws = self._make_mock_ws([("Label", "2024-12-31")])
⋮----
def test_skips_row_with_empty_label(self)
⋮----
rows = [
ws = self._make_mock_ws(rows)
result = proc._process_sheet(ws, "Income Statement")
# Only Revenue row should have facts
⋮----
def test_numeric_value_creates_fact(self)
⋮----
def test_integer_value_creates_fact(self)
⋮----
def test_boolean_cell_creates_fact(self)
⋮----
result = proc._process_sheet(ws, "Notes")
⋮----
assert result[0].unit is None  # No unit for booleans
⋮----
def test_string_numeric_value(self)
⋮----
def test_text_value_creates_fact(self)
⋮----
def test_arabic_label_sets_label_ar(self)
⋮----
def test_english_label_sets_label_en(self)
⋮----
def test_missing_period_skips_cell(self)
⋮----
"""Cells without header period dates should be skipped with an error."""
⋮----
# Header col 1 is unparseable -> no period for col 1
⋮----
def test_none_cell_value_skipped(self)
⋮----
# None cell skipped; only 2023 value
⋮----
def test_fiscal_year_header_period(self)
⋮----
def test_instant_period_from_header(self)
⋮----
"""_parse_date_string returns period_instant=None for ISO dates; period_end is set."""
⋮----
# process_workbook
⋮----
class TestProcessWorkbook
⋮----
"""Tests for process_workbook (lines 628-660)."""
⋮----
def test_no_openpyxl_raises(self, tmp_path)
⋮----
f = tmp_path / "filing.xlsx"
⋮----
def test_nonexistent_file(self, tmp_path)
⋮----
facts = proc.process_workbook(tmp_path / "missing.xlsx")
⋮----
def test_corrupt_workbook(self, tmp_path)
⋮----
f = tmp_path / "corrupt.xlsx"
⋮----
facts = proc.process_workbook(f)
⋮----
def test_workbook_sheet_error_continues(self)
⋮----
mock_wb = MagicMock()
⋮----
mock_ws1 = MagicMock()
⋮----
mock_ws2 = MagicMock()
⋮----
# Database functions
⋮----
class TestDatabaseFunctions
⋮----
"""Tests for insert_facts, create_filing, check_filing_exists, mark_* (lines 870-948)."""
⋮----
def test_insert_facts_dry_run(self)
⋮----
facts = [
count = insert_facts(None, facts, dry_run=True)
⋮----
def test_insert_facts_empty(self)
⋮----
count = insert_facts(None, [], dry_run=False)
⋮----
def test_insert_facts_batches(self)
⋮----
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
count = insert_facts(mock_conn, facts, dry_run=False)
⋮----
def test_create_filing_dry_run(self)
⋮----
result = create_filing(
⋮----
def test_create_filing_real(self)
⋮----
def test_check_filing_exists_true(self)
⋮----
result = check_filing_exists(mock_conn, "2222.SR", "http://example.com")
⋮----
def test_check_filing_exists_false(self)
⋮----
def test_mark_filing_complete_dry_run(self)
⋮----
def test_mark_filing_complete_real(self)
⋮----
def test_mark_filing_failed_dry_run(self)
⋮----
def test_mark_filing_failed_real(self)
⋮----
# process_single_file
⋮----
class TestProcessSingleFile
⋮----
"""Tests for process_single_file (lines 1003-1057)."""
⋮----
def test_skips_already_processed_filing(self, tmp_path)
⋮----
def test_dry_run_does_not_check_filing(self, tmp_path)
⋮----
def test_successful_processing(self, tmp_path)
⋮----
# Should have facts and called mark_complete
⋮----
def test_empty_facts_marks_failed(self, tmp_path)
⋮----
# Full pipeline integration scenarios
⋮----
class TestFullPipeline
⋮----
"""End-to-end pipeline tests for coverage of lines 1012-1057, 1061-1186."""
⋮----
def test_xml_pipeline_produces_facts(self, tmp_path)
⋮----
f = tmp_path / "aramco_2024.xml"
⋮----
proc = XBRLProcessor(ticker="2222.SR", filing_id="f-1")
facts = proc.process_filing(f)
⋮----
def test_xml_with_dimensions_pipeline(self, tmp_path)
⋮----
f = tmp_path / "segment_filing.xml"
⋮----
dim_facts = [fa for fa in facts if fa.dimension_member]
⋮----
def test_process_directory_multiple_xml(self, tmp_path)
⋮----
all_facts = proc.process_directory(tmp_path)
assert len(all_facts) >= 6  # 3+ per file
⋮----
def test_all_fact_hashes_unique_across_files(self, tmp_path)
⋮----
"""Different facts from the same file should have unique hashes only if they differ."""
⋮----
hashes = [fa.content_hash for fa in facts]
# All hashes should be strings of length 64
⋮----
def test_process_filing_xbrl_extension_dispatches_to_xml(self, tmp_path)
⋮----
f = tmp_path / "filing.xbrl"
⋮----
# Same content as XML, should produce facts
⋮----
def test_default_unit_used_when_no_unit_ref(self, tmp_path)
⋮----
"""When a fact has no unitRef, unit should be None (not default_unit)."""
⋮----
proc = XBRLProcessor(ticker="2222.SR", default_unit="USD")
⋮----
# Facts without unitRef should have unit=None
⋮----
pass  # other facts may have units
no_unit_numeric = [
⋮----
def test_errors_accumulated_across_directory(self, tmp_path)
⋮----
# Errors from bad.xml
⋮----
def test_filing_id_propagates_through_pipeline(self, tmp_path)
⋮----
proc = XBRLProcessor(ticker="2222.SR", filing_id="abc-123")
⋮----
def test_source_url_propagates_through_pipeline(self, tmp_path)
⋮----
proc = XBRLProcessor(ticker="2222.SR", source_url="https://tadawul.com/f.xml")
</file>

<file path="vanna-skill/references/architecture.md">
# Vanna Architecture

## System Overview

```
User Question -> Web Component -> Server -> Agent -> Tools -> Database
                      |              |
                   Cookies      User Resolver
                   /JWTs           |
                               LLM Service
```

## Core Components

### 1. Agent

The central orchestrator that:
- Receives user questions
- Routes to appropriate tools
- Manages conversation flow
- Streams responses

```python
from vanna import Agent, AgentConfig

agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=user_resolver,
    config=AgentConfig(stream_responses=True)
)
```

### 2. UserResolver

Extracts user identity from requests:

```python
from vanna.core.user import UserResolver, User, RequestContext

class MyUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        token = request_context.get_header('Authorization')
        user_data = self.decode_jwt(token)
        
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data['groups'],
            metadata={'role': user_data['role']}
        )
```

### 3. ToolRegistry

Manages available tools:

```python
from vanna.core.registry import ToolRegistry
from vanna.tools import RunSqlTool

tools = ToolRegistry()
tools.register_local_tool(
    RunSqlTool(sql_runner=postgres_runner),
    access_groups=['user', 'admin']
)
```

### 4. LlmService

Handles LLM interactions:

```python
from vanna.integrations.anthropic import AnthropicLlmService

llm = AnthropicLlmService(
    model="claude-sonnet-4-5",
    api_key="sk-ant-..."
)
```

### 5. SqlRunner

Executes SQL against databases:

```python
from vanna.integrations.postgres import PostgresRunner

runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password"
)
```

## Request Flow

```mermaid
sequenceDiagram
    participant U as User
    participant W as <vanna-chat>
    participant S as Server
    participant A as Agent
    participant T as Tools

    U->>W: "Show Q4 sales"
    W->>S: POST /api/vanna/v2/chat_sse (with auth)
    S->>A: User(id=alice, groups=[read_sales])
    A->>T: Execute SQL tool (user-aware)
    T->>T: Apply row-level security
    T->>A: Filtered results
    A->>W: Stream: Table -> Chart -> Summary
    W->>U: Display UI
```

## User-Aware Architecture

Every component knows the user identity:

1. **UserResolver** extracts identity from request
2. **Agent** passes User to all tools
3. **Tools** check permissions via `access_groups`
4. **SqlRunner** applies row-level security
5. **AuditLogger** tracks per-user activity

### Group-Based Access Control

```python
class MyTool(Tool):
    @property
    def access_groups(self) -> list[str]:
        return ["admin", "analyst"]  # Only these groups can use

# User with group_memberships=["user"] cannot access
# User with group_memberships=["admin"] can access
```

## Streaming Architecture

Responses stream via Server-Sent Events:

```python
# Server sends structured components
{
    "type": "progress",
    "data": {"message": "Generating SQL..."}
}
{
    "type": "code",
    "data": {"language": "sql", "content": "SELECT..."}
}
{
    "type": "table",
    "data": {"columns": [...], "rows": [...]}
}
{
    "type": "chart",
    "data": {"plotly_json": {...}}
}
{
    "type": "text",
    "data": {"content": "The top customers are..."}
}
```

## Configuration

### AgentConfig

```python
from vanna import AgentConfig

config = AgentConfig(
    stream_responses=True,
    max_iterations=10,
    temperature=0.7,
    pre_execution_hooks=[quota_hook],
    post_execution_hooks=[audit_hook]
)
```

### Server Configuration

```python
from vanna.servers.fastapi import VannaFastAPIServer

server = VannaFastAPIServer(
    agent,
    enable_cors=True,
    allowed_origins=["https://myapp.com"]
)
```

## Extension Points

1. **Custom Tools** - Extend `Tool` base class
2. **Lifecycle Hooks** - Pre/post execution hooks
3. **LLM Middlewares** - Caching, prompt engineering
4. **Context Enrichers** - RAG, memory, documentation
5. **Conversation Storage** - Persist history
6. **Audit Loggers** - Track activity
</file>

<file path="vanna-skill/references/database_integrations.md">
# Database Integrations

Vanna supports all major databases through SqlRunner implementations.

## PostgreSQL

```python
from vanna.integrations.postgres import PostgresRunner

runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password",
    port=5432
)

tools.register(RunSqlTool(sql_runner=runner))
```

### With Connection Pool

```python
runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password",
    port=5432,
    pool_size=10,
    max_overflow=20
)
```

### With SSL

```python
runner = PostgresRunner(
    host="db.example.com",
    dbname="mydb",
    user="user",
    password="password",
    ssl_mode="require",
    ssl_root_cert="/path/to/ca.pem"
)
```

## MySQL

```python
from vanna.integrations.mysql import MySQLRunner

runner = MySQLRunner(
    host="localhost",
    database="mydb",
    user="user",
    password="password",
    port=3306
)
```

### With SSL

```python
runner = MySQLRunner(
    host="db.example.com",
    database="mydb",
    user="user",
    password="password",
    ssl_ca="/path/to/ca.pem"
)
```

## SQLite

```python
from vanna.integrations.sqlite import SqliteRunner

# File-based
runner = SqliteRunner("./database.db")

# In-memory
runner = SqliteRunner(":memory:")
```

## Snowflake

```python
from vanna.integrations.snowflake import SnowflakeRunner

runner = SnowflakeRunner(
    account="xxx.snowflakecomputing.com",
    user="user",
    password="password",
    database="DB",
    schema="PUBLIC",
    warehouse="COMPUTE_WH",
    role="ANALYST"
)
```

### With Key-Pair Authentication

```python
runner = SnowflakeRunner(
    account="xxx.snowflakecomputing.com",
    user="user",
    private_key_path="/path/to/key.pem",
    private_key_passphrase="passphrase",
    database="DB",
    schema="PUBLIC",
    warehouse="COMPUTE_WH"
)
```

## BigQuery

```python
from vanna.integrations.bigquery import BigQueryRunner

# With service account
runner = BigQueryRunner(
    project="my-project",
    credentials_path="./service-account.json"
)

# With default credentials
runner = BigQueryRunner(
    project="my-project"
)
```

### With Dataset

```python
runner = BigQueryRunner(
    project="my-project",
    dataset="analytics",
    credentials_path="./credentials.json"
)
```

## DuckDB

```python
from vanna.integrations.duckdb import DuckDBRunner

# File-based
runner = DuckDBRunner("./analytics.duckdb")

# In-memory
runner = DuckDBRunner(":memory:")
```

### With Extensions

```python
runner = DuckDBRunner(
    path="./data.duckdb",
    extensions=["parquet", "json"]
)
```

## ClickHouse

```python
from vanna.integrations.clickhouse import ClickHouseRunner

runner = ClickHouseRunner(
    host="localhost",
    database="default",
    user="default",
    password="",
    port=9000
)
```

### With HTTP Interface

```python
runner = ClickHouseRunner(
    host="localhost",
    database="default",
    user="default",
    password="",
    port=8123,
    use_http=True
)
```

## Oracle

```python
from vanna.integrations.oracle import OracleRunner

runner = OracleRunner(
    host="localhost",
    service_name="ORCL",
    user="user",
    password="password",
    port=1521
)
```

## SQL Server

```python
from vanna.integrations.sqlserver import SQLServerRunner

runner = SQLServerRunner(
    host="localhost",
    database="mydb",
    user="sa",
    password="password",
    port=1433
)
```

### With Windows Authentication

```python
runner = SQLServerRunner(
    host="localhost",
    database="mydb",
    trusted_connection=True
)
```

## Redshift

```python
from vanna.integrations.redshift import RedshiftRunner

runner = RedshiftRunner(
    host="cluster.xxx.redshift.amazonaws.com",
    database="dev",
    user="user",
    password="password",
    port=5439
)
```

## Custom SqlRunner

Implement your own for unsupported databases:

```python
from vanna.core.sql_runner import SqlRunner
from typing import Any
import pandas as pd

class MyCustomRunner(SqlRunner):
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    async def execute(self, sql: str, user: User) -> pd.DataFrame:
        # Your implementation
        connection = self.get_connection()
        result = connection.execute(sql)
        return pd.DataFrame(result)
    
    async def get_schema(self) -> dict[str, Any]:
        # Return schema information
        return {"tables": [...], "columns": [...]}
```
</file>

<file path="vanna-skill/references/enterprise.md">
# Enterprise Features

Vanna 2.0+ includes production-ready enterprise features.

## User-Aware Security

### UserResolver

Extract user identity from requests:

```python
from vanna.core.user import UserResolver, User, RequestContext

class MyUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        # Extract from JWT
        token = request_context.get_header('Authorization')
        user_data = self.decode_jwt(token)
        
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data['groups'],
            metadata={'tenant_id': user_data['tenant_id']}
        )
```

### Group-Based Permissions

```python
class SensitiveTool(Tool):
    @property
    def access_groups(self) -> list[str]:
        return ["admin", "data_analyst"]  # Only these groups

# User with group_memberships=["user"] - DENIED
# User with group_memberships=["admin"] - ALLOWED
```

## Row-Level Security

Filter SQL queries per user:

```python
class SecureSqlRunner(PostgresRunner):
    async def execute(self, sql: str, user: User) -> DataFrame:
        # Get user's tenant from metadata
        tenant_id = user.metadata.get('tenant_id')
        
        # Inject filter into queries
        filtered_sql = self.apply_tenant_filter(sql, tenant_id)
        
        return await super().execute(filtered_sql, user)
    
    def apply_tenant_filter(self, sql: str, tenant_id: str) -> str:
        # Add WHERE tenant_id = '...' to all tables
        # Use SQL parser for robust implementation
        return modified_sql
```

## Audit Logging

Track all queries per user:

```python
from vanna.core.audit import AuditLogger
from datetime import datetime

class MyAuditLogger(AuditLogger):
    async def log_query(
        self,
        user_id: str,
        query: str,
        sql: str,
        result: any,
        execution_time: float
    ):
        await self.db.insert("audit_log", {
            "user_id": user_id,
            "query": query,
            "sql": sql,
            "row_count": len(result) if result else 0,
            "execution_time_ms": execution_time * 1000,
            "timestamp": datetime.utcnow()
        })

agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=user_resolver,
    audit_logger=MyAuditLogger()
)
```

## Rate Limiting

Implement per-user quotas:

```python
from vanna import AgentConfig

async def rate_limit_hook(context):
    user = context.user
    
    # Check requests in last hour
    requests = await get_request_count(user.id, period="1h")
    
    if requests > 100:
        raise RateLimitError("Rate limit exceeded: 100 requests/hour")
    
    # Check daily limit
    daily = await get_request_count(user.id, period="24h")
    
    if daily > 1000:
        raise RateLimitError("Daily limit exceeded: 1000 requests/day")

config = AgentConfig(
    pre_execution_hooks=[rate_limit_hook]
)
```

## Lifecycle Hooks

Add logic at key points in request lifecycle:

### Pre-Execution Hooks

Run before agent processes request:

```python
async def quota_check(context):
    user = context.user
    if await is_quota_exceeded(user.id):
        raise QuotaExceededError("Monthly quota exceeded")

async def content_filter(context):
    message = context.message
    if contains_pii(message):
        raise SecurityError("PII detected in query")

config = AgentConfig(
    pre_execution_hooks=[quota_check, content_filter]
)
```

### Post-Execution Hooks

Run after agent completes:

```python
async def log_usage(context, result):
    await record_usage(
        user_id=context.user.id,
        tokens=result.token_usage,
        cost=calculate_cost(result.token_usage)
    )

async def notify_admin(context, result):
    if result.contains_sensitive_data:
        await send_alert(
            f"Sensitive data accessed by {context.user.email}"
        )

config = AgentConfig(
    post_execution_hooks=[log_usage, notify_admin]
)
```

## Observability

### Built-in Tracing

```python
from vanna.core.tracing import TracingConfig

config = AgentConfig(
    tracing=TracingConfig(
        enabled=True,
        service_name="vanna-agent",
        exporter="otlp",
        endpoint="http://jaeger:4317"
    )
)
```

### Metrics

```python
from vanna.core.metrics import MetricsConfig

config = AgentConfig(
    metrics=MetricsConfig(
        enabled=True,
        port=9090,
        path="/metrics"
    )
)

# Exposes Prometheus metrics:
# - vanna_requests_total
# - vanna_request_duration_seconds
# - vanna_llm_tokens_total
# - vanna_sql_execution_seconds
```

### Logging

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Enable debug for Vanna
logging.getLogger('vanna').setLevel(logging.DEBUG)
```

## Conversation Storage

Persist conversation history:

```python
from vanna.core.storage import ConversationStorage

class PostgresConversationStorage(ConversationStorage):
    async def save(self, conversation_id: str, messages: list[dict]):
        await self.db.upsert("conversations", {
            "id": conversation_id,
            "messages": json.dumps(messages),
            "updated_at": datetime.utcnow()
        })
    
    async def load(self, conversation_id: str) -> list[dict]:
        row = await self.db.get("conversations", id=conversation_id)
        return json.loads(row['messages']) if row else []

agent = Agent(
    ...,
    conversation_storage=PostgresConversationStorage(connection)
)
```

## Multi-Tenant Architecture

### Tenant Isolation

```python
class TenantAwareUserResolver(UserResolver):
    async def resolve_user(self, ctx: RequestContext) -> User:
        token = ctx.get_header('Authorization')
        user = decode_jwt(token)
        
        return User(
            id=user['id'],
            email=user['email'],
            group_memberships=[f"tenant_{user['tenant_id']}"],
            metadata={'tenant_id': user['tenant_id']}
        )

class TenantAwareSqlRunner(PostgresRunner):
    async def execute(self, sql: str, user: User) -> DataFrame:
        tenant_id = user.metadata['tenant_id']
        
        # Connect to tenant-specific schema
        schema = f"tenant_{tenant_id}"
        self.set_search_path(schema)
        
        return await super().execute(sql, user)
```

### Per-Tenant Configuration

```python
class TenantConfigProvider:
    async def get_config(self, tenant_id: str) -> dict:
        return await self.db.get("tenant_configs", id=tenant_id)

# Use different LLMs per tenant
llm_service = TenantAwareLlmService(config_provider)
```

## Security Best Practices

1. **Always validate user identity** in UserResolver
2. **Use group-based permissions** on all sensitive tools
3. **Implement row-level security** for multi-tenant data
4. **Enable audit logging** for compliance
5. **Set rate limits** to prevent abuse
6. **Use HTTPS** for all API endpoints
7. **Sanitize SQL** before execution
8. **Never expose raw errors** to users
</file>

<file path="vanna-skill/references/getting_started.md">
# Getting Started with Vanna AI

## Overview

Vanna is a Python framework that converts natural language questions into accurate SQL queries using LLMs (Large Language Models) and Agentic Retrieval. Version 2.0 introduces a complete rewrite focused on user-aware agents and production deployments.

## Installation

### Basic Installation

```bash
pip install vanna
```

### With Specific Integrations

```bash
# Flask + Anthropic Claude
pip install 'vanna[flask,anthropic]'

# FastAPI + OpenAI
pip install 'vanna[fastapi,openai]'

# All integrations
pip install 'vanna[all]'
```

### Requirements

- Python 3.8+
- Database access (PostgreSQL, MySQL, SQLite, etc.)
- LLM API key (OpenAI, Anthropic, etc.) or local Ollama

## Quickstart (30 Seconds)

### 1. Minimal Setup with SQLite

```python
from vanna import Agent
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.sqlite import SqliteRunner
from vanna.tools import RunSqlTool
from vanna.core.registry import ToolRegistry

# Initialize LLM
llm = AnthropicLlmService(
    model="claude-haiku-4-5",
    api_key="your-api-key"
)

# Set up tools with database
tools = ToolRegistry()
tools.register(RunSqlTool(
    sql_runner=SqliteRunner("./chinook.db")
))

# Create agent
agent = Agent(llm_service=llm, tool_registry=tools)
```

### 2. Add Web Interface

```python
from vanna.servers.fastapi import VannaFastAPIServer

server = VannaFastAPIServer(agent)
server.run(host='0.0.0.0', port=8000)

# Visit http://localhost:8000 for web UI
```

### 3. Embed in Your Frontend

```html
<script src="https://img.vanna.ai/vanna-components.js"></script>
<vanna-chat
  sse-endpoint="http://localhost:8000/api/vanna/v2/chat_sse"
  theme="light">
</vanna-chat>
```

## Production Setup with Authentication

### Full FastAPI Integration

```python
from fastapi import FastAPI
from vanna import Agent
from vanna.servers.fastapi.routes import register_chat_routes
from vanna.servers.base import ChatHandler
from vanna.core.user import UserResolver, User, RequestContext
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.tools import RunSqlTool
from vanna.integrations.postgres import PostgresRunner
from vanna.core.registry import ToolRegistry

app = FastAPI()

# 1. Define user resolver
class MyUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        token = request_context.get_header('Authorization')
        user_data = self.decode_jwt(token)
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data['groups']
        )

# 2. Set up LLM
llm = AnthropicLlmService(model="claude-sonnet-4-5")

# 3. Set up tools
tools = ToolRegistry()
postgres_runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password",
    port=5432
)
tools.register(RunSqlTool(sql_runner=postgres_runner))

# 4. Create agent
agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=MyUserResolver()
)

# 5. Add routes
chat_handler = ChatHandler(agent)
register_chat_routes(app, chat_handler)

# Run: uvicorn main:app --host 0.0.0.0 --port 8000
```

## What You Get

When you ask a question, Vanna returns:

1. **Streaming Progress Updates** - Real-time status
2. **SQL Code Block** - Generated query (admin only by default)
3. **Interactive Data Table** - Results in table format
4. **Charts** - Plotly visualizations
5. **Natural Language Summary** - Human-readable explanation

All streamed in real-time via Server-Sent Events.

## Key Features

- **User-Aware**: Identity flows through every layer
- **Streaming**: Real-time responses via SSE
- **Row-Level Security**: Filter queries per user permissions
- **Audit Logging**: Track all queries per user
- **Multi-LLM Support**: OpenAI, Anthropic, Ollama, Azure, Gemini, Bedrock, Mistral
- **Multi-Database Support**: PostgreSQL, MySQL, SQLite, Snowflake, BigQuery, DuckDB, ClickHouse
- **Built-in Web UI**: `<vanna-chat>` component
- **Custom Tools**: Extend with your own functionality

## Next Steps

1. Read `architecture.md` for system design
2. See `database_integrations.md` for your database
3. See `llm_integrations.md` for your LLM provider
4. Check `migration.md` if upgrading from 0.x
</file>

<file path="vanna-skill/references/index.md">
# Vanna AI Reference Documentation

## Overview

Vanna is a Python framework for converting natural language to SQL using LLMs and Agentic Retrieval. Version 2.0+ introduces a complete rewrite focused on user-aware agents and production deployments.

## Reference Files

### Core Documentation

- **[getting_started.md](getting_started.md)** - Installation, quickstart, and first steps
- **[architecture.md](architecture.md)** - System design, components, and request flow

### Integrations

- **[database_integrations.md](database_integrations.md)** - PostgreSQL, MySQL, SQLite, Snowflake, BigQuery, DuckDB, ClickHouse, and more
- **[llm_integrations.md](llm_integrations.md)** - OpenAI, Anthropic, Azure, Ollama, Gemini, Bedrock, Mistral

### Development

- **[tools.md](tools.md)** - Creating custom tools, ToolContext, ToolResult
- **[web_ui.md](web_ui.md)** - `<vanna-chat>` component, styling, events

### Production

- **[enterprise.md](enterprise.md)** - Security, audit logging, rate limiting, observability
- **[migration.md](migration.md)** - Migrating from Vanna 0.x to 2.0+

## Quick Links

### Installation

```bash
pip install vanna
pip install 'vanna[flask,anthropic]'
```

### Minimal Example

```python
from vanna import Agent
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.sqlite import SqliteRunner
from vanna.tools import RunSqlTool
from vanna.core.registry import ToolRegistry

llm = AnthropicLlmService(model="claude-haiku-4-5")
tools = ToolRegistry()
tools.register(RunSqlTool(sql_runner=SqliteRunner("./db.sqlite")))

agent = Agent(llm_service=llm, tool_registry=tools)
```

### Web Component

```html
<script src="https://img.vanna.ai/vanna-components.js"></script>
<vanna-chat sse-endpoint="/api/vanna/v2/chat_sse"></vanna-chat>
```

## Key Concepts

1. **Agent** - Orchestrates LLM and tools
2. **UserResolver** - Extracts user identity
3. **ToolRegistry** - Manages available tools
4. **LlmService** - Handles LLM interactions
5. **SqlRunner** - Executes SQL queries

## Supported Databases

| Database | Runner Class |
|----------|--------------|
| PostgreSQL | `PostgresRunner` |
| MySQL | `MySQLRunner` |
| SQLite | `SqliteRunner` |
| Snowflake | `SnowflakeRunner` |
| BigQuery | `BigQueryRunner` |
| DuckDB | `DuckDBRunner` |
| ClickHouse | `ClickHouseRunner` |
| Oracle | `OracleRunner` |
| SQL Server | `SQLServerRunner` |
| Redshift | `RedshiftRunner` |

## Supported LLMs

| Provider | Service Class |
|----------|---------------|
| Anthropic | `AnthropicLlmService` |
| OpenAI | `OpenAILlmService` |
| Azure OpenAI | `AzureOpenAILlmService` |
| Ollama | `OllamaLlmService` |
| Google Gemini | `GeminiLlmService` |
| AWS Bedrock | `BedrockLlmService` |
| Mistral | `MistralLlmService` |

## External Resources

- **Documentation**: https://vanna.ai/docs
- **GitHub**: https://github.com/vanna-ai/vanna
- **Discussions**: https://github.com/vanna-ai/vanna/discussions
- **Issues**: https://github.com/vanna-ai/vanna/issues
- **Enterprise Support**: support@vanna.ai

## Version

- **Current Version**: 2.0+
- **Python**: 3.8+
- **License**: MIT
</file>

<file path="vanna-skill/references/llm_integrations.md">
# LLM Integrations

Vanna supports multiple LLM providers through LlmService implementations.

## Anthropic Claude

```python
from vanna.integrations.anthropic import AnthropicLlmService

llm = AnthropicLlmService(
    model="claude-sonnet-4-5",
    api_key="sk-ant-..."
)
```

### Available Models

- `claude-opus-4-5` - Most capable, best for complex queries
- `claude-sonnet-4-5` - Balanced performance and cost
- `claude-haiku-4-5` - Fastest, lowest cost

### With Configuration

```python
llm = AnthropicLlmService(
    model="claude-sonnet-4-5",
    api_key="sk-ant-...",
    max_tokens=4096,
    temperature=0.7
)
```

## OpenAI

```python
from vanna.integrations.openai import OpenAILlmService

llm = OpenAILlmService(
    model="gpt-4o",
    api_key="sk-..."
)
```

### Available Models

- `gpt-4o` - Multimodal, best quality
- `gpt-4-turbo` - Fast, high quality
- `gpt-3.5-turbo` - Fastest, lowest cost

### With Configuration

```python
llm = OpenAILlmService(
    model="gpt-4o",
    api_key="sk-...",
    max_tokens=4096,
    temperature=0.7,
    organization="org-..."
)
```

## Azure OpenAI

```python
from vanna.integrations.azure import AzureOpenAILlmService

llm = AzureOpenAILlmService(
    deployment_name="gpt-4",
    azure_endpoint="https://xxx.openai.azure.com/",
    api_key="...",
    api_version="2024-02-01"
)
```

### With Managed Identity

```python
from azure.identity import DefaultAzureCredential

llm = AzureOpenAILlmService(
    deployment_name="gpt-4",
    azure_endpoint="https://xxx.openai.azure.com/",
    credential=DefaultAzureCredential()
)
```

## Ollama (Local/Self-Hosted)

```python
from vanna.integrations.ollama import OllamaLlmService

llm = OllamaLlmService(
    model="llama3",
    base_url="http://localhost:11434"
)
```

### Available Models

- `llama3` - Meta Llama 3
- `mistral` - Mistral 7B
- `codellama` - Code-optimized Llama
- `mixtral` - Mixtral 8x7B

### With Configuration

```python
llm = OllamaLlmService(
    model="llama3:70b",
    base_url="http://localhost:11434",
    num_ctx=8192,
    temperature=0.7
)
```

## Google Gemini

```python
from vanna.integrations.gemini import GeminiLlmService

llm = GeminiLlmService(
    model="gemini-pro",
    api_key="..."
)
```

### Available Models

- `gemini-pro` - Text generation
- `gemini-pro-vision` - Multimodal

## AWS Bedrock

```python
from vanna.integrations.bedrock import BedrockLlmService

llm = BedrockLlmService(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    region_name="us-east-1"
)
```

### Available Models

- `anthropic.claude-3-sonnet-*` - Claude Sonnet
- `anthropic.claude-3-haiku-*` - Claude Haiku
- `amazon.titan-text-*` - Amazon Titan
- `meta.llama3-*` - Llama 3

### With Credentials

```python
llm = BedrockLlmService(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    region_name="us-east-1",
    aws_access_key_id="...",
    aws_secret_access_key="..."
)
```

## Mistral AI

```python
from vanna.integrations.mistral import MistralLlmService

llm = MistralLlmService(
    model="mistral-large-latest",
    api_key="..."
)
```

### Available Models

- `mistral-large-latest` - Most capable
- `mistral-medium` - Balanced
- `mistral-small` - Fast

## Custom LlmService

Implement your own for unsupported providers:

```python
from vanna.core.llm_service import LlmService
from typing import AsyncIterator

class MyLlmService(LlmService):
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
    
    async def generate(
        self,
        messages: list[dict],
        tools: list[dict] | None = None
    ) -> AsyncIterator[dict]:
        # Your implementation
        response = await self.client.chat(
            model=self.model,
            messages=messages,
            tools=tools
        )
        
        for chunk in response:
            yield {"type": "text", "content": chunk.text}
```

## LLM Middlewares

Add caching, logging, or prompt engineering:

```python
from vanna.core.middleware import LlmMiddleware

class CachingMiddleware(LlmMiddleware):
    async def process(self, messages, tools, next_fn):
        cache_key = self.compute_key(messages)
        
        if cached := await self.cache.get(cache_key):
            yield cached
            return
        
        result = []
        async for chunk in next_fn(messages, tools):
            result.append(chunk)
            yield chunk
        
        await self.cache.set(cache_key, result)

llm = AnthropicLlmService(...)
llm.add_middleware(CachingMiddleware())
```
</file>

<file path="vanna-skill/references/migration.md">
# Migration Guide: Vanna 0.x to 2.0+

Complete guide for migrating from legacy Vanna to the new agent framework.

## Overview of Changes

### What's New in 2.0+

- **User awareness** - Identity and permissions flow through every layer
- **Web component** - Pre-built UI with streaming responses
- **Tool registry** - Modular, extensible tool system
- **Rich UI components** - Tables, charts, status cards (not just text)
- **Streaming by default** - Progressive responses via SSE
- **Enterprise features** - Audit logs, rate limiting, observability
- **FastAPI/Flask servers** - Production-ready backends included

### What Changed from 0.x

| Feature | Vanna 0.x | Vanna 2.0+ |
|---------|-----------|------------|
| User Context | None | User flows through system |
| Interaction | `vn.ask()` | Agent-based with streaming |
| Tools | Monolithic methods | Modular Tool classes |
| Responses | Text/DataFrames | Rich UI components |
| Training | `vn.train()` | System prompts, RAG tools |
| Database | `vn.connect_to_*()` | SqlRunner implementations |
| Web UI | Custom | Built-in `<vanna-chat>` |
| Streaming | None | SSE by default |
| Permissions | None | Group-based access control |

## Migration Strategies

### Strategy 1: Legacy Adapter (Quick Migration)

Wrap your existing VannaBase instance:

```python
from vanna import Agent, AgentConfig
from vanna.servers.fastapi import VannaFastAPIServer
from vanna.core.user import UserResolver, User, RequestContext
from vanna.legacy.adapter import LegacyVannaAdapter
from vanna.integrations.anthropic import AnthropicLlmService

# Your existing 0.x setup (unchanged)
# vn = MyVanna(config={'model': 'gpt-4'})
# vn.connect_to_postgres(...)
# vn.train(ddl="...")

# NEW: Define user resolution
class SimpleUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        user_email = request_context.get_cookie('vanna_email')
        if not user_email:
            raise ValueError("Missing cookie")
        
        if user_email == "admin@example.com":
            return User(id="admin", email=user_email, group_memberships=['admin'])
        
        return User(id=user_email, email=user_email, group_memberships=['user'])

# NEW: Wrap with legacy adapter
tools = LegacyVannaAdapter(vn)

# NEW: Set up LLM for Agent
llm = AnthropicLlmService(
    model="claude-haiku-4-5",
    api_key="YOUR_ANTHROPIC_API_KEY"
)

# NEW: Create agent
agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=SimpleUserResolver(),
    config=AgentConfig()
)

# NEW: Run server
server = VannaFastAPIServer(agent)
server.run(host='0.0.0.0', port=8000)
```

**What LegacyVannaAdapter does:**
- Wraps `vn.run_sql()` as `run_sql` tool
- Exposes training data as searchable memory
- Maintains existing database connection

**Pros:**
- Minimal code changes
- Preserve existing training data
- Get new features immediately

**Cons:**
- Limited user awareness
- Can't leverage row-level security

### Strategy 2: Full Migration (Recommended)

#### Before (0.x)

```python
from vanna import VannaBase
from vanna.openai_chat import OpenAI_Chat
from vanna.chromadb import ChromaDB_VectorStore

class MyVanna(ChromaDB_VectorStore, OpenAI_Chat):
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)

vn = MyVanna(config={'model': 'gpt-4', 'api_key': 'key'})
vn.connect_to_postgres(...)
vn.train(ddl="CREATE TABLE customers ...")
vn.train(question="Top customers?", sql="SELECT ...")

sql = vn.generate_sql("Who are the top customers?")
df = vn.run_sql(sql)
```

#### After (2.0+)

```python
from vanna import Agent, AgentConfig
from vanna.servers.fastapi import VannaFastAPIServer
from vanna.core.registry import ToolRegistry
from vanna.core.user import UserResolver, User, RequestContext
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.tools import RunSqlTool
from vanna.integrations.postgres import PostgresRunner

# 1. Define user resolution
class MyUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        token = request_context.get_header('Authorization')
        user_data = await self.validate_token(token)
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data['groups']
        )

# 2. Set up tools
tools = ToolRegistry()
postgres_runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password"
)
tools.register_local_tool(
    RunSqlTool(sql_runner=postgres_runner),
    access_groups=['user', 'admin']
)

# 3. Create agent
llm = AnthropicLlmService(model="claude-sonnet-4-5")
agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=MyUserResolver(),
    config=AgentConfig(stream_responses=True)
)

# 4. Create server
server = VannaFastAPIServer(agent)
app = server.create_app()
```

## API Mapping

| 0.x Method | 2.0+ Equivalent |
|------------|-----------------|
| `vn.connect_to_postgres()` | `PostgresRunner(...)` |
| `vn.run_sql(sql)` | `RunSqlTool.execute()` |
| `vn.train(ddl=...)` | System prompts / context enrichers |
| `vn.train(question=..., sql=...)` | Memory tools / RAG |
| `vn.generate_sql(question)` | Agent handles automatically |
| `vn.ask(question)` | Agent with streaming components |

## Migration Checklist

- [ ] Install Vanna 2.0+: `pip install 'vanna[flask,anthropic]'`
- [ ] Choose migration strategy (Legacy Adapter or Full)
- [ ] Implement UserResolver for authentication
- [ ] Set up LlmService (OpenAI, Anthropic, etc.)
- [ ] Configure SqlRunner for your database
- [ ] Register tools in ToolRegistry
- [ ] Create Agent with configuration
- [ ] Set up server (FastAPI/Flask)
- [ ] Update frontend to use `<vanna-chat>`
- [ ] Test with sample queries
- [ ] Migrate training data if using Full Migration

## Recommended Path

1. **Start with Legacy Adapter** for quick migration
2. **Test thoroughly** with existing functionality
3. **Gradually migrate** critical paths to native 2.0+
4. **Remove Legacy Adapter** once fully migrated
</file>

<file path="vanna-skill/references/other.md">
# Vanna - Other

**Pages:** 1

---

## Documentation - Vanna AI

**URL:** https://vanna.ai/docs

---
</file>

<file path="vanna-skill/references/tools.md">
# Custom Tools

Extend Vanna with domain-specific tools by implementing the `Tool` base class.

## Tool Basics

### Simple Tool

```python
from vanna.core.tool import Tool, ToolContext, ToolResult
from pydantic import BaseModel, Field
from typing import Type

class GreetArgs(BaseModel):
    name: str = Field(description="Name to greet")

class GreetTool(Tool[GreetArgs]):
    @property
    def name(self) -> str:
        return "greet"
    
    @property
    def description(self) -> str:
        return "Greet a user by name"
    
    @property
    def access_groups(self) -> list[str]:
        return ["user", "admin"]
    
    def get_args_schema(self) -> Type[GreetArgs]:
        return GreetArgs
    
    async def execute(self, context: ToolContext, args: GreetArgs) -> ToolResult:
        return ToolResult(
            success=True,
            result_for_llm=f"Hello, {args.name}!"
        )

# Register
tools.register(GreetTool())
```

## Tool with Dependencies

```python
class EmailArgs(BaseModel):
    recipient: str = Field(description="Email recipient")
    subject: str = Field(description="Email subject")
    body: str = Field(description="Email body")

class EmailTool(Tool[EmailArgs]):
    def __init__(self, email_service: EmailService):
        self.email_service = email_service
    
    @property
    def name(self) -> str:
        return "send_email"
    
    @property
    def access_groups(self) -> list[str]:
        return ["send_email"]  # Only users with this group
    
    def get_args_schema(self) -> Type[EmailArgs]:
        return EmailArgs
    
    async def execute(self, context: ToolContext, args: EmailArgs) -> ToolResult:
        user = context.user  # Automatically injected
        
        await self.email_service.send(
            from_email=user.email,
            to=args.recipient,
            subject=args.subject,
            body=args.body
        )
        
        return ToolResult(
            success=True,
            result_for_llm=f"Email sent to {args.recipient}"
        )

# Register with dependency
email_service = EmailService(smtp_host="...")
tools.register(EmailTool(email_service))
```

## Tool with UI Components

Return rich UI components instead of text:

```python
from vanna.core.components import Table, Chart, CodeBlock

class AnalyzeArgs(BaseModel):
    query: str = Field(description="Analysis query")

class AnalyzeTool(Tool[AnalyzeArgs]):
    @property
    def name(self) -> str:
        return "analyze_data"
    
    async def execute(self, context: ToolContext, args: AnalyzeArgs) -> ToolResult:
        data = await self.get_data(args.query)
        
        return ToolResult(
            success=True,
            result_for_llm="Analysis complete",
            components=[
                Table(
                    columns=["Name", "Value"],
                    rows=data
                ),
                Chart(
                    type="bar",
                    data=data,
                    title="Analysis Results"
                )
            ]
        )
```

## ToolContext

Access request context in tools:

```python
async def execute(self, context: ToolContext, args: Args) -> ToolResult:
    # User information
    user = context.user
    user.id          # User ID
    user.email       # User email
    user.group_memberships  # Groups for permissions
    user.metadata    # Custom metadata
    
    # Request context
    request = context.request_context
    request.get_header("Authorization")
    request.get_cookie("session")
    
    # Conversation context
    context.conversation_id
    context.message_history
    
    return ToolResult(...)
```

## ToolResult

Structure tool responses:

```python
return ToolResult(
    success=True,                    # Was execution successful?
    result_for_llm="Analysis done",  # Text for LLM to use
    components=[                     # Rich UI components
        Table(...),
        Chart(...),
        CodeBlock(...)
    ],
    metadata={                       # Additional data
        "rows_affected": 100,
        "execution_time": 1.5
    }
)
```

## Built-in Tools

### RunSqlTool

Execute SQL queries:

```python
from vanna.tools import RunSqlTool

tools.register(RunSqlTool(
    sql_runner=postgres_runner,
    access_groups=["user", "admin"]
))
```

### SearchDocsTool

Search documentation via RAG:

```python
from vanna.tools import SearchDocsTool

tools.register(SearchDocsTool(
    vector_store=my_vector_store,
    access_groups=["user", "admin"]
))
```

### SearchMemoryTool

Search saved successful queries:

```python
from vanna.tools import SearchMemoryTool

tools.register(SearchMemoryTool(
    memory_store=my_memory_store,
    access_groups=["user", "admin"]
))
```

## Tool Registration

### Basic Registration

```python
tools = ToolRegistry()
tools.register(MyTool())
```

### With Access Groups Override

```python
tools.register_local_tool(
    MyTool(),
    access_groups=["admin"]  # Override tool's default
)
```

### List Registered Tools

```python
for tool in tools.list_tools():
    print(f"{tool.name}: {tool.description}")
```

## Error Handling

```python
async def execute(self, context: ToolContext, args: Args) -> ToolResult:
    try:
        result = await self.do_work(args)
        return ToolResult(success=True, result_for_llm=result)
    except PermissionError:
        return ToolResult(
            success=False,
            result_for_llm="Permission denied"
        )
    except Exception as e:
        return ToolResult(
            success=False,
            result_for_llm=f"Error: {str(e)}"
        )
```

## Async Tools

All tools are async by default:

```python
async def execute(self, context: ToolContext, args: Args) -> ToolResult:
    # Use async operations
    data = await self.async_fetch_data()
    await self.async_process(data)
    return ToolResult(success=True, result_for_llm="Done")
```
</file>

<file path="vanna-skill/references/web_ui.md">
# Web UI Component

Vanna includes a pre-built web component for chat interfaces.

## Basic Usage

```html
<!-- Include the component -->
<script src="https://img.vanna.ai/vanna-components.js"></script>

<!-- Use it -->
<vanna-chat
  sse-endpoint="https://your-api.com/api/vanna/v2/chat_sse"
  theme="light">
</vanna-chat>
```

## Configuration

### Endpoint

```html
<vanna-chat
  sse-endpoint="/api/vanna/v2/chat_sse">
</vanna-chat>
```

### Theme

```html
<!-- Light theme -->
<vanna-chat theme="light"></vanna-chat>

<!-- Dark theme -->
<vanna-chat theme="dark"></vanna-chat>

<!-- Auto (follows system) -->
<vanna-chat theme="auto"></vanna-chat>
```

### Custom Styling

```html
<vanna-chat
  style="--vanna-primary: #007bff; --vanna-bg: #f8f9fa;">
</vanna-chat>
```

### CSS Variables

```css
vanna-chat {
  --vanna-primary: #007bff;
  --vanna-primary-hover: #0056b3;
  --vanna-bg: #ffffff;
  --vanna-text: #333333;
  --vanna-border: #dee2e6;
  --vanna-code-bg: #f4f4f4;
  --vanna-font-family: system-ui, sans-serif;
  --vanna-border-radius: 8px;
}
```

## Authentication

The component uses your existing cookies/JWTs:

```html
<!-- Cookies are sent automatically -->
<vanna-chat
  sse-endpoint="/api/vanna/v2/chat_sse">
</vanna-chat>
```

### With Custom Headers

```html
<vanna-chat
  sse-endpoint="/api/vanna/v2/chat_sse"
  auth-header="Bearer your-token-here">
</vanna-chat>
```

### Dynamic Token

```javascript
const chat = document.querySelector('vanna-chat');
chat.setAuthHeader(`Bearer ${getToken()}`);
```

## Framework Integration

### React

```jsx
import { useEffect, useRef } from 'react';

function VannaChat() {
  const chatRef = useRef(null);
  
  useEffect(() => {
    // Import component
    import('https://img.vanna.ai/vanna-components.js');
  }, []);
  
  return (
    <vanna-chat
      ref={chatRef}
      sse-endpoint="/api/vanna/v2/chat_sse"
      theme="light"
    />
  );
}
```

### Vue

```vue
<template>
  <vanna-chat
    :sse-endpoint="endpoint"
    theme="dark"
  />
</template>

<script setup>
import { onMounted } from 'vue';

const endpoint = '/api/vanna/v2/chat_sse';

onMounted(() => {
  import('https://img.vanna.ai/vanna-components.js');
});
</script>
```

### Angular

```typescript
// component.ts
import { Component, OnInit } from '@angular/core';

@Component({
  selector: 'app-chat',
  template: `
    <vanna-chat
      sse-endpoint="/api/vanna/v2/chat_sse"
      theme="light">
    </vanna-chat>
  `
})
export class ChatComponent implements OnInit {
  ngOnInit() {
    import('https://img.vanna.ai/vanna-components.js');
  }
}
```

## Events

### Listen for Events

```javascript
const chat = document.querySelector('vanna-chat');

chat.addEventListener('vanna-message-sent', (e) => {
  console.log('User sent:', e.detail.message);
});

chat.addEventListener('vanna-response-complete', (e) => {
  console.log('Response:', e.detail);
});

chat.addEventListener('vanna-error', (e) => {
  console.error('Error:', e.detail.error);
});
```

### Available Events

- `vanna-message-sent` - User sent a message
- `vanna-response-started` - Response streaming started
- `vanna-response-complete` - Response finished
- `vanna-error` - Error occurred
- `vanna-table-rendered` - Table component rendered
- `vanna-chart-rendered` - Chart component rendered

## Methods

```javascript
const chat = document.querySelector('vanna-chat');

// Send a message programmatically
chat.sendMessage('Show me the top customers');

// Clear conversation
chat.clearConversation();

// Set auth header
chat.setAuthHeader('Bearer token');

// Get conversation history
const history = chat.getHistory();
```

## Streaming Components

The component renders these streamed components:

1. **Progress** - Status updates
2. **Code** - SQL with syntax highlighting
3. **Table** - Interactive data tables
4. **Chart** - Plotly visualizations
5. **Text** - Markdown-formatted text

## Responsive Design

The component is fully responsive:

- Desktop: Full layout with sidebar
- Tablet: Collapsed sidebar
- Mobile: Stacked layout

```css
/* Override breakpoints */
vanna-chat {
  --vanna-mobile-breakpoint: 480px;
  --vanna-tablet-breakpoint: 768px;
}
```

## Accessibility

The component follows WCAG 2.1 guidelines:

- Keyboard navigation
- Screen reader support
- Focus management
- High contrast mode
- Reduced motion support
</file>

<file path="vanna-skill/SKILL.md">
---
name: vanna
description: Vanna AI - Text-to-SQL generation using LLMs and Agentic Retrieval. Use when working with natural language to SQL queries, database querying via chat, training AI on database schemas, or building text-to-SQL applications.
---

# Vanna AI - Text-to-SQL with LLMs

Vanna is a Python framework for generating accurate SQL queries from natural language using LLMs and Agentic Retrieval. It's ideal for building chat-with-your-database applications with enterprise security features.

## When to Use This Skill

This skill should be activated when:
- Converting natural language questions to SQL queries
- Building text-to-SQL applications with streaming responses
- Implementing user-aware database querying with row-level security
- Creating chat interfaces for database analytics
- Integrating LLM-powered SQL generation with existing authentication systems
- Training AI models on database schemas and query patterns
- Working with the Vanna Python framework
- Debugging Vanna agent configurations
- Setting up Vanna with various databases (PostgreSQL, MySQL, Snowflake, BigQuery, etc.)
- Configuring LLM providers for Vanna (Anthropic, OpenAI, Ollama, etc.)

## Quick Start

### Installation

```bash
# Basic installation
pip install vanna

# With specific integrations
pip install 'vanna[flask,anthropic]'
pip install 'vanna[fastapi,openai]'
```

### Minimal Example (30 seconds)

```python
from vanna import Agent
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.sqlite import SqliteRunner
from vanna.tools import RunSqlTool
from vanna.core.registry import ToolRegistry

# Set up agent with SQLite
llm = AnthropicLlmService(model="claude-haiku-4-5")
tools = ToolRegistry()
tools.register(RunSqlTool(sql_runner=SqliteRunner("./chinook.db")))

agent = Agent(llm_service=llm, tool_registry=tools)
```

### Production Setup with Authentication

```python
from fastapi import FastAPI
from vanna import Agent
from vanna.servers.fastapi.routes import register_chat_routes
from vanna.servers.base import ChatHandler
from vanna.core.user import UserResolver, User, RequestContext
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.tools import RunSqlTool
from vanna.integrations.sqlite import SqliteRunner
from vanna.core.registry import ToolRegistry

app = FastAPI()

# Define user resolver for your auth system
class MyUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        token = request_context.get_header('Authorization')
        user_data = self.decode_jwt(token)
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data['groups']
        )

# Set up agent with tools
llm = AnthropicLlmService(model="claude-sonnet-4-5")
tools = ToolRegistry()
tools.register(RunSqlTool(sql_runner=SqliteRunner("./data.db")))

agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=MyUserResolver()
)

# Add Vanna routes
chat_handler = ChatHandler(agent)
register_chat_routes(app, chat_handler)
```

### Web Component (Frontend)

```html
<!-- Drop into any webpage -->
<script src="https://img.vanna.ai/vanna-components.js"></script>
<vanna-chat
  sse-endpoint="https://your-api.com/chat"
  theme="dark">
</vanna-chat>
```

---

## Core Concepts

### Architecture Overview

```text
User Question -> Web Component -> Server -> Agent -> Tools -> Database
                                    |
                              User Resolver (auth)
                                    |
                              LLM Service
```

**Key Components:**
1. **Agent** - Orchestrates LLM and tools to answer questions
2. **UserResolver** - Extracts user identity from requests (JWT, cookies)
3. **ToolRegistry** - Manages available tools (SQL, memory, custom)
4. **LlmService** - Handles LLM interactions (OpenAI, Anthropic, etc.)
5. **SqlRunner** - Executes SQL against your database

### User-Aware Design

Every component knows the user identity for:
- Row-level security (filter queries per user permissions)
- Audit logging (track all queries per user)
- Rate limiting (per-user quotas)
- Permission checks (tool access control)

---

## Database Integrations

### PostgreSQL

```python
from vanna.integrations.postgres import PostgresRunner

runner = PostgresRunner(
    host="localhost",
    dbname="mydb",
    user="user",
    password="password",
    port=5432
)
tools.register(RunSqlTool(sql_runner=runner))
```

### MySQL

```python
from vanna.integrations.mysql import MySQLRunner

runner = MySQLRunner(
    host="localhost",
    database="mydb",
    user="user",
    password="password"
)
```

### SQLite

```python
from vanna.integrations.sqlite import SqliteRunner

runner = SqliteRunner("./database.db")
```

### Snowflake

```python
from vanna.integrations.snowflake import SnowflakeRunner

runner = SnowflakeRunner(
    account="xxx.snowflakecomputing.com",
    user="user",
    password="password",
    database="DB",
    schema="PUBLIC",
    warehouse="COMPUTE_WH"
)
```

### BigQuery

```python
from vanna.integrations.bigquery import BigQueryRunner

runner = BigQueryRunner(
    project="my-project",
    credentials_path="./credentials.json"
)
```

### DuckDB

```python
from vanna.integrations.duckdb import DuckDBRunner

runner = DuckDBRunner("./analytics.duckdb")
```

### ClickHouse

```python
from vanna.integrations.clickhouse import ClickHouseRunner

runner = ClickHouseRunner(
    host="localhost",
    database="default",
    user="default",
    password=""
)
```

---

## LLM Integrations

### Anthropic Claude

```python
from vanna.integrations.anthropic import AnthropicLlmService

llm = AnthropicLlmService(
    model="claude-sonnet-4-5",  # or claude-haiku-4-5, claude-opus-4-5
    api_key="sk-ant-..."
)
```

### OpenAI

```python
from vanna.integrations.openai import OpenAILlmService

llm = OpenAILlmService(
    model="gpt-4o",
    api_key="sk-..."
)
```

### Azure OpenAI

```python
from vanna.integrations.azure import AzureOpenAILlmService

llm = AzureOpenAILlmService(
    deployment_name="gpt-4",
    azure_endpoint="https://xxx.openai.azure.com/",
    api_key="..."
)
```

### Ollama (Local)

```python
from vanna.integrations.ollama import OllamaLlmService

llm = OllamaLlmService(
    model="llama3",
    base_url="http://localhost:11434"
)
```

### Google Gemini

```python
from vanna.integrations.gemini import GeminiLlmService

llm = GeminiLlmService(
    model="gemini-pro",
    api_key="..."
)
```

### AWS Bedrock

```python
from vanna.integrations.bedrock import BedrockLlmService

llm = BedrockLlmService(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    region_name="us-east-1"
)
```

### Mistral

```python
from vanna.integrations.mistral import MistralLlmService

llm = MistralLlmService(
    model="mistral-large-latest",
    api_key="..."
)
```

---

## Custom Tools

Create domain-specific tools by extending the `Tool` base class:

```python
from vanna.core.tool import Tool, ToolContext, ToolResult
from pydantic import BaseModel, Field
from typing import Type

class EmailArgs(BaseModel):
    recipient: str = Field(description="Email recipient")
    subject: str = Field(description="Email subject")

class EmailTool(Tool[EmailArgs]):
    @property
    def name(self) -> str:
        return "send_email"

    @property
    def access_groups(self) -> list[str]:
        return ["send_email"]  # Permission check

    def get_args_schema(self) -> Type[EmailArgs]:
        return EmailArgs

    async def execute(self, context: ToolContext, args: EmailArgs) -> ToolResult:
        user = context.user  # Automatically injected

        await self.email_service.send(
            from_email=user.email,
            to=args.recipient,
            subject=args.subject
        )

        return ToolResult(success=True, result_for_llm=f"Email sent to {args.recipient}")

# Register
tools.register(EmailTool())
```

---

## Web Server Setup

### FastAPI Server

```python
from vanna.servers.fastapi import VannaFastAPIServer

server = VannaFastAPIServer(agent)
app = server.create_app()

# Run with: uvicorn main:app --host 0.0.0.0 --port 8000
```

### Flask Server

```python
from vanna.servers.flask import VannaFlaskServer

server = VannaFlaskServer(agent)
app = server.create_app()

# Run with: flask run --host 0.0.0.0 --port 8000
```

### Add to Existing FastAPI App

```python
from vanna.servers.fastapi.routes import register_chat_routes
from vanna.servers.base import ChatHandler

# Your existing app
app = FastAPI()

# Add Vanna routes
chat_handler = ChatHandler(agent)
register_chat_routes(app, chat_handler)

# Endpoints added:
# - POST /api/vanna/v2/chat_sse (streaming)
# - GET / (optional web UI)
```

---

## Enterprise Features

### Lifecycle Hooks

```python
from vanna import AgentConfig

async def quota_hook(context):
    user = context.user
    if await check_quota_exceeded(user.id):
        raise QuotaExceededError("Daily limit reached")

config = AgentConfig(
    pre_execution_hooks=[quota_hook]
)
agent = Agent(llm_service=llm, tool_registry=tools, config=config)
```

### Audit Logging

```python
from vanna.core.audit import AuditLogger

class MyAuditLogger(AuditLogger):
    async def log_query(self, user_id: str, query: str, sql: str, result: any):
        await self.db.insert("audit_log", {
            "user_id": user_id,
            "query": query,
            "sql": sql,
            "timestamp": datetime.now()
        })

agent = Agent(..., audit_logger=MyAuditLogger())
```

### Rate Limiting

```python
async def rate_limit_hook(context):
    user = context.user
    requests = await get_request_count(user.id, period="1h")
    if requests > 100:
        raise RateLimitError("Too many requests")

config = AgentConfig(pre_execution_hooks=[rate_limit_hook])
```

### Row-Level Security

```python
class SecureSqlRunner(PostgresRunner):
    async def execute(self, sql: str, user: User) -> DataFrame:
        # Inject user filter into queries
        filtered_sql = self.apply_rls(sql, user.group_memberships)
        return await super().execute(filtered_sql, user)
```

---

## Migration from Vanna 0.x

### Quick Migration with Legacy Adapter

```python
# Wrap existing VannaBase instance
from vanna.legacy.adapter import LegacyVannaAdapter

# Your existing 0.x setup
# vn = MyVanna(config={'model': 'gpt-4'})
# vn.connect_to_postgres(...)
# vn.train(ddl="...")

# Wrap with adapter
tools = LegacyVannaAdapter(vn)

# Create new agent
llm = AnthropicLlmService(model="claude-haiku-4-5")
agent = Agent(llm_service=llm, tool_registry=tools, user_resolver=SimpleUserResolver())

# Run server
server = VannaFastAPIServer(agent)
server.run(host='0.0.0.0', port=8000)
```

### Key Differences from 0.x

| Feature | Vanna 0.x | Vanna 2.0+ |
|---------|-----------|------------|
| User Context | None | User object flows through system |
| Interaction | `vn.ask()` | Agent-based with streaming |
| Tools | Monolithic methods | Modular Tool classes |
| Responses | Text/DataFrames | Rich UI components |
| Training | `vn.train()` | System prompts, RAG tools |
| Database | `vn.connect_to_*()` | SqlRunner implementations |
| Web UI | Custom | Built-in `<vanna-chat>` |
| Streaming | None | SSE by default |
| Permissions | None | Group-based access control |

---

## Streaming Responses

Vanna 2.0 streams rich UI components via Server-Sent Events:

1. **Progress Updates** - Real-time status
2. **SQL Code Block** - Generated query (admin only by default)
3. **Data Table** - Interactive results
4. **Charts** - Plotly visualizations
5. **Summary** - Natural language explanation

All rendered beautifully by the `<vanna-chat>` component.

---

## Common Patterns

### Multi-Tenant SaaS

```python
class TenantAwareUserResolver(UserResolver):
    async def resolve_user(self, ctx: RequestContext) -> User:
        token = ctx.get_header('Authorization')
        user = decode_jwt(token)
        return User(
            id=user['id'],
            email=user['email'],
            group_memberships=[f"tenant_{user['tenant_id']}"],
            metadata={'tenant_id': user['tenant_id']}
        )
```

### RAG with Documentation

```python
from vanna.tools import SearchDocsTool

tools.register(SearchDocsTool(
    vector_store=my_vector_store,
    access_groups=['user', 'admin']
))
```

### Conversation History

```python
from vanna.core.storage import ConversationStorage

storage = PostgresConversationStorage(connection_string)
agent = Agent(..., conversation_storage=storage)
```

---

## Troubleshooting

### Agent not generating SQL
- Check tool registration: `print(tools.list_tools())`
- Verify database connection in SqlRunner
- Check user has access to `run_sql` tool group

### Authentication errors
- Verify UserResolver extracts correct token
- Check group_memberships match tool access_groups
- Enable debug logging: `logging.basicConfig(level=logging.DEBUG)`

### Streaming not working
- Ensure `AgentConfig(stream_responses=True)`
- Check SSE endpoint URL in `<vanna-chat>`
- Verify CORS settings allow streaming

---

## Reference Documentation

For detailed API documentation and advanced topics, see:
- `references/getting_started.md` - Installation and quickstart
- `references/architecture.md` - System design and components
- `references/database_integrations.md` - All database connectors
- `references/llm_integrations.md` - All LLM providers
- `references/tools.md` - Custom tool development
- `references/web_ui.md` - Frontend component guide
- `references/migration.md` - 0.x to 2.0 migration guide
- `references/enterprise.md` - Security and audit features

---

## Resources

- **Documentation**: https://vanna.ai/docs
- **GitHub**: https://github.com/vanna-ai/vanna
- **Discussions**: https://github.com/vanna-ai/vanna/discussions
- **Issues**: https://github.com/vanna-ai/vanna/issues
- **Enterprise Support**: support@vanna.ai

---

## Quick Copy-Paste Patterns

### Complete FastAPI Setup

```python
from fastapi import FastAPI
from vanna import Agent, AgentConfig
from vanna.servers.fastapi.routes import register_chat_routes
from vanna.servers.base import ChatHandler
from vanna.core.user import UserResolver, User, RequestContext
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.postgres import PostgresRunner
from vanna.tools import RunSqlTool
from vanna.core.registry import ToolRegistry

app = FastAPI()

class MyUserResolver(UserResolver):
    async def resolve_user(self, ctx: RequestContext) -> User:
        token = ctx.get_header('Authorization')
        user_data = self.decode_jwt(token)
        return User(
            id=user_data['id'],
            email=user_data['email'],
            group_memberships=user_data.get('groups', ['user'])
        )

llm = AnthropicLlmService(model="claude-sonnet-4-5", api_key="sk-ant-...")
tools = ToolRegistry()
tools.register(RunSqlTool(sql_runner=PostgresRunner(
    host="localhost", dbname="mydb", user="user", password="pass", port=5432
)))

agent = Agent(
    llm_service=llm,
    tool_registry=tools,
    user_resolver=MyUserResolver(),
    config=AgentConfig(stream_responses=True)
)

register_chat_routes(app, ChatHandler(agent))
# Run: uvicorn main:app --host 0.0.0.0 --port 8000
```

### Simple SQLite Demo

```python
from vanna import Agent
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.sqlite import SqliteRunner
from vanna.tools import RunSqlTool
from vanna.core.registry import ToolRegistry
from vanna.servers.fastapi import VannaFastAPIServer

llm = AnthropicLlmService(model="claude-haiku-4-5", api_key="sk-ant-...")
tools = ToolRegistry()
tools.register(RunSqlTool(sql_runner=SqliteRunner("./data.db")))

agent = Agent(llm_service=llm, tool_registry=tools)
VannaFastAPIServer(agent).run(host='0.0.0.0', port=8000)
```

### Frontend HTML

```html
<!DOCTYPE html>
<html>
<head>
    <title>Chat with Database</title>
    <script src="https://img.vanna.ai/vanna-components.js"></script>
</head>
<body>
    <vanna-chat
        sse-endpoint="http://localhost:8000/api/vanna/v2/chat_sse"
        theme="light">
    </vanna-chat>
</body>
</html>
```

### Custom Tool Template

```python
from vanna.core.tool import Tool, ToolContext, ToolResult
from pydantic import BaseModel, Field
from typing import Type

class MyArgs(BaseModel):
    param1: str = Field(description="First parameter")
    param2: int = Field(description="Second parameter", default=10)

class MyCustomTool(Tool[MyArgs]):
    @property
    def name(self) -> str:
        return "my_tool"

    @property
    def description(self) -> str:
        return "Description of what this tool does"

    @property
    def access_groups(self) -> list[str]:
        return ["user", "admin"]

    def get_args_schema(self) -> Type[MyArgs]:
        return MyArgs

    async def execute(self, context: ToolContext, args: MyArgs) -> ToolResult:
        user = context.user
        # Your logic here
        result = f"Processed {args.param1} with {args.param2} for {user.email}"
        return ToolResult(success=True, result_for_llm=result)

# Register: tools.register(MyCustomTool())
```

---

## Key Imports Cheat Sheet

```python
# Core
from vanna import Agent, AgentConfig

# User Management
from vanna.core.user import UserResolver, User, RequestContext

# Tool System
from vanna.core.registry import ToolRegistry
from vanna.core.tool import Tool, ToolContext, ToolResult
from vanna.tools import RunSqlTool, SearchDocsTool, SearchMemoryTool

# LLM Services
from vanna.integrations.anthropic import AnthropicLlmService
from vanna.integrations.openai import OpenAILlmService
from vanna.integrations.azure import AzureOpenAILlmService
from vanna.integrations.ollama import OllamaLlmService
from vanna.integrations.gemini import GeminiLlmService
from vanna.integrations.bedrock import BedrockLlmService
from vanna.integrations.mistral import MistralLlmService

# Database Runners
from vanna.integrations.postgres import PostgresRunner
from vanna.integrations.mysql import MySQLRunner
from vanna.integrations.sqlite import SqliteRunner
from vanna.integrations.snowflake import SnowflakeRunner
from vanna.integrations.bigquery import BigQueryRunner
from vanna.integrations.duckdb import DuckDBRunner
from vanna.integrations.clickhouse import ClickHouseRunner

# Servers
from vanna.servers.fastapi import VannaFastAPIServer
from vanna.servers.fastapi.routes import register_chat_routes
from vanna.servers.flask import VannaFlaskServer
from vanna.servers.base import ChatHandler

# Migration (0.x compatibility)
from vanna.legacy.adapter import LegacyVannaAdapter
```

---

**Version**: Vanna 2.0+
**License**: MIT
**Python**: 3.8+
</file>

<file path="vercel.json">
{
  "buildCommand": "cd frontend && npm run build",
  "outputDirectory": "frontend/.next",
  "installCommand": "cd frontend && npm install",
  "framework": "nextjs",
  "rootDirectory": "frontend"
}
</file>

<file path="api/routes/news_stream.py">
"""
News Stream SSE endpoint.

Provides a Server-Sent Events stream that pushes notifications when new
news articles are detected in the store. Clients connect once and receive
events as they arrive, avoiding the need for polling.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/news", tags=["news-stream"])
⋮----
"""SSE endpoint that emits events when new articles appear.

    Polls the NewsStore every 30 seconds and sends an event containing
    the new article count whenever the latest article ID changes.
    Clients should reconnect on error.
    """
⋮----
async def event_generator()
⋮----
last_seen_id: Optional[str] = None
store = get_store()
⋮----
# Send an initial keepalive so the client knows the connection is open
⋮----
articles = await store.aget_latest_news(
⋮----
newest_id = articles[0]["id"]
⋮----
# First poll -- just record the current head
last_seen_id = newest_id
⋮----
# Find how many articles are newer than last_seen_id
new_items = []
⋮----
payload = json.dumps(
</file>

<file path="api/schemas/__init__.py">
"""Pydantic v2 request/response schemas for the TASI AI platform API."""
⋮----
__all__ = [
</file>

<file path="api/schemas/charts.py">
"""Pydantic schemas for chart data endpoints."""
⋮----
class ChartRequest(BaseModel)
⋮----
"""Request parameters for a chart query."""
⋮----
ticker: str = Field(..., min_length=1, max_length=20)
chart_type: str = Field("bar", max_length=50)
period: str = Field("1y", max_length=10)
⋮----
class ChartDataPoint(BaseModel)
⋮----
"""Single data point for a chart."""
⋮----
label: str
value: float
⋮----
class ChartResponse(BaseModel)
⋮----
"""Response model for chart data."""
⋮----
chart_type: str
title: str
data: List[ChartDataPoint]
</file>

<file path="auth/__init__.py">
"""
Authentication module for TASI AI Platform.

Provides JWT-based authentication with bcrypt password hashing,
access/refresh tokens, and FastAPI dependency injection.
"""
⋮----
__all__ = [
</file>

<file path="auth/jwt_handler.py">
"""
JWT token creation and verification.

Uses PyJWT with HS256 for signing. Supports two token types:
- access: short-lived, carries user claims for API authorization
- refresh: long-lived, used only to obtain new access tokens
"""
⋮----
def _get_auth_settings()
⋮----
def create_access_token(data: Dict[str, Any]) -> str
⋮----
"""Create a short-lived access token.

    Parameters
    ----------
    data : dict
        Claims to include in the token. Must include "sub" (user ID).

    Returns
    -------
    str
        Encoded JWT string.
    """
auth = _get_auth_settings()
to_encode = data.copy()
expire = datetime.now(timezone.utc) + timedelta(
⋮----
def create_refresh_token(data: Dict[str, Any]) -> str
⋮----
"""Create a long-lived refresh token.

    Parameters
    ----------
    data : dict
        Claims to include in the token. Must include "sub" (user ID).

    Returns
    -------
    str
        Encoded JWT string.
    """
⋮----
expire = datetime.now(timezone.utc) + timedelta(days=auth.refresh_token_expire_days)
⋮----
def decode_token(token: str, expected_type: Optional[str] = None) -> Dict[str, Any]
⋮----
"""Decode and validate a JWT token.

    Parameters
    ----------
    token : str
        The encoded JWT string.
    expected_type : str, optional
        If provided, verify the token's "type" claim matches. Use "access"
        or "refresh".

    Returns
    -------
    dict
        Decoded token payload.

    Raises
    ------
    jwt.ExpiredSignatureError
        If the token has expired.
    jwt.InvalidTokenError
        If the token is malformed or signature is invalid.
    ValueError
        If expected_type is provided and does not match the token's type claim.
    """
⋮----
payload = jwt.decode(token, auth.jwt_secret, algorithms=[auth.jwt_algorithm])
⋮----
token_type = payload.get("type")
</file>

<file path="auth/models.py">
"""
Pydantic models for authentication requests and responses.
"""
⋮----
class UserCreate(BaseModel)
⋮----
"""Request body for user registration.

    Accepts both ``display_name`` and ``name`` for the user's display name
    so the frontend can send ``{ "name": "..." }`` while the backend stores
    it as ``display_name``.
    """
⋮----
email: EmailStr
password: str = Field(..., min_length=8, max_length=128)
display_name: Optional[str] = Field(None, max_length=100, alias="name")
⋮----
model_config = {"populate_by_name": True}
⋮----
class UserLogin(BaseModel)
⋮----
"""Request body for user login."""
⋮----
password: str
⋮----
class TokenResponse(BaseModel)
⋮----
"""Response containing access and refresh tokens."""
⋮----
access_token: str
refresh_token: str
token_type: str = "bearer"
⋮----
class AuthResponse(BaseModel)
⋮----
"""Response returned by login and register endpoints.

    Includes both tokens and user info so the frontend can persist
    the user identity without an extra /me call.  Field names match
    what the frontend ``use-auth`` hook expects:
    ``token``, ``user_id``, ``name``.
    """
⋮----
token: str
⋮----
user_id: str
name: str
⋮----
class TokenRefreshRequest(BaseModel)
⋮----
"""Request body for refreshing an access token."""
⋮----
class UserProfile(BaseModel)
⋮----
"""Response model for the current user's profile."""
⋮----
id: str
email: str
display_name: Optional[str] = None
subscription_tier: str
usage_count: int
is_active: bool
created_at: Optional[datetime] = None
</file>

<file path="backend/middleware/__init__.py">
"""
Backend rate limiting middleware for Ra'd AI TASI Platform.

Provides a Redis-backed sliding window rate limiter with in-memory fallback,
Pydantic result models, and FastAPI middleware integration.

Usage::

    from backend.middleware import RateLimiter, RateLimitMiddleware, RateLimitResult

    limiter = RateLimiter(redis_url="redis://localhost:6379/1")
    app.add_middleware(
        RateLimitMiddleware,
        limiter=limiter,
        default_limit=60,
        default_window=60,
    )
"""
⋮----
__all__ = [
</file>

<file path="backend/middleware/rate_limit_config.py">
"""
Rate limit configuration with per-endpoint rules.

Loads settings from environment variables (prefix RATELIMIT_) with sensible
defaults for the Ra'd AI TASI platform.

Default endpoint rules:
- /api/v1/query:    50 req / 3600s (1 hour)  -- LLM-backed, expensive
- /api/auth:        20 req / 60s  (1 minute) -- brute-force protection
- /api/v1/export:   10 req / 3600s (1 hour)  -- heavy data export
- /api/v1:         1000 req / 3600s (1 hour)  -- general API
- (default):         60 req / 60s  (1 minute) -- catch-all

Health endpoints (/health, /health/live, /health/ready) are always skipped.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
class EndpointRateLimit(BaseModel)
⋮----
"""Rate limit rule for a path prefix.

    Attributes
    ----------
    path_prefix : str
        URL path prefix to match (longest prefix wins).
    limit : int
        Maximum requests allowed in the window.
    window : int
        Sliding window size in seconds.
    description : str
        Human-readable description of the rule.
    """
⋮----
path_prefix: str
limit: int
window: int
description: str = ""
⋮----
class RateLimitConfig(BaseSettings)
⋮----
"""Rate limit settings loaded from environment variables.

    Environment variables use the RATELIMIT_ prefix. The per-endpoint rules
    are defined as defaults and can be overridden by subclassing or by
    passing custom rules at middleware registration time.
    """
⋮----
model_config = SettingsConfigDict(env_prefix="RATELIMIT_")
⋮----
# Global defaults
default_limit: int = Field(default=60, description="Default requests per window")
default_window: int = Field(default=60, description="Default window in seconds")
⋮----
# Redis connection for rate limiting (db=1, separate from cache db=0)
redis_url: str = Field(
⋮----
# Whether to enable rate limiting at all
enabled: bool = Field(default=True, description="Enable rate limiting middleware")
⋮----
# Additional paths to skip (merged with built-in health/docs paths)
skip_paths: str = Field(
⋮----
@property
    def skip_paths_set(self) -> Set[str]
⋮----
"""Parse comma-separated skip paths into a set."""
⋮----
@property
    def endpoint_rules(self) -> List[EndpointRateLimit]
⋮----
"""Return the default per-endpoint rate limit rules.

        These are the built-in defaults for the Ra'd AI platform.
        Override by passing custom path_limits to the middleware.
        """
⋮----
def to_path_limits(self) -> Dict[str, Tuple[int, int]]
⋮----
"""Convert endpoint rules to the dict format expected by RateLimitMiddleware.

        Returns
        -------
        dict[str, tuple[int, int]]
            Mapping of path prefix -> (limit, window_seconds).
        """
⋮----
def log_config(self) -> None
⋮----
"""Log the active rate limit configuration."""
</file>

<file path="backend/middleware/rate_limiter.py">
"""
Sliding window rate limiter with Redis backend and in-memory fallback.

Uses Redis db=1 (separate from cache on db=0) for distributed rate limiting.
Falls back to an in-memory dict-of-deques implementation when Redis is
unavailable, making the limiter work identically in development without Redis.

Usage::

    limiter = RateLimiter(redis_url="redis://localhost:6379/1")
    result = limiter.check("user:123", limit=60, window=60)
    if not result.allowed:
        # reject request
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Cleanup stale in-memory entries every N checks
_CLEANUP_INTERVAL = 500
⋮----
class RateLimiter
⋮----
"""Sliding window rate limiter with Redis + in-memory fallback.

    Parameters
    ----------
    redis_url : str or None
        Redis connection URL. Uses db=1 by default to avoid conflicts with
        the cache layer (db=0). Pass None to skip Redis entirely.
    """
⋮----
def __init__(self, redis_url: Optional[str] = None) -> None
⋮----
# In-memory fallback structures
⋮----
def _init_redis(self, url: str) -> None
⋮----
"""Attempt to connect to Redis. Fail silently to in-memory mode."""
⋮----
@property
    def is_redis_available(self) -> bool
⋮----
"""Return True if the Redis backend is connected."""
⋮----
"""Check whether a request from *identifier* is within the rate limit.

        Parameters
        ----------
        identifier : str
            Unique client key (e.g. user ID, IP address).
        limit : int
            Maximum number of requests allowed in *window* seconds.
        window : int
            Sliding window size in seconds.
        bucket : str
            Logical bucket name for the rate limit rule (used in logging).

        Returns
        -------
        RateLimitResult
            Contains ``allowed``, ``remaining``, ``reset_after``, etc.
        """
key = f"rl:{bucket}:{identifier}"
⋮----
"""Sliding window counter using Redis sorted set.

        Each request is added as a member with score = current timestamp.
        Expired members (older than the window) are removed atomically via
        a pipeline to keep the operation consistent.
        """
now = time.time()
window_start = now - window
pipe = self._redis.pipeline(transaction=True)
⋮----
# Remove expired entries
⋮----
# Count current entries
⋮----
# Add current request (member = timestamp string to ensure uniqueness)
⋮----
# Set expiry on the key so it auto-cleans
⋮----
results = pipe.execute()
current_count = results[1]  # zcard result (before adding this request)
⋮----
# Over limit -- find when the oldest entry expires
oldest = self._redis.zrange(key, 0, 0, withscores=True)
⋮----
reset_after = max(1, int(oldest[0][1] + window - now) + 1)
⋮----
reset_after = window
⋮----
# Remove the request we just added since it's rejected
⋮----
remaining = max(0, limit - current_count - 1)
⋮----
"""In-memory sliding window using deque of timestamps."""
now = time.monotonic()
cutoff = now - window
⋮----
# Periodic cleanup
⋮----
timestamps = self._requests[key]
⋮----
reset_after = max(1, int(timestamps[0] - cutoff) + 1)
⋮----
remaining = max(0, limit - len(timestamps))
⋮----
def _cleanup_memory(self, now: float, window: int) -> None
⋮----
"""Remove in-memory keys with no recent requests."""
⋮----
stale = [k for k, ts in self._requests.items() if not ts or ts[-1] < cutoff]
⋮----
def close(self) -> None
⋮----
"""Close the Redis connection if open."""
</file>

<file path="backend/routes/health.py">
"""
Health check API routes for Ra'd AI TASI platform.

Provides three endpoints (no auth required):

    GET /health         -> Liveness probe (always 200 if process is running)
    GET /ready          -> Readiness probe (checks DB, Redis, Vanna/LLM)
    GET /metrics/basic  -> Basic operational metrics (uptime, request counts,
                           connections, error rate, circuit breaker states)
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(tags=["health"])
⋮----
_START_TIME = time.monotonic()
_REQUEST_COUNTER = 0
_ERROR_COUNTER = 0
_counter_lock = threading.Lock()
⋮----
def record_request(*, is_error: bool = False) -> None
⋮----
"""Increment global request counters (called from middleware)."""
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class LivenessResponse(BaseModel)
⋮----
"""Response for GET /health (liveness probe)."""
⋮----
status: str = Field(default="alive", description="Always 'alive' if process is up")
uptime_seconds: float = Field(description="Seconds since process start")
⋮----
class ComponentReadiness(BaseModel)
⋮----
"""Readiness status of a single dependency."""
⋮----
name: str = Field(description="Dependency name")
ready: bool = Field(description="Whether the dependency is reachable")
latency_ms: Optional[float] = Field(
message: str = Field(default="", description="Human-readable status detail")
⋮----
class ReadinessResponse(BaseModel)
⋮----
"""Response for GET /ready."""
⋮----
status: str = Field(description="'ready' or 'not_ready'")
components: List[ComponentReadiness] = Field(default_factory=list)
⋮----
class CircuitBreakerMetric(BaseModel)
⋮----
"""Snapshot of a single circuit breaker for the metrics endpoint."""
⋮----
name: str
state: str
failure_count: int
total_failures: int
total_successes: int
total_rejected: int
⋮----
class BasicMetricsResponse(BaseModel)
⋮----
"""Response for GET /metrics/basic."""
⋮----
uptime_seconds: float
total_requests: int
total_errors: int
error_rate: float = Field(description="Errors / requests (0.0 if no requests)")
circuit_breakers: List[CircuitBreakerMetric] = Field(default_factory=list)
⋮----
# Dependency check helpers
⋮----
async def _check_database() -> ComponentReadiness
⋮----
"""Check whether the database (SQLite or PostgreSQL) is reachable."""
start = time.monotonic()
⋮----
result = check_database()
latency = (time.monotonic() - start) * 1000
⋮----
async def _check_redis() -> ComponentReadiness
⋮----
"""Check whether Redis cache is reachable (non-fatal if disabled)."""
⋮----
result = check_redis()
⋮----
async def _check_llm() -> ComponentReadiness
⋮----
"""Check whether the LLM API key is configured."""
⋮----
result = check_llm()
⋮----
# Endpoints
⋮----
async def liveness() -> LivenessResponse
⋮----
"""Liveness probe -- always returns 200 if the process is up."""
uptime = time.monotonic() - _START_TIME
⋮----
async def readiness() -> Any
⋮----
"""Readiness probe -- returns 200 when all critical deps are reachable.

    Database is the only hard requirement; Redis and LLM being unavailable
    results in 503 but are logged as warnings rather than errors.
    """
components = [
⋮----
# Database is the hard gate; others are soft checks
db_ready = components[0].ready
all_ready = all(c.ready for c in components)
⋮----
async def basic_metrics() -> BasicMetricsResponse
⋮----
"""Return basic operational metrics for monitoring dashboards."""
⋮----
# Collect circuit breaker stats
cb_metrics: List[CircuitBreakerMetric] = []
⋮----
error_rate = (_ERROR_COUNTER / _REQUEST_COUNTER) if _REQUEST_COUNTER > 0 else 0.0
</file>

<file path="backend/security/allowlist.py">
"""Query allowlist with hot-reload support for Ra'd AI SQL security.

Manages lists of allowed tables and operations, loaded from a JSON
config file. Supports automatic reload when the config file changes
(checked via file modification time with a configurable TTL).
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Default path to the allowlist config file
_DEFAULT_CONFIG_PATH = (
⋮----
# How often to check if the config file has changed (seconds)
_DEFAULT_CACHE_TTL = 30.0
⋮----
class QueryAllowlist
⋮----
"""Manages allowed tables and operations for SQL query validation.

    Loads configuration from a JSON file and supports hot-reload:
    on each access, checks if the file has been modified (with a TTL
    to avoid excessive stat calls) and reloads if needed.

    Args:
        config_path: Path to the allowed_tables.json file.
            Defaults to config/allowed_tables.json in the project root.
        cache_ttl: Seconds between file modification checks.
            Defaults to 30 seconds.
    """
⋮----
def _load(self) -> None
⋮----
"""Load the allowlist configuration from the JSON file."""
⋮----
data: dict[str, Any] = json.load(f)
⋮----
# Fail safe: empty allowlist means nothing is allowed
⋮----
def _maybe_reload(self) -> None
⋮----
"""Check if the config file has changed and reload if needed."""
now = time.monotonic()
⋮----
current_mtime = os.path.getmtime(self._config_path)
⋮----
pass  # File might be temporarily unavailable
⋮----
def is_table_allowed(self, table_name: str) -> bool
⋮----
"""Check if a table name is in the allowlist.

        A table is allowed if it appears in the allowed_tables list
        and does NOT appear in the blocked_tables list.

        Args:
            table_name: The table name to check (case-insensitive).

        Returns:
            True if the table is allowed for querying.
        """
⋮----
lower = table_name.lower()
⋮----
def is_operation_allowed(self, operation: str) -> bool
⋮----
"""Check if a SQL operation is in the allowlist.

        Args:
            operation: The SQL operation to check (e.g., "SELECT").
                Case-insensitive.

        Returns:
            True if the operation is allowed.
        """
⋮----
def get_allowed_tables(self) -> list[str]
⋮----
"""Return the current list of allowed table names.

        Returns:
            Sorted list of allowed table names.
        """
⋮----
def get_blocked_tables(self) -> list[str]
⋮----
"""Return the current list of blocked table names.

        Returns:
            Sorted list of blocked table names.
        """
</file>

<file path="backend/security/sanitizer.py">
"""Input sanitization for natural language queries and SQL identifiers.

Provides functions to clean user input before it reaches the LLM,
preventing prompt injection and ensuring safe identifier usage.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Maximum allowed length for natural language queries
MAX_NL_QUERY_LENGTH = 2000
⋮----
# Maximum allowed length for SQL identifiers
MAX_IDENTIFIER_LENGTH = 128
⋮----
# Valid SQL identifier pattern: starts with letter or underscore,
# followed by letters, digits, or underscores
_IDENTIFIER_PATTERN = re.compile(r"^[a-zA-Z_][a-zA-Z0-9_]*$")
⋮----
# Control characters to strip (C0 and C1 control chars, excluding \n \r \t)
_CONTROL_CHARS_PATTERN = re.compile(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]")
⋮----
# Pattern to detect input that is purely SQL (not natural language)
_PURE_SQL_PATTERN = re.compile(
⋮----
def sanitize_nl_query(user_input: str) -> str
⋮----
"""Sanitize a natural language query from the user.

    Performs the following steps:
    1. Strip control characters (keeping newlines, tabs)
    2. Normalize Unicode (NFC form) to prevent homoglyph attacks
    3. Truncate to MAX_NL_QUERY_LENGTH characters
    4. Escape HTML entities to prevent XSS if rendered
    5. Reject input that appears to be raw SQL

    Args:
        user_input: The raw user input string.

    Returns:
        The sanitized input string.

    Raises:
        ValueError: If the input appears to be a raw SQL query rather
            than a natural language question.
    """
⋮----
# Strip control characters (keep \n, \r, \t)
cleaned = _CONTROL_CHARS_PATTERN.sub("", user_input)
⋮----
# Normalize Unicode to NFC form
cleaned = unicodedata.normalize("NFC", cleaned)
⋮----
# Strip leading/trailing whitespace
cleaned = cleaned.strip()
⋮----
# Truncate to max length
⋮----
cleaned = cleaned[:MAX_NL_QUERY_LENGTH]
⋮----
# Escape HTML entities
cleaned = html.escape(cleaned, quote=True)
⋮----
# Reject pure SQL input
⋮----
def sanitize_identifiers(identifier: str) -> str
⋮----
"""Validate and sanitize a SQL identifier (table name, column name).

    Only allows identifiers matching [a-zA-Z_][a-zA-Z0-9_]* pattern.
    This prevents SQL injection through dynamic identifier construction.

    Args:
        identifier: The identifier string to validate.

    Returns:
        The validated identifier (unchanged if valid).

    Raises:
        ValueError: If the identifier contains invalid characters or
            exceeds the maximum length.
    """
⋮----
# Strip whitespace
cleaned = identifier.strip()
</file>

<file path="backend/security/sql_validator.py">
"""SQL query validator for preventing SQL injection in AI-generated queries.

Uses sqlparse for proper SQL parsing to detect dangerous operations,
forbidden patterns, and potential injection attempts.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Operations that are never allowed in AI-generated queries
FORBIDDEN_OPERATIONS: set[str] = {
⋮----
"ANALYZE",  # SQLite ANALYZE can modify internal stats tables
⋮----
# Patterns that indicate injection attempts (case-insensitive)
INJECTION_PATTERNS: list[re.Pattern[str]] = [
⋮----
re.compile(r"0x[0-9a-fA-F]+"),  # Hex-encoded payloads
⋮----
),  # CHAR() obfuscation
re.compile(r"CONCAT\s*\(", re.IGNORECASE),  # String concatenation tricks
⋮----
),  # Time-based injection
⋮----
),  # File access
⋮----
),  # Schema probing
⋮----
# Risk score weights for different violation types
RISK_WEIGHTS: dict[str, float] = {
⋮----
class SqlQueryValidator
⋮----
"""Validates SQL queries for safety before execution.

    Uses sqlparse to parse and analyze SQL queries, detecting:
    - Write/DDL operations (INSERT, UPDATE, DELETE, DROP, etc.)
    - Stacked queries (multiple statements via semicolons)
    - UNION-based injection attempts
    - Comment-embedded SQL keywords
    - Schema probing (sqlite_master, information_schema)
    - Time-based and file-based injection patterns
    """
⋮----
def validate(self, sql: str) -> ValidationResult
⋮----
"""Run all validation checks on a SQL query.

        Args:
            sql: The SQL query string to validate.

        Returns:
            ValidationResult with is_valid, violations, risk_score, etc.
        """
⋮----
violations: list[str] = []
risk_scores: list[float] = []
⋮----
# Normalize whitespace for the sanitized version
normalized = self._normalize_sql(sql)
⋮----
# Parse with sqlparse
⋮----
parsed_statements = sqlparse.parse(normalized)
⋮----
# Check for stacked queries (multiple statements)
⋮----
# Analyze the primary statement
stmt = parsed_statements[0] if parsed_statements else None
⋮----
# Check read-only
forbidden = self.contains_forbidden_operations(sql)
⋮----
# Check injection patterns
⋮----
match = pattern.search(sql)
⋮----
# Check for comments containing SQL keywords
comment_violations = self._check_comments(stmt)
⋮----
# Extract tables
tables = self.extract_tables(sql)
⋮----
# Check for schema probing tables
schema_tables = {
probed = [t for t in tables if t.lower() in schema_tables]
⋮----
# Compute aggregate risk score (max of all, capped at 1.0)
risk_score = min(max(risk_scores, default=0.0), 1.0)
is_valid = len(violations) == 0
⋮----
def is_read_only(self, sql: str) -> bool
⋮----
"""Check if the SQL query is strictly read-only (SELECT only).

        Args:
            sql: The SQL query string to check.

        Returns:
            True if the query only performs read operations.
        """
⋮----
def contains_forbidden_operations(self, sql: str) -> list[str]
⋮----
"""Detect forbidden SQL operations in the query.

        Checks both the parsed token types and raw text patterns
        to catch operations even inside comments or strings.

        Args:
            sql: The SQL query string to check.

        Returns:
            List of forbidden operation names found.
        """
found: list[str] = []
⋮----
parsed_statements = sqlparse.parse(sql)
⋮----
# Check DML tokens (INSERT, UPDATE, DELETE, SELECT)
⋮----
upper_val = token.value.upper()
⋮----
# Check DDL tokens (CREATE, ALTER, DROP)
⋮----
# Check keywords (TRUNCATE, GRANT, etc.)
⋮----
# Also do a raw text scan for multi-word forbidden ops
sql_upper = sql.upper()
⋮----
return list(dict.fromkeys(found))  # Deduplicate preserving order
⋮----
def extract_tables(self, sql: str) -> list[str]
⋮----
"""Extract table names referenced in the SQL query.

        Handles FROM, JOIN, and subquery table references.

        Args:
            sql: The SQL query string to analyze.

        Returns:
            Deduplicated list of table names found.
        """
tables: list[str] = []
⋮----
# Deduplicate preserving order
seen: set[str] = set()
unique: list[str] = []
⋮----
lower = t.lower()
⋮----
def _extract_tables_from_tokens(self, tokens: list, tables: list[str]) -> None
⋮----
"""Recursively extract table names from token list."""
from_seen = False
⋮----
# Skip comments
⋮----
# Recurse into parentheses (subqueries)
⋮----
# Recurse into WHERE clauses (may contain subqueries)
⋮----
# Track FROM / JOIN keywords
⋮----
from_seen = True
⋮----
# Reset on other keywords like WHERE, GROUP BY, etc.
⋮----
# Extract table name after FROM/JOIN
⋮----
name = self._get_table_name(identifier)
⋮----
name = self._get_table_name(token)
⋮----
# Table-valued function - skip
⋮----
def _get_table_name(self, identifier: Identifier) -> str | None
⋮----
"""Extract the real table name from an Identifier token."""
# If identifier contains a subquery, skip it
⋮----
real_name = identifier.get_real_name()
⋮----
def _check_comments(self, stmt) -> list[str]
⋮----
"""Check for SQL keywords hidden inside comments."""
⋮----
dangerous_in_comments = {
⋮----
comment_upper = token.value.upper()
⋮----
def _normalize_sql(self, sql: str) -> str
⋮----
"""Normalize SQL whitespace and formatting."""
# Strip leading/trailing whitespace
normalized = sql.strip()
# Collapse multiple whitespace to single space (preserving newlines
# inside string literals is not critical for validation)
normalized = re.sub(r"\s+", " ", normalized)
# Remove trailing semicolons (we only allow single statements)
normalized = normalized.rstrip(";").strip()
</file>

<file path="backend/security/vanna_hook.py">
"""Vanna integration hook for SQL query validation.

This module provides the SINGLE ENTRY POINT for validating AI-generated
SQL queries before they are executed against the database. It combines
the SqlQueryValidator, QueryAllowlist, and input sanitizer into one
cohesive validation pipeline.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Module-level singletons (lazy-initialized)
_validator: SqlQueryValidator | None = None
_allowlist: QueryAllowlist | None = None
⋮----
def _get_validator() -> SqlQueryValidator
⋮----
"""Get or create the singleton SqlQueryValidator."""
⋮----
_validator = SqlQueryValidator()
⋮----
def _get_allowlist(config_path: str | Path | None = None) -> QueryAllowlist
⋮----
"""Get or create the singleton QueryAllowlist."""
⋮----
_allowlist = QueryAllowlist(config_path=config_path)
⋮----
"""Validate an AI-generated SQL query before execution.

    This is the SINGLE ENTRY POINT for all Vanna SQL validation.
    It runs the following pipeline:

    1. Basic validation (empty check, parse check)
    2. SqlQueryValidator checks (forbidden ops, injection patterns,
       stacked queries, comment injection, schema probing)
    3. Allowlist checks (table and operation permissions)

    Args:
        generated_sql: The SQL query generated by Vanna/LLM.
        original_query: The original natural language query (for logging).
        allowlist_config_path: Optional path to allowed_tables.json.

    Returns:
        ValidatedQuery with is_safe, sql, reason, risk_score, and
        validation_time_ms.
    """
start_time = time.perf_counter()
⋮----
# Empty query check
⋮----
elapsed = (time.perf_counter() - start_time) * 1000
⋮----
validator = _get_validator()
allowlist = _get_allowlist(config_path=allowlist_config_path)
⋮----
# Step 1: Run SqlQueryValidator
result = validator.validate(generated_sql)
⋮----
reason = "; ".join(result.violations)
⋮----
# Step 2: Check tables against allowlist
disallowed_tables = [
⋮----
reason = f"Access to table(s) not allowed: {', '.join(disallowed_tables)}"
⋮----
# Step 3: Check that the operation is allowed (SELECT only by default)
forbidden_ops = validator.contains_forbidden_operations(generated_sql)
# At this point forbidden_ops should be empty (caught in Step 1),
# but double-check the allowlist for the primary operation type
⋮----
primary_op = result.sanitized_sql.strip().split()[0].upper()
⋮----
reason = f"Operation '{primary_op}' is not in the allowlist"
⋮----
# All checks passed
⋮----
def reset_singletons() -> None
⋮----
"""Reset module-level singletons. Useful for testing."""
⋮----
_validator = None
_allowlist = None
</file>

<file path="backend/services/audit/models.py">
"""Pydantic v2 models for audit events.

Defines the data shapes for query audit trails and security events that are
persisted to the database and emitted as structured log entries.

All timestamps default to UTC.  IDs default to UUID4 hex strings so they can
be generated in application code without relying on the database.
"""
⋮----
# ---------------------------------------------------------------------------
# Security event enumerations
⋮----
class SecuritySeverity(str, Enum)
⋮----
"""Severity level for security events."""
⋮----
LOW = "low"
MEDIUM = "medium"
HIGH = "high"
CRITICAL = "critical"
⋮----
class SecurityEventType(str, Enum)
⋮----
"""Categorical type of a security event."""
⋮----
SQL_INJECTION_ATTEMPT = "sql_injection_attempt"
FORBIDDEN_KEYWORD = "forbidden_keyword"
RATE_LIMIT_EXCEEDED = "rate_limit_exceeded"
AUTH_FAILURE = "auth_failure"
INVALID_INPUT = "invalid_input"
SUSPICIOUS_PATTERN = "suspicious_pattern"
UNAUTHORIZED_ACCESS = "unauthorized_access"
⋮----
# Query audit event
⋮----
class QueryAuditEvent(BaseModel)
⋮----
"""Structured record of a single NL-to-SQL query lifecycle.

    Compatible with the ``query_audit_log`` PostgreSQL table defined in
    ``database/schema.sql`` and the migration in
    ``migrations/001_audit_table.sql``.
    """
⋮----
id: str = Field(default_factory=lambda: uuid.uuid4().hex)
request_id: Optional[str] = Field(
user_id: Optional[str] = None
timestamp: datetime = Field(
nl_query: str = Field(..., description="User's natural-language question")
generated_sql: Optional[str] = Field(
validation_result: Optional[str] = Field(
execution_time_ms: Optional[int] = Field(
row_count: Optional[int] = Field(
error: Optional[str] = Field(
ip_address: Optional[str] = None
risk_score: Optional[float] = Field(
⋮----
model_config = {"frozen": False, "extra": "ignore"}
⋮----
# Security event
⋮----
class SecurityEvent(BaseModel)
⋮----
"""Structured record of a security-relevant occurrence.

    Stored in the ``security_events`` table (see
    ``migrations/002_security_events_table.sql``).
    """
⋮----
event_type: SecurityEventType
severity: SecuritySeverity
⋮----
details: Optional[str] = Field(
</file>

<file path="backend/services/audit/structured_logger.py">
"""Structured logging for the audit subsystem.

Provides a JSON formatter and configuration function that integrate with
the existing ``config.logging_config`` module.  The audit logger adds
request-correlation fields (``request_id``) pulled from *contextvars*
so every log line emitted during a request can be traced back.

Usage::

    from backend.services.audit.structured_logger import configure_logging, get_logger

    configure_logging()                     # call once at startup
    logger = get_logger(__name__)
    logger.info("query executed", extra={"rows": 42})

Environment variables:
    LOG_LEVEL   - Root log level (default: INFO)
    LOG_FORMAT  - "json" (default) or "text" for human-readable output
"""
⋮----
class JSONFormatter(logging.Formatter)
⋮----
"""One-JSON-object-per-line formatter for production log aggregators.

    Automatically injects ``request_id`` from the correlation *contextvar*
    when available, ensuring every log line can be traced to an HTTP request.
    Extra fields passed via ``extra={}`` are merged into the JSON object.
    """
⋮----
# Standard LogRecord attributes to exclude from the "extra" sweep.
_SKIP_ATTRS: frozenset[str] = frozenset(
⋮----
def format(self, record: logging.LogRecord) -> str
⋮----
log_entry: dict = {
⋮----
# Inject correlation request_id when inside a request context.
request_id = get_current_request_id()
⋮----
# Include exception traceback if present.
⋮----
# Merge any extra fields the caller passed.
⋮----
class _PrettyFormatter(logging.Formatter)
⋮----
"""Human-readable formatter for development (non-JSON)."""
⋮----
_FMT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
⋮----
def __init__(self) -> None
⋮----
"""Configure the root logger for the audit subsystem.

    Safe to call multiple times; existing handlers are cleared first.

    Args:
        log_level: Logging level string (DEBUG, INFO, WARNING, ERROR, CRITICAL).
                   Falls back to ``LOG_LEVEL`` env var, then ``INFO``.
        json_format: If *True*, use :class:`JSONFormatter`.  If *False*, use
                     human-readable output.  If *None*, auto-detect from
                     ``LOG_FORMAT`` env var (``"json"`` vs ``"text"``).
    """
level_name = (log_level or os.environ.get("LOG_LEVEL", "INFO")).upper()
⋮----
log_fmt = os.environ.get("LOG_FORMAT", "json").lower()
json_format = log_fmt != "text"
⋮----
root = logging.getLogger()
⋮----
# Clear existing handlers to prevent duplicate output on re-init.
⋮----
handler = logging.StreamHandler(sys.stdout)
⋮----
# Suppress noisy third-party loggers.
⋮----
def get_logger(name: str) -> logging.Logger
⋮----
"""Return a named logger.

    Convenience wrapper matching ``config.logging_config.get_logger`` so
    audit code can import from a single place.

    Args:
        name: Logger name, usually ``__name__``.
    """
</file>

<file path="backend/services/cache/maintenance.py">
"""Cache maintenance: warm-up, cleanup, and statistics.

Provides a CacheMaintenance helper that can be invoked on a schedule or at
startup to pre-populate the cache, collect hit/miss stats, and purge stale
entries.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Common queries that benefit from warm-up (used as cache keys)
_WARM_UP_QUERIES: list[str] = [
⋮----
class CacheMaintenance
⋮----
"""Maintenance operations for the query cache.

    Args:
        redis: The RedisManager instance.
        query_cache: The QueryCache instance.
    """
⋮----
def __init__(self, redis: RedisManager, query_cache: QueryCache) -> None
⋮----
"""Pre-populate the cache with common queries.

        Args:
            execute_fn: An async callable ``(sql: str) -> list[dict]`` that
                executes a SQL query and returns rows. If None, warm-up is
                skipped (no DB access).

        Returns:
            Summary dict with ``warmed`` count and ``errors``.
        """
⋮----
warmed = 0
errors: list[str] = []
⋮----
rows = await execute_fn(sql)
⋮----
msg = f"{sql[:60]}... -> {exc}"
⋮----
async def cleanup_expired(self) -> dict[str, Any]
⋮----
"""Trigger cleanup of expired cache entries.

        Redis handles TTL expiration natively, so this method primarily
        serves as a stats-collection checkpoint. It scans keys with the
        cache prefix and reports how many are still alive.

        Returns:
            Dict with ``active_keys`` count.
        """
start = time.monotonic()
⋮----
health = await self._redis.health_check()
elapsed_ms = round((time.monotonic() - start) * 1000, 2)
⋮----
async def get_cache_stats(self) -> dict[str, Any]
⋮----
"""Aggregate cache statistics from the query cache and Redis.

        Returns:
            Combined dict of in-process stats and Redis health.
        """
cache_stats = self._cache.stats()
redis_health = await self._redis.health_check()
</file>

<file path="backend/services/cache/models.py">
"""Pydantic models for the caching layer.

Defines the cached result envelope, TTL tier classification, and pool
statistics used throughout the cache subsystem.
"""
⋮----
class TTLTier(str, Enum)
⋮----
"""Cache TTL tiers mapped to query categories.

    Each tier defines a default TTL suitable for the data volatility of
    that category.
    """
⋮----
MARKET = "market"  # 60 s  - live/recent market data
HISTORICAL = "historical"  # 3600 s - historical financials
SCHEMA = "schema"  # 86400 s - schema/metadata lookups
⋮----
@property
    def ttl_seconds(self) -> int
⋮----
"""Return the default TTL for this tier in seconds."""
⋮----
_TTL_MAP: dict[TTLTier, int] = {
⋮----
class CachedResult(BaseModel)
⋮----
"""Envelope stored in Redis for a cached query result.

    Attributes:
        query_hash: SHA-256 hex digest of the normalized SQL.
        sql: The original SQL query string.
        data: The query result payload (list of row dicts).
        tier: The TTL tier that was applied.
        created_at: UTC timestamp of when the entry was cached.
        ttl: TTL in seconds that was set on the key.
        row_count: Number of rows in the result set.
        compressed: Whether the payload was gzip-compressed before storage.
    """
⋮----
query_hash: str
sql: str
data: Any  # list[dict[str, Any]] — kept as Any for msgpack compat
tier: TTLTier
created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
ttl: int
row_count: int = 0
compressed: bool = False
⋮----
class PoolStats(BaseModel)
⋮----
"""Snapshot of database connection pool statistics.

    Attributes:
        pool_size: Configured maximum pool size.
        checked_out: Number of connections currently checked out.
        overflow: Number of overflow connections in use.
        checked_in: Number of idle connections in the pool.
    """
⋮----
pool_size: int = 0
checked_out: int = 0
overflow: int = 0
checked_in: int = 0
⋮----
class PoolConfig(BaseModel)
⋮----
"""Configuration for the async database connection pool.

    Attributes:
        url: SQLAlchemy-style async database URL.
        pool_size: Number of persistent connections.
        max_overflow: Extra connections allowed beyond pool_size.
        pool_timeout: Seconds to wait for a connection before raising.
        pool_recycle: Seconds after which a connection is recycled.
        echo: Whether to log all SQL statements.
    """
⋮----
url: str = "sqlite+aiosqlite:///saudi_stocks.db"
pool_size: int = 5
max_overflow: int = 10
pool_timeout: int = 30
pool_recycle: int = 1800
echo: bool = False
</file>

<file path="backend/services/cache/redis_client.py">
"""Async Redis client manager with connection pooling and auto-reconnection.

Provides a singleton-style RedisManager for the Ra'd AI platform that wraps
redis.asyncio with connection pooling, health checks, and graceful reconnection.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Default configuration
_DEFAULT_URL = "redis://localhost:6379/0"
_DEFAULT_MAX_CONNECTIONS = 20
_DEFAULT_SOCKET_TIMEOUT = 5.0
_DEFAULT_SOCKET_CONNECT_TIMEOUT = 5.0
_DEFAULT_RETRY_ON_TIMEOUT = True
_DEFAULT_DECODE_RESPONSES = False  # Keep bytes for msgpack serialization
⋮----
class RedisManager
⋮----
"""Async Redis client manager with connection pooling and auto-reconnect.

    Attributes:
        url: Redis connection URL.
        max_connections: Maximum pool size.
        password: Optional Redis password (overrides URL auth).
    """
⋮----
@property
    def is_connected(self) -> bool
⋮----
"""Whether the manager currently holds a live connection."""
⋮----
async def connect(self) -> None
⋮----
"""Initialize the connection pool and Redis client.

        Safe to call multiple times; subsequent calls are no-ops if already
        connected.
        """
⋮----
# Verify connectivity
⋮----
async def disconnect(self) -> None
⋮----
"""Close the Redis client and drain the connection pool."""
⋮----
async def _ensure_connection(self) -> Redis
⋮----
"""Return the Redis client, reconnecting if necessary.

        Raises:
            RuntimeError: If connect() was never called.
            RedisError: If reconnection fails.
        """
⋮----
# Attempt automatic reconnection
⋮----
# ------------------------------------------------------------------
# Core operations
⋮----
async def get(self, key: str) -> bytes | None
⋮----
"""Get a value by key.

        Args:
            key: The cache key.

        Returns:
            The raw bytes value, or None if the key does not exist.
        """
client = await self._ensure_connection()
⋮----
"""Set a key-value pair with an optional TTL.

        Args:
            key: The cache key.
            value: The value to store (bytes or str).
            ttl: Time-to-live in seconds. None means no expiry.

        Returns:
            True if the key was set successfully.
        """
⋮----
result = await client.setex(key, ttl, value)
⋮----
result = await client.set(key, value)
⋮----
async def delete(self, *keys: str) -> int
⋮----
"""Delete one or more keys.

        Args:
            keys: One or more cache keys to delete.

        Returns:
            Number of keys that were deleted.
        """
⋮----
async def exists(self, *keys: str) -> int
⋮----
"""Check if one or more keys exist.

        Args:
            keys: One or more cache keys to check.

        Returns:
            Number of provided keys that exist.
        """
⋮----
# Health
⋮----
async def health_check(self) -> dict[str, Any]
⋮----
"""Run a health check against the Redis server.

        Returns:
            A dict with keys: ``status`` (``"healthy"`` or ``"unhealthy"``),
            ``latency_ms``, ``connected``, and optionally ``error``.
        """
start = time.monotonic()
⋮----
latency_ms = round((time.monotonic() - start) * 1000, 2)
⋮----
info = await client.info(section="memory")
used_memory = info.get("used_memory_human", "unknown")
</file>

<file path="backend/services/resilience/circuit_breaker.py">
"""
Circuit breaker implementation for external service calls.

Prevents cascading failures by short-circuiting calls to services that are
experiencing repeated failures. Transitions through three states:

    CLOSED   -> calls pass through; failures are counted
    OPEN     -> calls are rejected immediately (raises CircuitBreakerOpen)
    HALF_OPEN -> a limited number of probe calls are allowed to test recovery

State transitions are logged for observability.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
T = TypeVar("T")
⋮----
class CircuitState(str, Enum)
⋮----
"""Possible states of a circuit breaker."""
⋮----
CLOSED = "closed"
OPEN = "open"
HALF_OPEN = "half_open"
⋮----
class CircuitStats(BaseModel)
⋮----
"""Observable statistics for a circuit breaker instance."""
⋮----
name: str = Field(description="Identifier of the protected service")
state: CircuitState = Field(description="Current circuit state")
failure_count: int = Field(default=0, description="Consecutive failure count")
success_count: int = Field(
total_failures: int = Field(default=0, description="Lifetime failure count")
total_successes: int = Field(default=0, description="Lifetime success count")
total_rejected: int = Field(default=0, description="Calls rejected while open")
last_failure_time: Optional[float] = Field(
last_success_time: Optional[float] = Field(
opened_at: Optional[float] = Field(
⋮----
class CircuitBreakerOpen(Exception)
⋮----
"""Raised when a call is attempted on an open circuit breaker."""
⋮----
def __init__(self, name: str, retry_after: float) -> None
⋮----
class CircuitBreaker
⋮----
"""Thread-safe circuit breaker for protecting external service calls.

    Args:
        name: Identifier for the protected service (used in logs and stats).
        failure_threshold: Number of consecutive failures before opening.
        recovery_timeout: Seconds to wait in OPEN state before transitioning
            to HALF_OPEN.
        half_open_max_calls: Number of probe calls allowed in HALF_OPEN state
            before deciding to close or re-open.
        success_threshold: Number of consecutive successes in HALF_OPEN state
            needed to transition back to CLOSED.
    """
⋮----
@property
    def state(self) -> CircuitState
⋮----
"""Return the current circuit state, considering timeout-based transitions."""
⋮----
def get_stats(self) -> CircuitStats
⋮----
"""Return a snapshot of current circuit breaker statistics."""
⋮----
async def call(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any
⋮----
"""Execute *func* through the circuit breaker.

        If the circuit is OPEN and the recovery timeout has not elapsed, raises
        ``CircuitBreakerOpen``.  In HALF_OPEN state, a limited number of probe
        calls are allowed through.

        Args:
            func: The async or sync callable to protect.
            *args: Positional arguments forwarded to *func*.
            **kwargs: Keyword arguments forwarded to *func*.

        Returns:
            The return value of *func*.

        Raises:
            CircuitBreakerOpen: If the circuit is open.
            Exception: Any exception raised by *func* (after recording the failure).
        """
⋮----
current_state = self.state
⋮----
retry_after = self.recovery_timeout - (
⋮----
# Transition from OPEN to HALF_OPEN if we got here
⋮----
# Execute outside the lock so we don't hold it during I/O
⋮----
result = await func(*args, **kwargs)
⋮----
result = func(*args, **kwargs)
⋮----
async def _record_success(self) -> None
⋮----
"""Record a successful call and potentially close the circuit."""
⋮----
now = time.monotonic()
⋮----
# Reset consecutive failure counter on success
⋮----
async def _record_failure(self, exc: Exception) -> None
⋮----
"""Record a failed call and potentially open the circuit."""
⋮----
# Any failure in half-open immediately re-opens
⋮----
def _transition(self, new_state: CircuitState) -> None
⋮----
"""Transition to a new state, logging the change."""
old_state = self._state
⋮----
async def reset(self) -> None
⋮----
"""Manually reset the circuit breaker to CLOSED state."""
⋮----
# ---------------------------------------------------------------------------
# Global registry of circuit breakers for health reporting
⋮----
_registry: dict[str, CircuitBreaker] = {}
⋮----
"""Return an existing circuit breaker or create a new one.

    This ensures a single ``CircuitBreaker`` instance per *name* across the
    application.
    """
⋮----
def get_all_stats() -> list[CircuitStats]
⋮----
"""Return stats for every registered circuit breaker."""
⋮----
def get_registry() -> dict[str, CircuitBreaker]
⋮----
"""Return the global circuit breaker registry (read-only access)."""
</file>

<file path="backend/services/resilience/degradation.py">
"""
Graceful degradation handler for external service failures.

Manages fallback behaviors when external services (Anthropic/Gemini LLM,
yfinance, Redis) are unavailable. Instead of returning 500 errors, the
system provides reduced-functionality responses.

Usage::

    manager = DegradationManager()

    manager.register_fallback(
        service="anthropic_llm",
        fallback=lambda *a, **kw: {"error": "LLM unavailable", "cached": True},
        description="Return cached/static response when LLM is down",
    )

    result = await manager.execute_with_fallback(
        service="anthropic_llm",
        func=call_anthropic_api,
        query="What is ARAMCO's P/E ratio?",
    )
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
@dataclass
class FallbackEntry
⋮----
"""A registered fallback for a service."""
⋮----
service: str
fallback: Callable[..., Any]
description: str = ""
⋮----
@dataclass
class DegradedServiceInfo
⋮----
"""Status information for a service currently in degraded mode."""
⋮----
degraded_since: float
last_error: str
fallback_invocations: int = 0
⋮----
class DegradationManager
⋮----
"""Manages fallback execution when external services are unavailable.

    Thread-safe for read operations; fallback registration should happen
    at startup before concurrent access.
    """
⋮----
def __init__(self) -> None
⋮----
"""Register a fallback function for a named service.

        Args:
            service: Unique service identifier (e.g., ``"anthropic_llm"``).
            fallback: Callable that returns a degraded response. May be sync
                or async. Receives the same ``*args, **kwargs`` as the primary
                function.
            description: Human-readable description of the fallback behavior.
        """
⋮----
"""Execute *func* with automatic fallback on failure.

        If *func* raises an exception and a fallback is registered for
        *service*, the fallback is invoked instead. The service is marked
        as degraded until a successful primary call clears the state.

        Args:
            service: Service identifier matching a registered fallback.
            func: The primary async or sync callable.
            *args: Positional arguments forwarded to both *func* and fallback.
            **kwargs: Keyword arguments forwarded to both *func* and fallback.

        Returns:
            The result of *func* on success, or the fallback result on failure.

        Raises:
            Exception: If *func* fails and no fallback is registered.
        """
⋮----
result = await func(*args, **kwargs)
⋮----
result = func(*args, **kwargs)
⋮----
# Success -- clear degraded state if previously degraded
⋮----
entry = self._fallbacks.get(service)
⋮----
# Mark as degraded
⋮----
# Execute the fallback
⋮----
def get_degraded_services(self) -> List[Dict[str, Any]]
⋮----
"""Return a list of currently degraded services with details."""
now = time.monotonic()
⋮----
def is_degraded(self, service: str) -> bool
⋮----
"""Check if a specific service is currently in degraded mode."""
⋮----
@property
    def total_fallback_calls(self) -> int
⋮----
"""Total number of times any fallback has been invoked."""
⋮----
def get_stats(self) -> Dict[str, Any]
⋮----
"""Return summary statistics for the degradation manager."""
⋮----
# ---------------------------------------------------------------------------
# Pre-configured fallbacks for known external services
⋮----
def _anthropic_fallback(*args: Any, **kwargs: Any) -> Dict[str, Any]
⋮----
"""Fallback when Anthropic/Gemini LLM is unavailable."""
⋮----
def _yfinance_fallback(*args: Any, **kwargs: Any) -> Dict[str, Any]
⋮----
"""Fallback when yfinance market data is unavailable."""
⋮----
def _redis_fallback(*args: Any, **kwargs: Any) -> None
⋮----
"""Fallback when Redis cache is unavailable -- operations become no-ops."""
⋮----
def create_default_manager() -> DegradationManager
⋮----
"""Create a ``DegradationManager`` pre-loaded with standard fallbacks.

    Registered services:
        - ``anthropic_llm``: Returns a user-friendly "unavailable" message.
        - ``yfinance``: Returns a "cached data" message.
        - ``redis``: Returns ``None`` (cache miss / no-op).
    """
manager = DegradationManager()
</file>

<file path="backend/services/resilience/retry.py">
"""
Retry and timeout decorators for resilient external service calls.

Provides two decorators:

    @with_retry   - Exponential backoff with jitter for transient failures
    @with_timeout - asyncio.wait_for wrapper with configurable deadline

Both decorators work with async functions and can be stacked.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
F = TypeVar("F", bound=Callable[..., Any])
⋮----
"""Decorator that retries an async function with exponential backoff + jitter.

    Args:
        max_attempts: Maximum number of attempts (including the first call).
        base_delay: Initial delay in seconds before the first retry.
        max_delay: Maximum delay in seconds between retries.
        exponential_base: Base for exponential backoff calculation.
        jitter: If True, adds random jitter to avoid thundering herd.
        retryable_exceptions: Tuple of exception types that trigger a retry.
            Non-matching exceptions propagate immediately.
        on_retry: Optional callback ``(attempt, exception, delay)`` called
            before each retry sleep.

    Returns:
        A decorated async function that transparently retries on failure.

    Example::

        @with_retry(max_attempts=3, retryable_exceptions=(ConnectionError, TimeoutError))
        async def fetch_data(url: str) -> dict:
            ...
    """
⋮----
def decorator(func: F) -> F
⋮----
@functools.wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> Any
⋮----
last_exception: Optional[BaseException] = None
⋮----
last_exception = exc
⋮----
# Calculate delay with exponential backoff
delay = min(
⋮----
# Add jitter: random value between 0 and delay
⋮----
delay = delay * random.uniform(0.5, 1.0)
⋮----
# Should not reach here, but just in case
⋮----
return wrapper  # type: ignore[return-value]
⋮----
"""Decorator that enforces a timeout on an async function.

    Uses ``asyncio.wait_for`` to cancel the coroutine if it exceeds the
    deadline.

    Args:
        timeout_seconds: Maximum execution time in seconds.
        timeout_message: Custom message for the ``asyncio.TimeoutError``.
            If not provided, a default message with the function name is used.

    Returns:
        A decorated async function that raises ``asyncio.TimeoutError``
        if the deadline is exceeded.

    Example::

        @with_timeout(5.0)
        async def query_database(sql: str) -> list:
            ...
    """
⋮----
msg = timeout_message or (
</file>

<file path="chart_engine/raid_chart_generator.py">
"""Ra'd AI custom chart generator with dark gold theme and improved heuristics.

Subclasses Vanna's PlotlyChartGenerator to fix:
1. 4-column table cutoff (raised to 8+)
2. Missing value heatmap (1 label + 3+ numeric)
3. Grouped bar chart using count instead of values
4. String dates not detected as datetime
5. Dark gold theme matching Ra'd AI design
"""
⋮----
# Ra'd AI gold color scale for heatmaps
GOLD_COLORSCALE = [
⋮----
# Ra'd AI brand palette
RAID_COLORWAY = [
⋮----
# Human-friendly column labels with units where applicable
COLUMN_LABELS = {
⋮----
class RaidChartGenerator(PlotlyChartGenerator)
⋮----
"""Chart generator with Ra'd AI dark gold theme and smarter heuristics."""
⋮----
def generate_chart(self, df: pd.DataFrame, title: str = "Chart") -> Dict[str, Any]
⋮----
# Step 1: Detect and convert date-like string columns
⋮----
sample = df[col].dropna().astype(str)
⋮----
# Step 2: Classify columns
numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
categorical_cols = df.select_dtypes(
datetime_cols = df.select_dtypes(include=["datetime64"]).columns.tolist()
⋮----
# Step 3: Table fallback only for very wide results (8+ columns)
⋮----
fig = self._create_table(df, title)
# Remove Plotly title to avoid duplication with the card-level title
⋮----
# Step 4: Apply heuristics (order matters)
⋮----
# Time series (date + numeric)
⋮----
fig = self._create_time_series_chart(
⋮----
# Value heatmap: 1 text label + 3+ numeric columns
⋮----
fig = self._create_value_heatmap(
⋮----
# Single numeric: histogram
⋮----
fig = self._create_histogram(df, numeric_cols[0], title)
⋮----
# 1 categorical + 1 numeric: bar chart
⋮----
fig = self._create_bar_chart(
⋮----
# 2 numeric: scatter
⋮----
fig = self._create_scatter_plot(df, numeric_cols[0], numeric_cols[1], title)
⋮----
# 1 categorical + 2 numeric: bar chart with first numeric
⋮----
# Only numeric (3+): correlation heatmap
⋮----
fig = self._create_correlation_heatmap(df, numeric_cols, title)
⋮----
# Multiple categorical: grouped bar (fixed)
⋮----
fig = self._create_grouped_bar_chart(df, categorical_cols, title)
⋮----
# Fallback
⋮----
fig = self._create_generic_chart(df, df.columns[0], df.columns[1], title)
⋮----
# ------------------------------------------------------------------
# Dark gold theme
⋮----
def _apply_standard_layout(self, fig: go.Figure) -> go.Figure
⋮----
# New: value heatmap
⋮----
"""Heatmap where each cell shows the actual metric value, color-scaled via z-score."""
labels = df[label_col].astype(str).tolist()
raw_values = df[numeric_cols].values.astype(float)
⋮----
# Z-score normalize each column independently for comparable color mapping
⋮----
means = np.nanmean(raw_values, axis=0)
stds = np.nanstd(raw_values, axis=0)
stds[stds == 0] = 1.0  # avoid division by zero
z_values = (raw_values - means) / stds
# Replace any remaining NaN in z_values with 0 (neutral color)
z_values = np.nan_to_num(z_values, nan=0.0)
⋮----
# Build text annotations with formatted actual values
# Use percentage formatting for percentage columns
pct_flags = [self._is_percentage_column(c) for c in numeric_cols]
text_matrix = []
⋮----
text_row = []
⋮----
humanized_cols = [self._humanize_header(c) for c in numeric_cols]
⋮----
fig = go.Figure(
⋮----
# Fix: grouped bar chart using actual values when numeric col exists
⋮----
# Use actual numeric values instead of counting
value_col = numeric_cols[0]
grouped = df.groupby(categorical_cols[:2])[value_col].sum().reset_index()
fig = go.Figure()
⋮----
# Fall back to parent behaviour for purely categorical data
⋮----
# Override table with dark theme
⋮----
def _create_table(self, df: pd.DataFrame, title: str) -> go.Figure
⋮----
df = df.fillna(pd.NA)  # normalize NaN types
header_values = [self._humanize_header(col) for col in df.columns]
⋮----
# Build per-column cell values with formatting, and per-column fill colors
cell_values = []
fill_colors = []
font_colors = []
base_stripe = ["#141414" if i % 2 == 0 else "#1a1a1a" for i in range(len(df))]
base_font = ["#E0E0E0"] * len(df)
⋮----
raw = df[col].tolist()
is_pct = self._is_percentage_column(col)
is_change = self._is_change_column(col)
⋮----
# Format numeric values nicely
formatted = []
⋮----
s = str(v)
⋮----
# Conditional coloring: green for positive, red for negative
col_fills = []
col_fonts = []
⋮----
fv = float(v) if v is not None else None
⋮----
fv = None
⋮----
# Override: bar chart with label rotation and theme
⋮----
"""Bar chart with horizontal orientation for 8+ items, label rotation for medium sets."""
labels = df[x_col].astype(str).tolist()
⋮----
# Horizontal bar chart for ranked lists - prevents label overlap
⋮----
yaxis=dict(autorange="reversed"),  # Keep original order (first = top)
⋮----
# For smaller datasets, use vertical bar with rotation if needed
fig = super()._create_bar_chart(df, x_col, y_col, title)
avg_len = sum(len(lbl) for lbl in labels) / max(len(labels), 1)
⋮----
# Utility
⋮----
# Common financial acronyms that should stay uppercase
_ACRONYMS = {"roe", "roa", "pe", "eps", "ebitda", "ebit", "ppe", "cfo", "cfi"}
⋮----
# Keywords that indicate a percentage column
_PCT_KEYWORDS = {
⋮----
# Keywords that indicate a value-change column (for conditional coloring)
_CHANGE_KEYWORDS = {"growth", "change", "return"}
⋮----
@staticmethod
    def _humanize_header(col_name: str) -> str
⋮----
"""Convert snake_case column names to human-friendly labels."""
⋮----
parts = col_name.split("_")
result = []
⋮----
@staticmethod
    def _is_percentage_column(col_name: str) -> bool
⋮----
"""Return True if column name suggests percentage data."""
lower = col_name.lower()
⋮----
@staticmethod
    def _is_change_column(col_name: str) -> bool
⋮----
"""Return True if column name suggests a growth/change value."""
⋮----
@staticmethod
    def _format_percentage(val) -> str
⋮----
"""Format a decimal value as percentage string. 0.2171 -> '21.71%'."""
⋮----
return "\u2014"  # em dash
⋮----
@staticmethod
    def _format_number(val: float) -> str
⋮----
"""Format numbers nicely: 1500000000 -> '1.5B', 0.2171 -> '0.2171'."""
</file>

<file path="config/env_validator.py">
"""
Startup environment variable validation.

Validates critical environment variables before the application loads
pydantic settings. Provides clear, actionable error messages for
missing or invalid configuration.

Usage:
    from config.env_validator import validate_env
    errors, warnings = validate_env()
    if errors:
        for e in errors:
            print(f"ERROR: {e}")
        sys.exit(1)
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_VALID_DB_BACKENDS = {"sqlite", "postgres"}
_VALID_LOG_LEVELS = {"DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"}
⋮----
def validate_env() -> Tuple[List[str], List[str]]
⋮----
"""Validate environment variables at startup.

    Returns:
        Tuple of (errors, warnings).
        errors: list of critical issues that should prevent startup.
        warnings: list of non-critical issues to log.
    """
errors: List[str] = []
warnings: List[str] = []
⋮----
# --- DB_BACKEND ---
db_backend = os.environ.get("DB_BACKEND", "sqlite").lower()
⋮----
# --- PostgreSQL vars (required when DB_BACKEND=postgres) ---
⋮----
pg_password = os.environ.get("POSTGRES_PASSWORD") or os.environ.get(
⋮----
pg_host = os.environ.get("POSTGRES_HOST") or os.environ.get("DB_PG_HOST")
⋮----
# --- LLM API key ---
llm_key = (
⋮----
# --- LOG_LEVEL ---
log_level = os.environ.get("LOG_LEVEL", "").upper()
⋮----
# --- AUTH_JWT_SECRET in production ---
environment = os.environ.get("ENVIRONMENT", "development").lower()
⋮----
jwt_secret = os.environ.get("AUTH_JWT_SECRET")
⋮----
# --- CORS origins ---
cors = os.environ.get("MW_CORS_ORIGINS", "")
⋮----
def validate_and_log() -> bool
⋮----
"""Run validation and log results.

    Returns:
        True if no critical errors, False otherwise.
    """
</file>

<file path="config/logging_config.py">
"""
Structured logging configuration for TASI AI Platform.

Provides JSON-structured logging for production and human-readable output
for development. Integrates with config/settings.py and coordinates with
middleware/request_logging.py for consistent log formatting.

Usage:
    from config.logging_config import setup_logging, get_logger

    setup_logging()  # Call once at startup
    logger = get_logger(__name__)
    logger.info("Server started", extra={"port": 8084})

Environment variables:
    LOG_LEVEL          - Root log level (default: INFO)
    IS_DEVELOPMENT     - "true" for dev-friendly output (also checks SERVER_DEBUG)
    SERVER_ENVIRONMENT - "development" enables pretty logging
"""
⋮----
class JsonFormatter(logging.Formatter)
⋮----
"""Structured JSON log formatter for production.

    Produces one JSON object per line with fields:
    timestamp, level, logger, message, and optional exception/extra fields.
    Compatible with Railway log aggregation and common log parsers.
    """
⋮----
def format(self, record: logging.LogRecord) -> str
⋮----
log_entry = {
⋮----
# Include exception traceback if present
⋮----
# Include extra fields from middleware (e.g. request_id, duration_ms)
_skip = {
⋮----
class PrettyFormatter(logging.Formatter)
⋮----
"""Human-readable log formatter for development.

    Format: HH:MM:SS | LEVEL    | logger.name | message
    Aligned with middleware/request_logging.py output style.
    """
⋮----
FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
⋮----
def __init__(self)
⋮----
def _is_dev_mode() -> bool
⋮----
"""Determine if running in development mode.

    Checks (in order):
    1. IS_DEVELOPMENT env var ("true", "1", "yes")
    2. SERVER_DEBUG env var ("true", "1", "yes")
    3. SERVER_ENVIRONMENT / ENVIRONMENT env var (== "development")

    Returns True if any indicate development mode.
    """
truthy = ("true", "1", "yes")
⋮----
env = os.environ.get(
⋮----
# Noisy third-party loggers to suppress (set to WARNING)
_NOISY_LOGGERS = [
⋮----
"""Configure the root logger for the application.

    Call this once during application startup (e.g. in the FastAPI lifespan).
    Safe to call multiple times; handlers are cleared before reconfiguration.

    Args:
        level: Log level string (DEBUG, INFO, WARNING, ERROR, CRITICAL).
               Falls back to LOG_LEVEL env var, then defaults to INFO.
        json_output: If True, use JSON formatter. If False, use pretty formatter.
                     If None, auto-detect from environment (dev = pretty, prod = JSON).
    """
log_level = (level or os.environ.get("LOG_LEVEL", "INFO")).upper()
⋮----
json_output = not _is_dev_mode()
⋮----
# Configure root logger
root = logging.getLogger()
⋮----
# Remove existing handlers to avoid duplicates on re-init
⋮----
handler = logging.StreamHandler(sys.stdout)
⋮----
# Suppress noisy third-party loggers
⋮----
def get_logger(name: str) -> logging.Logger
⋮----
"""Return a named logger for the given module.

    Convenience wrapper that ensures consistent logger naming across
    the codebase. Typically called as:

        logger = get_logger(__name__)

    Args:
        name: Logger name, usually __name__ of the calling module.

    Returns:
        A configured logging.Logger instance.
    """
</file>

<file path="database/__init__.py">
# Database package for TASI AI platform
⋮----
__all__ = ["DatabaseManager", "get_database_manager"]
</file>

<file path="database/csv_to_postgres.py">
"""
csv_to_postgres.py
==================
Converts the flat denormalized CSV file 'saudi_stocks_yahoo_data.csv' (500 stocks,
1062 columns) into a normalized PostgreSQL database.

Adapted from csv_to_sqlite.py with identical column mappings and unpivot logic.
Supports both initial load (create) and incremental upsert modes.

Tables created/populated:
  - companies             (core company info)
  - market_data           (price / volume / shares)
  - valuation_metrics     (PE, PB, EV ratios)
  - profitability_metrics (margins, growth)
  - dividend_data         (dividends)
  - financial_summary     (key financial aggregates)
  - analyst_data          (targets, recommendations)
  - balance_sheet         (unpivoted, multiple rows per ticker)
  - income_statement      (unpivoted, multiple rows per ticker)
  - cash_flow             (unpivoted, multiple rows per ticker)
  - sectors               (populated from unique sectors)
  - entities              (populated from companies)

Usage:
    # Initial load (applies schema, truncates, inserts)
    python database/csv_to_postgres.py

    # Upsert mode (updates existing rows, inserts new)
    python database/csv_to_postgres.py --upsert

    # Dry run
    python database/csv_to_postgres.py --dry-run

    # Custom CSV path
    python database/csv_to_postgres.py --csv-path /path/to/data.csv

Environment variables:
    PG_HOST, PG_PORT, PG_DBNAME, PG_USER, PG_PASSWORD
"""
⋮----
psycopg2 = None
⋮----
# ---------------------------------------------------------------------------
# Configuration
⋮----
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
CSV_PATH = PROJECT_DIR / "saudi_stocks_yahoo_data.csv"
SCHEMA_SQL_PATH = SCRIPT_DIR / "schema.sql"
BATCH_SIZE = 250
⋮----
# Column mappings (identical to csv_to_sqlite.py)
# Keys   = target column name in PostgreSQL
# Values = source column name in the CSV
⋮----
COMPANIES_COLS = {
⋮----
MARKET_DATA_COLS = {
⋮----
VALUATION_COLS = {
⋮----
PROFITABILITY_COLS = {
⋮----
DIVIDEND_COLS = {
⋮----
FINANCIAL_SUMMARY_COLS = {
⋮----
ANALYST_COLS = {
⋮----
# Financial statement field lists (identical to csv_to_sqlite.py)
⋮----
BS_FIELDS = [
⋮----
IS_FIELDS = [
⋮----
CF_FIELDS = [
⋮----
BS_PERIODS = {
⋮----
IS_PERIODS = {
⋮----
CF_PERIODS = {
⋮----
# Helper functions
⋮----
def extract_simple_table(df: pd.DataFrame, col_map: dict) -> pd.DataFrame
⋮----
"""Extract a subset of columns from the master DataFrame, renaming as needed."""
src_cols = list(col_map.values())
existing = [c for c in src_cols if c in df.columns]
missing = [c for c in src_cols if c not in df.columns]
⋮----
sub = df[existing].copy()
rename_map = {v: k for k, v in col_map.items() if v in existing}
⋮----
"""Unpivot flat financial columns into normalized rows.

    Identical logic to csv_to_sqlite.py.
    """
all_rows = []
tickers = df["ticker"].values
total = len(tickers)
⋮----
date_col = f"{prefix}_date"
⋮----
src_cols = [f"{prefix}_{f}" for f in fields]
⋮----
existing_src = [c for c in src_cols if c in df.columns]
existing_tgt = [
⋮----
dates = df[date_col].values
⋮----
data_block = df[existing_src].values
⋮----
date_val = dates[row_idx]
⋮----
row_dict = {
⋮----
val = data_block[row_idx, col_idx]
⋮----
# Fill missing columns with None
missing_src = [c for c in src_cols if c not in df.columns]
⋮----
field_name = c.split(f"{prefix}_", 1)[1].lower()
⋮----
def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame
⋮----
"""Replace NaN/Inf with None for proper NULL storage in PostgreSQL."""
df = df.replace([np.inf, -np.inf], np.nan)
df = df.where(pd.notnull(df), None)
⋮----
def df_to_tuples(df: pd.DataFrame) -> list
⋮----
"""Convert DataFrame rows to list of tuples for psycopg2."""
⋮----
def build_insert_sql(table: str, columns: list) -> str
⋮----
"""Build a parameterized INSERT statement."""
cols_str = ", ".join(columns)
placeholders = ", ".join(["%s"] * len(columns))
⋮----
def build_upsert_sql(table: str, columns: list, pk_columns: list) -> str
⋮----
"""Build an INSERT ... ON CONFLICT DO UPDATE statement."""
⋮----
pk_str = ", ".join(pk_columns)
update_cols = [c for c in columns if c not in pk_columns]
updates = ", ".join([f"{c} = EXCLUDED.{c}" for c in update_cols])
⋮----
# Primary keys for upsert mode
TABLE_PKS = {
⋮----
# Financial statement tables use (ticker, period_type, period_index) as logical key for upsert
FIN_UPSERT_KEY = ["ticker", "period_type", "period_index", "period_date"]
⋮----
# Insert/Upsert logic
⋮----
def insert_batch(pg_conn, sql: str, rows: list, batch_size: int) -> int
⋮----
"""Insert rows in batches. Returns total rows inserted."""
cur = pg_conn.cursor()
total = 0
⋮----
batch = rows[i : i + batch_size]
⋮----
"""Extract, clean, and load a simple table."""
sub = extract_simple_table(df, col_map)
sub = clean_dataframe(sub)
columns = list(sub.columns)
rows = df_to_tuples(sub)
⋮----
sql = build_upsert_sql(table, columns, TABLE_PKS[table])
⋮----
sql = build_insert_sql(table, columns)
⋮----
count = insert_batch(pg_conn, sql, rows, BATCH_SIZE)
⋮----
"""Unpivot and load a financial statement table."""
result_df = unpivot_financial(df, periods, fields, table)
⋮----
result_df = clean_dataframe(result_df)
columns = list(result_df.columns)
rows = df_to_tuples(result_df)
⋮----
sql = build_upsert_sql(table, columns, FIN_UPSERT_KEY)
⋮----
def populate_sectors(df: pd.DataFrame, pg_conn, dry_run: bool) -> dict
⋮----
"""Insert unique sectors and return name->id mapping."""
sectors = sorted(df["sector"].dropna().unique())
⋮----
sector_map = {}
⋮----
result = cur.fetchone()
⋮----
"""Populate entities table from companies data."""
rows = []
⋮----
ticker = row.get("ticker")
short_name = row.get("short_name")
sector = row.get("sector")
sector_id = sector_map.get(sector) if pd.notna(sector) else None
⋮----
sql = (
⋮----
# Main
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser(
⋮----
def main()
⋮----
args = parse_args()
t_start = time.time()
⋮----
mode = "DRY RUN" if args.dry_run else ("UPSERT" if args.upsert else "INSERT")
⋮----
# Read CSV
csv_path = Path(args.csv_path)
⋮----
df = pd.read_csv(str(csv_path), encoding="utf-8-sig", low_memory=False)
⋮----
# Connect to PostgreSQL
pg_conn = None
⋮----
pg_conn = psycopg2.connect(
⋮----
# Step 1: Apply schema
⋮----
schema_sql = SCHEMA_SQL_PATH.read_text(encoding="utf-8")
⋮----
# Step 2: Truncate tables (unless upsert or skip-truncate)
tables_to_truncate = [
⋮----
reason = (
⋮----
# Step 3: Load simple tables
⋮----
simple_tables = [
⋮----
total_rows = 0
table_counts = {}
⋮----
count = load_simple_table(
⋮----
# Step 4: Load financial statement tables
⋮----
fin_tables = [
⋮----
count = load_financial_table(
⋮----
# Step 5: Populate reference tables
⋮----
sector_map = populate_sectors(df, pg_conn, args.dry_run)
⋮----
entities_count = populate_entities(df, sector_map, pg_conn, args.dry_run)
⋮----
# Summary
elapsed = time.time() - t_start
</file>

<file path="database/manager.py">
"""
Centralized Database Manager
=============================
Encapsulates connection handling for both SQLite and PostgreSQL backends.
Provides context-managed connections with automatic commit/rollback.

Usage::

    from database.manager import get_database_manager

    db = get_database_manager()

    # Context-managed (auto commit/rollback):
    with db.connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT 1")

    # As a FastAPI generator dependency:
    def get_db():
        yield from db.get_connection_dependency()
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
class DatabaseManager
⋮----
"""Centralized database connection factory.

    Supports both SQLite and PostgreSQL backends. For PostgreSQL, uses the
    connection pool when available, falling back to direct connections.

    Parameters
    ----------
    backend : str
        Either "sqlite" or "postgres".
    sqlite_path : str or None
        Path to SQLite database file (required when backend is "sqlite").
    pg_settings : object or None
        DatabaseSettings-like object with pg_host, pg_port, pg_database,
        pg_user, pg_password attributes (required when backend is "postgres").
    """
⋮----
@property
    def backend(self) -> str
⋮----
def _get_raw_connection(self)
⋮----
"""Return a raw database connection (caller must manage lifecycle)."""
⋮----
conn = sqlite3.connect(self._sqlite_path)
⋮----
# PostgreSQL: prefer pool, fall back to direct connection
⋮----
@contextmanager
    def connection(self)
⋮----
"""Context manager that yields a connection.

        Commits on clean exit, rolls back on exception, and always closes
        the connection (returning it to pool if applicable).

        Usage::

            with db_manager.connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT 1")
        """
conn = self._get_raw_connection()
⋮----
@asynccontextmanager
    async def aconnection(self)
⋮----
"""Async context manager that runs the sync connection in a thread.

        Usage::

            async with db_manager.aconnection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT 1")
        """
conn = await asyncio.to_thread(self._get_raw_connection)
⋮----
def get_connection_dependency(self) -> Generator
⋮----
"""FastAPI-compatible generator dependency.

        Yields a connection and ensures cleanup on exit. Unlike
        ``connection()``, this does NOT auto-commit -- callers that need
        to write should commit explicitly. Read-only queries need no action.

        Usage::

            @router.get("/items")
            def list_items(conn=Depends(db_manager.get_connection_dependency)):
                ...
        """
⋮----
@lru_cache(maxsize=1)
def get_database_manager() -> DatabaseManager
⋮----
"""Return a cached singleton DatabaseManager configured from app settings.

    Falls back gracefully if config module is unavailable (e.g., in tests).
    """
⋮----
settings = get_settings()
</file>

<file path="database/migrate_sqlite_to_pg.py">
"""
migrate_sqlite_to_pg.py
=======================
Migrates all data from the SQLite database (saudi_stocks.db) into PostgreSQL.

Reads the 10 existing SQLite tables, maps types (REAL->NUMERIC, INTEGER->BIGINT),
handles NaN->NULL, populates the new 'sectors' and 'entities' tables from the
companies table, and inserts data in batches.

Usage:
    # Dry run (prints SQL, does not write)
    python database/migrate_sqlite_to_pg.py --dry-run

    # Full migration (requires PG connection)
    python database/migrate_sqlite_to_pg.py

    # With custom connection
    python database/migrate_sqlite_to_pg.py --pg-host localhost --pg-port 5432 \\
        --pg-dbname radai --pg-user radai --pg-password secret

    # Custom batch size
    python database/migrate_sqlite_to_pg.py --batch-size 500

Environment variables (override with CLI flags):
    PG_HOST, PG_PORT, PG_DBNAME, PG_USER, PG_PASSWORD
"""
⋮----
psycopg2 = None
⋮----
# ---------------------------------------------------------------------------
# Configuration
⋮----
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
SQLITE_DB_PATH = PROJECT_DIR / "saudi_stocks.db"
SCHEMA_SQL_PATH = SCRIPT_DIR / "schema.sql"
⋮----
# Tables to migrate in dependency order (companies first for FK references)
TABLES_ORDERED = [
⋮----
# Columns that are INTEGER in SQLite and should remain BIGINT in PG
# (volumes, counts - not the SERIAL 'id' columns which are auto-generated)
BIGINT_COLUMNS = {
⋮----
# The 'id' column in financial statement tables is SERIAL in PG
# and must be excluded from INSERT so the sequence auto-generates
SERIAL_ID_TABLES = {"balance_sheet", "income_statement", "cash_flow"}
⋮----
DEFAULT_BATCH_SIZE = 250
⋮----
# Helpers
⋮----
def get_sqlite_connection(db_path: Path) -> sqlite3.Connection
⋮----
"""Open a read-only SQLite connection."""
⋮----
conn = sqlite3.connect(str(db_path))
⋮----
def get_sqlite_columns(conn: sqlite3.Connection, table: str) -> list
⋮----
"""Return column names for a SQLite table."""
rows = conn.execute(f"PRAGMA table_info({table})").fetchall()
⋮----
def clean_value(val)
⋮----
"""Convert SQLite values for PostgreSQL compatibility.

    - NaN (float) -> None (NULL)
    - Infinity -> None (NULL)
    - Empty strings for numeric context -> None
    """
⋮----
def build_insert_sql(table: str, columns: list) -> str
⋮----
"""Build a parameterized INSERT statement."""
cols_str = ", ".join(columns)
placeholders = ", ".join(["%s"] * len(columns))
⋮----
def extract_sectors(sqlite_conn: sqlite3.Connection) -> list
⋮----
"""Extract unique sectors from the companies table for the sectors reference table."""
rows = sqlite_conn.execute(
⋮----
def extract_entities(sqlite_conn: sqlite3.Connection) -> list
⋮----
"""Extract data for the entities table from companies."""
⋮----
# Migration logic
⋮----
"""Migrate a single table from SQLite to PostgreSQL.

    Returns the number of rows migrated.
    """
# Get columns from SQLite
all_columns = get_sqlite_columns(sqlite_conn, table)
⋮----
# For SERIAL id tables, skip the 'id' column (PG auto-generates)
⋮----
columns = [c for c in all_columns if c != "id"]
⋮----
columns = all_columns
⋮----
# Read all rows from SQLite
rows = sqlite_conn.execute(f"SELECT * FROM {table}").fetchall()
total = len(rows)
⋮----
# Build INSERT SQL
insert_sql = build_insert_sql(table, columns)
⋮----
sample = rows[0]
sample_vals = tuple(clean_value(sample[c]) for c in columns)
⋮----
# Batch insert
pg_cur = pg_conn.cursor()
inserted = 0
⋮----
batch_end = min(batch_start + batch_size, total)
batch_rows = rows[batch_start:batch_end]
⋮----
values_list = []
⋮----
vals = tuple(clean_value(row[c]) for c in columns)
⋮----
"""Populate the sectors table from unique sectors in companies.

    Returns a dict mapping sector name -> sector id.
    """
sectors = extract_sectors(sqlite_conn)
sector_map = {}
⋮----
result = pg_cur.fetchone()
⋮----
"""Populate the entities table from companies data."""
entities = extract_entities(sqlite_conn)
total = len(entities)
⋮----
insert_sql = (
⋮----
sector_id = sector_map.get(sector)
⋮----
def apply_schema(pg_conn, dry_run: bool) -> None
⋮----
"""Apply the PostgreSQL schema from schema.sql."""
⋮----
schema_sql = SCHEMA_SQL_PATH.read_text(encoding="utf-8")
⋮----
# Main
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser(
⋮----
def main()
⋮----
args = parse_args()
t_start = time.time()
⋮----
# Open SQLite
sqlite_conn = get_sqlite_connection(Path(args.sqlite_path))
⋮----
# Open PostgreSQL (or skip if dry run)
pg_conn = None
⋮----
pg_conn = psycopg2.connect(
⋮----
# Step 1: Apply schema
⋮----
# Step 2: Migrate existing 10 tables
⋮----
total_rows = 0
table_counts = {}
⋮----
count = migrate_table(
⋮----
# Step 3: Populate sectors reference table
⋮----
sector_map = populate_sectors(sqlite_conn, pg_conn, args.dry_run)
⋮----
# Step 4: Populate entities from companies
entities_count = populate_entities(
⋮----
# Summary
elapsed = time.time() - t_start
⋮----
grand_total = total_rows + len(sector_map) + entities_count
</file>

<file path="database/postgres_utils.py">
"""
PostgreSQL utility helpers shared across the codebase.

These helpers are intentionally lightweight and import-safe:
they only import psycopg2 inside functions so the module can be
imported even when psycopg2 is not installed (SQLite-only installs).
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
def pg_available(timeout: int = 3) -> bool
⋮----
"""Return True if PostgreSQL is reachable via current env vars.

    Checks POSTGRES_HOST first; returns False immediately if unset.
    Attempts a real connection with *timeout* seconds connect_timeout.
    Logs a debug message on failure to avoid noisy test output.
    """
⋮----
conn = psycopg2.connect(
⋮----
def pg_connection_params() -> dict
⋮----
"""Return a dict of psycopg2 connection parameters from environment.

    Useful for building connection strings without repeating env-var logic.
    Does NOT include connect_timeout; callers add as needed.

    Example::

        import psycopg2
        from database.postgres_utils import pg_connection_params
        conn = psycopg2.connect(**pg_connection_params(), connect_timeout=5)
    """
⋮----
port = int(os.environ.get("POSTGRES_PORT", "5432"))
⋮----
port = 5432
</file>

<file path="database/queries.py">
"""
Centralized SQL query constants.

Named SQL strings used across route handlers. Keeping them here avoids
scattering raw SQL throughout the codebase and makes queries easier to
audit, test, and maintain.
"""
⋮----
# ---------------------------------------------------------------------------
# stock_data queries
⋮----
COMPANY_EXISTS = "SELECT 1 FROM companies WHERE ticker = ?"
⋮----
DIVIDEND_DATA_BY_TICKER = (
⋮----
FINANCIAL_SUMMARY_BY_TICKER = (
⋮----
COMPANY_NAMES_BY_TICKERS = (
⋮----
BATCH_QUOTES_SQL = """
⋮----
# market_analytics queries
⋮----
MOVERS_BASE = """
⋮----
MARKET_SUMMARY_AGGREGATES = """
⋮----
SECTOR_ANALYTICS = """
⋮----
HEATMAP = """
⋮----
# charts_analytics queries
⋮----
SECTOR_MARKET_CAP = """
⋮----
SECTOR_AVG_PE = """
⋮----
DIVIDEND_YIELD_TOP = """
⋮----
# sqlite_entities queries
⋮----
SECTOR_LIST = """
⋮----
ENTITY_FULL_DETAIL = """
</file>

<file path="frontend/.gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js
.yarn/install-state.gz

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

.vercel
</file>

<file path="frontend/e2e/tests/news-portal.spec.ts">
import { test, expect, type Page, type Route } from '@playwright/test';
⋮----
// ---------------------------------------------------------------------------
// Test data factory
// ---------------------------------------------------------------------------
⋮----
function makeArticle(index: number, overrides: Record<string, unknown> =
⋮----
function makeFeedResponse(count: number, total?: number)
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/** Intercept all news-related API calls with standard mocks. */
async function mockNewsApis(page: Page, articleCount = 20, total?: number)
⋮----
// News feed endpoint (GET /api/v1/news/feed?...)
⋮----
// Don't intercept batch endpoint — only the main feed
⋮----
// News sources endpoint
⋮----
// News search endpoint
⋮----
// Batch endpoint
⋮----
// Health endpoint (Header component polls this)
⋮----
/**
 * Intercept the SSE stream endpoint and return a controllable mock.
 *
 * The returned `emit` function pushes an SSE `data:` frame to the client.
 * Call `close()` to end the stream.
 */
async function mockSseStream(page: Page)
⋮----
// We'll collect route handlers and resolve them manually so we can
// stream data frame-by-frame.
⋮----
/** Wait until the browser actually opens the EventSource connection. */
⋮----
/**
     * Fulfill the SSE route with an initial comment + one data frame.
     * Playwright `route.fulfill` sends the whole body at once, so we
     * compose the full SSE payload upfront and fulfill once.
     */
⋮----
/** Switch the app to Arabic by clicking the language toggle in the Header. */
async function switchToArabic(page: Page)
⋮----
// The toggle button shows "عربي" when in English mode (clicking switches to Arabic).
// Its aria-label is "Switch to Arabic" when in English mode.
⋮----
// If button doesn't exist, we may already be in Arabic mode
⋮----
// Wait for dir attribute to flip
⋮----
// Already in Arabic — verify
⋮----
// ===========================================================================
// Test Suite
// ===========================================================================
⋮----
// -------------------------------------------------------------------------
// 1. RTL Layout Check
// -------------------------------------------------------------------------
⋮----
// Navigate to news page
⋮----
// Switch to Arabic
⋮----
// Verify <html dir="rtl"> is set
⋮----
// Wait for at least one article card to render
⋮----
// The ArticleCard sets `borderInlineEndWidth: '4px'` via inline style.
// In RTL mode (dir="rtl"), border-inline-end maps to border-left.
// Verify the inline style is present.
⋮----
// Check the actual inline style property
⋮----
// Wait for transition-all duration-200 on <article> to settle
⋮----
// Verify the computed border-inline-end-width resolves to 4px
⋮----
// Verify text direction via computed style on the page body
⋮----
// Verify the article title text is right-aligned (implicit from dir="rtl")
⋮----
// In RTL, the default text-align is 'start' which resolves to 'right'
⋮----
// -------------------------------------------------------------------------
// 2. Virtual Scroll Performance
// -------------------------------------------------------------------------
⋮----
// Mock 50 articles but tell the API total=50 so all are "loaded"
⋮----
// First page returns 50 articles (simulate "all loaded" scenario)
⋮----
// Wait for articles to render
⋮----
// The virtualizer uses @tanstack/react-virtual with overscan=5.
// With 3 columns at xl / 2 at md / 1 at mobile, the number of
// visible rows is limited. Even at 1 column, ~6-8 rows are visible
// in the viewport plus overscan=5 on each side = ~16-18 rows max.
// At 50 articles / 1 col, we expect significantly fewer than 50
// <article> elements in the DOM.
//
// At wider viewports with 2-3 cols, rows hold 2-3 articles each,
// so even fewer rows are needed — but each row renders all its
// articles. We conservatively check that DOM count < 40.
⋮----
// Verify the virtualizer container structure exists.
// The virtualizer renders a position:relative wrapper with a calculated height.
⋮----
// Check that the virtualizer has a calculated total height
⋮----
// If the virtualizer is active (scroll element found), DOM count < total.
// In headless CI without constrained layout, all items may render.
// We verify the structure is correct rather than strict DOM count.
⋮----
// -------------------------------------------------------------------------
// 3. Real-Time SSE — "New Articles Available" Banner
// -------------------------------------------------------------------------
⋮----
// Mock the feed API with initial articles
⋮----
// Set up SSE mock BEFORE navigating (so the route is intercepted)
⋮----
// Use 'domcontentloaded' — SSE mock holds the connection open, preventing 'networkidle'
⋮----
// Wait for articles to load
⋮----
// Wait for the EventSource to connect, then push a "new articles" event
⋮----
// The NewArticlesBanner renders inside a div[role="status"][aria-live="polite"]
// and contains text like "2 new articles - tap to refresh" (English)
// or "2 أخبار جديدة - اضغط للتحديث" (Arabic).
⋮----
// Check the banner text indicates 2 new articles
⋮----
// Should contain "new articles" or "أخبار جديدة"
⋮----
// Click the banner to dismiss and refresh
⋮----
// After clicking, the banner should disappear (count resets to 0)
⋮----
// -------------------------------------------------------------------------
// 4. Filter Interaction — Source Chip
// -------------------------------------------------------------------------
⋮----
// Track which source parameter was sent to the API
⋮----
// Return source-specific articles if a filter is active
⋮----
// Wait for initial articles to render
⋮----
// The source filter chips are buttons inside a div[role="group"].
// Each chip is a <button> with the Arabic source name as text.
// Click the "أرقام" (Argaam) source chip.
⋮----
// Verify the chip is now pressed (aria-pressed="true")
⋮----
// Wait for the feed to reload with the filtered source
⋮----
// Verify the API was called with source=أرقام
⋮----
// Verify the SSE stream was reconnected with the source filter
// (the EventSource URL should include ?source=أرقام)
// This is implicitly tested by the route being called.
⋮----
// All rendered articles should now be from أرقام.
// SourceBadge and SentimentBadge both use span.rounded-full;
// SentimentBadge appears first in DOM order. Search ALL badges for the source name.
⋮----
// Click "الكل" (All) to clear the filter
⋮----
// Verify أرقام chip is no longer pressed
</file>

<file path="frontend/scripts/lint-rtl.js">
/**
 * RTL lint script — catches physical Tailwind CSS properties that should use
 * logical equivalents for proper RTL/LTR support.
 *
 * Violations:
 *   ml-* -> ms-*    mr-* -> me-*
 *   pl-* -> ps-*    pr-* -> pe-*
 *   text-left -> text-start    text-right -> text-end
 *   border-l-* -> border-s-*  border-r-* -> border-e-*
 *   left-* -> start-*         right-* -> end-*
 *   rounded-l-* -> rounded-s-* rounded-r-* -> rounded-e-*
 *
 * Usage: node scripts/lint-rtl.js [--fix]
 */
⋮----
// Directories to skip
⋮----
// Physical -> Logical mapping (for --fix mode and reporting)
⋮----
// Combined detection regex (used for fast line-level scanning)
⋮----
// Lines containing these patterns are exempt from left-/right- violations
// (centering transforms, intentional LTR overrides for charts/code)
⋮----
/dir\s*=\s*["']ltr["']/,              // intentional LTR override
/left-1\/2/,                           // transform-based centering (direction-neutral)
/-translate-x-1\/2/,                   // paired with left-1/2 for centering
/inset-x-/,                            // already using logical inset
⋮----
function isExemptLine(line)
⋮----
function walkFiles(dir)
⋮----
function scanFile(filePath)
⋮----
// Skip comment-only lines
⋮----
// Skip import lines
⋮----
// Skip lines with intentional LTR overrides or centering patterns
⋮----
// Identify which specific violations are on this line
⋮----
// Reset lastIndex since we use /g flag
⋮----
// ---- main ----
</file>

<file path="frontend/sentry.client.config.ts">
// Sentry client-side configuration for Next.js
//
// IMPORTANT: next.config.mjs must wrap the config with withSentryConfig:
//   import { withSentryConfig } from '@sentry/nextjs';
//   export default analyze(withSentryConfig(nextConfig, { /* sentry options */ }));
// The bundle analyzer should remain the outermost wrapper.
⋮----
beforeSend(event)
⋮----
// Filter out noisy ResizeObserver errors
⋮----
// Filter out cancelled fetch requests (user navigated away)
⋮----
// Strip auth headers so JWT tokens are never sent to Sentry
⋮----
// Strip token values from extra context (e.g., localStorage snapshots)
</file>

<file path="frontend/src/app/admin/AdminPage.tsx">
import { useCallback, useEffect, useRef, useState } from 'react';
import { cn } from '@/lib/utils';
import { RoleGuard } from '@/components/auth/RoleGuard';
import { metricsCollector, type FrontendMetrics } from '@/lib/monitoring/metrics-collector';
import { getHealth, type HealthResponse } from '@/lib/api-client';
⋮----
function useAutoRefresh<T>(fetcher: (signal: AbortSignal) => Promise<T>, intervalMs: number)
⋮----
function StatusDot(
⋮----
className=
⋮----
function StatCard(
⋮----
function AdminDashboardContent()
⋮----
{/* System Health */}
⋮----
{/* Usage Stats */}
⋮----
{/* Web Vitals */}
⋮----
{/* Recent API Calls */}
</file>

<file path="frontend/src/app/api-docs/page.tsx">
import dynamic from 'next/dynamic';
⋮----
export default function ApiDocsPage()
⋮----
{/* Dark theme overrides for Swagger UI */}
</file>

<file path="frontend/src/app/charts/components/StockChartPanel.tsx">
import React from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { StockOHLCVChart } from '@/components/charts';
import { getTASIStockName } from '@/lib/tradingview-utils';
import { useStockDetail } from '@/lib/hooks/use-api';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector } from '@/lib/stock-translations';
⋮----
interface StockChartPanelProps {
  ticker: string;
  isFullscreen: boolean;
  onToggleFullscreen: () => void;
  onAddToCompare?: (ticker: string) => void;
  compareDisabled?: boolean;
}
⋮----
{/* Stock header */}
⋮----
className=
⋮----
{/* Add to comparison button */}
⋮----
onClick=
⋮----
{/* Fullscreen toggle */}
⋮----
{/* Stock OHLCV Candlestick Chart */}
⋮----
{/* Footer */}
</file>

<file path="frontend/src/app/charts/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Chart error icon */}
</file>

<file path="frontend/src/app/chat/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Chat error icon */}
</file>

<file path="frontend/src/app/chat/loading.tsx">
export default function ChatLoading()
⋮----
{/* Chat area */}
⋮----
{/* Input bar skeleton */}
</file>

<file path="frontend/src/app/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Gold warning icon */}
⋮----
{/* Decorative gold line */}
</file>

<file path="frontend/src/app/loading.tsx">
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Gold spinning indicator */}
⋮----
{/* Loading text */}
</file>

<file path="frontend/src/app/login/page.tsx">
import { useState, type FormEvent } from 'react';
import { useRouter, useSearchParams } from 'next/navigation';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useAuth } from '@/lib/hooks/use-auth';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
type Mode = 'login' | 'register';
⋮----
function parseBackendError(msg: string, t: (ar: string, en: string) => string): string
⋮----
// Inline validation
⋮----
const handleSubmit = async (e: FormEvent) =>
⋮----
// Client-side validation
⋮----
const handleGuestAccess = async () =>
⋮----
{/* Logo / Header */}
⋮----
{/* Card */}
⋮----
{/* Mode toggle */}
⋮----
onClick=
className=
⋮----
{/* Form */}
⋮----
onBlur=
⋮----
<p className=
⋮----
{/* Error message */}
⋮----
{/* Divider */}
⋮----
{/* Guest Access */}
⋮----
{/* Back link */}
</file>

<file path="frontend/src/app/market/error.tsx">
import { useEffect } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Market error icon */}
</file>

<file path="frontend/src/app/markets/components/CentralHub.tsx">
import React from 'react';
import { C, pctFmt } from './constants';
import { HUB_RADIUS } from '@/lib/market-graph';
import type { PortfolioStats } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// CentralHub - center of the constellation
// ---------------------------------------------------------------------------
⋮----
export interface CentralHubProps {
  stats: PortfolioStats;
  count: number;
  cx: number;
  cy: number;
  t: (ar: string, en: string) => string;
}
</file>

<file path="frontend/src/app/markets/components/ConstellationCanvas.tsx">
import React, { useMemo, useCallback } from 'react';
import { C } from './constants';
import { CentralHub } from './CentralHub';
import { InstrumentNode } from './InstrumentNode';
import { EdgeTooltip } from './EdgeTooltip';
import {
  CANVAS_W,
  CANVAS_H,
  CX,
  CY,
  toPosMap,
} from '@/lib/market-graph';
import type {
  Instrument,
  EdgeLabel,
  NodePosition,
  PortfolioStats,
} from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// ConstellationCanvas - desktop SVG constellation + nodes + tooltips
// ---------------------------------------------------------------------------
⋮----
export interface ConstellationCanvasProps {
  instruments: Instrument[];
  labels: EdgeLabel[];
  layout: NodePosition[];
  stats: PortfolioStats;
  hoveredKey: string | null;
  hoveredEdge: EdgeLabel | null;
  flashKeys: Set<string>;
  loaded: boolean;
  isRTL: boolean;
  language: 'ar' | 'en';
  t: (ar: string, en: string) => string;
  onHoverKey: (key: string | null) => void;
  onHoverEdge: (edge: EdgeLabel | null) => void;
}
⋮----
{/* SVG layer: orbit rings, correlation lines, radial lines */}
⋮----
aria-label=
⋮----
{/* Orbit rings */}
⋮----
{/* Correlation edge lines */}
⋮----
{/* Invisible wide hit area */}
⋮----
onMouseLeave=
onFocus=
onBlur=
onKeyDown=
⋮----
{/* Visible line */}
⋮----
{/* Radial lines from center to each node */}
⋮----
opacity=
⋮----
{/* Correlation % badges (HTML for crisp rendering) */}
⋮----
{/* Central hub */}
⋮----
{/* Instrument nodes */}
⋮----
isDimmed=
</file>

<file path="frontend/src/app/markets/components/InstrumentNode.tsx">
import React from 'react';
import { cn } from '@/lib/utils';
import { C, fmt, pctFmt } from './constants';
import { Sparkline } from './Sparkline';
import { StatBadge } from './StatBadge';
import type { Instrument } from '@/lib/market-graph';
⋮----
// ---------------------------------------------------------------------------
// InstrumentNode - Desktop instrument card positioned in constellation
// ---------------------------------------------------------------------------
⋮----
export interface InstrumentNodeProps {
  inst: Instrument;
  x: number;
  y: number;
  isHovered: boolean;
  isDimmed: boolean;
  isFlashing?: boolean;
  isRTL: boolean;
  language: 'ar' | 'en';
  onHover: () => void;
  onLeave: () => void;
}
⋮----
{/* Price row */}
⋮----

⋮----
{/* Opposite language name */}
⋮----
className=
</file>

<file path="frontend/src/app/markets/error.tsx">
import { useEffect, useState } from 'react';
import { RetryButton } from '@/components/common/RetryButton';
⋮----
export default function MarketsError({
  error,
  reset,
}: {
  error: Error & { digest?: string };
reset: ()
⋮----
const handleRetry = () =>
⋮----
{/* Gold warning icon */}
⋮----
{/* Network suggestion */}
</file>

<file path="frontend/src/app/news/components/index.ts">

</file>

<file path="frontend/src/app/news/components/NewArticlesBanner.tsx">
import { useState, useEffect, useRef, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
export interface NewArticlesBannerProps {
  count: number;
  onDismiss: () => void;
}
⋮----
// Wait for exit animation before calling onDismiss
⋮----
// Show banner when count becomes positive
⋮----
// Trigger progress bar after a micro-tick so the transition kicks in
⋮----
// Auto-dismiss timer
⋮----
className=
⋮----
{/* Refresh icon */}
⋮----
{/* Count badge */}
⋮----
{/* Close / dismiss X button */}
⋮----
{/* Auto-dismiss progress bar */}
</file>

<file path="frontend/src/app/news/utils.ts">
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
export const POLLING_FALLBACK_INTERVAL = 60_000; // 60 seconds polling fallback
⋮----
/** Map source names to their brand colors (includes alternate names) */
⋮----
// ---------------------------------------------------------------------------
// Utility functions
// ---------------------------------------------------------------------------
⋮----
export function getSourceColor(name: string): string
⋮----
export function timeAgo(dateStr: string | null, t: (ar: string, en: string) => string, language: string): string
⋮----
export function readingTime(body: string | null, t: (ar: string, en: string) => string): string | null
⋮----
export function getBookmarks(): Set<string>
⋮----
export function saveBookmarks(ids: Set<string>)
</file>

<file path="frontend/src/app/not-found.tsx">
import Link from 'next/link';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Gold 404 number */}
⋮----
{/* Heading */}
⋮----
{/* Description */}
⋮----
{/* Return home button */}
⋮----
{/* Decorative gold line */}
</file>

<file path="frontend/src/app/stock/[ticker]/components/StockReportsSection.tsx">
import React, { memo } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useReportsByTicker } from '@/lib/hooks/use-api';
import { LoadingSpinner } from '@/components/common/loading-spinner';
⋮----
interface StockReportsSectionProps {
  ticker: string;
  language: string;
  t: (ar: string, en: string) => string;
}
⋮----
aria-label=
</file>

<file path="frontend/src/app/watchlist/loading.tsx">
{/* Header skeleton */}
⋮----
{/* Tab chips skeleton */}
⋮----
{/* Add ticker skeleton */}
⋮----
{/* Table skeleton */}
</file>

<file path="frontend/src/components/charts/AreaChart.tsx">
import { useEffect, useRef, useState } from 'react';
import {
  createChart,
  type IChartApi,
  type ISeriesApi,
  type AreaData,
  type Time,
  ColorType,
} from 'lightweight-charts';
import { RAID_CHART_OPTIONS, LINE_COLOR, AREA_TOP_COLOR, AREA_BOTTOM_COLOR } from './chart-config';
import type { LineDataPoint, ChartTimeRange } from './chart-types';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';
import { cn } from '@/lib/utils';
⋮----
interface AreaChartProps {
  data: LineDataPoint[];
  height?: number;
  lineColor?: string;
  topColor?: string;
  bottomColor?: string;
  showTimeRange?: boolean;
  title?: string;
  className?: string;
  loading?: boolean;
  error?: string | null;
  refetch?: () => void;
}
⋮----
function filterByRange(data: LineDataPoint[], range: ChartTimeRange): LineDataPoint[]
⋮----
// Create chart once
⋮----
}, [loading, data.length === 0, height]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
// Update data when range or data changes
⋮----
className=
⋮----
{/* Toolbar */}
⋮----
{/* Chart */}
</file>

<file path="frontend/src/components/charts/DataSourceBadge.tsx">
import { useId, useMemo } from 'react';
import { cn } from '@/lib/utils';
import type { DataSource } from './chart-types';
⋮----
interface DataSourceBadgeProps {
  source: DataSource | null;
  lastUpdated?: string;
  className?: string;
}
⋮----
function formatTimeAgo(dateStr: string):
⋮----
<span className=
</file>

<file path="frontend/src/components/charts/LineChart.tsx">
import { useEffect, useRef, useState } from 'react';
import {
  createChart,
  type IChartApi,
  type ISeriesApi,
  type LineData,
  type Time,
  ColorType,
} from 'lightweight-charts';
import { RAID_CHART_OPTIONS, LINE_COLOR } from './chart-config';
import type { LineDataPoint, ChartTimeRange } from './chart-types';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';
import { cn } from '@/lib/utils';
⋮----
interface LineChartProps {
  data: LineDataPoint[];
  height?: number;
  color?: string;
  lineWidth?: 1 | 2 | 3 | 4;
  showTimeRange?: boolean;
  title?: string;
  className?: string;
  loading?: boolean;
  error?: string | null;
  refetch?: () => void;
}
⋮----
function filterByRange(data: LineDataPoint[], range: ChartTimeRange): LineDataPoint[]
⋮----
// Create chart once
⋮----
}, [loading, data.length === 0, height]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
// Update data when range or data changes
⋮----
className=
⋮----
{/* Toolbar */}
⋮----
{/* Chart */}
</file>

<file path="frontend/src/components/charts/MiniSparkline.tsx">
import { useEffect, useRef } from 'react';
import {
  createChart,
  type IChartApi,
  type ISeriesApi,
  type LineData,
  type Time,
  ColorType,
  CrosshairMode,
} from 'lightweight-charts';
import type { LineDataPoint } from './chart-types';
import { cn } from '@/lib/utils';
⋮----
interface MiniSparklineProps {
  data: LineDataPoint[];
  width?: number;
  height?: number;
  className?: string;
  loading?: boolean;
  error?: string | null;
}
⋮----
// Simplified loading state for tiny sparkline
⋮----
className=
⋮----
// Simplified error state for tiny sparkline
</file>

<file path="frontend/src/components/chat/LoadingDots.tsx">
interface LoadingDotsProps {
  /** Optional live progress message from the backend */
  progressText?: string;
}
⋮----
/** Optional live progress message from the backend */
</file>

<file path="frontend/src/components/common/ConnectionStatusBadge.tsx">
import { cn } from '@/lib/utils';
⋮----
type ConnectionStatus = 'live' | 'reconnecting' | 'offline';
⋮----
export function ConnectionStatusBadge(
⋮----
<span role="status" aria-live="polite" className=
</file>

<file path="frontend/src/components/common/GlobalKeyboardShortcuts.tsx">
/**
 * Global keyboard shortcuts.
 * Ctrl+K is now handled by CommandPalette directly, so it's removed here.
 * This component is kept for future global shortcuts.
 */
export function GlobalKeyboardShortcuts()
</file>

<file path="frontend/src/components/common/ScrollToTop.tsx">
import { useState, useEffect } from 'react';
import { cn } from '@/lib/utils';
⋮----
const handleScroll = () =>
⋮----
const scrollToTop = () =>
</file>

<file path="frontend/src/components/performance/Skeletons.tsx">
/**
 * Reusable skeleton loading components for lazy-loaded sections.
 * All follow the dark-gold Ra'd AI theme.
 */
⋮----
// ---------------------------------------------------------------------------
// Shared shimmer class from globals.css: .animate-shimmer
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// TableSkeleton
// ---------------------------------------------------------------------------
⋮----
interface TableSkeletonProps {
  rows?: number;
  columns?: number;
}
⋮----
{/* Header row */}
⋮----
// ---------------------------------------------------------------------------
// DashboardSkeleton
// ---------------------------------------------------------------------------
⋮----
{/* Stat cards row */}
⋮----
{/* Chart placeholder */}
⋮----
{/* Table placeholder */}
</file>

<file path="frontend/src/components/queries/ExportButton.tsx">
import { useState, useRef, useEffect } from 'react';
import { cn } from '@/lib/utils';
import { exportToCsv, exportToExcel, exportToPdf } from '@/lib/export/exporters';
import type { QueryResults } from '@/types/queries';
⋮----
interface ExportButtonProps {
  data: QueryResults;
  title?: string;
  className?: string;
}
⋮----
type ExportFormat = 'csv' | 'excel' | 'pdf';
⋮----
// Close dropdown on outside click
⋮----
const handler = (e: MouseEvent) =>
⋮----
const handleExport = async (format: ExportFormat) =>
⋮----
// Export failed silently
</file>

<file path="frontend/src/components/queries/QueryHistory.tsx">
import { useState, useEffect, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { queryStore } from '@/lib/queries/query-store';
import { QueryHistoryItem } from './QueryHistoryItem';
import type { QueryRecord, QuerySortField, SortDirection } from '@/types/queries';
⋮----
interface QueryHistoryProps {
  onRerun: (query: string) => void;
  onSave?: (record: QueryRecord) => void;
  className?: string;
}
⋮----
// IndexedDB unavailable
⋮----
const handleDelete = async (id: string) =>
⋮----
const handleToggleFavorite = async (id: string) =>
⋮----
const handleClearAll = async () =>
⋮----
const toggleSort = (field: QuerySortField) =>
⋮----
<div className=
{/* Header */}
⋮----
{/* Filters */}
⋮----
{/* Search */}
⋮----
{/* Filter row */}
⋮----
onClick=
⋮----
{/* Sort buttons */}
⋮----
{/* List */}
</file>

<file path="frontend/src/components/queries/SavedQueries.tsx">
import { useState, useEffect, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { queryStore } from '@/lib/queries/query-store';
import type { QueryRecord } from '@/types/queries';
⋮----
interface SavedQueriesProps {
  onRerun: (query: string) => void;
  className?: string;
}
⋮----
// IndexedDB unavailable
⋮----
// Gather all unique tags
⋮----
// Filter
⋮----
const handleRemoveFavorite = async (id: string) =>
⋮----
const handleShare = async (record: QueryRecord) =>
⋮----
// no-op
⋮----
<div className=
{/* Header */}
⋮----
{/* Search + Tag filters */}
⋮----
onClick=
⋮----
{/* Grid */}
⋮----
{/* Name / Query */}
⋮----
{/* Tags */}
⋮----
{/* Actions */}
</file>

<file path="frontend/src/components/ui/Tooltip.tsx">
import { cn } from '@/lib/utils';
⋮----
interface TooltipProps {
  text: string;
  position?: 'top' | 'bottom' | 'left' | 'right';
  children: React.ReactNode;
}
⋮----
export function Tooltip(
⋮----
className=
</file>

<file path="frontend/src/components/visualization/AutoChart.tsx">
import { useMemo } from 'react';
import dynamic from 'next/dynamic';
import { KPICard } from './chart-types';
⋮----
// Dynamically import recharts-based chart components to avoid bundling recharts in the main chunk
⋮----
export type ChartType = 'line' | 'bar' | 'pie' | 'scatter' | 'kpi';
⋮----
interface AutoChartProps {
  data: Record<string, unknown>[];
  title?: string;
  chartType?: ChartType;
}
⋮----
function isDateLike(key: string, values: unknown[]): boolean
⋮----
// Check if string values look like dates
⋮----
function detectChartType(data: Record<string, unknown>[]): ChartType
⋮----
// Single row, single numeric value -> KPI
⋮----
// Check for time series
⋮----
// Check for proportional data
⋮----
// Two numeric columns, no categorical -> scatter
⋮----
// One categorical + one or more numeric -> bar
⋮----
// Small dataset with a label column -> pie
⋮----
export function AutoChart(
</file>

<file path="frontend/src/components/visualization/chart-types/KPICard.tsx">
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface KPICardProps {
  data: Record<string, unknown>[];
  title?: string;
  config?: {
    valueKey?: string;
    labelKey?: string;
  };
}
⋮----
function formatValue(val: unknown, locale: string): string
</file>

<file path="frontend/src/components/visualization/ResultToolbar.tsx">
import { cn } from '@/lib/utils';
⋮----
export type ViewMode = 'chart' | 'table' | 'split';
⋮----
interface ResultToolbarProps {
  viewMode: ViewMode;
  onViewModeChange: (mode: ViewMode) => void;
  onExport?: () => void;
  onFullscreen?: () => void;
  isFullscreen?: boolean;
}
⋮----
{/* View mode toggle */}
⋮----
className=
⋮----
{/* Actions */}
</file>

<file path="frontend/src/components/widgets/LiveMarketWidgets.tsx">
import React, { useEffect, useMemo, useRef, useState, useCallback } from 'react';
import { cn } from '@/lib/utils';
import { ConnectionStatusBadge } from '@/components/common/ConnectionStatusBadge';
import { API_BASE } from '@/lib/config';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
type QuoteItem = {
  symbol: string;
  name: string;
  asset_class: 'crypto' | 'metal' | 'oil' | 'index' | 'fx' | 'other';
  price: number;
  currency: string;
  change?: number | null;
  change_pct?: number | null;
  ts_iso: string;
  source: string;
  is_delayed?: boolean;
  delay_minutes?: number;
};
⋮----
type ConnectionStatus = 'live' | 'reconnecting' | 'offline';
⋮----
// ---------------------------------------------------------------------------
// Asset-class icons (inline SVG, small)
// ---------------------------------------------------------------------------
⋮----
crypto: '\u20BF',   // Bitcoin symbol
metal: '\u2B50',    // Star (gold)
oil: '\u26FD',      // Fuel pump
index: '\u2191',    // Up arrow
fx: '\u0024',       // Dollar
other: '\u2022',    // Bullet
⋮----
// ---------------------------------------------------------------------------
// Memoized quote card to prevent re-renders when other quotes update
// ---------------------------------------------------------------------------
⋮----
{/* Name + asset icon */}
⋮----
{/* Price */}
⋮----
{/* Change % */}
⋮----
className=
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Number formatters, memoized per lang
⋮----
// Clean up any previous connection
⋮----
// Handle 'snapshot' event (full list of quotes)
⋮----
// ignore malformed data
⋮----
// Handle 'update' event (single quote update)
⋮----
// ignore malformed data
⋮----
// Exponential backoff: 1.5s, 3s, 6s, 12s, 24s, max 30s
⋮----
// If no quotes yet and still connecting, show a minimal skeleton
⋮----
// If offline with no data, show nothing
⋮----
{/* Header row */}
⋮----
{/* Scrollable quote cards */}
</file>

<file path="frontend/src/config/cdn.ts">
/**
 * CDN configuration for static asset delivery.
 *
 * When deploying behind a CDN (CloudFlare, CloudFront, etc.), set the
 * NEXT_PUBLIC_CDN_URL environment variable to the CDN origin URL.
 * This module provides helpers for building CDN-prefixed asset URLs.
 */
⋮----
/** CDN base URL (empty string = same origin, no CDN) */
⋮----
/**
 * Returns a CDN-prefixed URL for a static asset path.
 *
 * @param path - Asset path starting with "/" (e.g., "/images/logo.webp")
 * @returns Full URL with CDN prefix, or the original path if no CDN configured
 */
export function cdnAssetUrl(path: string): string
</file>

<file path="frontend/src/contexts/AuthContext.tsx">
import {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useMemo,
  useState,
  type ReactNode,
} from 'react';
import type { AuthUser, Role } from '@/types/auth';
import { hasPermission, hasRole } from '@/types/auth';
import { getAuthConfig } from '@/config/auth';
import { sessionManager } from '@/lib/auth/session';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
interface AuthContextValue {
  user: AuthUser | null;
  isLoading: boolean;
  isAuthenticated: boolean;
  login: (email: string, password: string) => Promise<void>;
  logout: () => void;
  refreshSession: () => void;
  /** Check if current user has a specific permission */
  checkPermission: (action: string, resource: string) => boolean;
  /** Check if current user meets a minimum role level */
  checkRole: (minimumRole: Role) => boolean;
}
⋮----
/** Check if current user has a specific permission */
⋮----
/** Check if current user meets a minimum role level */
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function decodeJwtPayload(token: string): Record<string, unknown>
⋮----
type AuthApiResponse = {
  token?: string;
  access_token?: string;
  refresh_token?: string;
  user_id?: string;
  name?: string;
  role?: string;
};
⋮----
function extractAccessToken(data: AuthApiResponse): string
⋮----
function extractRole(data: AuthApiResponse, claims: Record<string, unknown>): Role
⋮----
// ---------------------------------------------------------------------------
// Provider
// ---------------------------------------------------------------------------
⋮----
export function RBACAuthProvider(
⋮----
// Hydrate from localStorage on mount
⋮----
// Ensure role field exists (backward compat with old use-auth data)
⋮----
// Restore session tracking
⋮----
// ignore corrupt data
⋮----
// Set up session expiry callback
⋮----
// Auto-logout on session expiry
⋮----
// Periodic token refresh
⋮----
// Silent failure on refresh — session will expire naturally
⋮----
// continue with API-provided fields
⋮----
/**
 * Hook to access RBAC auth context. Throws if used outside RBACAuthProvider.
 */
export function useRBACAuth(): AuthContextValue
</file>

<file path="frontend/src/lib/auth/route-protection.ts">
/**
 * Route protection logic for Next.js middleware.
 *
 * The security-headers teammate creates middleware.ts and can import
 * `protectRoute()` from this module to enforce role-based access.
 */
⋮----
import { NextRequest, NextResponse } from 'next/server';
import type { Role } from '@/types/auth';
import { hasRole } from '@/types/auth';
import { getAuthConfig } from '@/config/auth';
⋮----
// ---------------------------------------------------------------------------
// Route -> minimum role mapping
// ---------------------------------------------------------------------------
⋮----
export interface RouteRule {
  /** Path prefix (matched with startsWith) */
  pattern: string;
  /** Minimum role required, or 'public' for unauthenticated access */
  access: Role | 'public';
}
⋮----
/** Path prefix (matched with startsWith) */
⋮----
/** Minimum role required, or 'public' for unauthenticated access */
⋮----
/**
 * Ordered route rules. First match wins, so more specific patterns go first.
 */
⋮----
// ---------------------------------------------------------------------------
// Token inspection (lightweight, no signature verification)
// ---------------------------------------------------------------------------
⋮----
interface TokenClaims {
  sub?: string;
  role?: string;
  exp?: number;
}
⋮----
function parseTokenClaims(token: string): TokenClaims | null
⋮----
function isTokenExpired(claims: TokenClaims): boolean
⋮----
function extractRoleFromClaims(claims: TokenClaims): Role
⋮----
// ---------------------------------------------------------------------------
// Main function
// ---------------------------------------------------------------------------
⋮----
/**
 * Evaluate route protection rules against the incoming request.
 *
 * Returns either:
 * - `null`              if access is allowed (caller should continue with NextResponse.next())
 * - a `NextResponse`    redirect/error if access is denied
 *
 * Usage in middleware.ts:
 * ```ts
 * import { protectRoute } from '@/lib/auth/route-protection';
 *
 * export function middleware(request: NextRequest) {
 *   const result = protectRoute(request);
 *   if (result) return result;
 *   return NextResponse.next();
 * }
 * ```
 */
export function protectRoute(request: NextRequest): NextResponse | null
⋮----
// Find matching rule
⋮----
// No rule matched — default to viewer (require auth)
⋮----
// Public routes need no auth check
⋮----
// Extract token from cookie or Authorization header
⋮----
// Redirect to login
⋮----
// Check role hierarchy
⋮----
// 403 — user is authenticated but lacks the role
⋮----
/**
 * Get the minimum role required for a given path.
 * Useful for UI components that need to know route requirements.
 */
export function getRouteRequirement(pathname: string): Role | 'public'
</file>

<file path="frontend/src/lib/chart-utils.ts">
/**
 * Chart utility functions for generating mock data.
 *
 * Used by data hooks for fallback when the backend API is unavailable.
 */
⋮----
import type { OHLCVData, LineDataPoint } from '@/components/charts/chart-types';
⋮----
// ---------------------------------------------------------------------------
// Mock data generation
// ---------------------------------------------------------------------------
⋮----
/** Simple deterministic hash from a string, used to seed per-ticker data. */
function hashTicker(ticker: string): number
⋮----
/** Seeded pseudo-random number generator (Mulberry32). */
function mulberry32(seed: number): () => number
⋮----
/**
 * Check if a date falls on a Saudi weekend (Friday or Saturday).
 */
function isSaudiWeekend(date: Date): boolean
⋮----
return day === 5 || day === 6; // Friday = 5, Saturday = 6
⋮----
/**
 * Format a Date as YYYY-MM-DD.
 */
function toDateStr(d: Date): string
⋮----
/**
 * Generate realistic OHLCV data for a given ticker.
 *
 * The ticker hash seeds the PRNG so data is consistent per ticker.
 * Base price ranges 10-500 SAR with 1-3% daily volatility.
 * Volume ranges 100K-10M shares. Saudi weekends (Fri-Sat) are skipped.
 */
export function generateMockOHLCV(ticker: string, days: number = 365): OHLCVData[]
⋮----
// Derive base price and volatility from ticker hash
const basePrice = 10 + (seed % 490); // 10 - 500 SAR
const volatility = 0.01 + (seed % 200) / 10000; // 1% - 3%
⋮----
const change = (rand() - 0.48) * volatility * price; // slight upward bias
⋮----
/**
 * Generate a price trend line from mock OHLCV close prices.
 */
export function generateMockPriceTrend(days: number = 365): LineDataPoint[]
⋮----
// Use a generic ticker to generate trend data
</file>

<file path="frontend/src/lib/formatters.ts">
/**
 * Shared number and date formatting utilities for the Ra'd AI platform.
 *
 * All formatters handle null/undefined inputs gracefully (return '-' or empty string).
 */
⋮----
/**
 * Format a stock price to 2 decimal places.
 * @example formatPrice(10.5) => "10.50"
 */
export function formatPrice(value: number | null | undefined): string
⋮----
/**
 * Format a percentage change with sign prefix and configurable decimals.
 * Input is a whole-number percent (e.g. 3.14 means "3.14%").
 * @example formatChangePercent(3.14) => "+3.14%"
 * @example formatChangePercent(-1.5, 1) => "-1.5%"
 */
export function formatChangePercent(
  value: number | null | undefined,
  decimals = 2
): string
⋮----
/**
 * Format a large number (volume) with K/M/B suffix.
 * @example formatVolume(1_500_000) => "1.5M"
 * @example formatVolume(42_000) => "42.0K"
 */
export function formatVolume(value: number | null | undefined): string
⋮----
/**
 * Format a market capitalization in SAR billions or millions.
 * @example formatMarketCap(500_000_000_000) => "500.0B"
 */
export function formatMarketCap(value: number | null | undefined): string
</file>

<file path="frontend/src/lib/hooks/use-market-data.ts">
import { useCallback, useEffect, useRef, useState } from 'react';
import { getMarketOverview } from '@/lib/api-client';
import { RAW_INSTRUMENTS, INSTRUMENT_META } from '@/lib/market-graph/data';
import type { RawInstrument } from '@/lib/market-graph/types';
⋮----
export interface UseMarketDataReturn {
  instruments: RawInstrument[];
  /** key -> 90 daily closes (for improved correlation calculations) */
  historicalData: Record<string, number[]>;
  isLoading: boolean;
  error: string | null;
  lastUpdated: Date | null;
  /** true if using live data, false if using static fallback */
  isLive: boolean;
  refetch: () => void;
}
⋮----
/** key -> 90 daily closes (for improved correlation calculations) */
⋮----
/** true if using live data, false if using static fallback */
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
const REFRESH_INTERVAL_MS = 60_000; // 60 seconds
⋮----
// ---------------------------------------------------------------------------
// Hook
// ---------------------------------------------------------------------------
⋮----
export function useMarketDataLive(): UseMarketDataReturn
⋮----
// Abort any in-flight request
⋮----
// Transform API response into RawInstrument[], filtering out failed instruments
⋮----
// Build historical data map
⋮----
// Fall back to static data
⋮----
// Fetch on mount
⋮----
// Auto-refresh every 60 seconds
</file>

<file path="frontend/src/lib/hooks/useWatchlist.ts">
import { useState, useEffect, useCallback, useRef } from 'react';
⋮----
export interface LocalWatchlist {
  id: string;
  name: string;
  tickers: string[];
}
⋮----
export function getDefaultWatchlists(): LocalWatchlist[]
⋮----
export function loadLocalWatchlists(): LocalWatchlist[]
⋮----
export function saveLocalWatchlists(lists: LocalWatchlist[]): void
⋮----
export function useWatchlist()
⋮----
// Load from localStorage on mount
⋮----
// Persist to localStorage on change - skip the very first render after load
⋮----
return; // skip initial load-triggered effect
</file>

<file path="frontend/src/lib/monitoring/swr-middleware.ts">
/**
 * SWR middleware for tracking API metrics.
 * Measures response times, error rates, and cache behavior.
 * Reports to metricsCollector singleton.
 *
 * Usage: Add to SWR global config or per-hook:
 *   <SWRConfig value={{ use: [metricsMiddleware] }}>
 */
⋮----
import type { Middleware } from 'swr';
import { metricsCollector } from './metrics-collector';
⋮----
/* eslint-disable @typescript-eslint/no-explicit-any */
export const metricsMiddleware: Middleware = (useSWRNext: any) =>
⋮----
// Check rate limit headers if available
</file>

<file path="frontend/src/lib/stock-translations.ts">
/**
 * Shared translations for sector names and stock aliases.
 *
 * Sector names in the database are in English. This module provides:
 * 1. Arabic translations for sector names (used when language is Arabic).
 * 2. A stock alias map for common nicknames and Arabic names, used to
 *    improve search matching on the market page.
 */
⋮----
import type { Language } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Sector name translations (English DB values -> Arabic display)
// ---------------------------------------------------------------------------
⋮----
// Extended sector names (in case backend adds TASI-style sector names)
⋮----
/**
 * Translate a sector name based on the current language.
 * Returns the Arabic translation if available, otherwise the original name.
 */
export function translateSector(sector: string | null | undefined, language: Language): string
⋮----
// ---------------------------------------------------------------------------
// Stock alias map (ticker -> common search terms)
// ---------------------------------------------------------------------------
⋮----
/**
 * Maps tickers to common aliases, abbreviations, and Arabic names
 * that users might search for. Used for client-side search matching.
 *
 * Each key is a ticker (e.g. "2222.SR") and the value is an array of
 * lowercase aliases. The search function normalizes input to lowercase
 * before matching.
 */
⋮----
// === Major Blue Chips ===
⋮----
// === Banks ===
⋮----
// === Telecom ===
⋮----
// === Mining & Energy ===
⋮----
// === Petrochemicals ===
⋮----
// === Food & Beverages ===
⋮----
// === Retail ===
⋮----
// === Healthcare ===
⋮----
// === Real Estate ===
⋮----
// === Insurance ===
⋮----
// === Transportation ===
⋮----
// === Cement ===
⋮----
// === Other Major Companies ===
⋮----
/**
 * Build a reverse lookup: alias (lowercase) -> ticker.
 * This is built once at module load for fast search matching.
 */
⋮----
/**
 * Given a search query, return matching ticker(s) from the alias map.
 * Performs partial matching: if the query is a substring of an alias,
 * or an alias is a substring of the query, the ticker is included.
 *
 * @returns An array of tickers that matched via alias.
 */
export function findTickersByAlias(query: string): string[]
⋮----
// Match if query is contained in alias or alias is contained in query
⋮----
/**
 * Check if a company summary item matches a search query.
 * Checks ticker, short_name, and aliases.
 */
export function matchesSearch(
  item: { ticker: string; short_name: string | null },
  query: string,
): boolean
⋮----
// Direct match on ticker or name
⋮----
// Also match ticker without ".SR" suffix
⋮----
// Alias match
</file>

<file path="frontend/src/lib/validators.ts">
/**
 * Lightweight runtime validators for chart API responses.
 *
 * No external dependencies (no Zod). Just basic shape checks
 * to catch malformed data before it hits chart components.
 */
⋮----
function isObject(val: unknown): val is Record<string, unknown>
⋮----
/**
 * Validate OHLCV data array (candlestick / bar chart data).
 * Each item must have `time` (string), `open` (number), `high` (number),
 * `low` (number), `close` (number). `volume` is optional.
 */
export function validateOHLCVData(data: unknown): data is Array<
⋮----
/**
 * Validate a TASI index API response envelope.
 */
export function validateTasiResponse(resp: unknown): resp is
</file>

<file path="frontend/src/providers/LanguageProvider.tsx">
import { createContext, useContext, useEffect, useState, useCallback } from 'react';
⋮----
export type Language = 'ar' | 'en';
⋮----
interface LanguageContextValue {
  language: Language;
  toggleLanguage: () => void;
  setLanguage: (lang: Language) => void;
  t: (ar: string, en: string) => string;
  isRTL: boolean;
}
⋮----
export function LanguageProvider(
⋮----
// No stored preference: detect from browser locale
⋮----
// Otherwise keep default 'ar'
⋮----
export function useLanguage()
</file>

<file path="frontend/src/styles/design-system.ts">
/**
 * Ra'd AI Design System Tokens
 * Extracted from the existing templates/index.html Ra'd AI theme.
 * Single source of truth for colors, spacing, typography, and layout constants.
 */
⋮----
// ---------------------------------------------------------------------------
// Colors
// ---------------------------------------------------------------------------
⋮----
// Light mode overrides
⋮----
// ---------------------------------------------------------------------------
// Spacing
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Border Radii
// Components use Tailwind defaults: rounded-md = 6px, rounded-lg = 8px,
// rounded-xl = 12px. The custom radii below are available for non-standard
// cases and are mapped into tailwind.config.ts.
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Typography
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Layout
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Transitions
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Gradients
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Breakpoints (matches Tailwind defaults)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Tailwind-compatible token export for tailwind.config.ts
// ---------------------------------------------------------------------------
</file>

<file path="ingestion/config.py">
"""
Ingestion pipeline configuration.

Settings for batch processing, rate limiting, and retry behavior.
Can be overridden via environment variables.
"""
⋮----
class IngestionConfig
⋮----
"""Configuration for ingestion pipeline components."""
⋮----
def __repr__(self) -> str
</file>

<file path="ingestion/price_loader.py">
"""
price_loader.py
===============
Loads daily OHLCV price data into the price_history PostgreSQL table.

Supports two modes:
  1. **yfinance**: Fetch live data from Yahoo Finance for Saudi stocks (.SR suffix)
  2. **CSV**: Load from CSV files (per-ticker or combined)

Features:
  - Batch processing: N tickers at a time (default: 10)
  - Rate limiting: configurable sleep between batches (default: 2s)
  - Exponential backoff on Yahoo Finance rate limiting
  - Dedup by (ticker, trade_date) via ON CONFLICT DO NOTHING
  - Computes change_amount and change_pct from previous close
  - Handles partial failures (continues processing remaining tickers)
  - Progress tracking with logging

Usage:
    # Fetch prices from Yahoo Finance for specific tickers
    python ingestion/price_loader.py --tickers 2222.SR 1010.SR --from-date 2024-01-01

    # Fetch prices for ALL tickers in the companies table
    python ingestion/price_loader.py --all --from-date 2024-01-01

    # Load from a single CSV
    python ingestion/price_loader.py --file data/prices/2222.SR.csv --ticker 2222.SR

    # Load from a directory of per-ticker CSVs
    python ingestion/price_loader.py --dir data/prices/

    # Dry run
    python ingestion/price_loader.py --tickers 2222.SR --from-date 2024-01-01 --dry-run
"""
⋮----
yf = None
⋮----
psycopg2 = None
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Configuration
⋮----
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
DB_BATCH_SIZE = 500
⋮----
# Columns expected in CSV imports
REQUIRED_COLUMNS = {
⋮----
# Alternate column name mappings (common variations)
COLUMN_ALIASES = {
⋮----
# INSERT SQL with ON CONFLICT DO NOTHING for incremental loading
INSERT_COLUMNS = [
⋮----
INSERT_SQL = (
⋮----
# PriceLoader class (yfinance-based)
⋮----
class PriceLoader
⋮----
"""Fetches OHLCV price data from Yahoo Finance and loads into PostgreSQL."""
⋮----
"""Initialize PriceLoader.

        Args:
            pg_conn: PostgreSQL connection (or None for dry run).
            config: Ingestion configuration for batch size / rate limits.
            dry_run: If True, don't write to database.
        """
⋮----
"""Fetch and load price data for a list of tickers.

        Processes tickers in batches with rate limiting between batches.

        Args:
            tickers: List of ticker symbols (e.g., ["2222.SR", "1010.SR"]).
            from_date: Start date for price data.
            to_date: End date (default: today).

        Returns:
            Total number of rows inserted.
        """
⋮----
to_date = date.today()
⋮----
total_inserted = 0
batch_size = self.config.batch_size
total_batches = (len(tickers) + batch_size - 1) // batch_size
⋮----
batch = tickers[batch_idx : batch_idx + batch_size]
batch_num = (batch_idx // batch_size) + 1
⋮----
count = self._fetch_and_insert_ticker(ticker, from_date, to_date)
⋮----
# Rate limit between batches (skip after last batch)
⋮----
sleep_time = self.config.rate_limit_seconds
⋮----
"""Fetch and load prices for ALL tickers in the companies table.

        Args:
            from_date: Start date for price data.
            to_date: End date (default: today).

        Returns:
            Total number of rows inserted.
        """
⋮----
cur = self.pg_conn.cursor()
⋮----
tickers = [row[0] for row in cur.fetchall()]
⋮----
"""Fetch data from Yahoo Finance for a single ticker and insert.

        Uses exponential backoff on rate limiting.
        """
df = self._fetch_with_retry(ticker, from_date, to_date)
⋮----
# Normalize yfinance output to our schema
df = self._normalize_yfinance_df(df, ticker)
⋮----
# Compute changes
df = compute_changes(df)
⋮----
# Clean for insertion
numeric = df.select_dtypes(include=[np.number]).columns
⋮----
rows = df_to_insert_tuples(df)
⋮----
"""Fetch price data from yfinance with exponential backoff on failure."""
max_retries = self.config.max_retries
backoff = self.config.backoff_factor
⋮----
stock = yf.Ticker(ticker)
df = stock.history(
⋮----
wait = backoff**attempt
⋮----
@staticmethod
    def _normalize_yfinance_df(df: pd.DataFrame, ticker: str) -> pd.DataFrame
⋮----
"""Normalize yfinance DataFrame to match our price_history schema."""
⋮----
df = df.reset_index()
⋮----
# Rename columns from yfinance names to our schema
rename_map = {
df = df.rename(columns=rename_map)
⋮----
# Add ticker
⋮----
# Convert trade_date to date (yfinance returns datetime with timezone)
⋮----
# Keep only relevant columns
keep_cols = [
available = [c for c in keep_cols if c in df.columns]
df = df[available]
⋮----
# Convert volume to Int64
⋮----
# Drop rows with no close price
df = df.dropna(subset=["trade_date", "close_price"])
⋮----
# CSV data processing (shared utilities)
⋮----
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame
⋮----
"""Rename columns to match expected names using aliases."""
rename_map = {}
⋮----
def compute_changes(df: pd.DataFrame) -> pd.DataFrame
⋮----
"""Compute change_amount and change_pct from previous close.

    For each ticker, sorts by trade_date and computes:
    - change_amount = close_price - previous_close_price
    - change_pct = (change_amount / previous_close_price) * 100
    """
df = df.sort_values(["ticker", "trade_date"]).copy()
⋮----
# First row for each ticker has no previous close
⋮----
df = df.drop(columns=["prev_close"])
⋮----
def prepare_dataframe(df: pd.DataFrame, ticker: str = None) -> pd.DataFrame
⋮----
"""Clean and prepare a CSV DataFrame for loading."""
df = normalize_columns(df)
⋮----
missing = REQUIRED_COLUMNS - set(df.columns)
⋮----
numeric_cols = ["open_price", "high_price", "low_price", "close_price"]
⋮----
def _clean_val(val)
⋮----
"""Convert NaN/NaT to None for psycopg2 compatibility."""
⋮----
def df_to_insert_tuples(df: pd.DataFrame) -> list
⋮----
"""Convert DataFrame to list of tuples for INSERT."""
tuples = []
⋮----
t = (
⋮----
# Database operations
⋮----
def insert_prices(pg_conn, rows: list, dry_run: bool = False) -> int
⋮----
"""Insert price rows into PostgreSQL. Returns count of rows inserted."""
⋮----
cur = pg_conn.cursor()
inserted = 0
⋮----
batch = rows[i : i + DB_BATCH_SIZE]
⋮----
# Main
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser(
source = parser.add_mutually_exclusive_group(required=True)
⋮----
"""Load a single CSV file. Returns row count."""
⋮----
df = pd.read_csv(str(file_path), encoding="utf-8-sig")
⋮----
df = prepare_dataframe(df, ticker)
⋮----
count = insert_prices(pg_conn, rows, dry_run)
⋮----
tickers = df["ticker"].nunique()
dates = (
suffix = " (dry run)" if dry_run else ""
⋮----
def main()
⋮----
args = parse_args()
t_start = time.time()
⋮----
# Build config from args
config = IngestionConfig(
⋮----
# Connect to PostgreSQL
pg_conn = None
⋮----
pg_conn = psycopg2.connect(
⋮----
total_rows = 0
⋮----
# yfinance mode
from_date_str = (
from_date = date.fromisoformat(from_date_str)
to_date = date.fromisoformat(args.to_date) if args.to_date else date.today()
⋮----
loader = PriceLoader(pg_conn=pg_conn, config=config, dry_run=args.dry_run)
⋮----
total_rows = loader.load_all_prices(from_date, to_date)
⋮----
total_rows = loader.load_prices(args.tickers, from_date, to_date)
⋮----
file_path = Path(args.file)
⋮----
total_rows = load_single_csv(file_path, args.ticker, pg_conn, args.dry_run)
⋮----
dir_path = Path(args.dir)
⋮----
files = sorted(dir_path.glob(args.pattern))
⋮----
ticker = args.ticker
⋮----
stem = file_path.stem
⋮----
ticker = stem.split("_")[0] if "_" in stem else stem
⋮----
ticker = None
⋮----
count = load_single_csv(file_path, ticker, pg_conn, args.dry_run)
⋮----
# Summary
elapsed = time.time() - t_start
</file>

<file path="ingestion/scheduler.py">
"""
scheduler.py
============
APScheduler-based ingestion scheduler for automated data loading.

Schedules:
  - price_loader: daily at 17:00 (after Saudi market close at 15:00 + buffer)
  - xbrl_processor: weekly on Friday at 20:00

Usage:
    python -m ingestion.scheduler

    # With custom PostgreSQL settings
    PG_HOST=myhost PG_PORT=5432 python -m ingestion.scheduler

Environment variables:
    PG_HOST, PG_PORT, PG_DBNAME, PG_USER, PG_PASSWORD
    INGESTION_BATCH_SIZE, INGESTION_RATE_LIMIT_SECONDS
"""
⋮----
BlockingScheduler = None
CronTrigger = None
⋮----
psycopg2 = None
⋮----
logger = logging.getLogger(__name__)
⋮----
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
⋮----
def _get_pg_conn()
⋮----
"""Create a PostgreSQL connection from environment variables."""
⋮----
def job_load_prices()
⋮----
"""Scheduled job: fetch yesterday's prices for all Saudi stocks."""
⋮----
pg_conn = None
⋮----
pg_conn = _get_pg_conn()
⋮----
config = IngestionConfig()
loader = PriceLoader(pg_conn=pg_conn, config=config)
⋮----
# Fetch last 3 days to handle weekends/holidays
from_date = date.today() - timedelta(days=3)
to_date = date.today()
⋮----
total = loader.load_all_prices(from_date, to_date)
⋮----
def job_process_xbrl()
⋮----
"""Scheduled job: process any new XBRL filings in the ingestion directory."""
⋮----
filings_dir = PROJECT_DIR / "data" / "filings"
⋮----
supported = {".xml", ".xbrl", ".xlsx", ".xls"}
files = [
⋮----
total_facts = 0
⋮----
# Extract ticker from filename (e.g., '2222.SR_annual.xml')
stem = file_path.stem
parts = stem.split("_")
⋮----
ticker = parts[0]
⋮----
filing_id = create_filing(
⋮----
processor = XBRLProcessor(
facts = processor.process_filing(file_path)
⋮----
count = insert_facts(pg_conn, facts)
⋮----
def main()
⋮----
"""Start the ingestion scheduler."""
⋮----
# Configure logging
⋮----
scheduler = BlockingScheduler()
⋮----
# Price loader: daily at 17:00 (Saudi market closes at 15:00, +2h buffer)
⋮----
# XBRL processor: weekly on Friday at 20:00
⋮----
# Graceful shutdown
def handle_signal(signum, frame)
</file>

<file path="ingestion/validators.py">
"""
Ingestion data validators.

Validation functions for price data, XBRL facts, and ticker formats
used by the ingestion pipeline before database insertion.
"""
⋮----
# Saudi stock tickers: 4 digits + .SR (e.g., 2222.SR, 1010.SR)
TICKER_PATTERN = re.compile(r"^\d{4}\.SR$")
⋮----
PRICE_REQUIRED_COLUMNS = {
⋮----
XBRL_REQUIRED_FIELDS = {"ticker", "concept"}
⋮----
def validate_ticker_format(ticker: str) -> bool
⋮----
"""Check that a ticker follows the ####.SR format for Saudi stocks.

    Args:
        ticker: Ticker string to validate.

    Returns:
        True if valid, False otherwise.
    """
⋮----
def validate_price_data(df: pd.DataFrame) -> list[str]
⋮----
"""Validate a price DataFrame before insertion.

    Checks:
    - Required columns present
    - No future dates
    - Price values are non-negative where present
    - Volume is non-negative where present
    - high >= low where both are present

    Args:
        df: DataFrame with price data.

    Returns:
        List of error strings. Empty list means valid.
    """
errors = []
⋮----
# Check required columns
missing = PRICE_REQUIRED_COLUMNS - set(df.columns)
⋮----
return errors  # Can't validate further without columns
⋮----
# Check for future dates
today = date.today()
⋮----
trade_dates = pd.to_datetime(df["trade_date"], errors="coerce").dt.date
future_mask = trade_dates > today
future_count = future_mask.sum()
⋮----
# Check non-negative prices
⋮----
numeric_vals = pd.to_numeric(df[col], errors="coerce")
neg_count = (numeric_vals < 0).sum()
⋮----
# Check non-negative volume
⋮----
numeric_vol = pd.to_numeric(df["volume"], errors="coerce")
neg_vol = (numeric_vol < 0).sum()
⋮----
# Check high >= low
⋮----
high = pd.to_numeric(df["high_price"], errors="coerce")
low = pd.to_numeric(df["low_price"], errors="coerce")
both_valid = high.notna() & low.notna()
invalid = (high[both_valid] < low[both_valid]).sum()
⋮----
def validate_xbrl_fact(fact_dict: dict) -> list[str]
⋮----
"""Validate an XBRL fact dictionary before insertion.

    Checks:
    - Required fields present and non-empty
    - At least one value field is set (numeric, text, or boolean)
    - Ticker format is valid

    Args:
        fact_dict: Dictionary with XBRL fact fields.

    Returns:
        List of error strings. Empty list means valid.
    """
⋮----
# Check required fields
⋮----
# Check ticker format if present
ticker = fact_dict.get("ticker", "")
⋮----
# Check at least one value field
has_value = any(
</file>

<file path="ingestion/xbrl_processor.py">
"""
xbrl_processor.py
=================
Processes XBRL financial data from XML filings and Excel workbooks,
inserting structured facts into the PostgreSQL xbrl_facts table.

Features:
  - XBRLFact dataclass for structured fact representation
  - Parses XBRL/XML filings using lxml (IFRS taxonomy)
  - Parses Excel workbooks with XBRL-like data using openpyxl
  - SHA-256 content hash for deduplication
  - Batch insert with ON CONFLICT DO NOTHING on content_hash
  - Skip already-processed filings
  - process_filing(file_path), process_directory(dir_path), process_url(url)

Usage:
    # Process a single XML filing
    python ingestion/xbrl_processor.py --file data/filing.xml --ticker 2222.SR

    # Process a single workbook
    python ingestion/xbrl_processor.py --file data/filing.xlsx --ticker 2222.SR

    # Process a directory of filings
    python ingestion/xbrl_processor.py --dir data/filings/ --ticker-pattern "*.xml"

    # Dry run
    python ingestion/xbrl_processor.py --file data/filing.xml --ticker 2222.SR --dry-run
"""
⋮----
etree = None
⋮----
openpyxl = None
⋮----
psycopg2 = None
⋮----
requests = None
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Configuration
⋮----
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
BATCH_SIZE = 250
⋮----
# XBRL namespace prefixes used in Saudi IFRS filings
XBRL_NAMESPACES = {
⋮----
# Data structures
⋮----
@dataclass
class XBRLFact
⋮----
"""A single XBRL fact extracted from a financial filing."""
⋮----
ticker: str
concept: str  # XBRL concept (e.g., 'ifrs-full:Revenue')
label_en: Optional[str] = None
label_ar: Optional[str] = None
value_numeric: Optional[float] = None
value_text: Optional[str] = None
value_boolean: Optional[bool] = None
unit: Optional[str] = None  # e.g., 'SAR', 'shares', 'pure'
decimals: Optional[int] = None
period_start: Optional[date] = None
period_end: Optional[date] = None
period_instant: Optional[date] = None
dimension_member: Optional[str] = None
dimension_value: Optional[str] = None
source_url: Optional[str] = None
filing_id: Optional[str] = None  # UUID string of the parent filing
content_hash: str = field(default="", init=False)
⋮----
def __post_init__(self)
⋮----
"""Compute content hash for deduplication."""
⋮----
def _compute_hash(self) -> str
⋮----
"""SHA-256 hash of the fact's identity fields for dedup."""
parts = [
raw = "|".join(parts)
⋮----
def to_insert_tuple(self) -> tuple
⋮----
"""Return a tuple matching the xbrl_facts INSERT column order."""
⋮----
# Column order for INSERT
XBRL_INSERT_COLUMNS = [
⋮----
XBRL_INSERT_SQL = (
⋮----
# XBRLProcessor
⋮----
class XBRLProcessor
⋮----
"""Parses XBRL XML filings and Excel workbooks, extracting financial facts."""
⋮----
# Common IFRS concept mappings for label-to-concept conversion
IFRS_CONCEPT_MAP = {
⋮----
# Common sheet names in Saudi XBRL Excel exports
EXPECTED_SHEETS = [
⋮----
# ------------------------------------------------------------------
# Public API: process_filing, process_directory, process_url
⋮----
def process_filing(self, file_path: Path) -> list[XBRLFact]
⋮----
"""Process a single filing (XML or Excel) and return extracted facts.

        Dispatches to XML or Excel parser based on file extension.

        Args:
            file_path: Path to the filing file (.xml, .xbrl, .xlsx, .xls).

        Returns:
            List of XBRLFact objects extracted from the filing.
        """
file_path = Path(file_path)
⋮----
suffix = file_path.suffix.lower()
⋮----
def process_directory(self, dir_path: Path, pattern: str = "*") -> list[XBRLFact]
⋮----
"""Process all filing files in a directory.

        Args:
            dir_path: Directory containing filing files.
            pattern: Glob pattern for matching files (default: all files).

        Returns:
            Combined list of XBRLFact objects from all files.
        """
dir_path = Path(dir_path)
⋮----
all_facts = []
supported_extensions = {".xml", ".xbrl", ".xlsx", ".xls"}
files = sorted(dir_path.glob(pattern))
⋮----
facts = self.process_filing(file_path)
⋮----
"""Download a filing from a URL and process it.

        Args:
            url: URL of the filing to download.
            download_dir: Directory to save the downloaded file.
                          Defaults to a temp location under SCRIPT_DIR.

        Returns:
            List of XBRLFact objects extracted from the filing.
        """
⋮----
download_dir = SCRIPT_DIR / "_downloads"
download_dir = Path(download_dir)
⋮----
# Derive filename from URL
parsed = urlparse(url)
filename = Path(parsed.path).name or "filing.xml"
local_path = download_dir / filename
⋮----
resp = requests.get(url, timeout=60)
⋮----
# XML/XBRL parsing (lxml)
⋮----
def process_xml(self, file_path: Path) -> list[XBRLFact]
⋮----
"""Parse an XBRL XML filing and extract facts.

        Handles IFRS taxonomy elements. Each fact element in the XBRL instance
        document is mapped to an XBRLFact.

        Args:
            file_path: Path to the XML/XBRL file.

        Returns:
            List of XBRLFact objects.
        """
⋮----
tree = etree.parse(str(file_path))
⋮----
root = tree.getroot()
nsmap = root.nsmap.copy()
# Ensure default namespace doesn't break XPath
⋮----
# Parse context elements for period information
contexts = self._parse_contexts(root, nsmap)
⋮----
# Parse unit elements
units = self._parse_units(root, nsmap)
⋮----
# Extract facts from all non-structural elements
⋮----
tag = elem.tag
⋮----
# Skip structural elements (contexts, units, footnotes, etc.)
local_name = etree.QName(tag).localname if "}" in tag else tag
namespace = etree.QName(tag).namespace if "}" in tag else ""
⋮----
# Skip XBRL infrastructure elements
skip_namespaces = {
⋮----
# This is a fact element
text = (elem.text or "").strip()
⋮----
# Build concept name
concept = self._build_concept_name(namespace, local_name, nsmap)
⋮----
# Get context reference for period info
context_ref = elem.get("contextRef")
period_info = contexts.get(context_ref, {}) if context_ref else {}
⋮----
# Get unit reference
unit_ref = elem.get("unitRef")
unit = units.get(unit_ref, self.default_unit) if unit_ref else None
⋮----
# Get decimals
decimals_str = elem.get("decimals")
decimals = None
⋮----
decimals = int(decimals_str)
⋮----
# Determine value type
value_numeric = None
value_text = None
value_boolean = None
⋮----
value_boolean = text.lower() == "true"
⋮----
value_numeric = float(text.replace(",", ""))
⋮----
value_text = text
⋮----
# Extract dimension info from context
dimension_member = period_info.get("dimension_member")
dimension_value = period_info.get("dimension_value")
⋮----
fact = XBRLFact(
⋮----
def _parse_contexts(self, root, nsmap: dict) -> dict
⋮----
"""Parse xbrli:context elements to build period lookup.

        Returns dict mapping context id -> {period_start, period_end, period_instant,
        dimension_member, dimension_value}.
        """
contexts = {}
⋮----
# Try multiple namespace resolution approaches
context_elements = root.findall(".//{http://www.xbrl.org/2003/instance}context")
⋮----
# Fallback: search all elements with 'context' local name
context_elements = [
⋮----
ctx_id = ctx.get("id")
⋮----
info = {
⋮----
# Parse period
⋮----
tag = period_elem.tag if isinstance(period_elem.tag, str) else ""
local = tag.split("}")[-1] if "}" in tag else tag
text = (period_elem.text or "").strip()
⋮----
# Parse dimension (scenario/segment explicit members)
⋮----
tag = member_elem.tag if isinstance(member_elem.tag, str) else ""
⋮----
dimension = member_elem.get("dimension", "")
value = (member_elem.text or "").strip()
⋮----
def _parse_units(self, root, nsmap: dict) -> dict
⋮----
"""Parse xbrli:unit elements to build unit lookup.

        Returns dict mapping unit id -> unit string (e.g., 'SAR').
        """
units = {}
⋮----
unit_elements = root.findall(".//{http://www.xbrl.org/2003/instance}unit")
⋮----
unit_elements = [
⋮----
unit_id = unit_elem.get("id")
⋮----
# Look for measure element
⋮----
tag = measure.tag if isinstance(measure.tag, str) else ""
⋮----
text = (measure.text or "").strip()
# Extract currency code from namespace-prefixed value
# e.g., "iso4217:SAR" -> "SAR"
⋮----
text = text.split(":")[-1]
⋮----
def _build_concept_name(self, namespace: str, local_name: str, nsmap: dict) -> str
⋮----
"""Build a prefixed concept name from namespace and local name.

        Tries to find a matching prefix in the document's namespace map.
        Falls back to known IFRS prefixes.
        """
# Try to find prefix in document namespace map
⋮----
# Check known prefixes
known = {
⋮----
# Fallback
⋮----
# Use last path segment as prefix
parts = namespace.rstrip("/").split("/")
prefix = parts[-1] if parts else "unknown"
⋮----
@staticmethod
    def _safe_parse_date(text: str) -> Optional[date]
⋮----
"""Parse a date string, returning None on failure."""
text = text.strip()
⋮----
# Excel/workbook parsing (openpyxl) - preserved from original
⋮----
def process_workbook(self, file_path: Path) -> list[XBRLFact]
⋮----
"""Process an Excel workbook and extract XBRL facts.

        Args:
            file_path: Path to the Excel file.

        Returns:
            List of XBRLFact objects extracted from the workbook.
        """
⋮----
wb = openpyxl.load_workbook(str(file_path), read_only=True, data_only=True)
⋮----
ws = wb[sheet_name]
sheet_facts = self._process_sheet(ws, sheet_name)
⋮----
def _process_sheet(self, ws, sheet_name: str) -> list[XBRLFact]
⋮----
"""Process a single worksheet.

        Expects a tabular layout where:
        - Column A: Concept/line item name (English or Arabic label)
        - Column B+: Period values (headers indicate dates)

        The first row is treated as headers containing period dates.
        """
facts = []
rows = list(ws.iter_rows(values_only=True))
⋮----
# Parse headers to extract period dates
headers = rows[0]
period_dates = self._parse_period_headers(headers)
⋮----
# Process data rows
⋮----
concept_label = str(row[0]).strip()
⋮----
# Derive concept identifier from label
concept = self._label_to_concept(concept_label, sheet_name)
⋮----
# Detect if label is Arabic
label_ar = concept_label if self._is_arabic(concept_label) else None
label_en = concept_label if not self._is_arabic(concept_label) else None
⋮----
# Extract values for each period column
⋮----
cell_value = row[col_idx]
⋮----
period_info = period_dates.get(col_idx, {})
period_end = period_info.get("period_end")
period_start = period_info.get("period_start")
period_instant = period_info.get("period_instant")
⋮----
# Skip if no period date context at all
⋮----
value_boolean = cell_value
⋮----
value_numeric = float(cell_value)
⋮----
text_val = str(cell_value).strip()
⋮----
value_numeric = float(text_val.replace(",", ""))
⋮----
value_text = text_val
⋮----
def _parse_period_headers(self, headers) -> dict
⋮----
"""Parse column headers to extract period date information."""
period_dates = {}
⋮----
header_str = str(header).strip()
parsed = self._parse_date_string(header_str)
⋮----
def _parse_date_string(self, s: str) -> Optional[dict]
⋮----
"""Parse a date string from a header into period context.

        Handles: '2024-12-31', '31/12/2024', 'FY 2024', 'Q1 2024', '2024'
        """
s = s.strip()
⋮----
# ISO format: YYYY-MM-DD and variants
⋮----
d = datetime.strptime(s, fmt).date()
⋮----
# Fiscal year: FY 2024, FY2024
s_upper = s.upper().replace(" ", "")
⋮----
year = int(s_upper[2:6])
⋮----
# Quarter: Q1 2024, Q2 2024, etc.
quarter_ends = {
⋮----
year = int(s_upper[len(q) : len(q) + 4])
quarter_start_month = month - 2
⋮----
# Year only: 2024
⋮----
year = int(s)
⋮----
def _label_to_concept(self, label: str, sheet_name: str) -> str
⋮----
"""Convert a human-readable label to an XBRL-like concept identifier."""
normalized = label.strip().lower()
⋮----
# Fallback: create a concept from the label in PascalCase
words = normalized.replace("-", " ").replace("_", " ").split()
pascal = "".join(w.capitalize() for w in words if w)
⋮----
# Determine prefix based on sheet context
prefix = "tasi"
⋮----
prefix = "ifrs-full"
⋮----
@staticmethod
    def _is_arabic(text: str) -> bool
⋮----
"""Check if text contains Arabic characters."""
⋮----
# Database operations
⋮----
def _get_connection(pg_conn_or_pool)
⋮----
"""Get a connection, supporting both direct connections and pool context managers."""
⋮----
def insert_facts(pg_conn, facts: list[XBRLFact], dry_run: bool = False) -> int
⋮----
"""Insert XBRL facts into PostgreSQL. Returns count of rows inserted."""
⋮----
rows = [f.to_insert_tuple() for f in facts]
cur = pg_conn.cursor()
inserted = 0
⋮----
batch = rows[i : i + BATCH_SIZE]
⋮----
"""Create a filing record and return its UUID."""
⋮----
filing_id = str(cur.fetchone()[0])
⋮----
def check_filing_exists(pg_conn, ticker: str, source_url: str) -> bool
⋮----
"""Check if a filing has already been processed (by ticker + source_url)."""
⋮----
def mark_filing_complete(pg_conn, filing_id: str, dry_run: bool = False) -> None
⋮----
"""Mark a filing as completed."""
⋮----
def mark_filing_failed(pg_conn, filing_id: str, dry_run: bool = False) -> None
⋮----
"""Mark a filing as failed."""
⋮----
# Main
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser(
group = parser.add_mutually_exclusive_group(required=True)
⋮----
"""Process a single filing file. Returns (facts_count, errors)."""
⋮----
# Skip already-processed filings
⋮----
# Create filing record
filing_date = date.today()
filing_id = create_filing(
⋮----
# Parse filing
processor = XBRLProcessor(
facts = processor.process_filing(file_path)
⋮----
# Insert facts
count = insert_facts(pg_conn, facts, dry_run)
⋮----
# Update filing status
⋮----
def main()
⋮----
args = parse_args()
t_start = time.time()
⋮----
# Connect to PostgreSQL
pg_conn = None
⋮----
pg_conn = psycopg2.connect(
⋮----
total_facts = 0
total_errors = []
⋮----
file_path = Path(args.file)
⋮----
processor = XBRLProcessor(ticker=args.ticker, source_url=args.url)
facts = processor.process_url(args.url)
⋮----
count = insert_facts(pg_conn, facts, args.dry_run)
⋮----
dir_path = Path(args.dir)
⋮----
files = sorted(dir_path.glob(args.ticker_pattern))
supported = {".xml", ".xbrl", ".xlsx", ".xls"}
files = [f for f in files if f.suffix.lower() in supported]
⋮----
# Try to extract ticker from filename (e.g., '2222.SR_annual.xlsx')
ticker = args.ticker
⋮----
stem = file_path.stem
parts = stem.split("_")
⋮----
ticker = parts[0]
⋮----
# Summary
elapsed = time.time() - t_start
</file>

<file path="requirements.in">
# requirements.in — Unpinned source constraints (edit this, not requirements.txt)
# Run: pip-compile requirements.in -o requirements.txt to regenerate the pinned lock

# Core framework
vanna>=2.0.2,<3.0
fastapi>=0.115.6,<1.0
uvicorn[standard]>=0.34.0,<1.0
python-dotenv>=1.0.1,<2.0
pydantic-settings>=2.0.0,<3.0
pydantic[email]>=2.5.0,<3.0

# LLM providers
openai>=1.20.0,<3.0
anthropic>=0.41.0,<1.0

# Database
psycopg2-binary>=2.9.10,<3.0
sqlalchemy[asyncio]>=2.0.0,<3.0
aiosqlite>=0.20.0,<1.0

# Data & visualization
pandas>=2.1.0,<3.0
numpy>=1.24.0,<3.0
plotly>=5.20.0,<7.0
yfinance>=0.2.35,<2.0

# Web scraping
lxml>=4.10.0,<7.0
beautifulsoup4>=4.12.0,<5.0
requests>=2.31.0,<3.0

# Auth
pyjwt>=2.8.1,<3.0
bcrypt>=4.1.0,<5.0

# Caching & messaging
redis>=5.0.0,<8.0
msgpack>=1.0.0,<2.0
prometheus-fastapi-instrumentator>=0.9.1,<3.0

# Scheduling
apscheduler>=3.10.4,<4.0

# SQL parsing
sqlparse>=0.5.0,<1.0
</file>

<file path="scripts/export_openapi.py">
#!/usr/bin/env python3
"""
Export the OpenAPI schema from the Ra'd AI TASI API.

Usage:
    python scripts/export_openapi.py                # Print to stdout
    python scripts/export_openapi.py -o openapi.json  # Write to file
    python scripts/export_openapi.py --yaml          # Output as YAML (requires pyyaml)
"""
⋮----
# Ensure project root is on sys.path
_PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
def get_openapi_schema() -> dict
⋮----
"""Import the FastAPI app and extract its OpenAPI schema."""
⋮----
def main() -> None
⋮----
parser = argparse.ArgumentParser(
⋮----
args = parser.parse_args()
⋮----
schema = get_openapi_schema()
⋮----
content = yaml.dump(schema, default_flow_style=False, allow_unicode=True)
⋮----
content = json.dumps(schema, indent=2, ensure_ascii=False)
⋮----
output_path = Path(args.output)
</file>

<file path="scripts/generate_system_prompt.py">
"""
Generate SYSTEM_PROMPT from live database schema.

Connects to SQLite or PostgreSQL (auto-detected from DB_BACKEND env var),
reads table and column metadata, and prints a formatted system prompt string
matching the format used in app.py.

Usage:
    python scripts/generate_system_prompt.py              # print to stdout
    python scripts/generate_system_prompt.py > prompt.txt # save to file
"""
⋮----
# Allow running from repo root or scripts/
⋮----
DB_BACKEND = os.environ.get("DB_BACKEND", "sqlite").lower()
⋮----
# Financial statement tables have multiple rows per company
_MULTI_ROW_TABLES = {"balance_sheet", "income_statement", "cash_flow"}
⋮----
def _get_sqlite_schema() -> dict[str, list[tuple[str, str]]]
⋮----
"""Return {table_name: [(col_name, col_type), ...]} from SQLite."""
⋮----
db_path = Path(__file__).resolve().parent.parent / os.environ.get(
conn = sqlite3.connect(str(db_path))
⋮----
cur = conn.cursor()
⋮----
tables = [row[0] for row in cur.fetchall()]
⋮----
schema: dict[str, list[tuple[str, str]]] = {}
⋮----
columns = [(row[1], row[2]) for row in cur.fetchall()]
⋮----
def _get_postgres_schema() -> dict[str, list[tuple[str, str]]]
⋮----
"""Return {table_name: [(col_name, col_type), ...]} from PostgreSQL."""
⋮----
conn = psycopg2.connect(
⋮----
def _format_prompt(schema: dict[str, list[tuple[str, str]]]) -> str
⋮----
"""Format the schema dict into the system prompt string."""
lines = [
⋮----
def main() -> None
⋮----
schema = _get_postgres_schema()
⋮----
schema = _get_sqlite_schema()
⋮----
prompt = _format_prompt(schema)
</file>

<file path="scripts/smoke_test.py">
"""
Ra'd AI Platform -- Smoke Test (Python)
========================================
Validates a running Ra'd AI instance end-to-end by hitting key endpoints.

Usage:
    python scripts/smoke_test.py [BASE_URL]

Examples:
    python scripts/smoke_test.py
    python scripts/smoke_test.py http://localhost:8084
    python scripts/smoke_test.py https://raid-ai-app-production.up.railway.app
"""
⋮----
DEFAULT_BASE_URL = "http://localhost:8084"
TIMEOUT = 10  # seconds
⋮----
# ---------------------------------------------------------------------------
# Colored output helpers
⋮----
class Colors
⋮----
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
CYAN = "\033[96m"
BOLD = "\033[1m"
RESET = "\033[0m"
⋮----
def _supports_color() -> bool
⋮----
"""Return True if stdout likely supports ANSI colors."""
⋮----
USE_COLOR = _supports_color()
⋮----
def _c(color: str, text: str) -> str
⋮----
# Result tracking
⋮----
_passed = 0
_failed = 0
_skipped = 0
⋮----
def _pass(name: str, detail: str = "") -> None
⋮----
suffix = f" ({detail})" if detail else ""
⋮----
def _fail(name: str, detail: str = "") -> None
⋮----
def _skip(name: str, reason: str = "") -> None
⋮----
suffix = f" ({reason})" if reason else ""
⋮----
# HTTP helpers (stdlib only -- no requests dependency)
⋮----
def _get(url: str, headers: dict | None = None) -> tuple[int, bytes]
⋮----
"""Perform a GET request, return (status_code, body_bytes)."""
req = urllib.request.Request(url, headers=headers or {})
⋮----
resp = urllib.request.urlopen(req, timeout=TIMEOUT)
⋮----
def _get_json(url: str, headers: dict | None = None) -> tuple[int, dict | None]
⋮----
"""GET request returning (status_code, parsed_json_or_None)."""
⋮----
# Test cases
⋮----
def test_tasi_health(base: str) -> None
⋮----
"""(1) /api/v1/charts/tasi/health returns JSON with status field."""
url = f"{base}/api/v1/charts/tasi/health"
⋮----
def test_tasi_index(base: str) -> None
⋮----
"""(2) /api/v1/charts/tasi/index returns data with count > 0."""
url = f"{base}/api/v1/charts/tasi/index"
⋮----
count = data.get("count", 0)
⋮----
def test_auth_protected_no_token(base: str) -> None
⋮----
"""(3) Protected endpoint without token returns 401."""
url = f"{base}/api/auth/me"
⋮----
def test_frontend_root(base: str) -> None
⋮----
"""(4a) Frontend root / returns 200."""
url = f"{base}/"
⋮----
def test_health_endpoint(base: str) -> None
⋮----
"""(4b) /health returns 200 or 503 with JSON."""
url = f"{base}/health"
⋮----
def test_openapi_json(base: str) -> None
⋮----
"""(5) /openapi.json returns valid JSON schema."""
url = f"{base}/openapi.json"
⋮----
paths_count = len(data.get("paths", {}))
⋮----
def test_guest_login(base: str) -> None
⋮----
"""(6) Guest login returns a JWT token."""
url = f"{base}/api/auth/guest"
req = urllib.request.Request(
⋮----
status = resp.status
body = resp.read()
⋮----
status = e.code
body = e.read()
⋮----
data = json.loads(body)
⋮----
def test_tasi_invalid_period(base: str) -> None
⋮----
"""(7) Invalid period returns 400."""
url = f"{base}/api/v1/charts/tasi/index?period=invalid"
⋮----
# Runner
⋮----
def main() -> int
⋮----
base_url = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_BASE_URL
base_url = base_url.rstrip("/")
⋮----
# Connectivity check
⋮----
# Run all tests
⋮----
# Summary
total = _passed + _failed
⋮----
summary = f"  Results: {_passed}/{total} passed, {_failed} failed"
</file>

<file path="scripts/smoke_test.sh">
#!/bin/bash
# =============================================================================
# Ra'd AI Platform - Smoke Test (Bash Wrapper)
# =============================================================================
# Runs the Python smoke test against a running Ra'd AI instance.
# Usage: ./scripts/smoke_test.sh [BASE_URL]
#
# Examples:
#   ./scripts/smoke_test.sh                           # localhost:8084
#   ./scripts/smoke_test.sh http://localhost:8084      # explicit local
#   ./scripts/smoke_test.sh https://raid-ai-app-production.up.railway.app
# =============================================================================

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BASE_URL="${1:-http://localhost:8084}"

echo "Running Ra'd AI smoke tests against: $BASE_URL"
echo ""

# Use Python smoke test script
python "$SCRIPT_DIR/smoke_test.py" "$BASE_URL"
exit $?
</file>

<file path="scripts/test_news_api.py">
"""Test news store and API readiness"""
⋮----
# Force UTF-8 output on Windows
⋮----
db_path = str(Path(__file__).resolve().parent.parent / "saudi_stocks.db")
store = NewsStore(db_path)
⋮----
# Fetch and store
articles = fetch_all_news()
stored = store.store_articles(articles)
⋮----
# Read back
latest = store.get_latest_news(limit=5)
⋮----
sources = store.get_sources()
⋮----
total = store.count_articles()
</file>

<file path="scripts/test_news_scraper.py">
"""Quick smoke test for the news scraper"""
⋮----
# Force UTF-8 output on Windows
⋮----
articles = fetch_all_news()
</file>

<file path="scripts/validate_charts.py">
"""Validate that all 8 suggestion chip queries produce the correct chart types.

Runs representative SQL queries against the real SQLite database
and verifies the RaidChartGenerator selects the expected chart type.
"""
⋮----
import pandas as pd  # noqa: E402
⋮----
# Ensure project root on path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from chart_engine import RaidChartGenerator  # noqa: E402
⋮----
DB_PATH = PROJECT_ROOT / "saudi_stocks.db"
⋮----
def main()
⋮----
conn = sqlite3.connect(str(DB_PATH))
gen = RaidChartGenerator()
⋮----
tests = [
⋮----
"expected_type": "scatter",  # Plotly line chart = scatter with mode=lines
⋮----
"expected_type": "heatmap",  # 1 text + 3 numeric = value heatmap
⋮----
passed = 0
failed = 0
⋮----
df = pd.read_sql(test["sql"], conn)
result = gen.generate_chart(df, test["chip"])
actual_type = result["data"][0]["type"]
ok = actual_type == test["expected_type"]
⋮----
status = "PASS" if ok else "FAIL"
</file>

<file path="scripts/validate_config.py">
"""
Configuration validation script for TASI AI Platform.

Validates that all required environment variables are set and consistent.
Can be run standalone or imported and called from app.py lifespan.

Usage:
    python scripts/validate_config.py          # standalone
    python -m scripts.validate_config          # as module

From app.py:
    from scripts.validate_config import validate_config
    issues = validate_config()
"""
⋮----
_log = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Validation checks
⋮----
def _check_llm_provider() -> list[str]
⋮----
"""Validate exactly one LLM provider is configured."""
issues: list[str] = []
anthropic_key = os.environ.get("ANTHROPIC_API_KEY", "").strip()
llm_key = os.environ.get("LLM_API_KEY", "").strip()
⋮----
def _check_database_config() -> list[str]
⋮----
"""Validate database settings match the chosen backend."""
⋮----
backend = os.environ.get("DB_BACKEND", "sqlite").lower()
⋮----
required_pg_vars = {
⋮----
db_val = os.environ.get(db_name, "").strip()
pg_val = os.environ.get(pg_name, "").strip()
⋮----
pw = db_val or pg_val
⋮----
def _check_jwt_secret() -> list[str]
⋮----
"""Validate JWT secret is set in production."""
⋮----
jwt_secret = os.environ.get("AUTH_JWT_SECRET", "").strip()
environment = os.environ.get("ENVIRONMENT", "development").lower()
debug = os.environ.get("SERVER_DEBUG", "true").lower()
is_production = environment == "production" or debug in ("false", "0", "no")
⋮----
def _check_env_naming_consistency() -> list[str]
⋮----
"""Warn if both DB_PG_* and POSTGRES_* are set with conflicting values."""
⋮----
pairs = [
⋮----
# Public API
⋮----
def validate_config(*, fail_fast: bool = False) -> list[str]
⋮----
"""
    Run all configuration checks.

    Args:
        fail_fast: If True, raise ValueError on the first FAIL-level issue.

    Returns:
        List of issue strings (prefixed with FAIL: or WARN:).
    """
all_issues: list[str] = []
⋮----
issues = check()
⋮----
def print_validation_report(issues: list[str] | None = None) -> bool
⋮----
"""
    Print a human-readable validation report.

    Returns True if all checks pass (no FAIL-level issues).
    """
⋮----
issues = validate_config()
⋮----
fails = [i for i in issues if i.startswith("FAIL:")]
warns = [i for i in issues if i.startswith("WARN:")]
⋮----
prefix = "  [X]" if issue.startswith("FAIL:") else "  [!]"
# Strip the FAIL:/WARN: prefix for cleaner display
msg = issue.split(":", 1)[1].strip() if ":" in issue else issue
tag = "FAIL" if issue.startswith("FAIL:") else "WARN"
⋮----
# Standalone entry point
⋮----
ok = print_validation_report()
</file>

<file path="services/cache_utils.py">
"""
Unified caching decorator with Redis / in-memory TTLCache fallback.

Usage::

    from services.cache_utils import cache_response

    @cache_response(ttl=300)
    def get_market_data(ticker: str) -> dict:
        ...

    @cache_response(ttl=60)
    async def get_live_price(ticker: str) -> dict:
        ...

The decorator transparently picks Redis when available, otherwise
falls back to a simple in-memory TTLCache.  Works with both sync
and async callables and preserves function metadata via functools.wraps.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
F = TypeVar("F", bound=Callable[..., Any])
⋮----
# Default in-memory TTL cache capacity
_DEFAULT_MAX_ENTRIES = 1024
⋮----
class TTLCache
⋮----
"""Thread-safe in-memory cache with per-entry TTL and LRU eviction.

    Args:
        default_ttl: Default time-to-live in seconds.
        max_entries: Maximum entries before LRU eviction kicks in.
    """
⋮----
def get(self, key: str) -> Optional[Any]
⋮----
entry = self._store.get(key)
⋮----
# Expired -- remove lazily
⋮----
def put(self, key: str, value: Any, ttl: Optional[int] = None) -> None
⋮----
effective_ttl = ttl if ttl is not None else self._default_ttl
⋮----
# Module-level singleton -- shared across all decorated functions.
_fallback_cache = TTLCache()
⋮----
# Redis client placeholder -- set externally via ``configure_redis``.
_redis_client: Any = None
⋮----
# Consecutive Redis failure tracking (S2-B3)
_redis_fail_count: int = 0
_REDIS_WARN_THRESHOLD: int = 3
⋮----
def configure_redis(client: Any) -> None
⋮----
"""Set a Redis client for the caching layer.

    Args:
        client: A ``redis.Redis`` (or compatible) instance.  If *None*,
            the decorator falls back to in-memory TTLCache.
    """
⋮----
_redis_client = client
⋮----
def _make_key(func: Callable, args: tuple, kwargs: dict) -> str
⋮----
"""Build a cache key from function qualname + call arguments."""
parts = [func.__module__, func.__qualname__]
⋮----
def _cache_get(key: str) -> Any
⋮----
"""Try Redis GET, then in-memory. Returns None on miss."""
⋮----
cached = _redis_client.get(key)
⋮----
_redis_fail_count = 0  # reset on success
⋮----
def _cache_put(key: str, value: Any, ttl: int) -> None
⋮----
"""Store in Redis and in-memory fallback."""
⋮----
def cache_response(ttl: int = 300) -> Callable[[F], F]
⋮----
"""Decorator that caches function return values.

    Uses Redis when available (via ``configure_redis``), otherwise
    falls back to in-memory ``TTLCache``.  Works with both sync and
    async functions.

    Args:
        ttl: Time-to-live in seconds for cached results.
    """
⋮----
def decorator(func: F) -> F
⋮----
@functools.wraps(func)
            async def async_wrapper(*args: Any, **kwargs: Any) -> Any
⋮----
key = _make_key(func, args, kwargs)
cached = _cache_get(key)
⋮----
result = await func(*args, **kwargs)
⋮----
return async_wrapper  # type: ignore[return-value]
⋮----
@functools.wraps(func)
            def sync_wrapper(*args: Any, **kwargs: Any) -> Any
⋮----
result = func(*args, **kwargs)
⋮----
return sync_wrapper  # type: ignore[return-value]
</file>

<file path="services/sqlite_pool.py">
"""Thread-safe SQLite connection pool with WAL mode."""
⋮----
logger = logging.getLogger(__name__)
⋮----
class SQLitePool
⋮----
def __init__(self, db_path: str, pool_size: int = 5)
⋮----
def _make_conn(self) -> sqlite3.Connection
⋮----
conn = sqlite3.connect(self._db_path, check_same_thread=False)
⋮----
def acquire(self, timeout: float = 10.0) -> sqlite3.Connection
⋮----
def release(self, conn: sqlite3.Connection) -> None
⋮----
class _Ctx
⋮----
def __init__(self, pool: "SQLitePool")
⋮----
def __enter__(self) -> sqlite3.Connection
⋮----
def __exit__(self, *_)
⋮----
def connection(self) -> "_Ctx"
⋮----
_pool: Optional[SQLitePool] = None
⋮----
def init_pool(db_path: str, pool_size: int = 5) -> None
⋮----
_pool = SQLitePool(db_path, pool_size)
⋮----
def get_pool() -> SQLitePool
</file>

<file path="services/yfinance_base.py">
"""
Shared yfinance utilities: LRU cache with TTL and circuit breaker.

Extracted from stock_ohlcv.py and tasi_index.py to eliminate ~200 lines
of duplicated cache + circuit breaker logic.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Default LRU cache capacity
DEFAULT_MAX_ENTRIES = 500
⋮----
class YFinanceCache
⋮----
"""Thread-safe LRU cache with TTL expiration.

    Uses OrderedDict to maintain insertion/access order and evicts
    the oldest entries when the cache exceeds ``max_entries``.

    Args:
        ttl: Time-to-live in seconds for cache entries.
        max_entries: Maximum number of entries before LRU eviction.
        name: Human-readable name for log messages.
    """
⋮----
@property
    def ttl(self) -> int
⋮----
def get(self, key: Union[str, Tuple]) -> Optional[Dict[str, Any]]
⋮----
"""Return cached payload if still fresh, else None."""
⋮----
entry = self._store.get(key)
⋮----
age = time.monotonic() - entry["fetched_at"]
⋮----
# Move to end (most-recently-used)
⋮----
def get_stale(self, key: Union[str, Tuple]) -> Optional[Dict[str, Any]]
⋮----
"""Return cached payload even if stale (for fallback on fetch failure)."""
⋮----
def put(self, key: Union[str, Tuple], payload: Dict[str, Any]) -> None
⋮----
"""Insert or update a cache entry, evicting oldest if over capacity."""
⋮----
# Evict oldest entries if over capacity
⋮----
def newest_entry(self) -> Optional[Dict[str, Any]]
⋮----
"""Return the most recently fetched entry, or None if empty."""
⋮----
# Last item in OrderedDict is most recently used/inserted
key = next(reversed(self._store))
⋮----
def clear(self) -> None
⋮----
"""Remove all entries from the cache."""
⋮----
def __getitem__(self, key: Union[str, Tuple]) -> Dict[str, Any]
⋮----
"""Dict-like access for backward compatibility with tests."""
⋮----
def __setitem__(self, key: Union[str, Tuple], value: Dict[str, Any]) -> None
⋮----
"""Dict-like assignment for backward compatibility with tests."""
⋮----
def __contains__(self, key: Union[str, Tuple]) -> bool
⋮----
def __len__(self) -> int
⋮----
def __bool__(self) -> bool
⋮----
class CircuitBreaker
⋮----
"""Thread-safe circuit breaker for external API calls.

    Opens the circuit after ``threshold`` consecutive failures and
    keeps it open for ``timeout`` seconds before allowing retries.

    Args:
        threshold: Number of consecutive failures before opening.
        timeout: Seconds to keep the circuit open.
        name: Human-readable name for log messages.
    """
⋮----
@property
    def threshold(self) -> int
⋮----
@property
    def timeout(self) -> int
⋮----
def is_open(self) -> bool
⋮----
"""Return True if the circuit breaker is currently open."""
⋮----
def record_failure(self) -> None
⋮----
"""Increment failure count; open circuit if threshold reached."""
⋮----
def record_success(self) -> None
⋮----
"""Reset circuit breaker on a successful call."""
⋮----
was_open = self.is_open()
⋮----
def get_status(self) -> Dict[str, Any]
⋮----
"""Return circuit breaker diagnostics for health endpoints."""
⋮----
is_open = self.is_open()
remaining = (
</file>

<file path="templates/raid-features.js">
/**
 * raid-features.js
 * Interactive features for the Ra'd AI legacy UI.
 * Loaded with defer so the DOM is ready.
 *
 * Features:
 *   1. Theme Toggle (Dark/Light)
 *   2. "New Chat" Button
 *   3. Suggestion Persistence
 *   4. Keyboard Shortcut (Ctrl+K / Cmd+K)
 *   5. Onboarding Overlay
 *   6. Enhanced CDN Fallback
 *   7. Data Freshness Indicator
 *   8. Error Boundary for Network
 */
⋮----
// =====================================================================
// 1. THEME TOGGLE (Dark/Light)
// =====================================================================
⋮----
// Original dark values (restored when toggling back)
⋮----
function applyThemeProperties(theme)
⋮----
function setTheme(mode)
⋮----
try { localStorage.setItem('raid-theme', mode); } catch (e) { /* ignored */ }
⋮----
function updateThemeIcon(mode)
⋮----
// Sun icon in dark mode (click to go light), moon icon in light mode (click to go dark)
⋮----
function getCurrentTheme()
⋮----
} catch (e) { /* ignored */ }
// Fall back to OS preference
⋮----
// Create theme toggle button
⋮----
// =====================================================================
// 2. "NEW CHAT" BUTTON
// =====================================================================
⋮----
// Re-show suggestions if they were hidden
⋮----
// Remove "Show Suggestions" button if it exists
⋮----
try { sessionStorage.removeItem('raid-suggestions-visible'); } catch (e) { /* ignored */ }
⋮----
// Insert buttons into .header-status (before status dot)
⋮----
// Insert new chat first, then theme toggle, so order is: [New Chat] [Theme] [dot] [label]
⋮----
// Apply saved/preferred theme on load
⋮----
// =====================================================================
// 3. SUGGESTION PERSISTENCE
// =====================================================================
⋮----
function createShowSuggestionsButton()
⋮----
if (showSuggestionsBtn) return; // already exists
⋮----
// Restore keyboard accessibility
⋮----
try { sessionStorage.setItem('raid-suggestions-visible', 'true'); } catch (e) { /* ignored */ }
⋮----
// Insert after hero section or before chat container
⋮----
// Insert after suggestions section (which is collapsed)
⋮----
// Use MutationObserver to detect when suggestions get hidden
⋮----
try { sessionStorage.setItem('raid-suggestions-visible', 'false'); } catch (e) { /* ignored */ }
⋮----
// Restore suggestion state from sessionStorage on load
⋮----
} catch (e) { /* ignored */ }
⋮----
// =====================================================================
// 4. KEYBOARD SHORTCUT (Ctrl+K / Cmd+K)
// =====================================================================
⋮----
// Focus the native chat input
⋮----
// =====================================================================
// 5. ONBOARDING OVERLAY
// =====================================================================
⋮----
try { hasOnboarded = localStorage.getItem('raid-onboarded') === 'true'; } catch (e) { /* ignored */ }
⋮----
// Find or create overlay container
⋮----
function showOnboardingStep(stepIndex)
⋮----
// Overlay background
⋮----
// Highlight target element
⋮----
// Tooltip card
⋮----
// Bind button events
⋮----
// Click overlay background to skip
⋮----
function clearStepHighlight(stepIndex)
⋮----
function dismissOnboarding()
⋮----
// Clear all highlights
⋮----
try { localStorage.setItem('raid-onboarded', 'true'); } catch (e) { /* ignored */ }
⋮----
// Start onboarding after a short delay to let animations finish
⋮----
// =====================================================================
// 6. (REMOVED - CDN fallback no longer needed; native chat UI is used)
// =====================================================================
⋮----
// =====================================================================
// 7. DATA FRESHNESS INDICATOR
// =====================================================================
⋮----
// Use current date as fallback
⋮----
// Append to footer
⋮----
// =====================================================================
// 8. ERROR BOUNDARY FOR NETWORK
// =====================================================================
⋮----
function getToastContainer()
⋮----
function showNetworkToast(message)
⋮----
// Remove previous toast if any
⋮----
// Bind dismiss
⋮----
// Bind retry
⋮----
// Auto-dismiss after 10 seconds
⋮----
function escapeHtml(text)
⋮----
function isNetworkError(error)
⋮----
// =====================================================================
// 9. (REMOVED - Shadow DOM overrides no longer needed; native chat UI)
// =====================================================================
⋮----
// =====================================================================
// 10. SUGGESTION KEYBOARD ACCESSIBILITY
// =====================================================================
⋮----
// Enhance the existing MutationObserver for suggestions to manage tabindex
⋮----
// Make collapsed chips non-focusable
</file>

<file path="tests/integration/test_api_chain.py">
"""
Integration Tests: Backend API Chain
=====================================
Tests the TASI index API pipeline end-to-end using FastAPI TestClient.

Covers:
  - GET /api/v1/charts/tasi/index with valid period -> response schema
  - GET /api/v1/charts/tasi/index with invalid period -> 400/422
  - GET /api/v1/charts/tasi/health -> enriched response
  - Mock yfinance failure -> circuit breaker fallback
  - Rate limiting -> 429

Markers:
  - integration: requires full app assembly (no external services)
"""
⋮----
# Ensure project root on sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture(scope="module")
def tasi_app()
⋮----
"""Create a minimal FastAPI app with just the TASI router."""
⋮----
app = FastAPI()
⋮----
@pytest.fixture(scope="module")
def tasi_client(tasi_app)
⋮----
"""TestClient for TASI endpoints."""
⋮----
@pytest.fixture(autouse=True)
def _clear_tasi_cache()
⋮----
"""Clear TASI cache between tests for isolation."""
⋮----
# Reset circuit breaker
⋮----
# TASI Index -- Valid periods
⋮----
class TestTASIIndexValidPeriods
⋮----
"""Test GET /api/v1/charts/tasi/index with valid periods."""
⋮----
@pytest.mark.integration
    def test_default_period_returns_200(self, tasi_client)
⋮----
resp = tasi_client.get("/api/v1/charts/tasi/index")
⋮----
@pytest.mark.integration
    def test_response_schema_has_required_fields(self, tasi_client)
⋮----
body = resp.json()
required = {"data", "source", "last_updated", "symbol", "period", "count"}
⋮----
@pytest.mark.integration
    def test_count_matches_data_length(self, tasi_client)
⋮----
@pytest.mark.integration
    def test_data_points_have_ohlcv_keys(self, tasi_client)
⋮----
point = body["data"][0]
⋮----
@pytest.mark.integration
    def test_explicit_period_3mo(self, tasi_client)
⋮----
resp = tasi_client.get("/api/v1/charts/tasi/index?period=3mo")
⋮----
@pytest.mark.integration
@pytest.mark.parametrize("period", ["1mo", "3mo", "6mo", "1y", "2y", "5y"])
    def test_all_valid_periods(self, tasi_client, period)
⋮----
resp = tasi_client.get(f"/api/v1/charts/tasi/index?period={period}")
⋮----
# TASI Index -- Invalid periods
⋮----
class TestTASIIndexInvalidPeriods
⋮----
"""Test GET /api/v1/charts/tasi/index with invalid periods."""
⋮----
@pytest.mark.integration
    def test_invalid_period_returns_400(self, tasi_client)
⋮----
resp = tasi_client.get("/api/v1/charts/tasi/index?period=invalid")
⋮----
@pytest.mark.integration
    def test_invalid_period_has_detail(self, tasi_client)
⋮----
resp = tasi_client.get("/api/v1/charts/tasi/index?period=bogus")
⋮----
@pytest.mark.integration
@pytest.mark.parametrize("bad_period", ["10y", "1d", "1w", "abc", ""])
    def test_various_invalid_periods(self, tasi_client, bad_period)
⋮----
resp = tasi_client.get(f"/api/v1/charts/tasi/index?period={bad_period}")
⋮----
# TASI Health
⋮----
class TestTASIHealth
⋮----
"""Test GET /api/v1/charts/tasi/health."""
⋮----
@pytest.mark.integration
    def test_health_returns_200(self, tasi_client)
⋮----
resp = tasi_client.get("/api/v1/charts/tasi/health")
⋮----
@pytest.mark.integration
    def test_health_has_status_and_message(self, tasi_client)
⋮----
@pytest.mark.integration
    def test_health_does_not_expose_internals(self, tasi_client)
⋮----
forbidden = {
exposed = forbidden & set(body.keys())
⋮----
# Circuit breaker (mock yfinance failure)
⋮----
class TestCircuitBreaker
⋮----
"""Test that repeated yfinance failures trip the circuit breaker."""
⋮----
@pytest.mark.integration
    def test_circuit_breaker_opens_after_threshold(self, tasi_client)
⋮----
"""After CIRCUIT_BREAKER_THRESHOLD failures, circuit should open."""
⋮----
mock_yf = MagicMock()
mock_ticker = MagicMock()
⋮----
# Fire enough requests to trip the circuit breaker
⋮----
assert resp.status_code == 200  # always 200 (falls back to mock)
⋮----
# Circuit should now be open
cb_status = mod.get_circuit_breaker_status()
⋮----
@pytest.mark.integration
    def test_fallback_to_mock_on_yfinance_failure(self, tasi_client)
⋮----
"""When yfinance fails, the endpoint returns mock data (not an error)."""
⋮----
# Rate limiting
⋮----
class TestRateLimiting
⋮----
"""Test rate limiter middleware returns 429 when limit is exceeded."""
⋮----
@pytest.mark.integration
@pytest.mark.slow
    def test_rate_limit_returns_429(self)
⋮----
"""Create an app with a very low rate limit and verify 429."""
⋮----
@app.get("/test")
        async def test_endpoint()
⋮----
# Very low limit: 3 requests per minute
⋮----
client = TestClient(app)
⋮----
statuses = []
⋮----
resp = client.get("/test")
⋮----
@pytest.mark.integration
    def test_rate_limit_skip_paths(self)
⋮----
"""Verify skip_paths bypass rate limiting."""
⋮----
@app.get("/health")
        async def health()
⋮----
# Should never get 429 for skipped path
⋮----
resp = client.get("/health")
</file>

<file path="tests/integration/test_auth_flow.py">
"""
Integration Tests: Authentication Flow
========================================
Tests the full JWT auth lifecycle using FastAPI TestClient.

Covers:
  - Guest login -> JWT token
  - Protected endpoint with valid token -> 200
  - Protected endpoint without token -> 401
  - Protected endpoint with invalid token -> 401/403
  - Token refresh flow

Markers:
  - integration: requires full app assembly (no external DB)
"""
⋮----
# Ensure project root on sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
@pytest.fixture(scope="module")
def auth_app()
⋮----
"""Create a minimal FastAPI app with the auth router."""
⋮----
app = FastAPI()
⋮----
@pytest.fixture(scope="module")
def auth_client(auth_app)
⋮----
"""TestClient for auth endpoints."""
⋮----
@pytest.fixture
def mock_auth_settings()
⋮----
"""Provide deterministic auth settings for JWT operations."""
⋮----
settings = AuthSettings(
⋮----
@pytest.fixture
def mock_db_user()
⋮----
"""Mock database to return a valid user for token verification."""
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
# Guest login -> JWT
⋮----
class TestGuestLogin
⋮----
"""Test POST /api/auth/guest returns a valid JWT."""
⋮----
@pytest.mark.integration
    def test_guest_login_returns_200(self, auth_client, mock_auth_settings)
⋮----
resp = auth_client.post("/api/auth/guest")
⋮----
@pytest.mark.integration
    def test_guest_login_returns_token(self, auth_client, mock_auth_settings)
⋮----
body = resp.json()
⋮----
@pytest.mark.integration
    def test_guest_login_returns_refresh_token(self, auth_client, mock_auth_settings)
⋮----
@pytest.mark.integration
    def test_guest_login_returns_user_id(self, auth_client, mock_auth_settings)
⋮----
@pytest.mark.integration
    def test_guest_login_returns_name(self, auth_client, mock_auth_settings)
⋮----
@pytest.mark.integration
    def test_guest_token_is_decodable(self, auth_client, mock_auth_settings)
⋮----
token = resp.json()["token"]
payload = pyjwt.decode(
⋮----
@pytest.mark.integration
    def test_each_guest_gets_unique_id(self, auth_client, mock_auth_settings)
⋮----
resp1 = auth_client.post("/api/auth/guest")
resp2 = auth_client.post("/api/auth/guest")
id1 = resp1.json()["user_id"]
id2 = resp2.json()["user_id"]
⋮----
# Protected endpoint with valid token -> 200
⋮----
class TestProtectedWithValidToken
⋮----
"""Test GET /api/auth/me with a valid token returns user profile."""
⋮----
# Get a guest token
⋮----
# Mock the DB lookup for /me
⋮----
resp = auth_client.get(
⋮----
# Protected endpoint without token -> 401/403
⋮----
class TestProtectedWithoutToken
⋮----
"""Test GET /api/auth/me without a token returns 401/403."""
⋮----
@pytest.mark.integration
    def test_me_without_token_returns_401_or_403(self, auth_client)
⋮----
resp = auth_client.get("/api/auth/me")
⋮----
@pytest.mark.integration
    def test_me_without_token_has_detail(self, auth_client)
⋮----
# Protected endpoint with invalid token -> 401/403
⋮----
class TestProtectedWithInvalidToken
⋮----
"""Test GET /api/auth/me with invalid tokens."""
⋮----
@pytest.mark.integration
    def test_me_with_garbage_token(self, auth_client, mock_auth_settings)
⋮----
@pytest.mark.integration
    def test_me_with_expired_token(self, auth_client, mock_auth_settings)
⋮----
expired = pyjwt.encode(
⋮----
@pytest.mark.integration
    def test_me_with_wrong_secret_token(self, auth_client, mock_auth_settings)
⋮----
wrong_secret_token = pyjwt.encode(
⋮----
@pytest.mark.integration
    def test_me_with_malformed_auth_header(self, auth_client)
⋮----
# Token refresh
⋮----
class TestTokenRefresh
⋮----
"""Test POST /api/auth/refresh with guest tokens."""
⋮----
@pytest.mark.integration
    def test_refresh_guest_token(self, auth_client, mock_auth_settings)
⋮----
# Get guest tokens
guest_resp = auth_client.post("/api/auth/guest")
refresh_tok = guest_resp.json()["refresh_token"]
⋮----
# Refresh
resp = auth_client.post(
⋮----
@pytest.mark.integration
    def test_refresh_with_invalid_token(self, auth_client, mock_auth_settings)
⋮----
@pytest.mark.integration
    def test_refresh_with_expired_token(self, auth_client, mock_auth_settings)
⋮----
expired_refresh = pyjwt.encode(
</file>

<file path="tests/integration/test_pg_path.py">
"""
Integration Tests: PostgreSQL Path
====================================
Tests the PostgreSQL database path end-to-end.

Skips automatically if PostgreSQL is not available (no POSTGRES_HOST or
unable to connect). When PG is available, tests:
  - Connection and table creation
  - Data insertion and retrieval
  - Query via TASI-style endpoints
  - Cleanup

Markers:
  - integration
  - pg_required: requires a running PostgreSQL instance
"""
⋮----
# Ensure project root on sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
# ---------------------------------------------------------------------------
# Skip if PostgreSQL is not available
⋮----
def _pg_available() -> bool
⋮----
"""Check if PostgreSQL is reachable."""
⋮----
conn = psycopg2.connect(
⋮----
PG_AVAILABLE = _pg_available()
pytestmark = [
⋮----
# Fixtures
⋮----
@pytest.fixture(scope="module")
def pg_conn()
⋮----
"""Provide a PostgreSQL connection for the test module."""
⋮----
@pytest.fixture
def test_table_name()
⋮----
"""Generate a unique temp table name for isolation."""
suffix = uuid.uuid4().hex[:8]
⋮----
@pytest.fixture
def temp_table(pg_conn, test_table_name)
⋮----
"""Create a temporary test table and drop it after the test."""
cur = pg_conn.cursor()
⋮----
# Cleanup
⋮----
# Connection tests
⋮----
class TestPGConnection
⋮----
"""Test basic PostgreSQL connectivity."""
⋮----
def test_connection_alive(self, pg_conn)
⋮----
result = cur.fetchone()
⋮----
def test_pg_version(self, pg_conn)
⋮----
version = cur.fetchone()[0]
⋮----
# Table creation tests
⋮----
class TestPGTableCreation
⋮----
"""Test table creation and schema verification."""
⋮----
def test_create_table(self, pg_conn, temp_table)
⋮----
columns = cur.fetchall()
⋮----
col_names = [c[0] for c in columns]
⋮----
# Data insertion and retrieval
⋮----
class TestPGDataOperations
⋮----
"""Test data insertion, retrieval, and query patterns."""
⋮----
def test_insert_and_query(self, pg_conn, temp_table)
⋮----
count = cur.fetchone()[0]
⋮----
def test_query_by_sector(self, pg_conn, temp_table)
⋮----
# Insert test data
⋮----
# Query by sector
⋮----
rows = cur.fetchall()
⋮----
def test_query_price_range(self, pg_conn, temp_table)
⋮----
# Schema verification (if main schema tables exist)
⋮----
class TestPGSchemaVerification
⋮----
"""Verify that the main application tables exist (if schema has been applied)."""
⋮----
def test_companies_table_exists(self, pg_conn)
⋮----
exists = cur.fetchone()[0]
⋮----
def test_companies_has_data(self, pg_conn)
</file>

<file path="tests/integration/test_rate_limiting.py">
"""
Integration Tests: Rate Limiting
=================================
Tests for rate limiting middleware behavior at the integration level.

Covers:
  - Requests within limit succeed with 200
  - Exceeding limit returns 429 with Retry-After header
  - Rate limit headers are present in 429 responses
  - Per-endpoint path-based rate limiting
  - Skip paths bypass rate limiting
  - Rate limit buckets reset after window expires
  - Independent bucket isolation (auth vs charts)
  - Error response body structure for 429

Uses FastAPI TestClient with RateLimitMiddleware.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
from middleware.rate_limit import RateLimitMiddleware  # noqa: E402
⋮----
"""Create a minimal FastAPI app with rate limiting."""
app = FastAPI()
⋮----
@app.get("/test")
    async def test_ep()
⋮----
@app.get("/health")
    async def health_ep()
⋮----
@app.post("/api/auth/login")
    async def login_ep()
⋮----
@app.get("/api/v1/charts/tasi/index")
    async def tasi_ep()
⋮----
@app.get("/api/v1/query")
    async def query_ep()
⋮----
# ===========================================================================
# Within limit: success
⋮----
class TestWithinLimit
⋮----
"""Test that requests within the rate limit succeed."""
⋮----
@pytest.mark.integration
    def test_all_requests_within_limit_return_200(self)
⋮----
app = _create_rate_limited_app(rpm=10)
client = TestClient(app)
⋮----
resp = client.get("/test")
⋮----
@pytest.mark.integration
    def test_exactly_at_limit_succeeds(self)
⋮----
app = _create_rate_limited_app(rpm=3)
⋮----
statuses = [client.get("/test").status_code for _ in range(3)]
⋮----
# Exceeding limit: 429
⋮----
class TestExceedingLimit
⋮----
"""Test that exceeding the rate limit returns 429."""
⋮----
@pytest.mark.integration
    def test_exceeding_limit_returns_429(self)
⋮----
app = _create_rate_limited_app(rpm=2)
⋮----
statuses = [client.get("/test").status_code for _ in range(5)]
⋮----
@pytest.mark.integration
    def test_429_response_has_retry_after_header(self)
⋮----
app = _create_rate_limited_app(rpm=1)
⋮----
retry_val = int(resp.headers["Retry-After"])
⋮----
@pytest.mark.integration
    def test_429_response_body_structure(self)
⋮----
body = resp.json()
⋮----
# Per-endpoint rate limits
⋮----
class TestPerEndpointLimits
⋮----
"""Test path-prefix-based rate limit tiers."""
⋮----
@pytest.mark.integration
    def test_auth_endpoint_separate_limit(self)
⋮----
app = _create_rate_limited_app(rpm=60, path_limits={"/api/auth": 2})
⋮----
r1 = client.post("/api/auth/login")
r2 = client.post("/api/auth/login")
⋮----
r3 = client.post("/api/auth/login")
⋮----
@pytest.mark.integration
    def test_charts_endpoint_separate_limit(self)
⋮----
app = _create_rate_limited_app(rpm=60, path_limits={"/api/v1/charts": 2})
⋮----
r = client.get("/api/v1/charts/tasi/index")
⋮----
# General endpoint should still work
r = client.get("/test")
⋮----
# Skip paths
⋮----
class TestSkipPaths
⋮----
"""Test that skip paths bypass rate limiting."""
⋮----
@pytest.mark.integration
    def test_health_endpoint_bypasses_rate_limit(self)
⋮----
app = _create_rate_limited_app(rpm=1, skip_paths=["/health"])
⋮----
# Exhaust the general limit
⋮----
# Health should still work
⋮----
resp = client.get("/health")
⋮----
# Bucket isolation
⋮----
class TestBucketIsolation
⋮----
"""Test that different path tiers have independent buckets."""
⋮----
@pytest.mark.integration
    def test_auth_and_charts_independent(self)
⋮----
app = _create_rate_limited_app(
⋮----
# Exhaust auth bucket
⋮----
r = client.post("/api/auth/login")
⋮----
# Charts bucket should still have capacity
⋮----
@pytest.mark.integration
    def test_exhausted_prefix_doesnt_affect_default(self)
⋮----
# Exhaust auth
⋮----
# Default bucket should still be available
</file>

<file path="tests/performance/test_concurrent_queries.py">
"""
Concurrent Query Performance Tests
====================================
Tests for measuring response time percentiles under concurrent load.

Measures p50, p95, p99 latency for key endpoints using asyncio.gather
for true concurrent execution.

Run:
  pytest tests/performance/test_concurrent_queries.py -v -s

Markers:
  - performance: requires a running server or uses TestClient
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
def percentile(data: list[float], p: float) -> float
⋮----
"""Calculate percentile from a sorted or unsorted list."""
⋮----
sorted_data = sorted(data)
k = (len(sorted_data) - 1) * (p / 100.0)
f = int(k)
c = f + 1
⋮----
def _report_latency(name: str, latencies: list[float]) -> dict
⋮----
"""Compute and print latency statistics."""
⋮----
stats = {
⋮----
# ===========================================================================
# Fixtures
⋮----
@pytest.fixture(scope="module")
def health_app()
⋮----
"""Create a minimal app with health endpoints."""
⋮----
app = FastAPI()
⋮----
@pytest.fixture(scope="module")
def health_client(health_app)
⋮----
@pytest.fixture(scope="module")
def auth_app()
⋮----
"""Create a minimal app with auth endpoints."""
⋮----
@pytest.fixture(scope="module")
def auth_client(auth_app)
⋮----
@pytest.fixture(scope="module")
def tasi_app()
⋮----
"""Create a minimal app with TASI endpoints."""
⋮----
@pytest.fixture(scope="module")
def tasi_client(tasi_app)
⋮----
# Concurrent health endpoint tests
⋮----
class TestConcurrentHealth
⋮----
"""Measure health endpoint latency under concurrent load."""
⋮----
@pytest.mark.performance
    def test_concurrent_health_live_50(self, health_client)
⋮----
"""50 concurrent /health/live requests."""
⋮----
latencies = []
⋮----
def _single_request()
⋮----
start = time.monotonic()
⋮----
resp = health_client.get("/health/live")
elapsed = time.monotonic() - start
⋮----
# Run 50 sequential (TestClient doesn't support true async)
statuses = [_single_request() for _ in range(50)]
⋮----
stats = _report_latency("/health/live (50 req)", latencies)
⋮----
@pytest.mark.performance
    def test_concurrent_health_live_200(self, health_client)
⋮----
"""200 concurrent /health/live requests."""
⋮----
statuses = [_single_request() for _ in range(200)]
success_rate = sum(1 for s in statuses if s == 200) / len(statuses)
⋮----
stats = _report_latency("/health/live (200 req)", latencies)
⋮----
# Concurrent auth endpoint tests
⋮----
class TestConcurrentAuth
⋮----
"""Measure auth endpoint latency under concurrent load."""
⋮----
@pytest.mark.performance
    def test_concurrent_guest_login_50(self, auth_client)
⋮----
"""50 concurrent guest login requests."""
⋮----
resp = auth_client.post("/api/auth/guest")
⋮----
stats = _report_latency("/api/auth/guest (50 req)", latencies)
⋮----
# Concurrent SQL validation tests
⋮----
class TestConcurrentSqlValidation
⋮----
"""Measure SQL validator performance under load."""
⋮----
@pytest.mark.performance
    def test_concurrent_validation_50(self)
⋮----
"""50 concurrent SQL validations."""
⋮----
validator = SqlQueryValidator()
queries = [
⋮----
] * 10  # 50 queries total
⋮----
result = validator.validate(sql)
⋮----
stats = _report_latency("SQL Validation (50 queries)", latencies)
⋮----
@pytest.mark.performance
    def test_concurrent_validation_injection_detection_200(self)
⋮----
"""200 injection detection validations."""
⋮----
attack_queries = [
⋮----
] * 50  # 200 queries total
⋮----
stats = _report_latency("SQL Injection Detection (200 queries)", latencies)
⋮----
@pytest.mark.performance
    def test_validation_throughput_500(self)
⋮----
"""500 mixed validations for throughput measurement."""
⋮----
safe_queries = [
⋮----
all_queries = safe_queries + attack_queries  # 500 total
⋮----
total_elapsed = time.monotonic() - start
⋮----
throughput = len(all_queries) / total_elapsed
⋮----
# Asyncio-based concurrent tests
⋮----
class TestAsyncConcurrent
⋮----
"""True async concurrent tests using asyncio.gather."""
⋮----
@pytest.mark.performance
@pytest.mark.asyncio
    async def test_async_concurrent_validation(self)
⋮----
"""Run SQL validations concurrently with asyncio."""
⋮----
async def validate_query(sql: str) -> tuple[bool, float]
⋮----
# Validator is sync, wrap in executor
loop = asyncio.get_event_loop()
result = await loop.run_in_executor(None, validator.validate, sql)
⋮----
] * 20  # 100 concurrent
⋮----
results = await asyncio.gather(*[validate_query(q) for q in queries])
⋮----
latencies = [elapsed for _, elapsed in results]
stats = _report_latency("Async Concurrent (100 queries)", latencies)
⋮----
valid_count = sum(1 for is_valid, _ in results if is_valid)
invalid_count = sum(1 for is_valid, _ in results if not is_valid)
⋮----
# 60 safe, 40 attack
</file>

<file path="tests/performance/test_load.py">
"""
Performance Load Tests (Locust)
================================
Locust load test profiles for the Ra'd AI TASI Platform.

Profiles:
  - Light load: 50 users, 10 users/sec ramp-up
  - Medium load: 200 users, 25 users/sec ramp-up
  - Heavy load: 500 users, 50 users/sec ramp-up

Tested endpoints:
  - GET /health (liveness, no auth)
  - GET /health/ready (readiness, no auth)
  - POST /api/auth/guest (guest token generation)
  - GET /api/v1/charts/tasi/index (TASI data, read-heavy)
  - GET /api/v1/news/feed (news list)

Run:
  locust -f tests/performance/test_load.py --host=http://localhost:8084

  # Light load (headless):
  locust -f tests/performance/test_load.py --host=http://localhost:8084 \
    --users 50 --spawn-rate 10 --run-time 60s --headless

  # Medium load:
  locust -f tests/performance/test_load.py --host=http://localhost:8084 \
    --users 200 --spawn-rate 25 --run-time 120s --headless

  # Heavy load:
  locust -f tests/performance/test_load.py --host=http://localhost:8084 \
    --users 500 --spawn-rate 50 --run-time 180s --headless
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
# Allow module to be imported for discovery without locust installed
⋮----
pytestmark = pytest.mark.skip(reason="locust not installed")
⋮----
class HttpUser
⋮----
def between(*a, **kw)
⋮----
def task(weight=1)
⋮----
def decorator(func)
⋮----
def tag(*tags)
⋮----
class HealthCheckUser(HttpUser)
⋮----
"""User that only hits health check endpoints.

    Simulates monitoring / load balancer probes.
    """
⋮----
wait_time = between(0.5, 1.5)
weight = 3
⋮----
@task(5)
@tag("health")
    def health_check(self)
⋮----
@task(3)
@tag("health")
    def liveness_check(self)
⋮----
@task(2)
@tag("health")
    def readiness_check(self)
⋮----
class GuestUser(HttpUser)
⋮----
"""User that creates guest tokens and accesses public endpoints.

    Simulates anonymous visitors exploring the platform.
    """
⋮----
wait_time = between(1, 3)
weight = 5
⋮----
def on_start(self)
⋮----
"""Get a guest token on start."""
resp = self.client.post("/api/auth/guest")
⋮----
data = resp.json()
⋮----
@property
    def _headers(self)
⋮----
@task(4)
@tag("tasi")
    def get_tasi_index(self)
⋮----
@task(3)
@tag("tasi")
    def get_tasi_3mo(self)
⋮----
@task(2)
@tag("news")
    def get_news_feed(self)
⋮----
@task(1)
@tag("auth")
    def guest_login(self)
⋮----
class ChartHeavyUser(HttpUser)
⋮----
"""User that heavily requests chart data.

    Simulates users viewing stock charts repeatedly.
    """
⋮----
wait_time = between(0.5, 2)
weight = 2
⋮----
@task(5)
@tag("tasi")
    def get_tasi_index_default(self)
⋮----
@task(3)
@tag("tasi")
    def get_tasi_1y(self)
⋮----
@task(2)
@tag("tasi")
    def get_tasi_6mo(self)
⋮----
@task(1)
@tag("health")
    def check_health(self)
</file>

<file path="tests/security/test_auth_bypass.py">
"""
Auth Bypass Security Tests
===========================
Tests for JWT authentication bypass vulnerabilities.

Covers:
  - Expired token rejection
  - Malformed JWT rejection (garbage, truncated, extra segments)
  - Missing Authorization header
  - Invalid signature (wrong secret)
  - Token type mismatch (refresh used as access)
  - Algorithm confusion (none, HS384)
  - Missing required claims (no sub, no type)
  - Replay with forged claims
  - Empty Bearer value

Uses FastAPI TestClient with the auth router.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
@pytest.fixture(scope="module")
def auth_app()
⋮----
"""Create a minimal FastAPI app with the auth router."""
⋮----
app = FastAPI()
⋮----
@pytest.fixture(scope="module")
def client(auth_app)
⋮----
@pytest.fixture
def mock_auth_settings()
⋮----
settings = AuthSettings(
⋮----
@pytest.fixture
def valid_token(mock_auth_settings)
⋮----
"""Generate a valid access token for testing."""
payload = {
⋮----
@pytest.fixture
def mock_db_user()
⋮----
"""Mock database returning a valid active user."""
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
# ===========================================================================
# Expired tokens
⋮----
class TestExpiredTokenBypass
⋮----
"""Test that expired tokens are properly rejected."""
⋮----
def test_expired_token_returns_401(self, client, mock_auth_settings)
⋮----
expired = pyjwt.encode(
resp = client.get(
⋮----
def test_just_expired_token_rejected(self, client, mock_auth_settings)
⋮----
"""Token that expired 1 second ago should still be rejected."""
just_expired = pyjwt.encode(
⋮----
# Malformed JWT tokens
⋮----
class TestMalformedTokenBypass
⋮----
"""Test that malformed tokens are rejected."""
⋮----
def test_garbage_token_rejected(self, client, mock_auth_settings)
⋮----
def test_empty_bearer_value_rejected(self, client, mock_auth_settings)
⋮----
def test_truncated_token_rejected(self, client, mock_auth_settings)
⋮----
"""Token with missing signature segment."""
token = pyjwt.encode(
# Remove the signature part
parts = token.split(".")
truncated = ".".join(parts[:2])
⋮----
def test_extra_segments_rejected(self, client, mock_auth_settings)
⋮----
"""Token with extra dot-segments."""
⋮----
# Missing Authorization header
⋮----
class TestMissingAuthHeader
⋮----
"""Test that missing auth headers are rejected."""
⋮----
def test_no_auth_header_returns_401_or_403(self, client)
⋮----
resp = client.get("/api/auth/me")
⋮----
def test_non_bearer_scheme_rejected(self, client)
⋮----
# Invalid signature (wrong secret)
⋮----
class TestInvalidSignatureBypass
⋮----
"""Test that tokens signed with wrong key are rejected."""
⋮----
def test_wrong_secret_rejected(self, client, mock_auth_settings)
⋮----
wrong_secret_token = pyjwt.encode(
⋮----
# Token type mismatch
⋮----
class TestTokenTypeMismatch
⋮----
"""Test that using refresh tokens for access is rejected."""
⋮----
def test_refresh_token_for_access_rejected(self, client, mock_auth_settings)
⋮----
refresh_token = pyjwt.encode(
⋮----
def test_access_token_for_refresh_rejected(self, client, mock_auth_settings)
⋮----
"""Access token used in refresh endpoint should fail."""
guest_resp = client.post("/api/auth/guest")
access_token = guest_resp.json()["token"]
⋮----
resp = client.post(
⋮----
# Algorithm confusion
⋮----
class TestAlgorithmConfusion
⋮----
"""Test algorithm confusion attacks."""
⋮----
def test_none_algorithm_rejected(self, client, mock_auth_settings)
⋮----
"""Crafting a token with alg=none should be rejected."""
⋮----
header = (
payload = (
fake_token = f"{header}.{payload}."
⋮----
# Missing required claims
⋮----
class TestMissingClaims
⋮----
"""Test that tokens with missing required claims are rejected."""
⋮----
def test_missing_sub_claim_rejected(self, client, mock_auth_settings)
⋮----
def test_empty_sub_claim_rejected(self, client, mock_auth_settings)
</file>

<file path="tests/test_cache.py">
"""
Cache Module Tests
==================
Tests for cache.redis_client and cache.decorators modules.

All tests use mocked Redis -- no running Redis server required.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
@pytest.fixture(autouse=True)
def reset_redis_client()
⋮----
"""Reset the redis_client module's global state before each test."""
⋮----
# ===========================================================================
# redis_client tests
⋮----
class TestRedisClientInit
⋮----
"""Tests for init_redis, close_redis, is_redis_available."""
⋮----
def test_init_redis_success(self)
⋮----
mock_redis_module = MagicMock()
mock_client = MagicMock()
⋮----
def test_init_redis_skips_if_already_initialized(self)
⋮----
def test_init_redis_failure_leaves_client_none(self)
⋮----
def test_is_redis_available_when_not_initialized(self)
⋮----
def test_close_redis_when_not_initialized(self)
⋮----
close_redis()  # Should not raise
⋮----
def test_close_redis_when_initialized(self)
⋮----
# Cache operation tests (cache_get, cache_set, cache_delete)
⋮----
class TestCacheOperations
⋮----
"""Tests for cache_get, cache_set, cache_delete."""
⋮----
def test_cache_get_returns_none_when_not_initialized(self)
⋮----
result = cache_get("some-key")
⋮----
def test_cache_set_returns_false_when_not_initialized(self)
⋮----
result = cache_set("key", "value")
⋮----
def test_cache_delete_returns_false_when_not_initialized(self)
⋮----
result = cache_delete("key")
⋮----
def test_cache_set_and_get(self)
⋮----
store = {}
⋮----
def mock_setex(key, ttl, value)
⋮----
def mock_get(key)
⋮----
result = rc.cache_set("test-key", "test-value", ttl=300)
⋮----
retrieved = rc.cache_get("test-key")
⋮----
def test_cache_delete_existing_key(self)
⋮----
result = rc.cache_delete("some-key")
⋮----
def test_cache_delete_nonexistent_key(self)
⋮----
result = rc.cache_delete("missing-key")
⋮----
def test_cache_get_handles_exception(self)
⋮----
result = rc.cache_get("key")
⋮----
def test_cache_set_handles_exception(self)
⋮----
result = rc.cache_set("key", "value")
⋮----
def test_cache_invalidate_pattern_when_not_initialized(self)
⋮----
result = cache_invalidate_pattern("news:*")
⋮----
def test_cache_invalidate_pattern(self)
⋮----
count = rc.cache_invalidate_pattern("news:*")
⋮----
# Cache decorator tests
⋮----
class TestCachedDecorator
⋮----
"""Tests for the @cached decorator."""
⋮----
def test_cached_bypasses_when_redis_unavailable(self)
⋮----
call_count = 0
⋮----
@cached(ttl=60, key_prefix="test")
        def my_func(x)
⋮----
# Redis is not initialized, so decorator should be a no-op
result = my_func(5)
⋮----
# Call again - should execute function again (no cache)
⋮----
def test_cached_returns_from_cache_on_hit(self)
⋮----
@cached(ttl=60, key_prefix="myprefix")
        def expensive_func(x)
⋮----
# First call: should execute function and cache
result1 = expensive_func(5)
⋮----
# Second call: should return from cache
result2 = expensive_func(5)
⋮----
assert call_count == 1  # Not called again
⋮----
def test_cached_different_args_different_keys(self)
⋮----
@cached(ttl=60, key_prefix="test")
        def func(x)
⋮----
assert call_count == 2  # Different args = different cache keys
⋮----
def test_cached_handles_non_serializable_gracefully(self)
⋮----
@cached(ttl=60, key_prefix="test")
        def func()
⋮----
# Even if cache fails, function should still return
result = func()
⋮----
class TestBuildKey
⋮----
"""Tests for cache key generation."""
⋮----
def test_build_key_deterministic(self)
⋮----
key1 = _build_key("prefix", "func", (1, 2), {"k": "v"})
key2 = _build_key("prefix", "func", (1, 2), {"k": "v"})
⋮----
def test_build_key_different_for_different_args(self)
⋮----
key1 = _build_key("prefix", "func", (1,), {})
key2 = _build_key("prefix", "func", (2,), {})
⋮----
def test_build_key_format(self)
⋮----
key = _build_key("news", "get_latest", (10,), {})
⋮----
def test_build_key_different_prefix(self)
⋮----
key1 = _build_key("news", "func", (1,), {})
key2 = _build_key("reports", "func", (1,), {})
</file>

<file path="tests/test_chart_engine.py">
"""Comprehensive tests for RaidChartGenerator chart engine."""
⋮----
# Add project root to path
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
def _extract_y_values(y_field)
⋮----
"""Extract numeric values from Plotly JSON y field (handles both list and bdata formats)."""
⋮----
raw = base64.b64decode(y_field["bdata"])
dtype = y_field.get("dtype", "f8")
fmt_map = {
fmt = fmt_map.get(dtype, "<d")
size = struct.calcsize(fmt)
⋮----
@pytest.fixture
def gen()
⋮----
"""Chart generator instance."""
⋮----
@pytest.fixture
def db_conn()
⋮----
"""Connection to the real Saudi stocks database."""
db_path = PROJECT_ROOT / "saudi_stocks.db"
⋮----
conn = sqlite3.connect(str(db_path))
⋮----
# ===================================================================
# Chart type detection tests
⋮----
class TestBarChart
⋮----
def test_1_text_1_numeric_produces_bar(self, gen)
⋮----
df = pd.DataFrame({"company": ["A", "B", "C"], "revenue": [100, 200, 300]})
result = gen.generate_chart(df, "Test")
⋮----
def test_bar_has_dark_theme(self, gen)
⋮----
def test_bar_with_real_data(self, gen, db_conn)
⋮----
df = pd.read_sql(
result = gen.generate_chart(df, "Top 10")
⋮----
class TestValueHeatmap
⋮----
def test_1_text_3_numeric_produces_heatmap(self, gen)
⋮----
df = pd.DataFrame(
result = gen.generate_chart(df, "Heatmap Test")
⋮----
def test_heatmap_has_text_annotations(self, gen)
⋮----
def test_heatmap_with_5_numeric_cols(self, gen)
⋮----
def test_heatmap_with_real_profitability_data(self, gen, db_conn)
⋮----
result = gen.generate_chart(df, "Profitability Heatmap")
⋮----
# Previously this was a TABLE due to 4-column cutoff
⋮----
def test_heatmap_uses_gold_colorscale(self, gen)
⋮----
colorscale = result["data"][0].get("colorscale")
⋮----
# Should contain gold colors
colors = [c[1] for c in colorscale]
⋮----
class TestScatterPlot
⋮----
def test_2_numeric_produces_scatter(self, gen)
⋮----
df = pd.DataFrame({"x": [1, 2, 3], "y": [4, 5, 6]})
⋮----
def test_scatter_with_real_data(self, gen, db_conn)
⋮----
result = gen.generate_chart(df, "Market Cap vs PE")
⋮----
class TestHistogram
⋮----
def test_1_numeric_produces_histogram(self, gen)
⋮----
df = pd.DataFrame({"value": [1, 2, 2, 3, 3, 3, 4, 4, 5]})
⋮----
def test_histogram_with_real_data(self, gen, db_conn)
⋮----
result = gen.generate_chart(df, "Dividend Distribution")
⋮----
class TestTimeSeries
⋮----
def test_string_dates_detected(self, gen)
⋮----
# Should be line chart (scatter type with mode=lines)
⋮----
def test_datetime64_column(self, gen)
⋮----
def test_time_series_with_real_data(self, gen, db_conn)
⋮----
result = gen.generate_chart(df, "Aramco Revenue")
⋮----
class TestCorrelationHeatmap
⋮----
def test_3_numeric_no_text_produces_correlation(self, gen)
⋮----
class TestTable
⋮----
def test_8_columns_produces_table(self, gen)
⋮----
df = pd.DataFrame({f"col_{i}": range(3) for i in range(8)})
⋮----
def test_7_columns_does_NOT_produce_table(self, gen)
⋮----
# 7 columns should NOT trigger table (old engine was 4+)
⋮----
# Should be value heatmap, NOT table
⋮----
def test_table_has_dark_theme(self, gen)
⋮----
header = result["data"][0]["header"]
⋮----
# Edge cases
⋮----
class TestEdgeCases
⋮----
def test_empty_dataframe_raises(self, gen)
⋮----
def test_single_row(self, gen)
⋮----
df = pd.DataFrame({"name": ["A"], "value": [42]})
⋮----
def test_nan_values_dont_crash(self, gen)
⋮----
def test_large_dataset(self, gen)
⋮----
# Dark theme verification
⋮----
class TestDarkTheme
⋮----
def test_transparent_background(self, gen)
⋮----
def test_gold_title(self, gen)
⋮----
"""Verify _apply_standard_layout sets gold title font color.
        Note: generate_chart() intentionally clears title text to avoid
        duplication with the card-level title, so we test the layout method directly."""
⋮----
fig = go.Figure(data=[go.Bar(x=[1, 2, 3], y=[4, 5, 6])])
⋮----
def test_light_text_color(self, gen)
⋮----
font_color = result["layout"]["font"]["color"]
⋮----
# Number formatting
⋮----
class TestNumberFormatting
⋮----
def test_format_billions(self)
⋮----
def test_format_millions(self)
⋮----
def test_format_thousands(self)
⋮----
def test_format_trillions(self)
⋮----
def test_format_small_decimal(self)
⋮----
def test_format_nan(self)
⋮----
def test_format_regular_number(self)
⋮----
# Regression: Prove old engine bugs are fixed
⋮----
class TestRegressions
⋮----
"""Verify that bugs in the original PlotlyChartGenerator are fixed."""
⋮----
def test_4_columns_NOT_table(self, gen)
⋮----
"""Original engine: 4 columns = table. RaidChartGenerator: heatmap."""
⋮----
assert result["data"][0]["type"] == "heatmap"  # NOT "table"
⋮----
def test_grouped_bar_not_count(self, gen)
⋮----
"""Original engine: grouped bar used .size() = count. Should use values."""
⋮----
# With 2 categorical + 1 numeric, should use actual score values
⋮----
# Collect all y values across all bar traces
all_y = []
⋮----
def test_string_dates_converted(self, gen)
⋮----
"""Original engine: string dates not detected. Should be line chart."""
⋮----
# Should be time series (scatter with lines), not bar chart
⋮----
# Integration with VisualizeDataTool
⋮----
class TestVisualizeDataToolIntegration
⋮----
def test_custom_generator_injected(self)
⋮----
"""Verify VisualizeDataTool accepts RaidChartGenerator."""
⋮----
tool = VisualizeDataTool(plotly_generator=RaidChartGenerator())
</file>

<file path="tests/test_db_compat.py">
"""Tests for services.db_compat – datetime_recent() validation."""
⋮----
# ---------------------------------------------------------------------------
# Helpers
⋮----
@pytest.fixture(params=[False, True], ids=["sqlite", "postgres"])
def backend(request, monkeypatch)
⋮----
"""Run each test against both SQLite and PostgreSQL backends."""
⋮----
# Valid inputs
⋮----
class TestDatetimeRecentValid
⋮----
"""Ensure valid column + interval combinations produce correct SQL."""
⋮----
@pytest.mark.parametrize("column", sorted(_ALLOWED_DATETIME_COLUMNS))
    def test_valid_columns(self, column, backend)
⋮----
result = datetime_recent(column, "1 day")
⋮----
def test_valid_intervals(self, interval, backend)
⋮----
result = datetime_recent("created_at", interval)
⋮----
def test_sqlite_output_format(self)
⋮----
result = datetime_recent("created_at", "1 day")
⋮----
def test_postgres_output_format(self)
⋮----
# Invalid column names
⋮----
class TestDatetimeRecentInvalidColumn
⋮----
"""Ensure non-whitelisted columns are rejected."""
⋮----
def test_invalid_column_raises(self, column, backend)
⋮----
# Invalid intervals
⋮----
class TestDatetimeRecentInvalidInterval
⋮----
"""Ensure malformed intervals are rejected."""
⋮----
def test_invalid_interval_raises(self, interval, backend)
⋮----
# SQL injection attempts
⋮----
class TestDatetimeRecentInjection
⋮----
"""Verify that SQL injection payloads are blocked."""
⋮----
def test_column_injection(self, backend)
⋮----
def test_interval_injection(self, backend)
⋮----
def test_union_injection_column(self, backend)
⋮----
def test_union_injection_interval(self, backend)
⋮----
# Whitelist / pattern sanity checks
⋮----
class TestWhitelistAndPattern
⋮----
"""Sanity-check the module-level constants."""
⋮----
def test_whitelist_is_frozenset(self)
⋮----
def test_whitelist_not_empty(self)
⋮----
def test_pattern_accepts_singular_units(self)
⋮----
def test_pattern_accepts_plural_units(self)
⋮----
def test_pattern_case_insensitive(self)
</file>

<file path="tests/test_ingestion.py">
"""
Ingestion Pipeline Tests
========================
Tests for validators, price_loader, and xbrl_processor modules.

All tests use mocked database and yfinance -- no real services required.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ===========================================================================
# Validator tests
⋮----
class TestTickerValidation
⋮----
"""Tests for ingestion.validators.validate_ticker_format."""
⋮----
def test_valid_ticker_format(self)
⋮----
def test_invalid_ticker_no_sr_suffix(self)
⋮----
def test_invalid_ticker_wrong_suffix(self)
⋮----
assert validate_ticker_format("2222.sr") is False  # case-sensitive
⋮----
def test_invalid_ticker_too_few_digits(self)
⋮----
def test_invalid_ticker_too_many_digits(self)
⋮----
def test_invalid_ticker_non_numeric(self)
⋮----
def test_invalid_ticker_none(self)
⋮----
def test_invalid_ticker_number(self)
⋮----
def test_invalid_ticker_empty_string(self)
⋮----
class TestPriceDataValidation
⋮----
"""Tests for ingestion.validators.validate_price_data."""
⋮----
def _valid_df(self)
⋮----
def test_valid_data_returns_no_errors(self)
⋮----
errors = validate_price_data(self._valid_df())
⋮----
def test_missing_required_columns(self)
⋮----
df = pd.DataFrame({"trade_date": ["2024-01-15"], "close_price": [32.5]})
errors = validate_price_data(df)
⋮----
def test_negative_prices(self)
⋮----
df = self._valid_df()
⋮----
def test_negative_volume(self)
⋮----
def test_high_less_than_low(self)
⋮----
def test_future_dates(self)
⋮----
class TestXBRLFactValidation
⋮----
"""Tests for ingestion.validators.validate_xbrl_fact."""
⋮----
def test_valid_fact(self)
⋮----
fact = {
errors = validate_xbrl_fact(fact)
⋮----
def test_missing_ticker(self)
⋮----
fact = {"concept": "ifrs-full:Revenue", "value_numeric": 100.0}
⋮----
def test_missing_concept(self)
⋮----
fact = {"ticker": "2222.SR", "value_numeric": 100.0}
⋮----
def test_invalid_ticker_format(self)
⋮----
def test_no_value_field(self)
⋮----
fact = {"ticker": "2222.SR", "concept": "ifrs-full:Revenue"}
⋮----
def test_text_value_is_valid(self)
⋮----
def test_boolean_value_is_valid(self)
⋮----
def test_empty_text_value_not_valid(self)
⋮----
# Price loader utility tests
⋮----
class TestPriceLoaderUtilities
⋮----
"""Tests for price_loader utility functions."""
⋮----
def test_normalize_columns(self)
⋮----
df = pd.DataFrame(
result = normalize_columns(df)
⋮----
def test_compute_changes(self)
⋮----
result = compute_changes(df)
⋮----
# First row should have no change
⋮----
# Second row: 33.0 - 32.0 = 1.0
⋮----
# Third row: 32.5 - 33.0 = -0.5
⋮----
def test_compute_changes_percentage(self)
⋮----
def test_df_to_insert_tuples(self)
⋮----
tuples = df_to_insert_tuples(df)
⋮----
def test_df_to_insert_tuples_handles_nan(self)
⋮----
assert tuples[0][2] is None  # open_price NaN -> None
assert tuples[0][7] is None  # change_amount NaN -> None
⋮----
def test_insert_prices_dry_run(self)
⋮----
rows = [
count = insert_prices(None, rows, dry_run=True)
⋮----
def test_insert_prices_empty_rows(self)
⋮----
count = insert_prices(None, [], dry_run=False)
⋮----
class TestPriceLoaderClass
⋮----
"""Tests for PriceLoader batch processing logic."""
⋮----
def test_price_loader_initialization(self)
⋮----
config = IngestionConfig(batch_size=5, rate_limit_seconds=1.0)
loader = PriceLoader(pg_conn=None, config=config, dry_run=True)
⋮----
def test_price_loader_load_all_requires_connection(self)
⋮----
loader = PriceLoader(pg_conn=None, dry_run=True)
⋮----
def test_price_loader_load_prices_requires_yfinance(self)
⋮----
def test_price_loader_normalize_yfinance_df(self)
⋮----
# Simulate yfinance output
⋮----
result = PriceLoader._normalize_yfinance_df(df, "2222.SR")
⋮----
def test_price_loader_normalize_empty_df(self)
⋮----
df = pd.DataFrame()
⋮----
# XBRL processor tests
⋮----
class TestXBRLFact
⋮----
"""Tests for ingestion.xbrl_processor.XBRLFact dataclass."""
⋮----
def test_xbrl_fact_creation(self)
⋮----
fact = XBRLFact(
⋮----
def test_xbrl_fact_content_hash_generated(self)
⋮----
assert len(fact.content_hash) == 64  # SHA-256 hex
⋮----
def test_xbrl_fact_different_values_different_hashes(self)
⋮----
f1 = XBRLFact(
f2 = XBRLFact(
⋮----
def test_xbrl_fact_same_values_same_hash(self)
⋮----
def test_xbrl_fact_to_insert_tuple(self)
⋮----
t = fact.to_insert_tuple()
⋮----
assert t[0] == "2222.SR"  # ticker
assert t[1] == "filing-123"  # filing_id
assert t[2] == "ifrs-full:Revenue"  # concept
⋮----
def test_xbrl_fact_with_period_dates(self)
⋮----
class TestXBRLProcessor
⋮----
"""Tests for XBRLProcessor class."""
⋮----
def test_processor_initialization(self)
⋮----
proc = XBRLProcessor(
⋮----
def test_process_filing_nonexistent_file(self)
⋮----
proc = XBRLProcessor(ticker="2222.SR")
facts = proc.process_filing(Path("/nonexistent/file.xml"))
⋮----
def test_process_filing_unsupported_extension(self)
⋮----
# Create a temp file with unsupported extension
facts = proc.process_filing(Path("test.pdf"))
⋮----
def test_label_to_concept_known_label(self)
⋮----
concept = proc._label_to_concept("total assets", "Balance Sheet")
⋮----
def test_label_to_concept_revenue(self)
⋮----
concept = proc._label_to_concept("revenue", "Income Statement")
⋮----
def test_label_to_concept_unknown_label(self)
⋮----
concept = proc._label_to_concept("some custom metric", "Balance Sheet")
assert ":" in concept  # Should still have prefix:PascalCase format
⋮----
def test_is_arabic_detection(self)
⋮----
def test_safe_parse_date_iso(self)
⋮----
d = XBRLProcessor._safe_parse_date("2024-12-31")
⋮----
def test_safe_parse_date_slash(self)
⋮----
d = XBRLProcessor._safe_parse_date("31/12/2024")
⋮----
def test_safe_parse_date_with_time(self)
⋮----
d = XBRLProcessor._safe_parse_date("2024-12-31T00:00:00")
⋮----
def test_safe_parse_date_invalid(self)
⋮----
d = XBRLProcessor._safe_parse_date("not-a-date")
⋮----
def test_parse_date_string_iso(self)
⋮----
result = proc._parse_date_string("2024-12-31")
⋮----
def test_parse_date_string_fiscal_year(self)
⋮----
result = proc._parse_date_string("FY 2024")
⋮----
def test_parse_date_string_quarter(self)
⋮----
result = proc._parse_date_string("Q1 2024")
⋮----
def test_parse_date_string_year_only(self)
⋮----
result = proc._parse_date_string("2024")
⋮----
def test_parse_date_string_invalid(self)
⋮----
result = proc._parse_date_string("not-a-date")
⋮----
class TestXBRLInsertFacts
⋮----
"""Tests for xbrl_processor.insert_facts."""
⋮----
def test_insert_facts_dry_run(self)
⋮----
facts = [
count = insert_facts(None, facts, dry_run=True)
⋮----
def test_insert_facts_empty(self)
⋮----
count = insert_facts(None, [], dry_run=False)
⋮----
class TestIngestionConfig
⋮----
"""Tests for ingestion.config.IngestionConfig."""
⋮----
def test_default_config(self)
⋮----
config = IngestionConfig()
⋮----
def test_custom_config(self)
⋮----
config = IngestionConfig(
⋮----
def test_config_repr(self)
⋮----
config = IngestionConfig(batch_size=5)
repr_str = repr(config)
⋮----
@patch.dict("os.environ", {"INGESTION_BATCH_SIZE": "25"})
    def test_config_from_env(self)
</file>

<file path="tests/test_query_router.py">
"""
Query Router Tests (Placeholder)
=================================
Tests for the hybrid query routing layer that will direct natural language
queries to the appropriate handler (Vanna SQL generation, knowledge base,
or direct API lookup).

This module is a placeholder for future implementation. The query router
will be responsible for:
  - Classifying user intent (SQL query, company lookup, news request, etc.)
  - Routing to the appropriate backend (Vanna agent, API routes, or cached data)
  - Handling fallback/retry logic when primary routing fails

Currently contains structural tests to validate the test framework is
working correctly.
"""
⋮----
class TestQueryRouterPlaceholder(unittest.TestCase)
⋮----
"""Placeholder test class for query routing logic."""
⋮----
def test_placeholder_passes(self)
⋮----
"""Verify test framework is operational for future query router tests."""
⋮----
def test_intent_classification_types(self)
⋮----
"""Validate expected intent types are defined."""
# These represent the future intent categories the router will support
expected_intents = [
⋮----
"sql_query",  # "What is the market cap of Aramco?"
"company_lookup",  # "Show me details for 2222.SR"
"sector_analysis",  # "Compare energy sector companies"
"news_search",  # "Latest news for Aramco"
"report_search",  # "Analyst reports for banking sector"
"chart_request",  # "Show me a chart of sector P/E ratios"
"general_chat",  # "Hello", "What can you do?"
⋮----
# Verify we have a reasonable set of intents planned
</file>

<file path="tests/test_rate_limiting.py">
"""
Rate Limiting Tests
===================
Tests for tiered rate limiting middleware (general, auth, chart tiers).

Uses FastAPI TestClient for integration-style testing.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
from middleware.rate_limit import RateLimitMiddleware  # noqa: E402
⋮----
# ===========================================================================
# Helper: create a minimal FastAPI app with tiered endpoints
⋮----
"""Create a minimal FastAPI app with rate limiting for testing."""
app = FastAPI()
⋮----
@app.get("/test")
    async def test_endpoint()
⋮----
@app.get("/health")
    async def health_endpoint()
⋮----
@app.post("/api/auth/login")
    async def login_endpoint()
⋮----
@app.post("/api/auth/register")
    async def register_endpoint()
⋮----
@app.get("/api/v1/charts/tasi/index")
    async def tasi_chart_endpoint()
⋮----
@app.get("/api/v1/charts/2222/ohlcv")
    async def stock_chart_endpoint()
⋮----
@app.get("/api/v1/market/movers")
    async def market_endpoint()
⋮----
# Tests: Default (single-tier) rate limiting
⋮----
class TestDefaultRateLimiting
⋮----
"""Tests for default (non-tiered) rate limiting."""
⋮----
def test_requests_within_limit_succeed(self)
⋮----
app = _create_test_app(requests_per_minute=10)
client = TestClient(app)
⋮----
response = client.get("/test")
⋮----
def test_requests_exceeding_limit_return_429(self)
⋮----
app = _create_test_app(requests_per_minute=3)
⋮----
responses = [client.get("/test") for _ in range(5)]
status_codes = [r.status_code for r in responses]
⋮----
def test_429_response_has_retry_after(self)
⋮----
app = _create_test_app(requests_per_minute=1)
⋮----
data = response.json()
⋮----
def test_skip_paths_bypass_rate_limit(self)
⋮----
app = _create_test_app(requests_per_minute=1, skip_paths=["/health"])
⋮----
client.get("/test")  # uses the single slot
⋮----
response = client.get("/health")
⋮----
def test_different_endpoints_share_default_bucket(self)
⋮----
"""Without path_limits, all non-skip paths share one bucket."""
⋮----
# 4th request should hit limit
response = client.get("/api/v1/market/movers")
⋮----
# Tests: Tiered (path-based) rate limiting
⋮----
class TestTieredRateLimiting
⋮----
"""Tests for path-prefix-based rate limit tiers."""
⋮----
def test_auth_tier_has_lower_limit(self)
⋮----
"""Auth endpoints should be limited independently at a lower rate."""
app = _create_test_app(
⋮----
# 2 requests within auth limit
r1 = client.post("/api/auth/login")
r2 = client.post("/api/auth/login")
⋮----
# 3rd should be rate limited
r3 = client.post("/api/auth/login")
⋮----
def test_chart_tier_has_separate_bucket(self)
⋮----
"""Chart endpoints should be limited independently."""
⋮----
# 3 chart requests within limit
⋮----
r = client.get("/api/v1/charts/tasi/index")
⋮----
# 4th chart request should be rate limited
⋮----
# But a general endpoint should still work (different bucket)
r = client.get("/test")
⋮----
def test_default_tier_still_works_with_path_limits(self)
⋮----
"""Endpoints not matching any prefix use the default limit."""
⋮----
# General endpoint uses default (3/min)
⋮----
def test_auth_and_chart_tiers_are_independent(self)
⋮----
"""Exhausting auth tier should not affect chart tier."""
⋮----
# Exhaust auth tier
⋮----
r = client.post("/api/auth/login")
⋮----
# Chart tier should still work
⋮----
def test_longest_prefix_match(self)
⋮----
"""More specific prefix should match over less specific."""
⋮----
# /api/auth/login matches the more specific prefix (limit=2)
⋮----
def test_register_has_separate_bucket_from_login(self)
⋮----
"""Login and register should have separate buckets."""
⋮----
# Exhaust login
⋮----
# Register should still work
r = client.post("/api/auth/register")
⋮----
def test_stock_ohlcv_uses_chart_tier(self)
⋮----
"""Per-stock OHLCV endpoint should match /api/v1/charts prefix."""
⋮----
# Both used the chart bucket, 3rd should be limited
r = client.get("/api/v1/charts/2222/ohlcv")
</file>

<file path="tests/test_schemas.py">
"""
Pydantic Schema Validation Tests
=================================
Tests for all API request/response schemas in api/schemas/.

All tests run without a database or external services.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ===========================================================================
# PaginationParams tests
⋮----
class TestPaginationParams
⋮----
"""Tests for api.schemas.common.PaginationParams."""
⋮----
def test_defaults(self)
⋮----
# PaginationParams uses FastAPI Query() defaults; calling with explicit
# values is the intended usage pattern.
p = PaginationParams(page=1, page_size=20)
⋮----
def test_offset_page_1(self)
⋮----
def test_offset_page_3(self)
⋮----
p = PaginationParams(page=3, page_size=10)
⋮----
def test_custom_page_size(self)
⋮----
p = PaginationParams(page=1, page_size=50)
⋮----
class TestPaginatedResponse
⋮----
"""Tests for api.schemas.common.PaginatedResponse."""
⋮----
def test_build_basic(self)
⋮----
resp = PaginatedResponse.build(
⋮----
assert resp.total_pages == 4  # ceil(10/3) = 4
⋮----
def test_build_single_page(self)
⋮----
resp = PaginatedResponse.build(items=["a"], total=1, page=1, page_size=20)
⋮----
def test_build_empty(self)
⋮----
resp = PaginatedResponse.build(items=[], total=0, page=1, page_size=20)
assert resp.total_pages == 1  # max(1, ...) ensures at least 1
⋮----
class TestErrorResponse
⋮----
"""Tests for api.schemas.common.ErrorResponse."""
⋮----
def test_basic(self)
⋮----
err = ErrorResponse(detail="Not found")
⋮----
def test_with_code(self)
⋮----
err = ErrorResponse(detail="Rate limited", code="RATE_LIMIT")
⋮----
# NewsCreate / NewsUpdate / NewsResponse
⋮----
class TestNewsSchemas
⋮----
"""Tests for api.schemas.news."""
⋮----
def test_news_create_valid_minimal(self)
⋮----
n = NewsCreate(title="Breaking News", content="Details here")
⋮----
def test_news_create_valid_full(self)
⋮----
n = NewsCreate(
⋮----
def test_news_create_missing_title(self)
⋮----
def test_news_create_missing_content(self)
⋮----
def test_news_create_empty_title(self)
⋮----
def test_news_create_title_too_long(self)
⋮----
def test_news_create_sentiment_score_out_of_range_high(self)
⋮----
def test_news_create_sentiment_score_out_of_range_low(self)
⋮----
def test_news_create_sentiment_score_boundary_values(self)
⋮----
n_pos = NewsCreate(title="Test", content="body", sentiment_score=1.0)
n_neg = NewsCreate(title="Test", content="body", sentiment_score=-1.0)
⋮----
def test_news_update_all_optional(self)
⋮----
u = NewsUpdate()
⋮----
def test_news_response_construction(self)
⋮----
resp = NewsResponse(id="news-1", title="Test", language="en")
⋮----
# ReportCreate / ReportUpdate / ReportResponse
⋮----
class TestReportSchemas
⋮----
"""Tests for api.schemas.reports."""
⋮----
def test_report_create_valid(self)
⋮----
r = ReportCreate(title="Aramco Analysis")
⋮----
def test_report_create_with_prices(self)
⋮----
r = ReportCreate(
⋮----
def test_report_create_missing_title(self)
⋮----
def test_report_create_negative_target_price(self)
⋮----
def test_report_create_zero_target_price(self)
⋮----
r = ReportCreate(title="Test", target_price=0)
⋮----
def test_report_update_all_optional(self)
⋮----
u = ReportUpdate()
⋮----
def test_report_response(self)
⋮----
resp = ReportResponse(id="r-1", title="Analysis")
⋮----
# AnnouncementCreate / AnnouncementResponse
⋮----
class TestAnnouncementSchemas
⋮----
"""Tests for api.schemas.announcements."""
⋮----
def test_announcement_create_minimal(self)
⋮----
a = AnnouncementCreate()
⋮----
def test_announcement_create_full(self)
⋮----
a = AnnouncementCreate(
⋮----
def test_announcement_response(self)
⋮----
resp = AnnouncementResponse(id="ann-1")
⋮----
# Entity schemas
⋮----
class TestEntitySchemas
⋮----
"""Tests for api.schemas.entities."""
⋮----
def test_company_summary(self)
⋮----
cs = CompanySummary(ticker="2222.SR", short_name="Aramco", sector="Energy")
⋮----
def test_company_detail(self)
⋮----
cd = CompanyDetail(
⋮----
def test_entity_list_response(self)
⋮----
resp = EntityListResponse(
⋮----
def test_sector_info(self)
⋮----
si = SectorInfo(sector="Energy", company_count=15)
⋮----
# Watchlist schemas
⋮----
class TestWatchlistSchemas
⋮----
"""Tests for api.schemas.watchlists."""
⋮----
def test_watchlist_create_defaults(self)
⋮----
req = WatchlistCreateRequest()
⋮----
def test_watchlist_create_with_tickers(self)
⋮----
req = WatchlistCreateRequest(
⋮----
def test_watchlist_add_request_valid(self)
⋮----
req = WatchlistAddRequest(ticker="2222.SR")
⋮----
def test_watchlist_add_request_empty_ticker(self)
⋮----
def test_watchlist_update_request(self)
⋮----
req = WatchlistUpdateRequest(name="Renamed")
⋮----
def test_watchlist_response(self)
⋮----
resp = WatchlistResponse(
⋮----
def test_alert_create_request(self)
⋮----
req = AlertCreateRequest(
⋮----
def test_alert_response_default_active(self)
⋮----
resp = AlertResponse(
⋮----
# Chart schemas
⋮----
class TestChartSchemas
⋮----
"""Tests for api.schemas.charts."""
⋮----
def test_chart_request_defaults(self)
⋮----
req = ChartRequest(ticker="2222.SR")
⋮----
def test_chart_request_empty_ticker(self)
⋮----
def test_chart_data_point(self)
⋮----
dp = ChartDataPoint(label="Energy", value=5000000.0)
⋮----
def test_chart_response(self)
⋮----
resp = ChartResponse(
⋮----
# Health schemas
⋮----
class TestHealthSchemas
⋮----
"""Tests for api.schemas.health."""
⋮----
def test_component_health_response(self)
⋮----
ch = ComponentHealthResponse(name="database", status="healthy", latency_ms=1.5)
⋮----
def test_component_health_response_defaults(self)
⋮----
ch = ComponentHealthResponse(name="redis", status="degraded")
⋮----
def test_health_response(self)
⋮----
resp = HealthResponse(
</file>

<file path="tests/test_services.py">
"""
Service Layer Tests
===================
Tests for all service modules: health, news, reports, announcements, user, audit.

Unit tests (always run):
  - Data class construction and serialization
  - Service instantiation with mock connection factories
  - Method existence and signatures

Integration tests (require PostgreSQL):
  - Skipped when POSTGRES_HOST is not set or PG is unreachable
  - Full CRUD operations against a live database
"""
⋮----
# ---------------------------------------------------------------------------
# PostgreSQL availability check
⋮----
def _pg_available() -> bool
⋮----
conn = psycopg2.connect(
⋮----
PG_AVAILABLE = _pg_available()
⋮----
# ===========================================================================
# Data class unit tests (always run, no DB required)
⋮----
class TestNewsDataClass(unittest.TestCase)
⋮----
"""Test NewsArticle data class."""
⋮----
def test_default_construction(self)
⋮----
article = NewsArticle()
⋮----
def test_construction_with_values(self)
⋮----
article = NewsArticle(
⋮----
def test_to_dict(self)
⋮----
article = NewsArticle(ticker="1010.SR", title="Test")
d = article.to_dict()
⋮----
# created_at should be removed when None (let DB default apply)
⋮----
def test_unique_ids(self)
⋮----
a1 = NewsArticle()
a2 = NewsArticle()
⋮----
class TestTechnicalReportDataClass(unittest.TestCase)
⋮----
"""Test TechnicalReport data class."""
⋮----
report = TechnicalReport()
⋮----
report = TechnicalReport(
⋮----
report = TechnicalReport(title="Test Report")
d = report.to_dict()
⋮----
class TestAnnouncementDataClass(unittest.TestCase)
⋮----
"""Test Announcement data class."""
⋮----
ann = Announcement()
⋮----
ann = Announcement(
⋮----
ann = Announcement(title_en="Test")
d = ann.to_dict()
⋮----
class TestUserDataClasses(unittest.TestCase)
⋮----
"""Test UserProfile, Watchlist, and UserAlert data classes."""
⋮----
def test_user_profile_defaults(self)
⋮----
user = UserProfile()
⋮----
def test_watchlist_defaults(self)
⋮----
wl = Watchlist()
⋮----
def test_watchlist_with_tickers(self)
⋮----
wl = Watchlist(tickers=["2222.SR", "1010.SR"])
⋮----
def test_user_alert_defaults(self)
⋮----
alert = UserAlert()
⋮----
class TestAuditDataClasses(unittest.TestCase)
⋮----
"""Test AuditEntry and UsageStats data classes."""
⋮----
def test_audit_entry_defaults(self)
⋮----
entry = AuditEntry()
⋮----
def test_usage_stats_defaults(self)
⋮----
stats = UsageStats()
⋮----
# Service construction tests (always run, mock connection factory)
⋮----
class TestServiceConstruction(unittest.TestCase)
⋮----
"""Verify all services can be instantiated with a mock connection factory."""
⋮----
def _mock_conn_factory(self)
⋮----
def test_news_service_construction(self)
⋮----
svc = NewsAggregationService(get_conn=self._mock_conn_factory)
⋮----
def test_reports_service_construction(self)
⋮----
svc = TechnicalReportsService(get_conn=self._mock_conn_factory)
⋮----
def test_announcement_service_construction(self)
⋮----
svc = AnnouncementService(get_conn=self._mock_conn_factory)
⋮----
def test_user_service_construction(self)
⋮----
svc = UserService(get_conn=self._mock_conn_factory)
⋮----
def test_audit_service_construction(self)
⋮----
svc = AuditService(get_conn=self._mock_conn_factory)
⋮----
# Service method signature tests (always run)
⋮----
class TestServiceMethodSignatures(unittest.TestCase)
⋮----
"""Verify all expected public methods exist on each service."""
⋮----
def test_news_service_methods(self)
⋮----
methods = [
⋮----
def test_reports_service_methods(self)
⋮----
def test_announcement_service_methods(self)
⋮----
def test_user_service_methods(self)
⋮----
def test_audit_service_methods(self)
⋮----
# Health service tests (always run against SQLite)
⋮----
class TestHealthService(unittest.TestCase)
⋮----
"""Test health service functions (works with SQLite backend)."""
⋮----
def test_health_status_enum(self)
⋮----
def test_component_health_construction(self)
⋮----
ch = ComponentHealth(name="test", status=HealthStatus.HEALTHY, message="ok")
⋮----
def test_health_report_to_dict(self)
⋮----
report = HealthReport(
⋮----
def test_check_database_sqlite(self)
⋮----
"""Verify SQLite health check works (DB_BACKEND defaults to sqlite)."""
⋮----
result = check_database()
⋮----
# Should be HEALTHY if saudi_stocks.db exists, UNHEALTHY otherwise
⋮----
def test_get_health(self)
⋮----
report = get_health()
⋮----
# PostgreSQL integration tests (skipped if PG unavailable)
⋮----
@unittest.skipUnless(PG_AVAILABLE, "PostgreSQL not available (set POSTGRES_HOST)")
class TestNewsServicePG(unittest.TestCase)
⋮----
"""Integration tests for NewsAggregationService against PostgreSQL."""
⋮----
@classmethod
    def setUpClass(cls)
⋮----
def test_store_and_retrieve_article(self)
⋮----
count = self.svc.store_articles([article])
⋮----
retrieved = self.svc.get_article_by_id(article.id)
⋮----
def test_get_latest_news(self)
⋮----
articles = self.svc.get_latest_news(limit=5)
⋮----
def test_count_articles(self)
⋮----
count = self.svc.count_articles()
⋮----
@unittest.skipUnless(PG_AVAILABLE, "PostgreSQL not available (set POSTGRES_HOST)")
class TestUserServicePG(unittest.TestCase)
⋮----
"""Integration tests for UserService against PostgreSQL."""
⋮----
def test_get_or_create_user(self)
⋮----
email = f"test-{uuid.uuid4().hex[:8]}@test.com"
user = self.svc.get_or_create_user(email=email, display_name="Test User")
⋮----
def test_create_and_get_watchlist(self)
⋮----
email = f"wl-{uuid.uuid4().hex[:8]}@test.com"
user = self.svc.get_or_create_user(email=email)
wl = self.svc.create_watchlist(
⋮----
watchlists = self.svc.get_watchlists(user_id=user.id)
⋮----
def test_create_and_deactivate_alert(self)
⋮----
email = f"alert-{uuid.uuid4().hex[:8]}@test.com"
⋮----
alert = self.svc.create_alert(
⋮----
deactivated = self.svc.deactivate_alert(alert_id=alert.id, user_id=user.id)
⋮----
@unittest.skipUnless(PG_AVAILABLE, "PostgreSQL not available (set POSTGRES_HOST)")
class TestAuditServicePG(unittest.TestCase)
⋮----
"""Integration tests for AuditService against PostgreSQL."""
⋮----
def test_log_query(self)
⋮----
entry_id = self.svc.log_query(
⋮----
def test_count_queries(self)
⋮----
count = self.svc.count_queries()
</file>

<file path="tests/test_tasi_endpoint.py">
"""
TASI Index Endpoint Tests
=========================
HTTP-level tests for the TASI index API routes using FastAPI TestClient.

Tests:
  - GET /api/v1/charts/tasi/index (default period)
  - GET /api/v1/charts/tasi/index?period=3mo
  - GET /api/v1/charts/tasi/index?period=invalid -> 400
  - GET /api/v1/charts/tasi/health
  - Response schema validation
"""
⋮----
def _create_test_app() -> FastAPI
⋮----
"""Create a minimal FastAPI app with just the TASI router."""
⋮----
app = FastAPI()
⋮----
class TestTASIIndexEndpoint(unittest.TestCase)
⋮----
"""Test GET /api/v1/charts/tasi/index."""
⋮----
@classmethod
    def setUpClass(cls)
⋮----
def setUp(self)
⋮----
def test_default_period_returns_200(self)
⋮----
resp = self.client.get("/api/v1/charts/tasi/index")
⋮----
body = resp.json()
⋮----
def test_explicit_period_3mo(self)
⋮----
resp = self.client.get("/api/v1/charts/tasi/index?period=3mo")
⋮----
def test_invalid_period_returns_400(self)
⋮----
resp = self.client.get("/api/v1/charts/tasi/index?period=invalid")
⋮----
def test_response_has_all_required_fields(self)
⋮----
required_fields = {
⋮----
def test_data_points_have_ohlcv_keys(self)
⋮----
point = body["data"][0]
⋮----
def test_count_matches_data_length(self)
⋮----
resp = self.client.get("/api/v1/charts/tasi/index?period=1mo")
⋮----
def test_all_valid_periods_return_200(self)
⋮----
resp = self.client.get(f"/api/v1/charts/tasi/index?period={period}")
⋮----
def test_response_content_type_is_json(self)
⋮----
class TestTASIHealthEndpoint(unittest.TestCase)
⋮----
"""Test GET /api/v1/charts/tasi/health."""
⋮----
def test_health_returns_200(self)
⋮----
resp = self.client.get("/api/v1/charts/tasi/health")
⋮----
def test_health_has_required_fields(self)
⋮----
"""Sanitized health response exposes only status and message."""
⋮----
required = {"status", "message"}
⋮----
def test_health_does_not_expose_internals(self)
⋮----
"""Sanitized health response must NOT leak infrastructure details."""
⋮----
forbidden = {
exposed = forbidden & set(body.keys())
⋮----
def test_health_status_ok_on_fresh_cache(self)
⋮----
# Populate cache by calling the index endpoint
⋮----
def test_health_status_values(self)
⋮----
def test_health_message_is_string(self)
⋮----
class TestTASIResponseModels(unittest.TestCase)
⋮----
"""Test Pydantic response model construction."""
⋮----
def test_ohlcv_point_model(self)
⋮----
pt = TASIOHLCVPoint(
⋮----
def test_index_response_model(self)
⋮----
resp = TASIIndexResponse(
⋮----
def test_health_response_model(self)
⋮----
resp = TASIHealthResponse(
⋮----
def test_health_response_model_degraded(self)
</file>

<file path="tests/test_tasi_index.py">
"""
TASI Index Service Tests
========================
Unit tests for services/tasi_index.py: fetching, caching, mock data,
thread safety, and structured logging.
"""
⋮----
class TestMockDataGenerator(unittest.TestCase)
⋮----
"""Test _generate_mock_data produces reasonable TASI-range values."""
⋮----
def test_mock_data_returns_list(self)
⋮----
data = _generate_mock_data("1y")
⋮----
def test_mock_data_point_has_required_keys(self)
⋮----
required_keys = {"time", "open", "high", "low", "close", "volume"}
⋮----
def test_mock_data_values_in_tasi_range(self)
⋮----
def test_mock_data_is_deterministic(self)
⋮----
data_a = _generate_mock_data("1y")
data_b = _generate_mock_data("1y")
⋮----
def test_mock_data_period_lengths(self)
⋮----
lengths = {}
⋮----
data = _generate_mock_data(period)
⋮----
def test_mock_data_time_format(self)
⋮----
data = _generate_mock_data("1mo")
⋮----
# Should be YYYY-MM-DD
⋮----
def test_mock_data_unknown_period_defaults(self)
⋮----
data = _generate_mock_data("99y")
# Falls back to 252 days (the default)
⋮----
class TestFetchTasiIndex(unittest.TestCase)
⋮----
"""Test fetch_tasi_index with mocked yfinance."""
⋮----
def setUp(self)
⋮----
# Clear the module-level cache before each test
⋮----
def _make_yf_dataframe(self, rows=10)
⋮----
"""Build a fake yfinance-style DataFrame."""
dates = pd.date_range("2025-01-01", periods=rows, freq="B")
⋮----
@patch("services.tasi_index.yf", create=True)
    def test_fetch_real_data_success(self, mock_yf)
⋮----
"""yfinance returns valid data -> source='real'."""
⋮----
# Patch the import inside the function
fake_ticker = MagicMock()
⋮----
result = mod.fetch_tasi_index("1y")
⋮----
# Verify each point has correct keys
⋮----
@patch("services.tasi_index.yf", create=True)
    def test_fetch_yfinance_exception_falls_to_mock(self, mock_yf)
⋮----
"""yfinance raises -> fallback to mock data."""
⋮----
result = mod.fetch_tasi_index("1mo")
⋮----
def test_fetch_yfinance_import_error_falls_to_mock(self)
⋮----
"""yfinance not installed -> fallback to mock data."""
⋮----
result = mod.fetch_tasi_index("3mo")
⋮----
def test_cache_hit_skips_yfinance(self)
⋮----
"""Second call uses cache, no yfinance fetch."""
⋮----
# Populate cache with mock (yfinance unavailable)
⋮----
result1 = mod.fetch_tasi_index("1y")
⋮----
# Second call should return cached data (mock cached is still source=mock)
result2 = mod.fetch_tasi_index("1y")
⋮----
def test_stale_cache_fallback(self)
⋮----
"""Expired cache + yfinance failure -> source='cached'."""
⋮----
# Populate cache
⋮----
# Expire it by backdating fetched_at
entry = mod._cache["6mo"]
⋮----
# Mock yfinance to fail
mock_yf = MagicMock()
⋮----
result = mod.fetch_tasi_index("6mo")
⋮----
def test_result_payload_schema(self)
⋮----
"""Verify response dict has all required top-level keys."""
⋮----
def test_valid_periods_constant(self)
⋮----
class TestCacheStatus(unittest.TestCase)
⋮----
"""Test get_cache_status diagnostic function."""
⋮----
def test_empty_cache(self)
⋮----
info = get_cache_status()
⋮----
def test_fresh_cache(self)
⋮----
# Populate
⋮----
def test_stale_cache(self)
⋮----
# Expire it
⋮----
class TestThreadSafety(unittest.TestCase)
⋮----
"""Test concurrent access to fetch_tasi_index."""
⋮----
def test_concurrent_fetches_all_succeed(self)
⋮----
"""Multiple threads calling simultaneously all get valid results."""
⋮----
futures = [pool.submit(mod.fetch_tasi_index, "1y") for _ in range(16)]
results = [f.result() for f in concurrent.futures.as_completed(futures)]
⋮----
def test_concurrent_fetches_same_data(self)
⋮----
"""All threads get the same data (deterministic mock)."""
⋮----
futures = [pool.submit(mod.fetch_tasi_index, "3mo") for _ in range(8)]
results = [f.result() for f in futures]
⋮----
first_data = results[0]["data"]
⋮----
class TestStructuredLogging(unittest.TestCase)
⋮----
"""Verify that fetch_tasi_index emits structured log messages."""
⋮----
def test_log_on_cache_hit(self)
⋮----
log_output = "\n".join(cm.output)
⋮----
def test_log_on_mock_fallback(self)
</file>

<file path=".github/workflows/deploy.yml">
name: Deploy

on:
  workflow_run:
    workflows: [CI]
    types: [completed]
    branches: [master]

jobs:
  deploy:
    name: Deploy to Railway
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    environment: production
    steps:
      - uses: actions/checkout@v4

      - name: Install Railway CLI
        run: npm install -g @railway/cli@3

      - name: Deploy to Railway
        run: railway up --service raid-ai-app
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
          RAILWAY_PROJECT_ID: ${{ secrets.RAILWAY_PROJECT_ID }}

      - name: Post-deploy health check
        run: |
          echo "Waiting 30s for deployment to stabilize..."
          sleep 30
          HEALTH_URL="${{ vars.DEPLOY_URL || 'https://raid-ai-app-production.up.railway.app' }}/health"
          echo "Checking health at $HEALTH_URL"
          for i in 1 2 3 4 5; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$HEALTH_URL" || true)
            if [ "$STATUS" = "200" ]; then
              echo "Health check passed (attempt $i)"
              exit 0
            fi
            echo "Attempt $i: status=$STATUS, retrying in 15s..."
            sleep 15
          done
          echo "Health check failed after 5 attempts"
          exit 1

      - name: Rollback on health check failure
        if: failure()
        run: railway rollback --service raid-ai-app
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
          RAILWAY_PROJECT_ID: ${{ secrets.RAILWAY_PROJECT_ID }}
</file>

<file path="api/routes/widgets_stream.py">
"""Live Market Widgets SSE endpoint.

Streams real-time market quotes (crypto, metals, oil, indices) to the
frontend via Server-Sent Events. Uses Redis Pub/Sub when available,
falls back to polling the in-memory snapshot from the quotes hub.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/widgets", tags=["widgets"])
⋮----
_REDIS_KEY = "widgets:quotes:latest"
_REDIS_CHANNEL = "widgets:quotes:pubsub"
⋮----
def _get_redis()
⋮----
"""Return the Redis client, or None."""
⋮----
@router.get("/quotes/stream")
async def widgets_quotes_stream(request: Request)
⋮----
"""SSE stream of live market quotes.

    Sends the latest snapshot immediately on connect, then streams
    updates via Redis Pub/Sub or in-memory polling.
    """
redis = _get_redis()
⋮----
# In-memory fallback (no Redis)
⋮----
def _sse_headers() -> dict
⋮----
async def _memory_event_generator(request: Request)
⋮----
"""Stream quotes from in-memory snapshot (no Redis)."""
⋮----
last_sent = ""
⋮----
# Fast first paint
snapshot = get_latest_snapshot()
⋮----
last_sent = snapshot
⋮----
event = get_snapshot_event()
⋮----
# Wait for new data or timeout
⋮----
# Send keepalive comment
⋮----
async def _redis_event_generator(request: Request, redis)
⋮----
"""Stream quotes from Redis Pub/Sub."""
# Fast first paint: send cached snapshot immediately
⋮----
snapshot = await asyncio.to_thread(redis.get, _REDIS_KEY)
⋮----
# Subscribe to Pub/Sub channel for live updates
pubsub = redis.pubsub()
⋮----
msg = await asyncio.to_thread(pubsub.get_message, timeout=1.0)
⋮----
data = msg["data"]
⋮----
data = data.decode("utf-8")
</file>

<file path="api/schemas/entities.py">
"""Pydantic schemas for entity (company/stock) endpoints."""
⋮----
class CompanySummary(BaseModel)
⋮----
"""Brief company summary for list views."""
⋮----
ticker: str
short_name: Optional[str] = None
sector: Optional[str] = None
industry: Optional[str] = None
current_price: Optional[float] = None
market_cap: Optional[float] = None
change_pct: Optional[float] = None
⋮----
class CompanyDetail(BaseModel)
⋮----
"""Full company detail with market data, valuation, and profitability."""
⋮----
exchange: Optional[str] = None
currency: Optional[str] = None
⋮----
previous_close: Optional[float] = None
day_high: Optional[float] = None
day_low: Optional[float] = None
week_52_high: Optional[float] = None
week_52_low: Optional[float] = None
volume: Optional[int] = None
⋮----
beta: Optional[float] = None
trailing_pe: Optional[float] = None
forward_pe: Optional[float] = None
price_to_book: Optional[float] = None
trailing_eps: Optional[float] = None
roe: Optional[float] = None
profit_margin: Optional[float] = None
revenue_growth: Optional[float] = None
recommendation: Optional[str] = None
target_mean_price: Optional[float] = None
analyst_count: Optional[int] = None
# valuation_metrics (expanded)
price_to_sales: Optional[float] = None
enterprise_value: Optional[float] = None
ev_to_revenue: Optional[float] = None
peg_ratio: Optional[float] = None
forward_eps: Optional[float] = None
book_value: Optional[float] = None
# profitability_metrics (expanded)
roa: Optional[float] = None
operating_margin: Optional[float] = None
gross_margin: Optional[float] = None
ebitda_margin: Optional[float] = None
earnings_growth: Optional[float] = None
# market_data (expanded)
avg_50d: Optional[float] = None
avg_200d: Optional[float] = None
avg_volume: Optional[int] = None
shares_outstanding: Optional[float] = None
pct_held_insiders: Optional[float] = None
pct_held_institutions: Optional[float] = None
# analyst_data (expanded)
target_high_price: Optional[float] = None
target_low_price: Optional[float] = None
target_median_price: Optional[float] = None
⋮----
class EntityListResponse(BaseModel)
⋮----
"""Paginated list of company summaries."""
⋮----
items: List[CompanySummary]
count: int
total: int = 0
⋮----
class SectorInfo(BaseModel)
⋮----
"""Sector with company count."""
⋮----
sector: str
company_count: int
</file>

<file path="api/schemas/health.py">
"""Pydantic schemas for health check endpoint."""
⋮----
class ComponentHealthResponse(BaseModel)
⋮----
"""Health status of a single platform component."""
⋮----
name: str
status: str
latency_ms: Optional[float] = None
message: str = ""
⋮----
class HealthResponse(BaseModel)
⋮----
"""Aggregated health status for the platform."""
⋮----
service: str = "raid-ai-tasi"
version: str = "1.0.0"
uptime_seconds: float = 0.0
components: List[ComponentHealthResponse]
pool_stats: Optional[Dict[str, Any]] = None
</file>

<file path="backend/middleware/cost_controller.py">
"""
API cost tracking controller for Anthropic/LLM usage.

Tracks per-user API costs in Redis (db=1) with daily and monthly buckets.
Falls back to in-memory tracking when Redis is unavailable.

Usage::

    controller = CostController(redis_url="redis://localhost:6379/1")
    controller.record_cost("user:123", input_tokens=500, output_tokens=200)
    usage = controller.get_usage("user:123")
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Anthropic Claude pricing (USD per 1M tokens, approximate)
_DEFAULT_INPUT_COST_PER_M = 3.00  # $3 per 1M input tokens
_DEFAULT_OUTPUT_COST_PER_M = 15.00  # $15 per 1M output tokens
⋮----
class UsageSummary(BaseModel)
⋮----
"""Summary of API usage for a user.

    Attributes
    ----------
    user_id : str
        The user identifier.
    daily_input_tokens : int
        Input tokens used today.
    daily_output_tokens : int
        Output tokens used today.
    daily_cost_usd : float
        Estimated USD cost for today.
    monthly_input_tokens : int
        Input tokens used this month.
    monthly_output_tokens : int
        Output tokens used this month.
    monthly_cost_usd : float
        Estimated USD cost for this month.
    """
⋮----
user_id: str
daily_input_tokens: int = 0
daily_output_tokens: int = 0
daily_cost_usd: float = 0.0
monthly_input_tokens: int = 0
monthly_output_tokens: int = 0
monthly_cost_usd: float = 0.0
⋮----
class CostLimitConfig(BaseModel)
⋮----
"""Per-user cost limits.

    Attributes
    ----------
    daily_cost_limit_usd : float
        Maximum daily spend per user in USD. 0 = unlimited.
    monthly_cost_limit_usd : float
        Maximum monthly spend per user in USD. 0 = unlimited.
    daily_token_limit : int
        Maximum total tokens (input + output) per day. 0 = unlimited.
    """
⋮----
daily_cost_limit_usd: float = Field(default=0.0, ge=0)
monthly_cost_limit_usd: float = Field(default=0.0, ge=0)
daily_token_limit: int = Field(default=0, ge=0)
⋮----
class CostController
⋮----
"""Tracks per-user LLM API costs with Redis or in-memory storage.

    Parameters
    ----------
    redis_url : str or None
        Redis URL (should use db=1). Pass None for in-memory only.
    input_cost_per_m : float
        Cost per 1M input tokens in USD.
    output_cost_per_m : float
        Cost per 1M output tokens in USD.
    limits : CostLimitConfig or None
        Per-user cost/token limits. None = no limits.
    """
⋮----
# In-memory fallback: key -> {field: value}
⋮----
def _init_redis(self, url: str) -> None
⋮----
"""Attempt to connect to Redis."""
⋮----
def _calc_cost(self, input_tokens: int, output_tokens: int) -> float
⋮----
"""Calculate USD cost from token counts."""
⋮----
@staticmethod
    def _daily_key(user_id: str) -> str
⋮----
"""Redis key for daily usage bucket."""
day = datetime.now(timezone.utc).strftime("%Y%m%d")
⋮----
@staticmethod
    def _monthly_key(user_id: str) -> str
⋮----
"""Redis key for monthly usage bucket."""
month = datetime.now(timezone.utc).strftime("%Y%m")
⋮----
"""Record token usage for a user.

        Parameters
        ----------
        user_id : str
            User identifier (e.g. "user:123" or "ip:10.0.0.1").
        input_tokens : int
            Number of input tokens consumed.
        output_tokens : int
            Number of output tokens consumed.
        """
daily_key = self._daily_key(user_id)
monthly_key = self._monthly_key(user_id)
⋮----
pipe = self._redis.pipeline(transaction=False)
⋮----
pipe.expire(daily_key, 86400 + 3600)  # 25 hours
⋮----
pipe.expire(monthly_key, 32 * 86400)  # 32 days
⋮----
# In-memory fallback
⋮----
def get_usage(self, user_id: str) -> UsageSummary
⋮----
"""Get current usage summary for a user.

        Parameters
        ----------
        user_id : str
            User identifier.

        Returns
        -------
        UsageSummary
            Current daily and monthly token counts and estimated costs.
        """
⋮----
daily_in = 0
daily_out = 0
monthly_in = 0
monthly_out = 0
⋮----
daily_data = self._redis.hgetall(daily_key)
monthly_data = self._redis.hgetall(monthly_key)
daily_in = int(daily_data.get("input_tokens", 0))
daily_out = int(daily_data.get("output_tokens", 0))
monthly_in = int(monthly_data.get("input_tokens", 0))
monthly_out = int(monthly_data.get("output_tokens", 0))
⋮----
daily_in = self._memory[daily_key].get("input_tokens", 0)
daily_out = self._memory[daily_key].get("output_tokens", 0)
monthly_in = self._memory[monthly_key].get("input_tokens", 0)
monthly_out = self._memory[monthly_key].get("output_tokens", 0)
⋮----
def check_limits(self, user_id: str) -> tuple
⋮----
"""Check if a user has exceeded their cost/token limits.

        Parameters
        ----------
        user_id : str
            User identifier.

        Returns
        -------
        tuple[bool, str]
            (allowed, reason). allowed=True if under all limits.
        """
⋮----
usage = self.get_usage(user_id)
⋮----
total_daily = usage.daily_input_tokens + usage.daily_output_tokens
⋮----
def close(self) -> None
⋮----
"""Close the Redis connection if open."""
</file>

<file path="backend/services/cache/query_cache.py">
"""Query-level cache built on top of RedisManager.

Provides SHA-256 keyed caching with tiered TTLs (market / historical /
schema) and msgpack serialization for compact Redis storage.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_KEY_PREFIX = "raid:qcache:"
⋮----
def _normalize_sql(sql: str) -> str
⋮----
"""Collapse whitespace and lowercase for consistent hashing."""
⋮----
def _make_key(sql: str) -> str
⋮----
"""Return the Redis key for a given SQL query."""
digest = hashlib.sha256(_normalize_sql(sql).encode("utf-8")).hexdigest()
⋮----
def classify_tier(sql: str) -> TTLTier
⋮----
"""Heuristically classify a SQL query into a TTL tier.

    Rules (applied against the lowercased, whitespace-normalized SQL):
    - Contains ``information_schema``, ``sqlite_master``, ``pg_catalog``,
      or looks like a ``DESCRIBE``/``SHOW`` statement -> SCHEMA.
    - Contains date-range predicates (``period_type``, ``period_index``,
      ``WHERE.*date``) or targets ``balance_sheet``, ``income_statement``,
      ``cash_flow`` -> HISTORICAL.
    - Everything else (``market_data``, ``companies``, etc.) -> MARKET.
    """
norm = _normalize_sql(sql)
⋮----
# Schema / metadata queries
schema_indicators = (
⋮----
# Historical / financial-statement queries
historical_indicators = (
⋮----
# Default: live market data
⋮----
class QueryCache
⋮----
"""Tiered query cache backed by Redis + msgpack.

    Args:
        redis: An initialized RedisManager instance.
        enabled: Master switch; when False all operations are no-ops.
    """
⋮----
def __init__(self, redis: RedisManager, *, enabled: bool = True) -> None
⋮----
@property
    def enabled(self) -> bool
⋮----
@property
    def hit_count(self) -> int
⋮----
@property
    def miss_count(self) -> int
⋮----
@property
    def hit_rate(self) -> float
⋮----
total = self._hits + self._misses
⋮----
async def get(self, sql: str) -> list[dict[str, Any]] | None
⋮----
"""Look up a cached result for *sql*.

        Returns:
            The cached row list, or ``None`` on a miss.
        """
⋮----
key = _make_key(sql)
⋮----
raw = await self._redis.get(key)
⋮----
envelope: dict[str, Any] = msgpack.unpackb(raw, raw=False)
⋮----
"""Store a query result in the cache.

        Args:
            sql: The SQL query.
            data: The result rows.
            tier: Explicit TTL tier. If None, auto-classified from *sql*.

        Returns:
            True if the value was stored.
        """
⋮----
tier = classify_tier(sql)
⋮----
ttl = tier.ttl_seconds
⋮----
envelope = CachedResult(
⋮----
packed = msgpack.packb(
⋮----
async def invalidate(self, sql: str) -> bool
⋮----
"""Remove a specific query from the cache.

        Returns:
            True if the key existed and was deleted.
        """
⋮----
deleted = await self._redis.delete(key)
⋮----
def stats(self) -> dict[str, Any]
⋮----
"""Return in-process cache statistics."""
</file>

<file path="config/__init__.py">
"""
TASI AI Platform configuration module.

Usage:
    from config import get_settings
    settings = get_settings()
    print(settings.db.backend)       # "sqlite" or "postgresql"
    print(settings.server.port)      # 8084
    print(settings.get_llm_api_key())  # effective API key
"""
⋮----
__all__ = [
</file>

<file path="config/lifecycle.py">
"""
Application lifecycle handlers for startup and shutdown.

Provides structured logging of startup diagnostics and graceful shutdown
coordination. Designed to be called from the FastAPI lifespan context.

Usage in app.py:
    from config.lifecycle import on_startup, on_shutdown

    async def lifespan(app):
        on_startup()
        yield
        on_shutdown()
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_APP_VERSION = "1.0.0"
_start_time: float = 0.0
⋮----
def on_startup() -> None
⋮----
"""Run startup diagnostics.

    Logs version, Python info, database backend, and validates environment.
    """
⋮----
_start_time = time.monotonic()
⋮----
# Validate environment variables
⋮----
# Log database backend
⋮----
settings = get_settings()
⋮----
# Log connection pool status
⋮----
_settings = _get_settings()
⋮----
# Log Prometheus availability
⋮----
import prometheus_fastapi_instrumentator  # noqa: F401
⋮----
def on_shutdown() -> None
⋮----
"""Run graceful shutdown procedures.

    Logs shutdown timing and flushes log handlers.
    """
elapsed = time.monotonic() - _start_time if _start_time else 0
⋮----
# Flush all log handlers to ensure nothing is lost
</file>

<file path="entrypoint.sh">
#!/bin/bash
set -e

# Map POSTGRES_* vars to PG_* vars expected by csv_to_postgres.py
export PG_HOST="${POSTGRES_HOST:-localhost}"
export PG_PORT="${POSTGRES_PORT:-5432}"
export PG_DBNAME="${POSTGRES_DB:-raid_ai}"
export PG_USER="${POSTGRES_USER:-raid}"
export PG_PASSWORD="${POSTGRES_PASSWORD:-}"

# Initialize database schema and load data on first run
if [ "$DB_BACKEND" = "postgres" ]; then
    echo "Checking if database needs initialization..."
    python -c "
import psycopg2, os, sys
conn = psycopg2.connect(
    host=os.environ['PG_HOST'],
    port=os.environ['PG_PORT'],
    dbname=os.environ['PG_DBNAME'],
    user=os.environ['PG_USER'],
    password=os.environ['PG_PASSWORD']
)
cur = conn.cursor()
cur.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'companies'\")
exists = cur.fetchone()[0] > 0
cur.close()
conn.close()
if exists:
    print('TABLES_EXIST')
else:
    print('NEED_INIT')
" > /tmp/db_check.txt 2>&1 || echo "NEED_INIT" > /tmp/db_check.txt

    DB_STATUS=$(cat /tmp/db_check.txt)

    if echo "$DB_STATUS" | grep -q "NEED_INIT"; then
        echo "Initializing database schema..."
        if ! PGPASSWORD="$PG_PASSWORD" psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" \
             -d "$PG_DBNAME" -f database/schema.sql 2>&1; then
            echo "ERROR: Schema initialization failed. Check database/schema.sql and DB credentials." >&2
            exit 1
        fi

        echo "Loading data from CSV..."
        if [ ! -f database/csv_to_postgres.py ]; then
            echo "ERROR: CSV loader not found at database/csv_to_postgres.py" >&2
            exit 1
        fi
        if ! python database/csv_to_postgres.py --csv-path saudi_stocks_yahoo_data.csv; then
            echo "ERROR: CSV data loading failed. App cannot start without data." >&2
            exit 1
        fi
        echo "Database initialization complete."
    else
        echo "Database already initialized, skipping."
    fi
fi

# Start the application
exec uvicorn app:app --host 0.0.0.0 --port ${PORT:-8084} --timeout-graceful-shutdown 30
</file>

<file path="frontend/src/app/admin/page.tsx">
import AdminPage from './AdminPage';
⋮----
export default function Page()
</file>

<file path="frontend/src/app/chat/page.tsx">
import { Suspense } from 'react';
import { AIChatInterface } from '@/components/chat/AIChatInterface';
⋮----
function ChatContent()
⋮----
export default function ChatPage()
</file>

<file path="frontend/src/app/markets/MarketOverviewClient.tsx">
import { useState, useEffect, useMemo, useRef } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
import { useFormatters } from '@/lib/hooks/useFormatters';
import { Breadcrumb } from '@/components/common/Breadcrumb';
import type { MarketGraphModel, EdgeLabel } from '@/lib/market-graph';
⋮----
import {
  MarketHeader,
  ConstellationCanvas,
  MobileSummary,
  CategoryLegend,
  MARKET_KEYFRAMES,
} from './components';
⋮----
// ---------------------------------------------------------------------------
// Main client component
// ---------------------------------------------------------------------------
⋮----
// Detect price changes and trigger flash animation
⋮----
// Compute connection status: 'live' | 'stale' | 'offline'
⋮----
}, [isLive, lastUpdated, time]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
{/* CSS keyframes */}
⋮----
{/* Breadcrumb */}
⋮----
{/* Header */}
⋮----
{/* Desktop constellation */}
⋮----
{/* Mobile view */}
⋮----
{/* Legend + explainer */}
</file>

<file path="frontend/src/app/news/components/FilterBar.tsx">
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { SOURCE_FILTERS } from '../utils';
import { SearchInput } from './SearchInput';
⋮----
export interface FilterBarProps {
  searchQuery: string;
  onSearchChange: (q: string) => void;
  activeSource: string | null;
  showSaved: boolean;
  isSearching: boolean;
  bookmarkCount: number;
  sourceCounts: Record<string, number>;
  showAdvancedFilters: boolean;
  advancedFilterCount: number;
  activeSentiment: string | null;
  dateFrom: string;
  dateTo: string;
  isSticky: boolean;
  onSourceChange: (source: string | null) => void;
  onShowSaved: () => void;
  onToggleAdvancedFilters: () => void;
  onSentimentChange: (sentiment: string) => void;
  onDateFromChange: (value: string) => void;
  onDateToChange: (value: string) => void;
  onClearAdvancedFilters: () => void;
}
⋮----
className=
⋮----
{/* Search input */}
⋮----
{/* Source filter chips + saved tab */}
⋮----
onSearchChange('');
onSourceChange(source.key);
⋮----
{/* Saved articles tab */}
⋮----
{/* Advanced filters toggle */}
⋮----
{/* Advanced filters panel */}
⋮----
{/* Sentiment chips */}
⋮----
{/* Date range */}
⋮----
{/* Clear filters */}
</file>

<file path="frontend/src/app/news/components/SearchInput.tsx">
import { useState, useRef, useEffect } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
export interface SearchInputProps {
  value: string;
  onChange: (v: string) => void;
  placeholder?: string;
}
⋮----
const handleChange = (v: string) =>
⋮----
{/* Search icon */}
⋮----
onChange=
⋮----
aria-label=
⋮----
{/* Clear button */}
⋮----
setLocal('');
onChange('');
</file>

<file path="frontend/src/app/news/components/SkeletonCard.tsx">
function ShimmerOverlay(
⋮----
export function SkeletonCard(
⋮----
{/* Main content area */}
⋮----
{/* Sentiment badge placeholder */}
⋮----
{/* Title placeholder - two lines */}
⋮----
{/* Body text placeholder */}
⋮----
{/* Footer: source badge + time + chevron */}
⋮----
{/* Source icon placeholder */}
⋮----
{/* Shimmer overlay */}
</file>

<file path="frontend/src/app/news/error.tsx">
import { useEffect, useState } from 'react';
⋮----
const handleRetry = () =>
⋮----
{/* News error icon with gold glow */}
⋮----
{/* Gold accent separator */}
</file>

<file path="frontend/src/app/news/hooks/useNewsFilters.ts">
import { useState, useCallback, useMemo } from 'react';
⋮----
interface NewsFiltersState {
  page: number;
  activeSource: string | null;
  searchQuery: string;
  showSaved: boolean;
  activeSentiment: string | null;
  dateFrom: string;
  dateTo: string;
  showAdvancedFilters: boolean;
  advancedFilterCount: number;
}
⋮----
interface NewsFiltersActions {
  setPage: (p: number | ((prev: number) => number)) => void;
  setSearchQuery: (q: string) => void;
  setShowSaved: (v: boolean) => void;
  setShowAdvancedFilters: (v: boolean | ((prev: boolean) => boolean)) => void;
  handleSourceChange: (source: string | null) => void;
  handleClearAdvancedFilters: () => void;
  handleSentimentChange: (sentiment: string) => void;
  handleDateFromChange: (value: string) => void;
  handleDateToChange: (value: string) => void;
}
⋮----
export function useNewsFilters(
  resetAllArticles: () => void,
): NewsFiltersState & NewsFiltersActions
</file>

<file path="frontend/src/components/charts/chart-types.ts">
export interface OHLCVData {
  time: string;
  open: number;
  high: number;
  low: number;
  close: number;
  volume?: number;
}
⋮----
export interface LineDataPoint {
  time: string;
  value: number;
}
⋮----
export type ChartTimeRange = '1W' | '1M' | '3M' | '6M' | '1Y' | 'ALL';
⋮----
export interface ChartContainerProps {
  height?: number;
  className?: string;
  title?: string;
}
⋮----
export type DataSource = 'real' | 'mock' | 'cached';
⋮----
export interface ChartDataResult<T> {
  data: T | null;
  loading: boolean;
  error: string | null;
  source: DataSource | null;
  refetch: () => void;
}
</file>

<file path="frontend/src/components/charts/index.tsx">
import dynamic from 'next/dynamic';
import { ChartSkeleton } from './ChartSkeleton';
</file>

<file path="frontend/src/components/chat/AssistantContent.tsx">
import dynamic from 'next/dynamic';
import type { ChatMessage, SSEEvent, SSEProgressData, SSECodeData, SSETableData, SSEChartData, SSETextData } from '@/lib/types';
import { DataTable } from './DataTable';
import { LoadingDots } from './LoadingDots';
⋮----
interface AssistantContentProps {
  message: ChatMessage;
  /** Live progress text from the SSE stream */
  progressText?: string;
}
⋮----
/** Live progress text from the SSE stream */
</file>

<file path="frontend/src/components/chat/SQLBlock.tsx">
import { useState } from 'react';
import { cn } from '@/lib/utils';
import type { SSECodeData } from '@/lib/types';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { oneDark } from 'react-syntax-highlighter/dist/esm/styles/prism';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface SQLBlockProps {
  data: SSECodeData;
}
⋮----
const handleCopy = async () =>
⋮----
{/* Header */}
⋮----
{/* Code */}
</file>

<file path="frontend/src/components/common/error-display.tsx">
import { useState } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface ErrorDisplayProps {
  message: string;
  onRetry?: () => void;
  className?: string;
}
⋮----
/**
 * Parse the error message to extract the HTTP status code (if present).
 * ApiError messages follow the format: "[API_ERROR:<status>] ..."
 */
function parseErrorStatus(message: string): number | null
⋮----
/**
 * Returns a user-friendly bilingual error message based on the error string.
 */
function getUserFriendlyMessage(
  message: string,
  t: (ar: string, en: string) => string,
): string
⋮----
// Other HTTP errors (4xx, etc.)
⋮----
// Network / fetch errors (no status code)
⋮----
// Fallback for any other unknown error
⋮----
<div className=
⋮----
{/* Technical detail toggle */}
⋮----
onClick=
⋮----
className=
</file>

<file path="frontend/src/components/common/Toast.tsx">
import {
  createContext,
  useContext,
  useState,
  useCallback,
  useRef,
  type ReactNode,
} from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
type ToastType = 'success' | 'error' | 'info' | 'warning';
⋮----
interface ToastItem {
  id: number;
  message: string;
  type: ToastType;
  exiting: boolean;
}
⋮----
interface ToastContextValue {
  showToast: (message: string, type?: ToastType) => void;
}
⋮----
// ---------------------------------------------------------------------------
// Context
// ---------------------------------------------------------------------------
⋮----
export function useToast(): ToastContextValue
⋮----
// ---------------------------------------------------------------------------
// Color map
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Single toast component
// ---------------------------------------------------------------------------
⋮----
className=
⋮----
// ---------------------------------------------------------------------------
// Provider
// ---------------------------------------------------------------------------
⋮----
// Remove from DOM after exit animation
⋮----
// Auto-dismiss after 3 seconds
⋮----
{/* Toast container */}
</file>

<file path="frontend/src/components/layout/AppShell.tsx">
import { useState, useCallback } from 'react';
import { Header } from './Header';
import { Sidebar } from './Sidebar';
import { Footer } from './Footer';
import { CommandPalette } from '@/components/common/CommandPalette';
import { MobileBottomNav } from '@/components/common/MobileBottomNav';
import { ToastProvider } from '@/components/common/Toast';
import { LiveMarketWidgets } from '@/components/widgets/LiveMarketWidgets';
import { OfflineBanner } from '@/components/common/OfflineBanner';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface AppShellProps {
  children: React.ReactNode;
}
⋮----
export function AppShell(
</file>

<file path="frontend/src/components/queries/QueryHistoryItem.tsx">
import { useState } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import type { QueryRecord } from '@/types/queries';
⋮----
function timeAgo(timestamp: number, t: (ar: string, en: string) => string): string
⋮----
interface QueryHistoryItemProps {
  record: QueryRecord;
  onRerun: (query: string) => void;
  onDelete: (id: string) => void;
  onToggleFavorite: (id: string) => void;
  onSave?: (record: QueryRecord) => void;
}
⋮----
const handleCopySql = async () =>
⋮----
// Fallback: no-op
⋮----
className=
⋮----
{/* Collapsed header */}
⋮----
{/* Expand chevron */}
⋮----
{/* Favorite star */}
⋮----
{/* Expanded content with accordion animation */}
⋮----
{/* SQL block */}
⋮----
{/* Result preview */}
⋮----
{/* Tags */}
⋮----
{/* Action buttons */}
⋮----
onClick=
</file>

<file path="frontend/src/components/queries/SaveQueryModal.tsx">
import { useState, useEffect, useRef } from 'react';
import { cn } from '@/lib/utils';
import { queryStore } from '@/lib/queries/query-store';
import type { QueryRecord } from '@/types/queries';
⋮----
interface SaveQueryModalProps {
  record: QueryRecord;
  onClose: () => void;
  onSaved: (updated: QueryRecord) => void;
}
⋮----
// Close on Escape + focus trapping
⋮----
const handler = (e: KeyboardEvent) =>
⋮----
const handleSave = async () =>
⋮----
{/* Header */}
⋮----
{/* Body */}
⋮----
{/* Query preview */}
⋮----
{/* Name */}
⋮----
{/* Tags */}
⋮----
{/* Notes */}
⋮----
{/* Footer */}
</file>

<file path="frontend/src/components/visualization/DataTable.tsx">
import { useCallback, useMemo, useRef, useState } from 'react';
import { useVirtualizer } from '@tanstack/react-virtual';
import { cn } from '@/lib/utils';
import { DataTableHeader, type SortDirection } from './DataTableHeader';
⋮----
interface DataTableProps {
  columns: string[];
  rows: (string | number | null)[][];
  pageSize?: number;
  className?: string;
}
⋮----
function compareValues(a: string | number | null, b: string | number | null, dir: SortDirection): number
⋮----
function exportCSV(columns: string[], rows: (string | number | null)[][])
⋮----
const escape = (val: string | number | null) =>
⋮----
// Filter rows
⋮----
// Sort rows
⋮----
// Paginate
⋮----
const renderRow = (row: (string | number | null)[], rowIdx: number, style?: React.CSSProperties) => (
    <tr
      key={rowIdx}
      className={cn(
        'border-b border-[#2A2A2A]',
        'hover:bg-gold/5 transition-colors',
        rowIdx % 2 === 1 && 'bg-[#1A1A1A]/50',
      )}
      style={style}
    >
      <td className="px-3 py-2 text-[#707070] text-xs whitespace-nowrap">
        {page * pageSize + rowIdx + 1}
      </td>
      {row.map((cell, cellIdx) => (
        <td key={cellIdx} className="px-3 py-2 text-[#B0B0B0] whitespace-nowrap text-sm">
          {cell !== null && cell !== undefined ? String(cell) : '-'}
        </td>
      ))}
    </tr>
  );
⋮----
<div className=
{/* Toolbar */}
⋮----
onClick=
⋮----
{/* Table */}
⋮----

⋮----
{/* Footer / pagination */}
</file>

<file path="frontend/src/components/visualization/MobileQueryInput.tsx">
import { useCallback, useState } from 'react';
import { cn } from '@/lib/utils';
⋮----
interface MobileQueryInputProps {
  onSubmit: (query: string) => void;
  isLoading?: boolean;
  placeholder?: string;
  className?: string;
}
⋮----
onChange=
</file>

<file path="frontend/src/lib/chart-cache.ts">
import useSWR, { type SWRConfiguration, type KeyedMutator } from 'swr';
import { metricsMiddleware } from '@/lib/monitoring/swr-middleware';
⋮----
// ---------------------------------------------------------------------------
// Default SWR config for chart data
// ---------------------------------------------------------------------------
⋮----
dedupingInterval: 60_000,       // 60s dedup window
refreshInterval: 360_000,       // 6min — offset from backend 5min TTL to avoid always hitting cache
⋮----
use: [metricsMiddleware],        // collect API timing + error-rate metrics
⋮----
// ---------------------------------------------------------------------------
// Cache key factories
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Generic SWR hook wrapper for chart data
// ---------------------------------------------------------------------------
⋮----
export interface ChartCacheResult<T> {
  data: T | undefined;
  error: Error | undefined;
  isLoading: boolean;
  isValidating: boolean;
  mutate: KeyedMutator<T>;
}
⋮----
/**
 * Wraps SWR with chart-specific defaults.
 * `key` should come from `chartKeys.*`.
 * `fetcher` is the async function that returns data.
 */
export function useChartCache<T>(
  key: readonly unknown[] | null,
  fetcher: () => Promise<T>,
  config?: SWRConfiguration,
): ChartCacheResult<T>
</file>

<file path="frontend/src/lib/types.ts">
/**
 * Types for the Vanna 2.0 SSE chat interface.
 */
⋮----
/** SSE event types sent by the Vanna backend */
export type SSEEventType = 'progress' | 'code' | 'table' | 'chart' | 'text';
⋮----
/** A single SSE event from the backend (normalized format) */
export interface SSEEvent {
  type: SSEEventType;
  data: SSEProgressData | SSECodeData | SSETableData | SSEChartData | SSETextData;
}
⋮----
export interface SSEProgressData {
  message: string;
}
⋮----
export interface SSECodeData {
  language: string;
  content: string;
}
⋮----
export interface SSETableData {
  columns: string[];
  rows: (string | number | null)[][];
}
⋮----
export interface SSEChartData {
  plotly_json: Record<string, unknown>;
}
⋮----
export interface SSETextData {
  content: string;
}
⋮----
// ---------------------------------------------------------------------------
// Vanna 2.0 raw SSE types (rich/simple envelope)
// ---------------------------------------------------------------------------
⋮----
/** The raw rich component sent by the Vanna 2.0 backend */
export interface Vanna2RichComponent {
  id: string;
  type: string;
  lifecycle?: string;
  children?: unknown[];
  timestamp?: string;
  visible?: boolean;
  interactive?: boolean;
  data: Record<string, unknown>;
}
⋮----
/** The raw simple fallback sent alongside the rich component */
export interface Vanna2SimpleComponent {
  type?: string;
  semantic_type?: string | null;
  metadata?: unknown;
  text?: string;
}
⋮----
/** A raw SSE message in Vanna 2.0 format (rich/simple envelope) */
export interface Vanna2RawEvent {
  rich: Vanna2RichComponent;
  simple: Vanna2SimpleComponent | null;
  conversation_id?: string;
  request_id?: string;
  timestamp?: number;
}
⋮----
/** A chat message in the conversation */
export interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  /** Streamed components from the assistant */
  components?: SSEEvent[];
  /** Whether the assistant is still streaming */
  isStreaming?: boolean;
  /** Whether this message resulted in an error */
  isError?: boolean;
}
⋮----
/** Streamed components from the assistant */
⋮----
/** Whether the assistant is still streaming */
⋮----
/** Whether this message resulted in an error */
</file>

<file path="middleware/cors.py">
"""
CORS middleware setup for TASI AI Platform.

Configures FastAPI's built-in CORSMiddleware with allowed origins from settings.
"""
⋮----
def setup_cors(app: "FastAPI", allowed_origins: List[str]) -> None
⋮----
"""Add CORS middleware to the FastAPI application.

    Parameters
    ----------
    app : FastAPI
        The FastAPI application instance.
    allowed_origins : list[str]
        List of allowed origin URLs (e.g. ["http://localhost:3000"]).
    """
</file>

<file path="models/validators.py">
"""
Shared input validators for API route handlers.

Provides reusable validation functions and FastAPI Path/Query dependencies
for common parameters like ticker symbols and period strings.
"""
⋮----
# ---------------------------------------------------------------------------
# Ticker validation
⋮----
# Saudi stock tickers: 4-digit number optionally followed by .SR
# Also allow ^TASI for the index
_TICKER_PATTERN = re.compile(r"^(\d{4}(\.SR)?|\^TASI)$", re.IGNORECASE)
⋮----
# Max ticker length to prevent abuse (longest valid: "2222.SR" = 7 chars)
_MAX_TICKER_LENGTH = 10
⋮----
def validate_ticker(ticker: str) -> str
⋮----
"""Validate and normalize a Saudi stock ticker.

    Accepts: "2222", "2222.SR", "^TASI"
    Raises HTTPException 400 for invalid formats.
    """
stripped = ticker.strip()
⋮----
"""Validate a comma-separated list of tickers.

    Returns a list of validated ticker strings.
    Raises HTTPException 400 for invalid formats or count violations.
    """
ticker_list = [t.strip() for t in tickers_csv.split(",") if t.strip()]
⋮----
# Period validation
⋮----
# Valid periods for yfinance-backed OHLCV endpoints
VALID_OHLCV_PERIODS = frozenset(
⋮----
# Valid period types for financial statements
VALID_PERIOD_TYPES = frozenset({"annual", "quarterly", "ttm"})
⋮----
# Valid financial statement names
VALID_STATEMENT_TYPES = frozenset({"balance_sheet", "income_statement", "cash_flow"})
⋮----
def validate_ohlcv_period(period: str) -> str
⋮----
"""Validate an OHLCV period string."""
</file>

<file path="pyproject.toml">
# =============================================================================
# Ra'd AI Platform -- Project Configuration
# =============================================================================

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

# Custom markers for test taxonomy
markers = [
    "fast: Unit tests that run quickly with no external dependencies",
    "slow: Tests that may take longer (network, heavy computation)",
    "integration: Integration tests requiring app assembly or multiple components",
    "pg_required: Tests requiring a running PostgreSQL instance (skip if unavailable)",
    "performance: Performance and load tests (may take longer to run)",
]

# Default: run all tests except pg_required (use -m to override)
addopts = [
    "-v",
    "--strict-markers",
    "--tb=short",
]

# Filter common warnings
filterwarnings = [
    "ignore::DeprecationWarning:pkg_resources",
    "ignore::DeprecationWarning:google.rpc",
]

[tool.pytest-cov]
# Coverage configuration for pytest-cov
source = [
    "api",
    "auth",
    "config",
    "services",
    "middleware",
    "chart_engine",
    "database",
    "cache",
    "models",
]

[tool.coverage.run]
source = [
    "api",
    "auth",
    "config",
    "services",
    "middleware",
    "chart_engine",
    "database",
    "cache",
    "models",
]
omit = [
    "*/test_*",
    "*/tests/*",
    "*/__pycache__/*",
    "*/migrations/*",
    "scripts/*",
    "vanna-skill/*",
    "vanna_docs/*",
    "frontend/*",
]

[tool.coverage.report]
show_missing = true
precision = 1
fail_under = 0
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if TYPE_CHECKING:",
    "if __name__ == .__main__.",
    "raise NotImplementedError",
    "pass",
    "\\.\\.\\.",
]

[tool.coverage.html]
directory = "htmlcov"
</file>

<file path="railway.toml">
[build]
builder = "DOCKERFILE"
dockerfilePath = "Dockerfile"

[deploy]
healthcheckPath = "/health"
healthcheckTimeout = 120
startCommand = "./entrypoint.sh"
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 3
numReplicas = 1
</file>

<file path="services/audit_service.py">
"""
Audit Service
=============
Logging and retrieval for the query_audit_log table. Provides methods to
record every AI query and retrieve usage statistics per user or globally.

Requires a psycopg2 connection factory passed at init.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Data class
⋮----
@dataclass
class AuditEntry
⋮----
"""Mirrors the query_audit_log table in database/schema.sql."""
⋮----
id: str = field(default_factory=lambda: str(uuid.uuid4()))
user_id: Optional[str] = None
natural_language_query: str = ""
generated_sql: Optional[str] = None
execution_time_ms: Optional[int] = None
row_count: Optional[int] = None
was_successful: Optional[bool] = None
error_message: Optional[str] = None
ip_address: Optional[str] = None
user_agent: Optional[str] = None
created_at: Optional[datetime] = None
⋮----
@dataclass
class UsageStats
⋮----
"""Aggregated usage statistics for a time period."""
⋮----
period: str = ""  # e.g. '2026-02-07' or '2026-02'
query_count: int = 0
successful_count: int = 0
failed_count: int = 0
avg_execution_time_ms: Optional[float] = None
unique_users: int = 0
⋮----
# Service
⋮----
class AuditService
⋮----
"""Service layer for the query_audit_log table.

    Parameters
    ----------
    get_conn : callable
        A zero-argument callable that returns a psycopg2 connection.
        The service calls ``conn.close()`` after each operation.
    """
⋮----
def __init__(self, get_conn)
⋮----
# -- helpers -------------------------------------------------------------
⋮----
def _conn(self)
⋮----
@staticmethod
    def _row_to_entry(row: Dict[str, Any]) -> AuditEntry
⋮----
# -- public API ----------------------------------------------------------
⋮----
"""Insert an audit log entry. Returns the entry id."""
entry_id = str(uuid.uuid4())
⋮----
sql = """
⋮----
conn = self._conn()
⋮----
"""Return a user's query history, newest first."""
clauses = ["q.user_id = %(user_id)s"]
params: Dict[str, Any] = {
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
sql = f"""
⋮----
"""Return daily usage stats for the last N days."""
clauses = ["q.created_at >= NOW() - %(interval)s::interval"]
params: Dict[str, Any] = {"interval": f"{days} days"}
⋮----
"""Return monthly usage stats for the last N months."""
⋮----
params: Dict[str, Any] = {"interval": f"{months} months"}
⋮----
"""Return total query count with optional filters."""
clauses: List[str] = []
params: Dict[str, Any] = {}
⋮----
where = ("WHERE " + " AND ".join(clauses)) if clauses else ""
⋮----
sql = f"SELECT COUNT(*) FROM query_audit_log q {where}"
</file>

<file path="services/auth_service.py">
"""
Authentication service for user registration, login, and token management.

Separates database operations and auth logic from route handlers.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
@dataclass
class AuthResult
⋮----
"""Result of an authentication operation."""
⋮----
success: bool
user_id: Optional[str] = None
email: Optional[str] = None
error: Optional[str] = None
error_code: int = 401
⋮----
class AuthService
⋮----
"""Handles user authentication against the PostgreSQL users table."""
⋮----
def __init__(self, get_conn: Callable)
⋮----
"""Register a new user account.

        Returns AuthResult with success=True and tokens on success,
        or success=False with error details on failure.
        """
password_hash = hash_password(password)
⋮----
conn = self._get_conn()
⋮----
user_id = str(cur.fetchone()[0])
⋮----
def login(self, email: str, password: str) -> AuthResult
⋮----
"""Authenticate a user by email and password.

        Returns AuthResult with user_id on success, or error on failure.
        """
⋮----
row = cur.fetchone()
⋮----
def verify_user_active(self, user_id: str) -> AuthResult
⋮----
"""Verify that a user exists and is active."""
⋮----
@staticmethod
    def build_token_claims(user_id: str, email: str) -> Dict[str, Any]
⋮----
"""Build JWT claims for a user."""
⋮----
@staticmethod
    def create_tokens(claims: Dict[str, Any]) -> Dict[str, str]
⋮----
"""Create access and refresh tokens from claims."""
</file>

<file path="services/db_compat.py">
"""
Database Compatibility Layer
=============================
Provides backend-aware query helpers for code that needs to work with
both SQLite and PostgreSQL backends.

Primary consumers:
  - ``services/health_service.py`` (health checks that query entity/market data)

For route-level dual-backend queries, prefer ``api/db_helper.py`` instead.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_HERE = Path(__file__).resolve().parent.parent
_SQLITE_PATH = str(_HERE / "saudi_stocks.db")
⋮----
def is_postgres() -> bool
⋮----
"""Return True when the active backend is PostgreSQL."""
settings = get_settings()
⋮----
def get_read_connection()
⋮----
"""Return a read-only database connection for the active backend.

    * **SQLite**: opens ``saudi_stocks.db`` with ``sqlite3.Row`` factory.
    * **PostgreSQL**: checks the connection pool first, falls back to direct.

    The caller **must** close the connection in a ``finally`` block.
    """
⋮----
# Fallback to direct connection
⋮----
# SQLite
db_path = _SQLITE_PATH
⋮----
db_path = str(settings.db.resolved_sqlite_path)
⋮----
conn = sqlite3.connect(db_path, timeout=5)
⋮----
"""Execute SQL and return all rows as dicts, for either backend."""
⋮----
rows = conn.execute(sql, params or ()).fetchall()
⋮----
"""Execute SQL and return first row as dict, for either backend."""
⋮----
row = cur.fetchone()
⋮----
row = conn.execute(sql, params or ()).fetchone()
⋮----
def scalar_compat(conn, sql: str, params: Optional[tuple] = None) -> Any
⋮----
"""Execute SQL and return a single scalar value, for either backend."""
⋮----
def table_exists(conn, table_name: str) -> bool
⋮----
"""Check if a table exists, for either backend."""
⋮----
sql = (
⋮----
sql = "SELECT 1 FROM sqlite_master WHERE type='table' AND name=?"
row = conn.execute(sql, (table_name,)).fetchone()
⋮----
# Whitelist of allowed column names for datetime filtering
_ALLOWED_DATETIME_COLUMNS = frozenset(
⋮----
# Interval must match pattern like "1 day", "24 hours", "30 minutes"
_INTERVAL_PATTERN = re.compile(
⋮----
def datetime_recent(column: str, interval: str) -> str
⋮----
"""Return a WHERE clause fragment for recent records.

    Args:
        column: Column name - must be in the allowed whitelist.
        interval: Interval string (e.g. '1 day', '24 hours').

    Returns:
        SQL fragment like ``created_at > datetime('now', '-1 day')`` for SQLite
        or ``created_at > NOW() - INTERVAL '1 day'`` for PostgreSQL.

    Raises:
        ValueError: If column or interval fails validation.
    """
⋮----
# SQLite: convert '1 day' -> '-1 day'
</file>

<file path="services/news_paraphraser.py">
"""
Arabic News Paraphraser
========================
Simple Arabic text paraphraser that applies minor modifications to news articles:
- Replaces common financial Arabic terms with synonyms
- Adds slight variations to sentence openings
- Keeps meaning identical -- this is NOT creative rewriting

Usage:
    from services.news_paraphraser import paraphrase_article
    modified = paraphrase_article(article_dict)
"""
⋮----
# ---------------------------------------------------------------------------
# Synonym pairs: (original, replacement)
# Each pair can be applied in either direction.
⋮----
SYNONYM_PAIRS: List[Tuple[str, str]] = [
⋮----
# Build a flat lookup dict: word -> replacement
_SYNONYM_MAP: Dict[str, str] = {}
⋮----
# Sentence opening variations (prepended alternatives)
OPENING_VARIATIONS: List[Tuple[str, str]] = [
⋮----
def _apply_synonyms(text: str) -> str
⋮----
"""Replace financial terms with their synonyms.

    Applies replacements with ~50% probability per occurrence to avoid
    making the text feel mechanically transformed.
    """
⋮----
result = text
⋮----
# Replace only the first occurrence to keep changes minimal
result = result.replace(original, replacement, 1)
⋮----
def _vary_openings(text: str) -> str
⋮----
"""Apply slight variations to sentence openings."""
⋮----
# Only modify if it appears at the start of a sentence
# (beginning of text or after a period/newline)
pattern = rf"(^|[.\n]\s*){re.escape(original)}"
⋮----
result = re.sub(pattern, rf"\1{replacement}", result, count=1)
⋮----
def paraphrase_text(text: str) -> str
⋮----
"""Apply minor paraphrasing to Arabic text.

    Returns the modified text with synonym replacements and
    opening variations applied.
    """
⋮----
result = _apply_synonyms(text)
result = _vary_openings(result)
⋮----
def paraphrase_article(article: dict) -> dict
⋮----
"""Paraphrase the title and body of an article dict.

    Returns a new dict (does not modify the original).
    Handles None/empty body gracefully -- always returns a string for body.
    """
modified = dict(article)
title = article.get("title") or ""
body = article.get("body") or ""
</file>

<file path="services/news_service.py">
"""
News Aggregation Service
========================
CRUD operations for news_articles table. Provides methods to store, retrieve,
and filter news articles by ticker, sector, date range, and sentiment.

Accepts a ``get_conn`` callable at init. When the connection pool is active,
use ``database.pool.get_pool_connection`` as the callable -- pool-returned
connections auto-return to the pool on ``close()``.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Data class
⋮----
@dataclass
class NewsArticle
⋮----
"""Mirrors the news_articles table in database/schema.sql."""
⋮----
id: str = field(default_factory=lambda: str(uuid.uuid4()))
ticker: Optional[str] = None
title: str = ""
body: Optional[str] = None
source_name: Optional[str] = None
source_url: Optional[str] = None
published_at: Optional[datetime] = None
sentiment_score: Optional[float] = None
sentiment_label: Optional[str] = None
entities_extracted: Optional[Dict[str, Any]] = None
language: str = "ar"
created_at: Optional[datetime] = None
⋮----
def to_dict(self) -> Dict[str, Any]
⋮----
"""Convert to dict suitable for database insertion."""
d = asdict(self)
# Remove created_at so the DB default (NOW()) applies
⋮----
# Service
⋮----
class NewsAggregationService
⋮----
"""Service layer for the news_articles table.

    Parameters
    ----------
    get_conn : callable
        A zero-argument callable that returns a psycopg2 connection.
        The service calls ``conn.close()`` after each operation so the
        caller can supply a connection-pool checkout (e.g.
        ``pool.getconn``).
    """
⋮----
def __init__(self, get_conn)
⋮----
# -- helpers -------------------------------------------------------------
⋮----
def _conn(self)
⋮----
@staticmethod
    def _row_to_article(row: Dict[str, Any]) -> NewsArticle
⋮----
# -- public API ----------------------------------------------------------
⋮----
def store_articles(self, articles: List[NewsArticle]) -> int
⋮----
"""Insert one or more articles. Returns the number of rows inserted.

        Duplicates (same id) are silently skipped via ON CONFLICT DO NOTHING.
        """
⋮----
sql = """
⋮----
conn = self._conn()
⋮----
# Use Json adapter for JSONB column
rows = []
⋮----
d = a.to_dict()
⋮----
"""Return the most recent articles across all tickers."""
clauses: List[str] = []
params: Dict[str, Any] = {"limit": limit, "offset": offset}
⋮----
where = ("WHERE " + " AND ".join(clauses)) if clauses else ""
⋮----
sql = f"""
⋮----
"""Return articles for a specific ticker, newest first."""
clauses = ["n.ticker = %(ticker)s"]
params: Dict[str, Any] = {
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
"""Return articles for all companies in a given sector, newest first.

        Joins news_articles to companies on ticker to filter by sector.
        """
clauses = ["c.sector ILIKE %(sector)s"]
⋮----
def get_article_by_id(self, article_id: str) -> Optional[NewsArticle]
⋮----
"""Return a single article by its UUID, or None if not found."""
sql = "SELECT * FROM news_articles WHERE id = %(id)s"
⋮----
row = cur.fetchone()
⋮----
"""Return total article count with optional ticker/sector filter."""
⋮----
params: Dict[str, Any] = {}
join_companies = False
⋮----
join_companies = True
⋮----
join = "JOIN companies c ON c.ticker = n.ticker" if join_companies else ""
⋮----
sql = f"SELECT COUNT(*) FROM news_articles n {join} {where}"
</file>

<file path="tests/integration/test_health.py">
"""
Integration Tests: Health Endpoints
=====================================
Tests for /health, /health/live, /health/ready endpoints.

Covers:
  - /health returns structured report with status, version, components
  - /health/live returns alive status without checking dependencies
  - /health/ready returns ready when DB is reachable
  - /health/ready returns 503 when DB is down
  - Health endpoints do not require authentication
  - Health response does not leak internal implementation details

Uses FastAPI TestClient with the health router + mocked health service.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
@pytest.fixture(scope="module")
def health_app()
⋮----
"""Create a minimal FastAPI app with the health router."""
⋮----
app = FastAPI()
⋮----
@pytest.fixture(scope="module")
def client(health_app)
⋮----
@pytest.fixture
def mock_healthy_report()
⋮----
"""Mock a fully healthy HealthReport."""
⋮----
report = HealthReport(
⋮----
@pytest.fixture
def mock_unhealthy_report()
⋮----
"""Mock an unhealthy HealthReport (DB down)."""
⋮----
# ===========================================================================
# /health endpoint
⋮----
class TestHealthEndpoint
⋮----
"""Test GET /health full health report."""
⋮----
@pytest.mark.integration
    def test_health_returns_200_when_healthy(self, client, mock_healthy_report)
⋮----
resp = client.get("/health")
⋮----
@pytest.mark.integration
    def test_health_response_has_required_fields(self, client, mock_healthy_report)
⋮----
body = resp.json()
⋮----
@pytest.mark.integration
    def test_health_returns_503_when_unhealthy(self, client, mock_unhealthy_report)
⋮----
@pytest.mark.integration
    def test_health_components_list_populated(self, client, mock_healthy_report)
⋮----
comp = body["components"][0]
⋮----
# /health/live endpoint
⋮----
class TestLivenessEndpoint
⋮----
"""Test GET /health/live liveness probe."""
⋮----
@pytest.mark.integration
    def test_live_returns_200(self, client)
⋮----
resp = client.get("/health/live")
⋮----
@pytest.mark.integration
    def test_live_returns_alive_status(self, client)
⋮----
# /health/ready endpoint
⋮----
class TestReadinessEndpoint
⋮----
"""Test GET /health/ready readiness probe."""
⋮----
@pytest.mark.integration
    def test_ready_returns_200_when_db_healthy(self, client)
⋮----
healthy_db = ComponentHealth(
⋮----
resp = client.get("/health/ready")
⋮----
@pytest.mark.integration
    def test_ready_returns_503_when_db_unhealthy(self, client)
⋮----
unhealthy_db = ComponentHealth(
⋮----
# No auth required
⋮----
class TestHealthNoAuth
⋮----
"""Test that health endpoints do not require authentication."""
⋮----
@pytest.mark.integration
    def test_health_accessible_without_token(self, client)
⋮----
# Should not be 401 or 403
⋮----
@pytest.mark.integration
    def test_live_accessible_without_token(self, client)
⋮----
@pytest.mark.integration
    def test_ready_accessible_without_token(self, client)
</file>

<file path="tests/integration/test_query_flow.py">
"""
Integration Tests: Query Lifecycle Flow
========================================
Tests the full NL -> validation -> SQL -> execution pipeline.

Covers:
  - Valid SELECT queries pass validation and execute
  - SQL injection attempts are blocked pre-execution
  - Read-only enforcement (no mutations allowed)
  - Table extraction from complex queries
  - Risk score calculation for borderline queries
  - Sanitized SQL output for valid queries
  - Multi-table join query flow
  - Parameterized-style queries

Uses SqlQueryValidator and test SQLite database from conftest.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
from backend.security.sql_validator import SqlQueryValidator  # noqa: E402
⋮----
@pytest.fixture
def validator()
⋮----
# ===========================================================================
# Valid query lifecycle (NL -> validation -> execution)
⋮----
class TestValidQueryFlow
⋮----
"""Test that valid queries pass validation and can be executed."""
⋮----
def test_simple_select_validates_and_executes(self, validator, test_db)
⋮----
sql = "SELECT ticker, short_name FROM companies"
result = validator.validate(sql)
⋮----
# Execute against test DB
rows = test_db["cursor"].execute(result.sanitized_sql).fetchall()
⋮----
tickers = [r[0] for r in rows]
⋮----
def test_where_clause_validates_and_executes(self, validator, test_db)
⋮----
sql = "SELECT ticker, current_price FROM market_data WHERE current_price > 50"
⋮----
def test_join_validates_and_executes(self, validator, test_db)
⋮----
sql = (
⋮----
# Ordered by price DESC: RIBL (80) first
⋮----
def test_aggregation_validates_and_executes(self, validator, test_db)
⋮----
sql = "SELECT sector, COUNT(*) as cnt FROM companies GROUP BY sector"
⋮----
sectors = {r[0] for r in rows}
⋮----
def test_subquery_validates_and_executes(self, validator, test_db)
⋮----
def test_limit_validates_and_executes(self, validator, test_db)
⋮----
sql = "SELECT ticker FROM companies LIMIT 1"
⋮----
def test_financial_statement_query(self, validator, test_db)
⋮----
# Aramco has higher net_income
⋮----
def test_multi_join_validates_and_executes(self, validator, test_db)
⋮----
# Injection blocked pre-execution
⋮----
class TestInjectionBlockedPreExecution
⋮----
"""Test that SQL injection is blocked before reaching the database."""
⋮----
def test_drop_blocked_before_execution(self, validator, test_db)
⋮----
sql = "DROP TABLE companies"
⋮----
# Verify table still exists
count = (
⋮----
def test_stacked_delete_blocked(self, validator, test_db)
⋮----
sql = "SELECT * FROM companies; DELETE FROM companies"
⋮----
# Verify no rows were deleted
⋮----
def test_union_schema_probe_blocked(self, validator)
⋮----
sql = "SELECT ticker FROM companies UNION ALL SELECT name FROM sqlite_master"
⋮----
def test_insert_blocked(self, validator, test_db)
⋮----
sql = "INSERT INTO companies (ticker, short_name) VALUES ('EVIL', 'Evil Corp')"
⋮----
# Verify no new row
⋮----
# Read-only enforcement
⋮----
class TestReadOnlyEnforcement
⋮----
"""Test that only read operations are allowed."""
⋮----
def test_mutation_blocked(self, validator, operation)
⋮----
def test_select_allowed(self, validator, operation)
⋮----
# Table extraction
⋮----
class TestTableExtraction
⋮----
"""Test table name extraction from various query patterns."""
⋮----
def test_single_table(self, validator)
⋮----
tables = validator.extract_tables("SELECT * FROM companies")
⋮----
def test_aliased_table(self, validator)
⋮----
tables = validator.extract_tables("SELECT * FROM companies c")
⋮----
def test_multiple_joins(self, validator)
⋮----
tables = validator.extract_tables(
⋮----
def test_subquery_table(self, validator)
⋮----
# Risk score calculation
⋮----
class TestRiskScoreCalculation
⋮----
"""Test risk score assignment for different query types."""
⋮----
def test_safe_query_zero_risk(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies")
⋮----
def test_forbidden_op_high_risk(self, validator)
⋮----
result = validator.validate("DROP TABLE companies")
⋮----
def test_stacked_query_high_risk(self, validator)
⋮----
result = validator.validate("SELECT 1; DROP TABLE companies")
⋮----
def test_injection_pattern_medium_risk(self, validator)
⋮----
result = validator.validate(
</file>

<file path="tests/security/test_sql_injection.py">
"""
SQL Injection Prevention Tests
===============================
Tests for SqlQueryValidator OWASP injection pattern detection.

Covers:
  - Classic SQL injection (tautologies, UNION-based, stacked queries)
  - Blind/time-based injection (SLEEP, BENCHMARK, WAITFOR, PG_SLEEP)
  - Comment-based obfuscation (inline, block, nested)
  - DDL/DML mutation attempts (DROP, INSERT, UPDATE, DELETE, ALTER, TRUNCATE)
  - Schema probing (sqlite_master, information_schema, pg_catalog)
  - Encoding tricks (hex, CHAR(), CONCAT())
  - File access attempts (LOAD_FILE, INTO OUTFILE, INTO DUMPFILE)
  - Valid SELECT queries pass validation

Uses SqlQueryValidator from backend.security.sql_validator.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
⋮----
from backend.security.sql_validator import SqlQueryValidator  # noqa: E402
⋮----
@pytest.fixture
def validator()
⋮----
# ===========================================================================
# Valid queries that SHOULD pass
⋮----
class TestValidQueries
⋮----
"""Ensure safe SELECT queries are accepted."""
⋮----
def test_simple_select(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies")
⋮----
def test_select_with_where(self, validator)
⋮----
result = validator.validate(
⋮----
def test_select_with_join(self, validator)
⋮----
def test_select_with_aggregation(self, validator)
⋮----
def test_select_with_subquery(self, validator)
⋮----
def test_select_with_limit(self, validator)
⋮----
def test_select_count(self, validator)
⋮----
result = validator.validate("SELECT COUNT(*) FROM companies")
⋮----
# Classic SQL injection (tautologies, UNION-based)
⋮----
class TestClassicInjection
⋮----
"""Test classic SQL injection patterns."""
⋮----
def test_tautology_or_1_equals_1(self, validator)
⋮----
# Should detect the comment containing no dangerous keyword, but the query
# itself is structurally a valid SELECT. The tautology is logic-level, not
# syntactically forbidden. Validator should still accept pure SELECTs.
# However, if the comment contains dangerous keywords, that's a violation.
# A bare "--" with no dangerous keyword is OK.
⋮----
def test_union_select_schema_probe(self, validator)
⋮----
def test_union_select_information_schema(self, validator)
⋮----
def test_union_with_null_padding(self, validator)
⋮----
# Stacked queries (multiple statements)
⋮----
class TestStackedQueries
⋮----
"""Test multiple statement (stacked query) detection."""
⋮----
def test_stacked_select_drop(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies; DROP TABLE companies")
⋮----
def test_stacked_select_insert(self, validator)
⋮----
def test_stacked_select_update(self, validator)
⋮----
def test_stacked_select_delete(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies; DELETE FROM companies")
⋮----
# DDL/DML mutation attempts
⋮----
class TestForbiddenOperations
⋮----
"""Test detection of write/DDL operations."""
⋮----
def test_drop_table(self, validator)
⋮----
result = validator.validate("DROP TABLE companies")
⋮----
def test_alter_table(self, validator)
⋮----
result = validator.validate("ALTER TABLE companies ADD COLUMN hacked TEXT")
⋮----
def test_insert_into(self, validator)
⋮----
def test_update_set(self, validator)
⋮----
def test_delete_from(self, validator)
⋮----
result = validator.validate("DELETE FROM companies WHERE ticker = '2222.SR'")
⋮----
def test_truncate_table(self, validator)
⋮----
result = validator.validate("TRUNCATE TABLE companies")
⋮----
def test_create_table(self, validator)
⋮----
result = validator.validate("CREATE TABLE evil (id INTEGER PRIMARY KEY)")
⋮----
def test_grant_permissions(self, validator)
⋮----
result = validator.validate("GRANT ALL PRIVILEGES ON companies TO attacker")
⋮----
def test_revoke_permissions(self, validator)
⋮----
result = validator.validate("REVOKE ALL PRIVILEGES ON companies FROM admin")
⋮----
# Time-based injection (blind injection)
⋮----
class TestTimeBasedInjection
⋮----
"""Test time-based blind injection pattern detection."""
⋮----
def test_sleep_injection(self, validator)
⋮----
def test_benchmark_injection(self, validator)
⋮----
def test_waitfor_injection(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies; WAITFOR DELAY '00:00:05'")
⋮----
def test_pg_sleep_injection(self, validator)
⋮----
# Comment-based obfuscation
⋮----
class TestCommentObfuscation
⋮----
"""Test SQL keyword hiding inside comments."""
⋮----
def test_inline_comment_with_drop(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies -- DROP TABLE companies")
⋮----
def test_block_comment_with_delete(self, validator)
⋮----
def test_semicolon_in_comment_with_insert(self, validator)
⋮----
# Encoding tricks (hex, CHAR, CONCAT)
⋮----
class TestEncodingTricks
⋮----
"""Test detection of encoding-based obfuscation."""
⋮----
def test_hex_encoded_payload(self, validator)
⋮----
result = validator.validate("SELECT * FROM companies WHERE ticker = 0x41424344")
⋮----
def test_char_obfuscation(self, validator)
⋮----
def test_concat_trick(self, validator)
⋮----
# File access injection
⋮----
class TestFileAccessInjection
⋮----
"""Test detection of file access injection attempts."""
⋮----
def test_load_file(self, validator)
⋮----
result = validator.validate("SELECT LOAD_FILE('/etc/passwd')")
⋮----
def test_into_outfile(self, validator)
⋮----
def test_into_dumpfile(self, validator)
⋮----
# Schema probing
⋮----
class TestSchemaProbing
⋮----
"""Test detection of schema metadata probing."""
⋮----
def test_sqlite_master(self, validator)
⋮----
result = validator.validate("SELECT * FROM sqlite_master")
⋮----
def test_pg_catalog(self, validator)
⋮----
result = validator.validate("SELECT * FROM pg_catalog.pg_tables")
⋮----
def test_pg_tables(self, validator)
⋮----
result = validator.validate("SELECT tablename FROM pg_tables")
⋮----
def test_information_schema_tables(self, validator)
⋮----
result = validator.validate("SELECT table_name FROM information_schema.tables")
⋮----
# SQLite-specific attacks
⋮----
class TestSQLiteSpecific
⋮----
"""Test SQLite-specific injection patterns."""
⋮----
def test_attach_database(self, validator)
⋮----
result = validator.validate("ATTACH DATABASE '/tmp/evil.db' AS evil")
⋮----
def test_pragma_table_info(self, validator)
⋮----
# sqlparse tokenizes PRAGMA as Token.Name, not Token.Keyword,
# so the token-based check misses it. The raw-text scan should
# catch it if PRAGMA is in FORBIDDEN_OPERATIONS.
result = validator.validate("PRAGMA table_info(companies)")
# PRAGMA may not be caught by sqlparse token analysis; check
# that the raw text scan in contains_forbidden_operations works.
ops = validator.contains_forbidden_operations("PRAGMA table_info(companies)")
⋮----
# Known gap: sqlparse doesn't classify PRAGMA as a keyword.
# The validator should ideally add a raw-text check for PRAGMA.
⋮----
def test_analyze(self, validator)
⋮----
result = validator.validate("ANALYZE companies")
⋮----
# Edge cases
⋮----
class TestEdgeCases
⋮----
"""Test edge cases and boundary conditions."""
⋮----
def test_empty_query(self, validator)
⋮----
result = validator.validate("")
⋮----
def test_whitespace_only_query(self, validator)
⋮----
result = validator.validate("   \n\t  ")
⋮----
def test_is_read_only_for_select(self, validator)
⋮----
def test_is_read_only_for_insert(self, validator)
⋮----
def test_is_read_only_for_drop(self, validator)
⋮----
def test_extract_tables_from_join(self, validator)
⋮----
tables = validator.extract_tables(
⋮----
def test_risk_score_capped_at_1(self, validator)
⋮----
# Query with many violations should still cap at 1.0
</file>

<file path="tests/test_api_routes.py">
"""
API Routes Tests
================
Tests for all FastAPI API route modules.

Unit tests (always run):
  - Pydantic response model construction and validation
  - Route module imports and router configuration
  - Endpoint registration (path, methods, response_model)

Integration tests (require PostgreSQL):
  - Skipped when POSTGRES_HOST is not set or PG is unreachable
  - Full HTTP request/response cycle via FastAPI TestClient
"""
⋮----
# ---------------------------------------------------------------------------
# PostgreSQL availability check
⋮----
def _pg_available() -> bool
⋮----
conn = psycopg2.connect(
⋮----
PG_AVAILABLE = _pg_available()
⋮----
# ===========================================================================
# Pydantic response model tests (always run, no DB required)
⋮----
class TestNewsPydanticModels(unittest.TestCase)
⋮----
"""Test news route Pydantic response models."""
⋮----
def test_news_article_response(self)
⋮----
resp = NewsArticleResponse(
⋮----
def test_news_list_response(self)
⋮----
resp = NewsListResponse(
⋮----
class TestReportsPydanticModels(unittest.TestCase)
⋮----
"""Test reports route Pydantic response models."""
⋮----
def test_report_response(self)
⋮----
resp = ReportResponse(
⋮----
def test_report_list_response(self)
⋮----
resp = ReportListResponse(items=[], count=0)
⋮----
class TestAnnouncementsPydanticModels(unittest.TestCase)
⋮----
"""Test announcements route Pydantic response models."""
⋮----
def test_announcement_response(self)
⋮----
resp = AnnouncementResponse(
⋮----
def test_announcement_list_response(self)
⋮----
resp = AnnouncementListResponse(items=[], count=0)
⋮----
class TestEntitiesPydanticModels(unittest.TestCase)
⋮----
"""Test entities route Pydantic response models."""
⋮----
def test_company_summary(self)
⋮----
resp = CompanySummary(
⋮----
def test_company_detail(self)
⋮----
resp = CompanyDetail(
⋮----
def test_entity_list_response(self)
⋮----
resp = EntityListResponse(
⋮----
def test_sector_info(self)
⋮----
resp = SectorInfo(sector="Energy", company_count=15)
⋮----
class TestWatchlistPydanticModels(unittest.TestCase)
⋮----
"""Test watchlist route Pydantic models."""
⋮----
def test_watchlist_response(self)
⋮----
resp = WatchlistResponse(
⋮----
def test_watchlist_create_request(self)
⋮----
req = WatchlistCreateRequest()
⋮----
def test_alert_response(self)
⋮----
resp = AlertResponse(
⋮----
class TestChartsPydanticModels(unittest.TestCase)
⋮----
"""Test charts route Pydantic models."""
⋮----
def test_chart_data_point(self)
⋮----
dp = ChartDataPoint(label="Energy", value=5000000.0)
⋮----
def test_chart_response(self)
⋮----
resp = ChartResponse(
⋮----
# Router configuration tests (always run)
⋮----
class TestRouterConfiguration(unittest.TestCase)
⋮----
"""Verify each route module exports a properly configured router."""
⋮----
def test_news_router(self)
⋮----
def test_reports_router(self)
⋮----
def test_announcements_router(self)
⋮----
def test_entities_router(self)
⋮----
def test_watchlists_router(self)
⋮----
def test_charts_router(self)
⋮----
def test_health_router(self)
⋮----
class TestRouteEndpoints(unittest.TestCase)
⋮----
"""Verify expected endpoints are registered on each router."""
⋮----
def _route_paths(self, router)
⋮----
"""Extract route paths from a router."""
⋮----
def test_news_routes(self)
⋮----
paths = self._route_paths(router)
⋮----
def test_reports_routes(self)
⋮----
def test_announcements_routes(self)
⋮----
def test_entities_routes(self)
⋮----
def test_watchlists_routes(self)
⋮----
def test_charts_routes(self)
⋮----
def test_health_route(self)
⋮----
# Integration tests with TestClient (require PostgreSQL)
⋮----
@unittest.skipUnless(PG_AVAILABLE, "PostgreSQL not available (set POSTGRES_HOST)")
class TestAPIRoutesWithTestClient(unittest.TestCase)
⋮----
"""Integration tests using FastAPI TestClient against live PostgreSQL.

    These test the full request/response cycle including dependency injection,
    database queries, and response serialization.
    """
⋮----
@classmethod
    def setUpClass(cls)
⋮----
# Set DB_BACKEND to postgres before importing app
⋮----
def test_health_endpoint(self)
⋮----
resp = self.client.get("/health")
⋮----
data = resp.json()
⋮----
def test_entities_list(self)
⋮----
resp = self.client.get("/api/entities?limit=5")
⋮----
def test_entities_sectors(self)
⋮----
resp = self.client.get("/api/entities/sectors")
⋮----
def test_entities_detail(self)
⋮----
resp = self.client.get("/api/entities/2222.SR")
⋮----
def test_entities_detail_not_found(self)
⋮----
resp = self.client.get("/api/entities/XXXX.SR")
⋮----
def test_news_list(self)
⋮----
resp = self.client.get("/api/news?limit=5")
⋮----
def test_reports_list(self)
⋮----
resp = self.client.get("/api/reports?limit=5")
⋮----
def test_announcements_list(self)
⋮----
resp = self.client.get("/api/announcements?limit=5")
⋮----
def test_charts_sector_market_cap(self)
⋮----
resp = self.client.get("/api/charts/sector-market-cap")
⋮----
def test_charts_top_companies(self)
⋮----
resp = self.client.get("/api/charts/top-companies?limit=5")
</file>

<file path="tests/test_auth.py">
"""
Authentication Module Tests
============================
Tests for JWT handling, password hashing, auth models, and auth dependencies.

All tests run without a database or external services.
"""
⋮----
# Ensure project root is on sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ===========================================================================
# Password hashing tests
⋮----
class TestPasswordHashing
⋮----
"""Tests for auth.password module."""
⋮----
def test_hash_password_returns_string(self)
⋮----
result = hash_password("mysecretpassword")
⋮----
def test_hash_password_not_plaintext(self)
⋮----
password = "mysecretpassword"
hashed = hash_password(password)
⋮----
def test_hash_password_unique_salts(self)
⋮----
h1 = hash_password("samepassword")
h2 = hash_password("samepassword")
⋮----
def test_verify_password_correct(self)
⋮----
password = "correcthorsebatterystaple"
⋮----
def test_verify_password_wrong(self)
⋮----
hashed = hash_password("realpassword")
⋮----
def test_verify_password_empty_string(self)
⋮----
hashed = hash_password("")
⋮----
def test_hash_password_unicode(self)
⋮----
password = "كلمة_سر_عربية"
⋮----
# JWT token tests
⋮----
class TestJWTTokenCreation
⋮----
"""Tests for auth.jwt_handler token creation and verification."""
⋮----
@pytest.fixture(autouse=True)
    def _mock_auth_settings(self, auth_settings)
⋮----
"""Patch _get_auth_settings in jwt_handler for all tests in this class."""
⋮----
def test_create_access_token_returns_string(self)
⋮----
token = create_access_token({"sub": "user-1", "email": "user@test.com"})
⋮----
def test_create_access_token_contains_claims(self)
⋮----
payload = jwt.decode(
⋮----
def test_create_access_token_has_expiration(self)
⋮----
token = create_access_token({"sub": "user-1"})
⋮----
def test_create_refresh_token_returns_string(self)
⋮----
token = create_refresh_token({"sub": "user-1"})
⋮----
def test_create_refresh_token_type_is_refresh(self)
⋮----
def test_access_and_refresh_tokens_differ(self)
⋮----
data = {"sub": "user-1"}
access = create_access_token(data)
refresh = create_refresh_token(data)
⋮----
def test_decode_valid_access_token(self)
⋮----
token = create_access_token({"sub": "user-1", "email": "a@b.com"})
payload = decode_token(token, expected_type="access")
⋮----
def test_decode_valid_refresh_token(self)
⋮----
payload = decode_token(token, expected_type="refresh")
⋮----
def test_decode_token_wrong_type_raises_value_error(self)
⋮----
def test_decode_token_no_type_check(self)
⋮----
payload = decode_token(token)  # no expected_type
⋮----
def test_decode_expired_token_raises(self)
⋮----
expired_payload = {
expired_token = jwt.encode(
⋮----
def test_decode_token_with_invalid_secret_raises(self)
⋮----
payload = {
token = jwt.encode(payload, "wrong-secret", algorithm="HS256")
⋮----
def test_decode_malformed_token_raises(self)
⋮----
def test_token_preserves_custom_claims(self)
⋮----
token = create_access_token(
payload = decode_token(token)
⋮----
# Auth models tests
⋮----
class TestAuthModels
⋮----
"""Tests for auth.models Pydantic models."""
⋮----
def test_user_create_valid(self)
⋮----
user = UserCreate(email="test@example.com", password="securepass123")
⋮----
def test_user_create_with_display_name(self)
⋮----
user = UserCreate(
⋮----
def test_user_create_invalid_email(self)
⋮----
with pytest.raises(Exception):  # ValidationError
⋮----
def test_user_create_password_too_short(self)
⋮----
def test_user_create_password_too_long(self)
⋮----
def test_user_create_password_max_length(self)
⋮----
user = UserCreate(email="test@example.com", password="x" * 128)
⋮----
def test_user_login_valid(self)
⋮----
login = UserLogin(email="test@example.com", password="anypass")
⋮----
def test_token_response_default_type(self)
⋮----
resp = TokenResponse(access_token="abc", refresh_token="def")
⋮----
def test_token_refresh_request(self)
⋮----
req = TokenRefreshRequest(refresh_token="some-refresh-token")
⋮----
def test_user_profile(self)
⋮----
profile = UserProfile(
⋮----
def test_user_profile_with_all_fields(self)
⋮----
# Auth dependencies tests
⋮----
class TestAuthDependencies
⋮----
"""Tests for auth.dependencies (get_current_user, require_admin)."""
⋮----
def _make_credentials(self, token)
⋮----
creds = MagicMock()
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_with_valid_token(self)
⋮----
token = create_access_token({"sub": "user-42", "email": "u@t.com"})
creds = self._make_credentials(token)
⋮----
fake_row = (
⋮----
user = await get_current_user(creds)
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_expired_token_raises_401(self)
⋮----
creds = self._make_credentials(expired_token)
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_invalid_token_raises_401(self)
⋮----
creds = self._make_credentials("totally-invalid-token")
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_missing_sub_raises_401(self)
⋮----
# Token without "sub" claim
⋮----
token = jwt.encode(
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_deactivated_account_raises_401(self)
⋮----
token = create_access_token({"sub": "user-99"})
⋮----
@pytest.mark.asyncio
    async def test_get_current_user_not_found_raises_401(self)
⋮----
token = create_access_token({"sub": "nonexistent"})
⋮----
def test_require_admin_with_enterprise_user(self)
⋮----
user = {"subscription_tier": "enterprise", "id": "admin-1"}
result = require_admin(user)
⋮----
def test_require_admin_with_free_user_raises_403(self)
⋮----
user = {"subscription_tier": "free", "id": "user-1"}
</file>

<file path="tests/test_connection_pool.py">
"""
Connection Pool Tests
=====================
Tests for database.pool module (PostgreSQL connection pooling).

All tests use mocked psycopg2 -- no real database required.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
@pytest.fixture(autouse=True)
def reset_pool()
⋮----
"""Reset the pool module's global state before each test."""
⋮----
class TestInitPool
⋮----
"""Tests for pool initialization."""
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_init_pool_creates_pool(self, mock_pool_cls)
⋮----
mock_settings = MagicMock()
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_init_pool_skips_if_already_initialized(self, mock_pool_cls)
⋮----
# First call
⋮----
# Second call should skip
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_init_pool_failure_leaves_pool_none(self, mock_pool_cls)
⋮----
class TestGetConnection
⋮----
"""Tests for the get_connection context manager."""
⋮----
def test_get_connection_raises_without_init(self)
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_get_connection_returns_and_commits(self, mock_pool_cls)
⋮----
mock_pool_instance = MagicMock()
mock_conn = MagicMock()
⋮----
# Verify commit was called on clean exit
⋮----
# Verify connection was returned to pool (with unique key)
⋮----
call_args = mock_pool_instance.putconn.call_args
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_get_connection_rollback_on_exception(self, mock_pool_cls)
⋮----
# Verify rollback was called (not commit)
⋮----
# Connection should still be returned to pool (with unique key)
⋮----
class TestGetPoolConnection
⋮----
"""Tests for get_pool_connection (raw connection)."""
⋮----
def test_get_pool_connection_raises_without_init(self)
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_get_pool_connection_wraps_close(self, mock_pool_cls)
⋮----
_original_close = mock_conn.close
⋮----
conn = get_pool_connection()
⋮----
# close() on wrapper should return to pool
⋮----
# Should return to pool instead of truly closing (with unique key)
⋮----
class TestClosePool
⋮----
"""Tests for close_pool."""
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_close_pool(self, mock_pool_cls)
⋮----
def test_close_pool_when_not_initialized(self)
⋮----
# Should not raise
⋮----
@patch("database.pool.ThreadedConnectionPool")
    def test_close_pool_handles_exception(self, mock_pool_cls)
⋮----
# Should not raise even if closeall fails
</file>

<file path="tests/test_news_scraper.py">
"""
News Scraper Tests
===================
Tests for services/news_scraper.py and services/news_paraphraser.py.
Covers scraper instantiation, priorities, relevance filtering,
deduplication, paraphrasing, and error handling.
"""
⋮----
# Ensure project root is on sys.path
⋮----
# -----------------------------------------------------------------------
# Scraper instantiation and priority tests
⋮----
class TestScraperInstantiation(unittest.TestCase)
⋮----
"""Each scraper can be instantiated and has correct metadata."""
⋮----
def test_alarabiya_source_name(self)
⋮----
s = AlarabiyaScraper()
⋮----
def test_alarabiya_priority(self)
⋮----
def test_asharq_source_name(self)
⋮----
s = AsharqBusinessScraper()
⋮----
def test_asharq_priority(self)
⋮----
def test_argaam_source_name(self)
⋮----
s = ArgaamScraper()
⋮----
def test_argaam_priority(self)
⋮----
def test_maaal_source_name(self)
⋮----
s = MaaalScraper()
⋮----
def test_maaal_priority(self)
⋮----
def test_mubasher_source_name(self)
⋮----
s = MubasherScraper()
⋮----
def test_mubasher_priority(self)
⋮----
def test_all_scrapers_has_five_entries(self)
⋮----
def test_priorities_are_ascending_1_to_5(self)
⋮----
priorities = [cls().priority for cls in ALL_SCRAPERS]
⋮----
def test_all_scrapers_have_source_url(self)
⋮----
s = cls()
⋮----
# Article dict and relevance
⋮----
class TestArticleStructure(unittest.TestCase)
⋮----
"""Articles produced by _make_article have required keys."""
⋮----
def _make(self)
⋮----
def test_required_keys_present(self)
⋮----
a = self._make()
⋮----
def test_language_is_ar(self)
⋮----
def test_priority_matches_scraper(self)
⋮----
class TestRelevanceFilter(unittest.TestCase)
⋮----
"""_is_relevant detects Saudi market keywords."""
⋮----
def test_relevant_article_with_keyword(self)
⋮----
article = {"title": "ارتفاع مؤشر تاسي بنسبة 1.5%", "body": ""}
⋮----
def test_irrelevant_article(self)
⋮----
article = {"title": "Weather forecast for London", "body": "Some body text"}
⋮----
def test_relevant_keyword_in_body_only(self)
⋮----
article = {"title": "Breaking news", "body": "الأسهم السعودية ترتفع"}
⋮----
# Deduplication
⋮----
class TestDeduplication(unittest.TestCase)
⋮----
"""_deduplicate removes near-duplicate titles, keeping highest priority."""
⋮----
def test_exact_duplicates_keep_higher_priority(self)
⋮----
articles = [
result = _deduplicate(articles)
⋮----
def test_different_titles_kept(self)
⋮----
def test_empty_list(self)
⋮----
result = _deduplicate([])
⋮----
# Error handling: mocked HTTP failures
⋮----
class TestScraperErrorHandling(unittest.TestCase)
⋮----
"""fetch_articles returns [] on network errors (never raises)."""
⋮----
@patch("services.news_scraper.requests.Session.get")
    def test_timeout_returns_empty(self, mock_get)
⋮----
result = s.fetch_articles()
⋮----
@patch("services.news_scraper.requests.Session.get")
    def test_connection_error_returns_empty(self, mock_get)
⋮----
@patch("services.news_scraper.requests.Session.get")
    def test_http_error_returns_empty(self, mock_get)
⋮----
resp = MagicMock()
⋮----
# fetch_all_news integration (mocked scrapers)
⋮----
class TestFetchAllNews(unittest.TestCase)
⋮----
"""fetch_all_news aggregates, paraphrases, deduplicates, and sorts."""
⋮----
@patch("services.news_scraper.time.sleep")
    def test_returns_list(self, mock_sleep)
⋮----
# Patch all scrapers to return canned data
fake_article = {
⋮----
result = fetch_all_news()
⋮----
@patch("services.news_scraper.time.sleep")
    def test_articles_sorted_by_priority(self, mock_sleep)
⋮----
a1 = {
a2 = {
⋮----
# Rate limiting delay
⋮----
class TestRateLimiting(unittest.TestCase)
⋮----
"""INTER_REQUEST_DELAY exists and is positive."""
⋮----
def test_delay_constant_exists(self)
⋮----
def test_delay_is_positive(self)
⋮----
# Paraphraser tests
⋮----
class TestParaphraser(unittest.TestCase)
⋮----
"""Paraphraser applies synonyms without changing meaning."""
⋮----
def test_empty_string(self)
⋮----
def test_none_returns_none(self)
⋮----
# paraphrase_text checks `if not text: return text`
⋮----
def test_synonym_pairs_not_empty(self)
⋮----
def test_apply_synonyms_with_known_word(self)
⋮----
"""Seeded random so replacement always fires."""
⋮----
text = "ارتفع المؤشر"
result = _apply_synonyms(text)
# With seed(0), random.random() < 0.5 may or may not trigger;
# either original or replacement is acceptable
⋮----
def test_output_similar_length(self)
⋮----
text = "أعلنت أرامكو عن أرباح كبيرة في قطاع البتروكيماويات"
result = paraphrase_text(text)
# Output should not be drastically different in length
ratio = len(result) / len(text) if len(text) > 0 else 1
⋮----
def test_paraphrase_article_returns_new_dict(self)
⋮----
article = {"title": "عنوان", "body": "نص", "extra": "value"}
result = paraphrase_article(article)
⋮----
def test_paraphrase_article_with_empty_title(self)
⋮----
article = {"title": "", "body": "نص المقال"}
⋮----
# URL helpers
⋮----
class TestAbsoluteUrl(unittest.TestCase)
⋮----
"""_absolute_url correctly builds full URLs."""
⋮----
def test_already_absolute(self)
⋮----
result = BaseNewsScraper._absolute_url(
⋮----
def test_protocol_relative(self)
⋮----
def test_root_relative(self)
⋮----
def test_path_relative(self)
⋮----
def test_empty_href(self)
⋮----
result = BaseNewsScraper._absolute_url("https://example.com", "")
</file>

<file path="tests/test_ui_enhancements.py">
"""Tests for Ra'd AI UI enhancements: chart engine, CSS, JS, WCAG, HTML integration, static serving.

Run with:
    python -m pytest tests/test_ui_enhancements.py -v
"""
⋮----
# Ensure project root is on the path so we can import chart_engine
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ---------------------------------------------------------------------------
# Fixtures
⋮----
TEMPLATES_DIR = PROJECT_ROOT / "templates"
⋮----
@pytest.fixture(scope="module")
def generator()
⋮----
@pytest.fixture(scope="module")
def index_html_content()
⋮----
"""Read index.html once for all HTML integration tests."""
html_path = TEMPLATES_DIR / "index.html"
⋮----
@pytest.fixture(scope="module")
def css_content()
⋮----
css_path = TEMPLATES_DIR / "raid-enhancements.css"
⋮----
@pytest.fixture(scope="module")
def js_content()
⋮----
js_path = TEMPLATES_DIR / "raid-features.js"
⋮----
# WCAG Helpers
⋮----
def _relative_luminance(hex_color: str) -> float
⋮----
"""Calculate relative luminance per WCAG 2.0."""
r = int(hex_color[1:3], 16) / 255
g = int(hex_color[3:5], 16) / 255
b = int(hex_color[5:7], 16) / 255
⋮----
def linearize(c)
⋮----
def _contrast_ratio(fg: str, bg: str) -> float
⋮----
l1 = _relative_luminance(fg)
l2 = _relative_luminance(bg)
lighter = max(l1, l2)
darker = min(l1, l2)
⋮----
# ===================================================================
# 1. Chart Engine Tests
⋮----
class TestHumanizeHeader
⋮----
def test_humanize_header_basic(self)
⋮----
def test_humanize_header_acronyms(self)
⋮----
result = RaidChartGenerator._humanize_header("trailing_pe")
⋮----
def test_humanize_header_multi_word(self)
⋮----
class TestIsPercentageColumn
⋮----
def test_percentage_columns_detected(self, col)
⋮----
def test_non_percentage_columns_rejected(self, col)
⋮----
class TestFormatPercentage
⋮----
def test_format_normal_value(self)
⋮----
def test_format_none(self)
⋮----
def test_format_nan(self)
⋮----
def test_format_zero(self)
⋮----
def test_format_negative(self)
⋮----
class TestTableCreation
⋮----
def test_table_uses_humanized_headers(self, generator)
⋮----
df = pd.DataFrame(
# 9 columns >= 8, so generate_chart should create a table
result = generator.generate_chart(df, "Test Table")
# Result should be valid plotly JSON with data
⋮----
table_data = result["data"][0]
⋮----
header_values = table_data["header"]["values"]
# Headers should be humanized
⋮----
class TestBarChartLabelRotation
⋮----
def test_long_labels_get_rotation(self, generator)
⋮----
result = generator.generate_chart(df, "Market Cap Comparison")
layout = result.get("layout", {})
xaxis = layout.get("xaxis", {})
⋮----
class TestValueHeatmap
⋮----
def test_heatmap_uses_humanized_headers(self, generator)
⋮----
# 1 categorical + 3 numeric -> value heatmap
result = generator.generate_chart(df, "Profitability Heatmap")
⋮----
heatmap_data = result["data"][0]
⋮----
x_labels = heatmap_data["x"]
⋮----
# 2. File Existence and Content Tests
⋮----
class TestFileExistence
⋮----
def test_css_file_exists(self)
⋮----
def test_js_file_exists(self)
⋮----
class TestCSSKeySelectors
⋮----
def test_focus_visible(self, css_content)
⋮----
def test_prefers_reduced_motion(self, css_content)
⋮----
def test_skip_to_content(self, css_content)
⋮----
def test_print_media(self, css_content)
⋮----
class TestJSKeyFunctions
⋮----
def test_theme_toggle(self, js_content)
⋮----
def test_onboarding(self, js_content)
⋮----
def test_keyboard_shortcut(self, js_content)
⋮----
def test_new_chat(self, js_content)
⋮----
# 3. WCAG Contrast Ratio Tests
⋮----
class TestWCAGContrast
⋮----
def test_text_muted_contrast_on_card(self)
⋮----
"""#999999 on #1A1A1A should pass WCAG AA 4.5:1."""
ratio = _contrast_ratio("#999999", "#1A1A1A")
⋮----
def test_text_muted_contrast_on_bg(self)
⋮----
"""#999999 on #0E0E0E should pass WCAG AA 4.5:1."""
ratio = _contrast_ratio("#999999", "#0E0E0E")
⋮----
def test_footer_text_contrast(self)
⋮----
"""#A0A0A0 on #0E0E0E should pass WCAG AA 4.5:1."""
ratio = _contrast_ratio("#A0A0A0", "#0E0E0E")
⋮----
def test_luminance_black(self)
⋮----
"""Sanity check: black has luminance 0."""
⋮----
def test_luminance_white(self)
⋮----
"""Sanity check: white has luminance 1."""
⋮----
def test_black_white_contrast(self)
⋮----
"""Black on white should be 21:1."""
ratio = _contrast_ratio("#000000", "#FFFFFF")
⋮----
# 4. HTML Integration Tests
⋮----
class TestHTMLIntegration
⋮----
def test_css_link_in_html(self, index_html_content)
⋮----
def test_js_script_in_html(self, index_html_content)
⋮----
def test_skip_to_content_link(self, index_html_content)
⋮----
def test_aria_role_banner(self, index_html_content)
⋮----
def test_aria_role_main(self, index_html_content)
⋮----
def test_aria_role_contentinfo(self, index_html_content)
⋮----
def test_main_content_id(self, index_html_content)
⋮----
def test_suggestion_chips_data_query(self, index_html_content)
⋮----
"""All 8 suggestion chips should have data-query attributes."""
count = index_html_content.count("data-query=")
⋮----
def test_text_muted_updated(self, index_html_content)
⋮----
"""--text-muted should be #999999 (or #999) for WCAG compliance."""
⋮----
def test_onboarding_overlay_container(self, index_html_content)
⋮----
def test_data_freshness_display(self, index_html_content)
⋮----
def test_native_chat_ui(self, index_html_content)
⋮----
"""Native chat UI replaced CDN vanna-chat component."""
⋮----
def test_plotly_cdn_script(self, index_html_content)
⋮----
"""Plotly.js is loaded for chart rendering."""
⋮----
# 5. Static File Serving Test
⋮----
class TestStaticServing
⋮----
def test_static_mount_serves_css(self)
⋮----
async def _test()
⋮----
transport = ASGITransport(app=app)
⋮----
resp = await client.get("/static/raid-enhancements.css")
⋮----
def test_static_mount_serves_js(self)
⋮----
resp = await client.get("/static/raid-features.js")
⋮----
# 6. Column Labels Tests
⋮----
class TestColumnLabels
⋮----
def test_column_labels_dict_exists(self)
⋮----
def test_humanize_uses_labels(self)
⋮----
result = RaidChartGenerator._humanize_header("market_cap")
⋮----
def test_humanize_roe_uses_label(self)
⋮----
result = RaidChartGenerator._humanize_header("roe")
⋮----
def test_humanize_fallback_for_unknown(self)
⋮----
result = RaidChartGenerator._humanize_header("unknown_column")
⋮----
# 7. Null Handling Tests
⋮----
class TestNullHandling
⋮----
def test_format_number_nan_returns_dash(self)
⋮----
result = RaidChartGenerator._format_number(float("nan"))
⋮----
def test_format_percentage_none_returns_dash(self)
⋮----
result = RaidChartGenerator._format_percentage(None)
⋮----
def test_format_percentage_nan_returns_dash(self)
⋮----
result = RaidChartGenerator._format_percentage(float("nan"))
⋮----
# 8. Horizontal Bar Tests
⋮----
class TestHorizontalBar
⋮----
def test_many_items_uses_horizontal(self, generator)
⋮----
result = generator.generate_chart(df, "Top 10")
trace = result["data"][0]
⋮----
def test_few_items_uses_vertical(self, generator)
⋮----
result = generator.generate_chart(df, "Small")
⋮----
# Should NOT be horizontal (no orientation or orientation='v')
⋮----
# 9. No Duplicate Title Tests
⋮----
class TestNoDuplicateTitle
⋮----
def test_bar_chart_title_empty(self, generator)
⋮----
result = generator.generate_chart(df, "Test")
⋮----
title = layout.get("title", "")
⋮----
def test_table_title_empty(self, generator)
⋮----
result = generator.generate_chart(df, "Table Test")
⋮----
# 10. Shadow DOM Features Tests
⋮----
class TestNativeChatFeatures
⋮----
"""Tests for native chat UI (replaced Shadow DOM CDN component)."""
⋮----
def test_js_has_suggestion_observer(self, js_content)
⋮----
def test_js_manages_tabindex(self, js_content)
⋮----
def test_js_has_theme_toggle(self, js_content)
⋮----
def test_js_has_keyboard_shortcut(self, js_content)
⋮----
# 11. Keyboard Trap Fix Tests
⋮----
class TestKeyboardTrapFix
⋮----
def test_askquestion_sets_aria_hidden(self, index_html_content)
⋮----
def test_askquestion_sets_tabindex(self, index_html_content)
⋮----
# The askQuestion function should set tabindex=-1 on collapsed chips
⋮----
# 12. Responsive CSS Tests
⋮----
class TestResponsiveCSS
⋮----
def test_overflow_x_auto(self, css_content)
⋮----
def test_word_wrap(self, css_content)
⋮----
def test_pre_wrap(self, css_content)
⋮----
def test_plotly_max_width(self, css_content)
⋮----
def test_sr_only_utility(self, css_content)
⋮----
# 13. Format Number Tests
⋮----
class TestFormatNumber
⋮----
def test_format_billions(self)
⋮----
def test_format_millions(self)
⋮----
def test_format_thousands(self)
⋮----
def test_format_trillions(self)
⋮----
def test_format_small_decimal(self)
⋮----
def test_format_regular(self)
</file>

<file path="tests/test_vanna_pipeline.py">
"""Integration tests for the Vanna AI query pipeline.

Tests the core NL -> SQL -> response flow with mocked LLM service
to avoid real API calls while validating the pipeline assembly and execution.
"""
⋮----
# ---------------------------------------------------------------------------
# Paths
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
REAL_DB_PATH = PROJECT_ROOT / "saudi_stocks.db"
⋮----
# Fixtures
⋮----
@pytest.fixture
def sqlite_runner(test_db)
⋮----
"""SqliteRunner backed by the temporary test database."""
⋮----
@pytest.fixture
def real_sqlite_runner()
⋮----
"""SqliteRunner backed by the real saudi_stocks.db (read-only queries)."""
⋮----
@pytest.fixture
def tool_registry(sqlite_runner)
⋮----
"""ToolRegistry with RunSqlTool and VisualizeDataTool registered."""
registry = ToolRegistry()
⋮----
@pytest.fixture
def demo_memory()
⋮----
"""DemoAgentMemory with a small limit for testing."""
⋮----
@pytest.fixture
def test_user()
⋮----
"""A standard test user with 'user' group membership."""
⋮----
@pytest.fixture
def tool_context(test_user, demo_memory)
⋮----
"""Minimal ToolContext for executing tools in tests."""
⋮----
# ===========================================================================
# Test: Agent assembly components
⋮----
class TestAgentAssemblyComponents
⋮----
"""Verify the Agent constructs correctly with all required components."""
⋮----
def test_agent_assembly_components(self, tool_registry, demo_memory)
⋮----
"""Agent must accept all 5 required components without error."""
mock_llm = MagicMock()
mock_resolver = MagicMock()
⋮----
agent = Agent(
⋮----
def test_agent_with_custom_system_prompt_builder(self, tool_registry, demo_memory)
⋮----
"""Agent accepts a custom SystemPromptBuilder."""
⋮----
class StubPromptBuilder(SystemPromptBuilder)
⋮----
async def build_system_prompt(self, user, tools)
⋮----
# Test: Tool registry has required tools
⋮----
class TestToolRegistryTools
⋮----
"""Verify RunSqlTool and VisualizeDataTool are registered correctly."""
⋮----
@pytest.mark.asyncio
    async def test_tool_registry_has_required_tools(self, tool_registry)
⋮----
"""Registry must contain both RunSqlTool and VisualizeDataTool."""
tool_names = await tool_registry.list_tools()
⋮----
@pytest.mark.asyncio
    async def test_tool_schemas_available_for_user(self, tool_registry, test_user)
⋮----
"""Schemas visible to a user in the 'user' group."""
schemas = await tool_registry.get_schemas(user=test_user)
⋮----
@pytest.mark.asyncio
    async def test_tool_schemas_available_for_admin(self, tool_registry)
⋮----
"""Schemas visible to an admin user."""
admin = User(id="admin", email="admin@test.com", group_memberships=["admin"])
schemas = await tool_registry.get_schemas(user=admin)
⋮----
# Test: System prompt contains schema
⋮----
class TestSystemPromptContainsSchema
⋮----
"""Verify the SaudiStocksSystemPromptBuilder produces expected content."""
⋮----
def test_system_prompt_contains_schema(self)
⋮----
"""The system prompt must reference key database tables."""
⋮----
expected_tables = [
⋮----
def test_system_prompt_mentions_tickers(self)
⋮----
"""The system prompt should mention Saudi ticker format."""
⋮----
# Test: System prompt builder signature
⋮----
class TestSystemPromptBuilderSignature
⋮----
"""Verify build_system_prompt(self, user, tools) works correctly."""
⋮----
@pytest.mark.asyncio
    async def test_system_prompt_builder_signature(self, test_user)
⋮----
"""SaudiStocksSystemPromptBuilder.build_system_prompt must be async
        and accept (user, tools) arguments."""
⋮----
builder = SaudiStocksSystemPromptBuilder()
result = await builder.build_system_prompt(user=test_user, tools=[])
⋮----
assert len(result) > 100  # Should be a substantial prompt
⋮----
def test_system_prompt_builder_is_subclass(self)
⋮----
"""Must be a subclass of SystemPromptBuilder."""
⋮----
# Test: SQL tool executes valid query
⋮----
class TestSqlToolExecution
⋮----
"""Verify RunSqlTool executes queries against the real database."""
⋮----
@pytest.mark.asyncio
    async def test_sql_tool_executes_valid_query(self, real_sqlite_runner, demo_memory)
⋮----
"""RunSqlTool should successfully execute SELECT COUNT(*) FROM companies."""
tool = RunSqlTool(sql_runner=real_sqlite_runner)
user = User(id="test", email="t@t.com", group_memberships=["user"])
ctx = ToolContext(
args = RunSqlToolArgs(sql="SELECT COUNT(*) FROM companies")
result = await tool.execute(ctx, args)
⋮----
assert "500" in result.result_for_llm  # 500 companies in real DB
⋮----
"""RunSqlTool should execute a filtered SELECT query."""
⋮----
args = RunSqlToolArgs(
⋮----
@pytest.mark.asyncio
    async def test_sql_tool_with_test_db(self, sqlite_runner, tool_context)
⋮----
"""RunSqlTool works with the temporary test database fixture."""
tool = RunSqlTool(sql_runner=sqlite_runner)
⋮----
result = await tool.execute(tool_context, args)
⋮----
# Test DB has 2 companies
⋮----
# Test: SQL tool rejects destructive queries
⋮----
class TestSqlToolRejectsDestructive
⋮----
"""Verify RunSqlTool rejects dangerous SQL statements."""
⋮----
@pytest.mark.asyncio
    async def test_sql_tool_handles_invalid_sql(self, sqlite_runner, tool_context)
⋮----
"""RunSqlTool returns an error for syntactically invalid SQL."""
⋮----
args = RunSqlToolArgs(sql="SELECTT * FROOM companies")
⋮----
"""RunSqlTool returns an error when querying a nonexistent table."""
⋮----
args = RunSqlToolArgs(sql="SELECT * FROM nonexistent_table")
⋮----
@pytest.mark.asyncio
    async def test_sql_tool_does_not_block_writes(self, sqlite_runner, tool_context)
⋮----
"""RunSqlTool does NOT block write operations at the tool level.

        IMPORTANT FINDING: Vanna 2.0's RunSqlTool relies on LLM behavior
        and database-level permissions (read-only users) to prevent writes,
        NOT on tool-level SQL filtering. This test documents that behavior
        so production deployments use read-only database credentials.
        """
⋮----
# INSERT succeeds (tool does not filter it)
insert_args = RunSqlToolArgs(
insert_result = await tool.execute(tool_context, insert_args)
⋮----
# Verify the row was actually inserted
check_args = RunSqlToolArgs(sql="SELECT COUNT(*) FROM companies")
check_result = await tool.execute(tool_context, check_args)
⋮----
# Test: Agent memory stores and retrieves
⋮----
class TestAgentMemory
⋮----
"""Test DemoAgentMemory can store and retrieve items."""
⋮----
@pytest.mark.asyncio
    async def test_agent_memory_stores_and_retrieves(self, demo_memory, tool_context)
⋮----
"""save_text_memory / get_recent_text_memories roundtrip."""
⋮----
memories = await demo_memory.get_recent_text_memories(tool_context, limit=10)
memory_texts = [m.content for m in memories]
⋮----
def test_agent_memory_respects_max_items(self)
⋮----
"""DemoAgentMemory should accept max_items parameter."""
mem = DemoAgentMemory(max_items=5)
⋮----
@pytest.mark.asyncio
    async def test_agent_memory_clear(self, demo_memory, tool_context)
⋮----
"""clear_memories should remove stored items."""
⋮----
memories_before = await demo_memory.get_recent_text_memories(tool_context)
⋮----
memories_after = await demo_memory.get_recent_text_memories(tool_context)
⋮----
# Test: AgentConfig settings
⋮----
class TestAgentConfigSettings
⋮----
"""Verify AgentConfig has expected defaults and accepts overrides."""
⋮----
def test_agent_config_defaults(self)
⋮----
"""Default AgentConfig should have sensible defaults."""
cfg = AgentConfig()
⋮----
def test_agent_config_custom_values(self)
⋮----
"""AgentConfig accepts custom max_tool_iterations and stream_responses."""
cfg = AgentConfig(max_tool_iterations=5, stream_responses=False)
⋮----
def test_agent_config_from_app(self)
⋮----
"""The config used in app.py should have stream_responses=True."""
⋮----
# Test: End-to-end query pipeline with mocked LLM
⋮----
class MockUserResolver(UserResolver)
⋮----
"""Resolver that always returns a fixed test user."""
⋮----
async def resolve_user(self, request_context)
⋮----
class TestEndToEndQueryPipeline
⋮----
"""Test the full Agent.send_message flow with a mocked LLM.

    The LLM is mocked to return tool_calls that invoke RunSqlTool,
    verifying the pipeline wires: user message -> LLM -> tool call ->
    SQL execution -> tool result -> LLM final response -> UI components.
    """
⋮----
@pytest.fixture
    def mock_llm_service(self)
⋮----
"""LlmService mock that returns a RunSqlTool call then a text response."""
llm = MagicMock(spec=LlmService)
⋮----
# First call: LLM returns a tool call to run SQL
sql_tool_response = LlmResponse(
⋮----
# Second call: LLM returns a text summary
text_response = LlmResponse(
⋮----
@pytest.fixture
    def pipeline_agent(self, mock_llm_service, tool_registry, demo_memory)
⋮----
"""Agent assembled with mocked LLM for pipeline testing."""
⋮----
@pytest.mark.asyncio
    async def test_send_message_invokes_sql_tool(self, pipeline_agent)
⋮----
"""Agent.send_message should invoke RunSqlTool when LLM requests it."""
request_ctx = RequestContext(headers={}, cookies={})
components = []
⋮----
# The LLM was called (at least the first request)
⋮----
# We should have received UI components (dataframe from SQL + text)
⋮----
@pytest.mark.asyncio
    async def test_pipeline_passes_sql_result_back_to_llm(self, pipeline_agent)
⋮----
"""After RunSqlTool executes, the result is sent back to the LLM."""
⋮----
# LLM should be called twice: once to get tool call, once with result
⋮----
# The second call should include the tool result in messages
second_call_args = pipeline_agent.llm_service.send_request.call_args_list[1]
request = second_call_args[0][0]  # First positional arg (LlmRequest)
# Messages should include the tool result
messages = request.messages
assert len(messages) >= 3  # user msg + assistant tool_call + tool result
⋮----
# Test: Chart generation pipeline
⋮----
class TestChartGenerationPipeline
⋮----
"""Test that VisualizeDataTool is invoked when LLM requests it."""
⋮----
@pytest.fixture
    def mock_plotly_generator(self)
⋮----
"""Mock plotly chart generator."""
gen = MagicMock()
⋮----
@pytest.fixture
    def chart_tool_registry(self, sqlite_runner, mock_plotly_generator)
⋮----
"""Registry with both tools, using a trackable plotly generator."""
⋮----
@pytest.fixture
    def chart_agent(self, chart_tool_registry, demo_memory)
⋮----
"""Agent that will receive LLM responses requesting chart generation."""
⋮----
# Step 1: LLM calls run_sql
sql_response = LlmResponse(
⋮----
# Step 2: LLM calls visualize_data with the CSV filename
chart_response = LlmResponse(
⋮----
# Step 3: LLM returns final text
⋮----
@pytest.mark.asyncio
    async def test_chart_pipeline_calls_visualize_tool(self, chart_agent)
⋮----
"""Agent should invoke VisualizeDataTool when LLM requests it."""
⋮----
# LLM should be called 3 times (sql, chart, final text)
⋮----
# Test: Error handling in the pipeline
⋮----
class TestPipelineErrorHandling
⋮----
"""Test that the pipeline handles errors gracefully."""
⋮----
@pytest.mark.asyncio
    async def test_pipeline_handles_bad_sql_from_llm(self, tool_registry, demo_memory)
⋮----
"""When LLM generates invalid SQL, the error is captured and
        sent back to the LLM for a corrective response."""
⋮----
# LLM returns bad SQL
bad_sql_response = LlmResponse(
⋮----
# After getting the error, LLM apologizes
apology_response = LlmResponse(
⋮----
# Pipeline should not crash -- LLM gets the error and responds
⋮----
@pytest.mark.asyncio
    async def test_pipeline_handles_empty_results(self, tool_registry, demo_memory)
⋮----
"""Pipeline handles SQL that returns zero rows gracefully."""
⋮----
# LLM returns SQL that matches nothing
empty_response = LlmResponse(
⋮----
# LLM reports no results
⋮----
"""Agent stops after max_tool_iterations to prevent infinite loops."""
⋮----
# LLM keeps requesting tool calls indefinitely
infinite_tool_call = LlmResponse(
⋮----
# Should stop after max_tool_iterations (3) + 1 final call
</file>

<file path=".dockerignore">
# Secrets and VCS
.env
.env.*
!.env.example
.git
.gitignore

# Python bytecode and caches
__pycache__
*.pyc
*.pyo
.mypy_cache
.ruff_cache
.pytest_cache

# Data files (generated at runtime or during build)
*.db
*.csv
!saudi_stocks_yahoo_data.csv

# Documentation and planning
*.md
.planning/
.claude/

# Reference docs (read-only, not needed at runtime)
vanna_docs/
vanna-skill/

# Test files
test_*.py
tests/
pytest.ini
conftest.py

# Frontend (built separately, not in Python container)
frontend/
node_modules/
vitest.config.ts
*.test.ts
*.test.tsx
package*.json

# Scripts (deployment helpers, not runtime)
scripts/

# CI/CD
.github/

# IDE and editor
.vscode/
.idea/
*.swp
*.swo

# OS artifacts
.DS_Store
Thumbs.db
nul

# Docker (avoid recursive context)
docker-compose*.yml

# Misc
*.log
*.jsonl
plan.md
repo.md
</file>

<file path="api/routes/charts.py">
"""
Chart data API routes.

Provides pre-built data endpoints for frontend charting (sector breakdown,
top companies by market cap, etc.). The frontend renders charts via
TradingView Lightweight Charts or Plotly.
"""
⋮----
router = APIRouter(prefix="/api/charts", tags=["charts"])
⋮----
def _pg_fetchall(sql: str, params=None) -> List[Dict[str, Any]]
⋮----
"""Run a PG query synchronously, returning list of dicts."""
conn = get_db_connection()
⋮----
# ---------------------------------------------------------------------------
# Routes
⋮----
@router.get("/sector-market-cap", response_model=ChartResponse)
async def sector_market_cap() -> ChartResponse
⋮----
"""Return total market cap by sector for a pie/bar chart."""
sql = """
rows = await asyncio.to_thread(_pg_fetchall, sql)
⋮----
"""Return top N companies by market cap."""
clauses = ["m.market_cap IS NOT NULL"]
params: Dict[str, Any] = {"limit": limit}
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
sql = f"""
rows = await asyncio.to_thread(_pg_fetchall, sql, params)
⋮----
@router.get("/sector-pe", response_model=ChartResponse)
async def sector_avg_pe() -> ChartResponse
⋮----
"""Return average trailing P/E ratio by sector."""
⋮----
"""Return top N companies by dividend yield."""
⋮----
rows = await asyncio.to_thread(_pg_fetchall, sql, {"limit": limit})
</file>

<file path="api/routes/news.py">
"""
News articles API routes.

Provides read endpoints (public) and a write endpoint (authenticated).
"""
⋮----
# Backward-compatible aliases used by tests/test_api_routes.py
NewsArticleResponse = NewsResponse
⋮----
class NewsListResponse(BaseModel)
⋮----
"""Legacy response model kept for backward compatibility."""
⋮----
items: List[NewsResponse]
count: int
⋮----
router = APIRouter(prefix="/api/news", tags=["news"])
⋮----
def _to_response(a: NewsArticle) -> NewsResponse
⋮----
# ---------------------------------------------------------------------------
# Read endpoints (public)
⋮----
"""Return the latest news articles across all tickers."""
articles = await asyncio.to_thread(
total = await asyncio.to_thread(svc.count_articles)
⋮----
"""Return news articles for a specific ticker."""
⋮----
total = await asyncio.to_thread(svc.count_articles, ticker=ticker)
⋮----
"""Return news articles for all companies in a sector."""
⋮----
total = await asyncio.to_thread(svc.count_articles, sector=sector)
⋮----
"""Return a single news article by ID."""
article = await asyncio.to_thread(svc.get_article_by_id, article_id)
⋮----
# Write endpoints (authenticated)
⋮----
"""Create a new news article. Requires authentication."""
article = NewsArticle(
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to the Ra'd AI TASI Platform are documented in this file.

Format based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/).

## [Unreleased] - 2026-02-20

### Added (S1–S4 Remediation Sprint — 25 tasks)

**S1 — Foundations**
- SQLite connection pool (`services/sqlite_pool.py`): thread-safe pooled connections for concurrent request handling
- JWT validation moved to startup: `env_validator.py` enforces `JWT_SECRET_KEY` before accepting traffic (PostgreSQL mode)
- `config/lifecycle.py`: structured startup diagnostics — logs version, Python, database backend, pool status, and Prometheus availability
- `config/env_validator.py`: startup validation with fail-fast on missing required env vars
- `middleware/request_context.py`: `ContextVar`-based request ID propagation so all log records within a request share the same `request_id`
- `middleware/chat_auth.py`: `ChatAuthMiddleware` class that enforces JWT on `/api/vanna/v2/chat_sse` and `/api/vanna/v2/chat_poll` endpoints (PostgreSQL mode only)

**S1 — Fixes**
- Resolved LLM provider confusion: `settings.py` now uses `ANTHROPIC_API_KEY` consistently; removed Gemini/OpenRouter fallbacks
- Removed stale `config/logging.py` duplicate (consolidated into `config/logging_config.py`)
- Fixed `entrypoint.sh` silent failure: added `set -euo pipefail` and explicit error checks
- Replaced hardcoded Windows SQLite paths in all test files and `api/db_helper.py` with env-var-driven resolution via `DB_SQLITE_PATH`
- Fixed CI/CD: lint job now blocks all downstream jobs; tests were previously running in parallel with linting

**S2 — Performance & Bug Fixes**
- PostgreSQL connection pool (`database/pool.py`): singleton `ThreadedConnectionPool` with UUID-keyed checkout for async FastAPI safety
- Pool stats exposed in health response: `get_pool_stats()` added to `health_service.py`, returned in `/health` endpoint
- Batched news inserts: `store_articles()` now uses `executemany` instead of per-row `execute` (significant throughput improvement for large fetches)
- Fixed `_sql_convert` naive string replace: now uses `sqlparse`-aware token replacement for SQLite→PostgreSQL compatibility
- Composite indexes added: `(source_name, created_at)` on `news_articles`, `(sector)` on `companies`
- Deduplicated cache wrappers: removed inline `@cache` usage; all caching goes through `@cache_response` from `cache_utils.py`
- Fixed Redis failure logging: changed `log.exception()` to `log.warning(..., exc_info=True)` to avoid noisy tracebacks in expected failure scenarios
- Frontend: memoized chart MA calculations, CommandPalette results, ArticleCard render with `React.memo` and `useMemo`
- Frontend: converted multiple `useState` to `useReducer` for market filters state
- Frontend: extracted `useWatchlist` hook; consolidated date/number formatters into `lib/formatters.ts`
- Frontend: added `aria-label` to all 15+ icon-only buttons across navigation, chat, and market pages

**S3 — Infrastructure**
- Dependency lockfile: `pip-compile requirements.in -o requirements.lock` workflow; CI verifies lock is up to date
- Split `requirements.txt` into `requirements.in` (unpinned constraints) and `requirements-dev.txt` (test/lint tools)
- Docker: added commented production env var template to `docker-compose.yml` app service
- Docker: pinned pgAdmin from `:latest` to `dpage/pgadmin4:8`
- Deploy: pinned Railway CLI to `@railway/cli@3` in `deploy.yml`; added health-check rollback step on deployment failure
- CI coverage threshold raised to 75% (from 70%)
- Added `pg_schema_version` session fixture to `tests/conftest.py`: verifies `schema.sql` was applied in PostgreSQL CI jobs
- Prometheus metrics: added `prometheus-fastapi-instrumentator` dependency; metrics exposed at `/metrics` with graceful `ImportError` fallback
- Request ID ContextVar: `error_handler.py` now sets `request_id` in both `request.state` and `ContextVar` so log records include it even outside middleware

**S4 — Backend Backlog**
- Pre-rendered HTML template: `_TEMPLATE_HTML` computed at module load from `templates/index.html`, not per-request
- `NewsScheduler` stats: added `last_run_at`, `total_articles_stored`, `run_count`, and `get_stats()` method

**S4 — Frontend Backlog**
- `useLocalStorageTTL<T>` hook (`lib/hooks/useLocalStorageTTL.ts`): generic hook with TTL envelope `{ value, expiresAt }` and SSR guard
- Sentry PII redaction: `sentry.client.config.ts` `beforeSend` hook strips `Authorization` headers and redacts `token`/`accessToken`/`refreshToken`/`jwt` from `event.extra`
- SWR metrics wired: `metricsMiddleware` now registered in `chartCacheConfig.use` (was defined but never applied)
- Accessibility: Space key handling added to all `role="button"` elements in `AIChatInterface`, `DataTable`, and `DataTableHeader`; `e.preventDefault()` prevents page scroll on Space

**S4 — Infra**
- `database/postgres_utils.py` (new): shared `pg_available(timeout=3)` and `pg_connection_params()` helpers; replaces duplicated PG connection code across test files
- Moved `test_database.py` and `test_app_assembly_v2.py` from project root to `tests/`; updated `_SQLITE_PATH` fallback to `_HERE.parent / "saudi_stocks.db"`; CI test paths updated accordingly
- CI: added `security-scan` job (bandit, severity/confidence medium, excludes frontend/tests/venv/vanna dirs)
- CI: added `type-check` job (mypy on `config/ services/ api/ middleware/ database/ backend/`)
- `config/lifecycle.py`: startup now logs connection pool size and Prometheus availability

### Changed
- Test files `test_database.py` and `test_app_assembly_v2.py` moved from project root to `tests/` directory

### Fixed
- Request ID filter installed with duplicate guard (`if not any(isinstance...)`) to prevent double-registration on hot-reload

## [Unreleased] - 2026-02-18

### Added
- E2E Playwright tests for markets page (sector filter, sort, pagination, search, mobile card view, RTL layout)
- E2E Playwright tests for stock detail page (financials tab, dividends tab, watchlist toggle, news section, reports section)
- Backend test coverage expansion: XBRL processor (`ingestion/xbrl_processor.py`) 26.7% → 70%+ with 40+ new tests
- Backend test coverage expansion: price loader (`ingestion/price_loader.py`) 33.5% → 70%+ with 35+ new tests
- Backend test coverage expansion: stock OHLCV service (`services/stock_ohlcv.py`) 18.8% → 70%+ with 30+ new tests
- Backend test coverage expansion: Redis client (`backend/services/cache/redis_client.py`) 22.2% → 70%+ with mocked operations, pub/sub, and connection management tests
- Backend test coverage expansion: ingestion scheduler (`ingestion/scheduler.py`) 0% → 70%+ with 25+ new tests
- Frontend API module tests for remaining domains: charts, entities, market, and health (`frontend/src/__tests__/lib/api/`)
- Redis integration tests: mocked client operations, pub/sub flow, connection lifecycle, error recovery

### Changed
- ConstellationCanvas (`frontend/src/components/`) performance optimized: replaced `setInterval` animation loop with `requestAnimationFrame`; added `React.memo` to particle child components; debounced resize handler to prevent layout thrashing; added `will-change: transform` CSS hint for GPU compositing

### Improved
- Backend test coverage on critical data ingestion modules from an average of ~20% to 70%+

## [Unreleased] - 2026-02-17

### Added
- Frontend component decomposition: charts page split into 9 subcomponents (`frontend/src/app/charts/components/`), markets page split into 12 subcomponents (`frontend/src/app/markets/components/`)
- API client modularized into domain modules under `frontend/src/lib/api/` (stocks, news, charts, market, auth, health, widgets, reports, announcements); backward-compatible shim retained at `api-client.ts`
- Auth system enhancements: token refresh, guest login flow, profile enrichment in auth hook and auth service
- Stock detail page enriched with financials tab, dividends tab, reports section, related news feed, and watchlist toggle
- Chat SSE message batching and message persistence across sessions
- Tailwind CSS keyframe animations added to global styles
- Performance config files consolidated (Next.js + Tailwind + PostCSS)
- New frontend test suites: auth hook tests, API client module tests, stock detail page tests
- New backend test suites: auth service tests, widget system tests, health and config module tests
- UX/UI deep dive audit with 100 recommendations from 10-agent analysis
- 122 new tests: 95 db_compat tests + 27 Vanna pipeline integration tests

### Changed
- API client architecture: flat `api-client.ts` moved to modular domain structure under `frontend/src/lib/api/`; original file now re-exports from modules for backward compatibility
- Apply ruff 0.15.1 formatting across 49 Python files
- Reduce DemoAgentMemory from 10,000 to 500 items (OOM prevention)
- Replace global `_fetch_lock` with per-ticker locks in stock_ohlcv and tasi_index services
- `requirements-dev.txt` with separated development dependencies (pytest, pytest-timeout, ruff)
- `.gitignore` expansion to cover `.env.*`, keys, and build artifacts (49 patterns)

### Fixed
- RTL lint violations: physical direction Tailwind classes replaced with logical properties across charts and markets pages
- ESLint warnings resolved across new frontend component files
- CI pipeline: install `requirements-dev.txt` for test jobs
- CI pipeline: add `--timeout=60` per-test timeout to prevent yfinance network hangs
- CI pipeline: skip `tests/performance/` directory (network-dependent load tests)
- Add `pytest-timeout` to dev dependencies
- Resolve 21 unused imports, 1 ambiguous variable, 4 E402 noqa annotations
- Resolve 7 critical audit findings from 14-agent code audit (180+ findings)

### Security
- Add whitelist + regex validation to `datetime_recent()` (SQL injection defense)
- Add `validate_ticker()` to entity and reports routes (3 files)
- Pin dependency version bounds in `requirements.txt`

## [0.9.0] - 2026-02-16

### Added
- News page UX/UI polish (14 agents across 2 teams):
  - Full card clickable ArticleCard with hover prefetch
  - Breaking news treatment (pulsing badge + red accent bar for priority >= 5)
  - Search term highlighting in article cards
  - Horizontal scroll filter chips with snap-x and active ring
  - Toast-style NewArticlesBanner with auto-dismiss progress bar
  - Gold shimmer SkeletonCard with staggered entrance animations
  - Article detail: back-to-top, reading progress, drop-cap, auto-retry, word count
  - Related articles with staggered entrance animations
  - Bookmark toast notification on save/unsave
  - Header article count badge
  - Error/empty states redesign with floating icons and bilingual text
  - CSS keyframe animations: fade-in-up, fade-in, shimmer, float
- Live market widgets system (SSE + Redis pub/sub): crypto, metals, oil, indices
- ConnectionStatusBadge component (live/reconnecting/offline indicator)
- Unified cache decorator (`@cache_response` with LRU + TTL)
- Shared yfinance base extraction with circuit breaker
- SQL centralization in `database/queries.py`
- News scheduler per-source logging
- RTL lint script (`npm run lint:rtl`)
- Route-level error and loading boundaries for news, market, charts, chat
- Markets page with MarketOverviewClient

### Fixed
- Reports service now works with SQLite backend (dual-backend support)
- Auto-create `technical_reports` table in SQLite on first access
- Reports router registered for both backends in `app.py`
- Add `vercel.json` to fix Vercel build error
- Replace stale Vercel URL with env-driven frontend link (`{{FRONTEND_URL}}`)
- Raise `/api/v1/charts` rate limit from 30 to 120 rpm (sparkline burst support)
- `next.config` health proxy fix

## [0.8.0] - 2026-02-14

### Added
- Pydantic-settings configuration for scraper (`ScraperSettings` with all constants)
- Frontend runtime config (`frontend/src/lib/config.ts`) for API timeouts and intervals
- Playwright E2E tests for news portal (RTL, virtual scroll, SSE, filters)
- News SSE stream route (`/api/v1/news/stream`)
- SSE buffering headers in `next.config.mjs` for stream endpoints
- `DEVELOPMENT.md` developer guide (async patterns, adding news sources, RTL, AbortController)

### Changed
- Extract hardcoded config values into pydantic-settings and env variables
- Replace 30+ hardcoded 15,000ms timeouts in `api-client.ts` with config defaults
- Replace hardcoded 60,000ms cache TTL with config defaults
- Replace hardcoded 30,000ms health poll interval in `Header.tsx`
- Decompose 1,212-line `news/page.tsx` into 8 focused modules
- Finalize dead code cleanup and async conversion across codebase
- Synchronize all documentation with new architecture

### Fixed
- AbortController added to 15 unsafe `fetch()` calls preventing race conditions on unmount
- Replace 25 hardcoded LTR Tailwind classes with logical properties (ms/me/ps/pe) for RTL
- Wrap sync sqlite3/psycopg2 calls in `asyncio.to_thread()` across 14 backend route files
- Fix blocking sync call in `news_stream.py` SSE endpoint
- E2E test flakes (RTL timing, SSE network idle, Badge selectors)
- Docker: add missing `models/` COPY to Dockerfile
- Docker: add `--timeout-graceful-shutdown 30` to uvicorn for clean SSE disconnect
- Docker: rename `REDIS_URL` to `CACHE_REDIS_URL` in `docker-compose.yml`
- Build regressions from refactor verified and fixed

## [0.7.0] - 2026-02-13

### Added
- Frontend security: CSP, HSTS, host validation middleware, CSRF protection, secure cookies
- Frontend monitoring: Sentry integration (client/server/edge), error boundaries, Web Vitals
- Frontend performance: bundle analyzer, lazy-loading wrappers, skeletons, OptimizedImage
- Frontend auth: RBAC types (admin/analyst/viewer), AuthContext with JWT, permission guards
- Query UX: IndexedDB query store, history UI with search/sort/favorites, saved queries
- Data export: CSV, Excel, and PDF export from query results
- AutoChart with smart type detection (recharts), DataTable with virtual scrolling
- Playwright E2E tests (27 tests), Locust load tests, Lighthouse CI
- 72 new Vitest unit tests (139 total)
- OpenAPI 3.0 spec (35+ endpoints), Swagger UI
- PRIVACY.md (GDPR/PDPL), SLA.md documentation

### Changed
- Production-readiness audit: 38 tasks across all layers (7-agent team)
- Chat bundle reduced from 362kB to 105kB (71% reduction)
- Tiered rate limiting (10/30/60 rpm by endpoint category)
- Unified error responses with `request_id` correlation

### Fixed
- 12 information leak fixes across API endpoints
- CORS hardening with restricted methods and headers

### Security
- Ticker input validation on 7 endpoints
- 54-endpoint auth audit completed
- Dependency security scan integrated
- IP-anonymized request logging

### Infrastructure
- Multi-stage Dockerfile with tini init and non-root user
- CI/CD pipeline with Buildx cache (`.github/workflows/ci.yml` + `deploy.yml`)
- Environment validation on startup (`env_validator.py`)
- Liveness (`/health/live`) and readiness (`/health/ready`) probes
- Graceful shutdown handler
- Railway deployment configuration
- `db_compat.py` SQLite/PostgreSQL abstraction layer
- Structured JSON logging (production) / pretty logging (development)
- Pluggable error tracking (`error_tracking.py`)
- 56 new backend tests (552 total, 0 regressions)

## [0.6.0] - 2026-02-12

### Added
- Bilingual chat suggestion chips (Arabic/English)
- Stock detail: financial statements tabbed section
- Watchlist: batch quote fetching with skeleton loading states
- Login page with email/password + guest access
- Guest token endpoint for anonymous chat access
- Health endpoint: TASI cache, news scheduler, build info checks (6 components)

### Changed
- Switch default LLM to Claude Sonnet 4.5 (`AnthropicLlmService`), remove Gemini references
- Chart rendering: `normalizeSSEEvent` falls back to `richData` when `plotly_json`/`fig` wrappers absent
- Batch SSE events via 50ms flush interval (reduce React re-renders ~10-20x)
- Separate progress events from content components in chat UI
- Show live progress text in LoadingDots during streaming

### Fixed
- Chat SSE: use `RequestContext.get_header()` instead of `.request` (Vanna 2.0 API)
- Arabic name corrected: replace all "رائد" with "رعد" across frontend
- SSE chat: normalize Vanna 2.0 event format to frontend types
- Dividend yield: remove 100x multiplier (DB stores percentages)
- TASI index: reduce circuit breaker timeout, add data freshness metadata
- Stock detail: add `.SR` ticker normalization (frontend + backend)
- Filter 113 SABE null-data entities from listings (387 real companies remain)
- News scrapers: replace broken Al Arabiya/Asharq with Google News RSS proxy
- Market search: add stock alias matching (Aramco -> 2222.SR)
- Sector names: add Arabic translation map across all pages
- Unicode escape sequences replaced with actual Arabic characters
- `useLanguage` i18n added to 6 chat components
- `timeAgo()`, `readingTime()`, `formatDate()` made bilingual
- Sidebar CSS dark mode conflict fixed
- Pool connection: UUID-based unique keys per checkout (replaces thread-ID collision)
- psycopg2 pool: `_PooledConnection` wrapper (read-only C extension attribute fix)
- Dual-backend: entities/market/charts/stocks routes use PostgreSQL on Railway
- Entities API: SQLite fallback always registered with proper error handling
- Market analytics: NULL handling with COALESCE, 503 on missing DB
- News pipeline: full article body fetching, `published_at` fallback
- Watchlist: localStorage fallback for anonymous users
- Announcements: user-friendly error display with collapsible details
- News counter: updates correctly when Saved filter active
- News 404: clean error UI instead of raw JSON
- localStorage keys: unified to `rad-ai-*` prefix with migration
- CommandPalette `useMemo` missing language dependency
- Mobile: flex-wrap breadcrumbs, break-words company names
- Ruff format applied to 28 files, lint errors fixed for CI compliance
- CI: upgrade Node 18 to 20 for Vitest ESM compatibility

### Security
- JWT made optional (anonymous access allowed) with proper guards
- Security headers: X-Content-Type-Options, X-Frame-Options, Referrer-Policy
- API client: 15s timeout with in-memory cache

## [0.5.0] - 2026-02-11

### Added
- English/Arabic language toggle with full RTL/LTR support (LanguageProvider)
- 5-source Arabic news scraper, paraphraser, SQLite store, background scheduler
- News REST API (`/api/v1/news/feed`, `/feed/{id}`, `/sources`, `/search`)
- Frontend news page with full Arabic RTL support
- Announcements page, news detail page, loading states, 404 page
- Command palette (Ctrl+K), mobile bottom nav, scroll-to-top, toast system
- TASI index chart (lightweight-charts), TradingView widget integration
- Stock comparison chart
- Market analytics, news feed, SQLite entities API routes
- Database manager, config prompts module
- Gemini model compatibility check and auto-fallback

### Changed
- Switch LLM from Gemini to Claude Sonnet 4.5 via Anthropic API
- Home page made fully bilingual (Arabic/English)

### Fixed
- Frontend deployment: CORS origins, navigation, language toggle, API wiring, legacy UI
- API path mismatches for market and stocks endpoints
- `BACKEND_URL` env var for Next.js rewrite destination
- Railway deployment: warn instead of crash on missing `AUTH_JWT_SECRET`
- Gemini 3 Flash tool round-trip error (switch to Gemini 2.5 Flash as intermediate step)

## [0.4.0] - 2026-02-10

### Added
- Per-stock OHLCV endpoint (`GET /api/v1/charts/{ticker}/ohlcv`) with yfinance, 300s cache, circuit breaker
- TASI index endpoint with same pattern
- Dedicated `/charts` page with stock search, quick-pick chips, and candlestick charts
- Pagination for news and reports pages
- Ctrl+K chat shortcut, print styles, onboarding banner
- DataSourceBadge, ChartErrorBoundary components
- 53 frontend Vitest tests (7 files), 42 new Python tests for TASI/OHLCV
- MSW integration tests

### Changed
- Switch LLM from Anthropic Claude to Google Gemini 3 Flash (via OpenAI-compatible endpoint)

### Fixed
- OHLCV hooks: unwrap `StockOHLCVResponse` correctly
- Wire error/loading/empty states across all chart pages

### Security
- Fix CRITICAL watchlist IDOR (SA-01): replace `X-User-Id` header with `get_optional_current_user()`
- Add DOMPurify to marked.js rendering (SA-02)
- Startup warnings for JWT secret and debug mode

## [0.3.0] - 2026-02-09

### Added
- Comprehensive codebase analysis with security findings and action plan

### Changed
- Dockerfile: add non-root user (`appuser`) and HEALTHCHECK
- CORS: restrict `allow_methods` and `allow_headers` to specific values
- `app.py`: use `config/settings.py` exclusively, replace deprecated `@app.on_event` with lifespan

### Fixed
- `middleware/rate_limit.py`: replace `list.pop(0)` with `deque.popleft()` (O(1) performance)
- `test_database.py`: fix index existence assertion

### Security
- Docker Compose: require passwords (no weak defaults), bind ports to 127.0.0.1, add resource limits
- Redis: enable persistence + auth
- Frontend: add DOMPurify for markdown XSS prevention, add `crossorigin` attrs to CDN scripts
- `requirements.txt`: pin all deps to major.minor ranges, update minimum versions
- CI: add security audit job (pip-audit), Docker build verification with non-root user check
- Extract auth logic into `services/auth_service.py` for separation of concerns

## [0.2.0] - 2026-02-08

### Added
- TradingView Lightweight Charts for Next.js frontend:
  - CandlestickChart with volume overlay, MA20/MA50, time ranges
  - LineChart with configurable time range selector
  - AreaChart with gradient fill for index trends
  - MiniSparkline for stock cards
  - Shared hooks (`useChart.ts`), config, and types
  - Chart state components (skeleton, error, empty)
- Native SSE chat UI replacing broken CDN vanna-chat component
- Railway deployment configuration (`railway.toml`)
- Entrypoint script for auto DB initialization on Railway
- Chart data layer: mock generators, formatters, MA calculation, data hooks
- 26 API routes, 5 CRUD services

### Changed
- Ra'd AI full-stack build with UI/UX hardening:
  - WCAG AA contrast (#999999 on dark), focus-visible, skip-to-content
  - Theme toggle (dark/light), onboarding overlay
  - Chart readability: larger fonts (14px), automargin, COLUMN_LABELS with units
  - Responsive: overflow-x handling, 44px touch targets
  - Remove user admin group to suppress Vanna diagnostic messages

### Fixed
- PostgresRunner parameter: `dbname` changed to `database`
- Health endpoint test: remove `components` assertion (Vanna returns `{status, service}`)
- CI test failures: add `openai` dependency, fix PG migration params
- Docker build: unblock CSV in `.dockerignore`, drop `.db` COPY
- All ruff lint and format errors resolved for CI (46 files formatted)

### Infrastructure
- Docker Compose: PostgreSQL 16 Alpine + app + optional pgAdmin
- Dockerfile: Python 3.11 FastAPI container
- Custom RaidChartGenerator: dark gold theme, value heatmaps, smart formatting
- JWT auth, Pydantic validation, Redis cache, rate limiting
- CI/CD pipeline established
- 513 tests pass, 0 failures, 38 skipped (PostgreSQL-only)

## [0.1.0] - 2026-02-06

### Added
- Initial release of Ra'd AI TASI Platform
- Normalized 1,062-column CSV into 10-table SQLite database (~500 Saudi stocks)
- Vanna 2.0 FastAPI server with Claude Sonnet 4.5 (Anthropic API)
- Ra'd AI gold-themed dark UI with responsive design (Tajawal font)
- SQL and visualization tools (RunSqlTool, VisualizeDataTool)
- Full schema documentation in system prompt
- 7 simple tables (1 row per ticker): companies, market_data, valuation_metrics, profitability_metrics, dividend_data, financial_summary, analyst_data
- 3 financial statement tables (multiple rows per ticker): balance_sheet, income_statement, cash_flow
- CLAUDE.md for Claude Code guidance
- AGENTS.md for agent configuration and behavioral rules
- Vanna skill references and scraped Vanna documentation
- Comprehensive test suite (44 tests)
- Environment configuration via `.env` file
</file>

<file path="database/pool.py">
"""
PostgreSQL Connection Pool
==========================
Singleton connection pool using psycopg2's ThreadedConnectionPool.
Lazy-initialized via ``init_pool()`` -- never imports or connects at module level.

Important: FastAPI async handlers share a single event-loop thread, so the
default ThreadedConnectionPool key (thread ID) would cause all concurrent
requests to share one connection.  We use unique keys per checkout to avoid
this.

Usage::

    from database.pool import init_pool, get_connection, close_pool

    # At startup
    init_pool(settings.db)

    # In service code
    with get_connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT 1")

    # At shutdown
    close_pool()
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Module-level singleton (lazy -- None until init_pool is called)
⋮----
_pool: Optional[ThreadedConnectionPool] = None
⋮----
"""Initialize the global connection pool.

    Parameters
    ----------
    db_settings : DatabaseSettings
        Must expose ``pg_host``, ``pg_port``, ``pg_database``, ``pg_user``,
        ``pg_password``.
    min_connections : int
        Minimum connections kept open (default 2).
    max_connections : int
        Maximum connections allowed (default 10).
    """
⋮----
_pool = ThreadedConnectionPool(
⋮----
_pool = None
⋮----
def _unique_key() -> str
⋮----
"""Generate a unique key for each pool checkout.

    ThreadedConnectionPool uses thread ID as the default key, which causes
    all async handlers (sharing one event-loop thread) to receive the same
    connection.  A UUID key ensures every checkout gets its own connection.
    """
⋮----
@contextmanager
def get_connection()
⋮----
"""Context manager that checks out a connection and returns it on exit.

    Commits on clean exit, rolls back on exception, and always returns the
    connection to the pool.

    Raises ``RuntimeError`` if the pool has not been initialized.
    """
⋮----
key = _unique_key()
conn = _pool.getconn(key=key)
⋮----
class _PooledConnection
⋮----
"""Thin wrapper around a psycopg2 connection that returns it to the pool
    on ``close()`` instead of destroying it.

    psycopg2 connections are C extension objects whose ``close`` attribute
    is read-only, so we cannot monkey-patch it.  Instead we delegate all
    attribute access to the underlying connection while overriding ``close``.
    """
⋮----
__slots__ = ("_conn", "_pool", "_key")
⋮----
def __init__(self, conn, pool, key: str)
⋮----
def close(self)
⋮----
conn = object.__getattribute__(self, "_conn")
pool = object.__getattribute__(self, "_pool")
key = object.__getattribute__(self, "_key")
⋮----
def cursor(self, *args, **kwargs)
⋮----
def commit(self)
⋮----
def rollback(self)
⋮----
def __getattr__(self, name)
⋮----
def __enter__(self)
⋮----
def __exit__(self, *args)
⋮----
def get_pool_connection()
⋮----
"""Return a wrapped connection from the pool.

    The caller should call ``conn.close()`` when done -- this returns the
    connection to the pool instead of destroying it.

    Prefer :func:`get_connection` (context manager) instead when possible.

    This function exists so services that use the ``get_conn`` callable pattern
    can be wired to the pool: ``Service(get_conn=get_pool_connection)``.
    """
⋮----
def close_pool() -> None
⋮----
"""Close all connections in the pool. Safe to call even if not initialized."""
⋮----
def is_pool_initialized() -> bool
⋮----
"""Return True if the connection pool has been initialized."""
</file>

<file path="frontend/src/components/charts/CandlestickChart.tsx">
import { useEffect, useRef, useState, useCallback } from 'react';
import { cn } from '@/lib/utils';
import {
  createChart,
  type IChartApi,
  type ISeriesApi,
  type CandlestickData,
  type HistogramData,
  type LineData,
  type Time,
} from 'lightweight-charts';
import {
  RAID_CHART_OPTIONS,
  CANDLE_COLORS,
  VOLUME_UP_COLOR,
  VOLUME_DOWN_COLOR,
  MA20_COLOR,
  MA50_COLOR,
} from './chart-config';
import type { OHLCVData, ChartTimeRange } from './chart-types';
import { formatVolume } from '@/lib/formatters';
import dynamic from 'next/dynamic';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';
⋮----
// ---------------------------------------------------------------------------
// Props
// ---------------------------------------------------------------------------
⋮----
interface CandlestickChartProps {
  data: OHLCVData[];
  height?: number;
  showVolume?: boolean;
  showMA20?: boolean;
  showMA50?: boolean;
  title?: string;
  ticker?: string;
  className?: string;
  loading?: boolean;
  error?: string | null;
  refetch?: () => void;
}
⋮----
// ---------------------------------------------------------------------------
// Time range definitions
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function calculateMA(data: OHLCVData[], period: number): LineData[]
⋮----
function filterByTimeRange(data: OHLCVData[], range: ChartTimeRange): OHLCVData[]
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Responsive height: 500 XL, 400 desktop, 300 tablet, 250 mobile
⋮----
function handleResize()
⋮----
// Build chart once
⋮----
// Candlestick series
⋮----
// Volume histogram on a separate price scale
⋮----
// MA20 line
⋮----
// MA50 line
⋮----
// Crosshair tooltip
⋮----
// ResizeObserver
⋮----
// Create chart on mount, destroy on unmount
⋮----
}, [loading, data.length > 0, buildChart]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
// Update chart height when responsive size changes
⋮----
// Update series data when data, range, or toggles change
⋮----
// Candlestick data
⋮----
// Volume data
⋮----
// MA lines
⋮----
// Fit content to visible range
⋮----
// Loading state
⋮----
// Error state
⋮----
// Empty state
⋮----
{/* Toolbar */}
⋮----
{/* Left: Title / ticker */}
⋮----
{/* Right: Toggles + Time range */}
⋮----
{/* MA toggles */}
⋮----
{/* Separator */}
⋮----
{/* Time range buttons */}
⋮----
className=
⋮----
{/* Crosshair tooltip bar */}
⋮----
{/* Chart container */}
⋮----
// SSR-safe dynamic wrapper -- pages should import this default export
</file>

<file path="frontend/src/components/charts/ChartEmpty.tsx">
interface ChartEmptyProps {
  message?: string;
  height?: number;
}
⋮----
export function ChartEmpty({
  message = 'No data available',
  height = 400,
}: ChartEmptyProps)
</file>

<file path="frontend/src/components/charts/ChartError.tsx">
interface ChartErrorProps {
  message?: string;
  onRetry?: () => void;
  height?: number;
}
</file>

<file path="frontend/src/components/charts/ChartSkeleton.tsx">
interface ChartSkeletonProps {
  height?: number;
}
⋮----
{/* Shimmer overlay */}
⋮----
{/* Toolbar skeleton */}
⋮----
{/* Chart area skeleton with fake candle hints */}
</file>

<file path="frontend/src/components/chat/ChartBlock.tsx">
import { useRef, useState, useEffect, useCallback } from 'react';
import dynamic from 'next/dynamic';
import { cn } from '@/lib/utils';
import type { SSEChartData } from '@/lib/types';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
// Plotly must be loaded client-side only (no SSR)
⋮----
interface ChartBlockProps {
  data: SSEChartData;
}
⋮----
// Responsive chart height
⋮----
function updateHeight()
⋮----
// Extract data and layout from the Plotly JSON
⋮----
const handleDownload = async () =>
⋮----
// Find the plotly div inside the container
⋮----
// Access Plotly from the global scope (loaded by react-plotly.js)
⋮----
// Silently fail if download not available
⋮----
{/* Action buttons */}
<div className=
{/* Download button */}
⋮----
title=
aria-label=
⋮----
{/* Fullscreen button */}
⋮----
{/* Close button */}
</file>

<file path="frontend/src/components/chat/MessageBubble.tsx">
import { useState } from 'react';
import { cn } from '@/lib/utils';
import type { ChatMessage } from '@/lib/types';
import { AssistantContent } from './AssistantContent';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface MessageBubbleProps {
  message: ChatMessage;
  onRetry?: () => void;
  /** Live progress text for the currently-streaming message */
  progressText?: string;
}
⋮----
/** Live progress text for the currently-streaming message */
⋮----
className=
⋮----
{/* Assistant avatar */}
⋮----
{/* Retry button for error messages */}
⋮----
{/* Timestamp on hover */}
⋮----
{/* User avatar */}
</file>

<file path="frontend/src/components/visualization/DataTableHeader.tsx">
import { cn } from '@/lib/utils';
⋮----
export type SortDirection = 'asc' | 'desc' | null;
⋮----
interface DataTableHeaderProps {
  columns: string[];
  sortColumn: string | null;
  sortDirection: SortDirection;
  onSort: (column: string) => void;
  onFilterChange: (column: string, value: string) => void;
  filters: Record<string, string>;
  showFilters: boolean;
}
⋮----
function SortArrow(
⋮----
className=
⋮----
aria-sort=
</file>

<file path="frontend/src/lib/hooks/use-chart-data.ts">
import { useCallback } from 'react';
import { getOHLCVData, getTasiIndex } from '@/lib/api-client';
import {
  generateMockOHLCV,
  generateMockPriceTrend,
} from '@/lib/chart-utils';
import type { OHLCVData, LineDataPoint } from '@/lib/chart-utils';
import type { DataSource, ChartDataResult } from '@/components/charts/chart-types';
import { useChartCache, chartKeys } from '@/lib/chart-cache';
import { validateOHLCVData, validateTasiResponse } from '@/lib/validators';
⋮----
// ---------------------------------------------------------------------------
// Internal type for fetched data with source tracking
// ---------------------------------------------------------------------------
⋮----
interface SourcedData<T> {
  data: T;
  source: DataSource;
}
⋮----
// ---------------------------------------------------------------------------
// Adapter: convert SWR result to ChartDataResult<T>
// ---------------------------------------------------------------------------
⋮----
function toChartResult<T>(
  swrData: SourcedData<T> | undefined,
  isLoading: boolean,
  error: Error | undefined,
  mutate: () => void,
): ChartDataResult<T>
⋮----
// ---------------------------------------------------------------------------
// Dev-only mock fallback warning
// ---------------------------------------------------------------------------
⋮----
function warnMockFallback(hook: string, detail: Record<string, unknown>)
⋮----
// ---------------------------------------------------------------------------
// Chart data hooks
// ---------------------------------------------------------------------------
⋮----
/**
 * Fetch OHLCV data for a ticker.
 * Tries the backend API first; falls back to mock data on error/404.
 */
export function useOHLCVData(ticker: string): ChartDataResult<OHLCVData[]>
⋮----
/**
 * Fetch OHLCV data for a ticker with a specific period.
 * Used by StockOHLCVChart to support period switching (3M/6M/1Y/2Y/5Y).
 * Tries the backend API first; falls back to mock data on error/404.
 */
export function useStockOHLCV(ticker: string, period: string = '1y'): ChartDataResult<OHLCVData[]>
⋮----
/**
 * Derive a price trend (close prices as LineDataPoint[]) from OHLCV data.
 */
export function usePriceTrend(
  ticker: string,
  days: number = 365,
): ChartDataResult<LineDataPoint[]>
⋮----
/**
 * Fetch TASI index OHLCV data for candlestick charts.
 * Falls back to mock data on error.
 */
export function useTasiOHLCV(period: string = '1y'): ChartDataResult<OHLCVData[]>
⋮----
/**
 * Fetch TASI index data from the real backend API.
 * Falls back to mock data on error.
 */
export function useMarketIndex(period: string = '1y'): ChartDataResult<LineDataPoint[]>
⋮----
/**
 * Returns last 30 days of close prices as LineDataPoint[].
 * Optimized for sparklines (minimal data).
 */
export function useMiniChartData(ticker: string): ChartDataResult<LineDataPoint[]>
⋮----
// Request only 1 month of data to reduce payload ~90% for sparklines
</file>

<file path="frontend/tailwind.config.ts">
import type { Config } from 'tailwindcss';
import { tailwindTokens } from './src/styles/design-system';
</file>

<file path="middleware/rate_limit.py">
"""
In-memory sliding window rate limiter middleware.

Tracks requests per IP address using a dict of timestamps.
Returns 429 Too Many Requests when the limit is exceeded.

Supports path-based tiered limits via ``path_limits``: a dict mapping
path prefixes to per-minute limits.  When a request matches a prefix,
it is tracked in a separate bucket with the specified limit.  Requests
that do not match any prefix use the default ``requests_per_minute``.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# Cleanup stale entries every N requests
_CLEANUP_INTERVAL = 500
⋮----
class RateLimitMiddleware(BaseHTTPMiddleware)
⋮----
"""Per-IP sliding window rate limiter with optional path-based tiers.

    Parameters
    ----------
    app : ASGIApp
        The ASGI application.
    requests_per_minute : int
        Maximum requests allowed per IP per minute (default 60).
    skip_paths : list[str]
        Paths that bypass rate limiting (e.g. ["/health"]).
    path_limits : dict[str, int] | None
        Mapping of path prefixes to per-minute limits.
        Example: ``{"/api/auth": 10, "/api/v1/charts": 30}``
        The **longest matching prefix** wins.  If no prefix matches,
        the default ``requests_per_minute`` is used.
    """
⋮----
# Sort path_limits by prefix length descending for longest-match-first
⋮----
# (IP, bucket) -> deque of request timestamps
⋮----
def _resolve_limit(self, path: str) -> tuple[str, int]
⋮----
"""Return (bucket_key, limit) for the given path.

        Returns the longest matching prefix from ``path_limits``, or
        ``("_default", self.requests_per_minute)`` if no prefix matches.
        """
⋮----
async def dispatch(self, request: Request, call_next)
⋮----
path = request.url.path
⋮----
client_ip = request.client.host if request.client else "unknown"
now = time.monotonic()
⋮----
# Periodic cleanup of stale entries
⋮----
key = f"{client_ip}:{bucket}"
⋮----
# Sliding window: remove timestamps older than window
timestamps = self._requests[key]
cutoff = now - self.window_seconds
# Remove expired entries from the front (O(1) with deque)
⋮----
# Calculate Retry-After: time until the oldest request expires
retry_after = int(timestamps[0] - cutoff) + 1
request_id = getattr(request.state, "request_id", "unknown")
⋮----
def _cleanup(self, now: float) -> None
⋮----
"""Remove keys with no recent requests."""
⋮----
stale_keys = [
</file>

<file path="middleware/request_logging.py">
"""
Request logging middleware.

Logs method, path, status code, duration, client IP (anonymized), and
request_id for each request as structured JSON-compatible log records.
Uses Python logging with extra fields compatible with config/logging_config.py.
"""
⋮----
logger = logging.getLogger("tasi.access")
⋮----
# Default paths to skip (health, docs, static assets)
_DEFAULT_SKIP_PATHS = {"/health", "/favicon.ico", "/docs", "/redoc", "/openapi.json"}
⋮----
def _anonymize_ip(ip: str) -> str
⋮----
"""Anonymize the last octet of an IPv4 address, or last segment of IPv6."""
⋮----
# IPv4: replace last octet
parts = ip.rsplit(".", 1)
⋮----
# IPv6: replace last segment
parts = ip.rsplit(":", 1)
⋮----
class RequestLoggingMiddleware(BaseHTTPMiddleware)
⋮----
"""Logs HTTP requests with timing, request_id, and anonymized client IP.

    Parameters
    ----------
    app : ASGIApp
        The ASGI application.
    skip_paths : list[str]
        Paths to exclude from logging (e.g. ["/health", "/favicon.ico"]).
        Merged with a default set that includes /docs, /redoc, /openapi.json.
    """
⋮----
def __init__(self, app, skip_paths: List[str] | None = None) -> None
⋮----
async def dispatch(self, request: Request, call_next)
⋮----
path = request.url.path
⋮----
raw_ip = request.client.host if request.client else "unknown"
client_ip = _anonymize_ip(raw_ip)
request_id = getattr(request.state, "request_id", "unknown")
start = time.perf_counter()
⋮----
response = await call_next(request)
⋮----
duration_ms = (time.perf_counter() - start) * 1000
status_code = response.status_code
⋮----
# Structured log data (JSON-compatible)
log_data = {
⋮----
msg = json.dumps(log_data, ensure_ascii=False)
</file>

<file path="services/announcement_service.py">
"""
Announcement Service
====================
CRUD operations for the announcements table. Provides methods to store,
retrieve, and filter CMA/Tadawul announcements by ticker, sector, category,
date range, and materiality.

Accepts a ``get_conn`` callable at init. When the connection pool is active,
use ``database.pool.get_pool_connection`` as the callable -- pool-returned
connections auto-return to the pool on ``close()``.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Data class
⋮----
@dataclass
class Announcement
⋮----
"""Mirrors the announcements table in database/schema.sql."""
⋮----
id: str = field(default_factory=lambda: str(uuid.uuid4()))
ticker: Optional[str] = None
title_ar: Optional[str] = None
title_en: Optional[str] = None
body_ar: Optional[str] = None
body_en: Optional[str] = None
source: Optional[str] = None  # 'CMA', 'Tadawul'
announcement_date: Optional[datetime] = None
category: Optional[str] = None
classification: Optional[str] = None
is_material: bool = False
embedding_flag: bool = False
source_url: Optional[str] = None
created_at: Optional[datetime] = None
⋮----
def to_dict(self) -> Dict[str, Any]
⋮----
"""Convert to dict suitable for database insertion."""
d = asdict(self)
# Remove created_at so the DB default (NOW()) applies
⋮----
# Service
⋮----
class AnnouncementService
⋮----
"""Service layer for the announcements table.

    Parameters
    ----------
    get_conn : callable
        A zero-argument callable that returns a psycopg2 connection.
        The service calls ``conn.close()`` after each operation.
    """
⋮----
def __init__(self, get_conn)
⋮----
# -- helpers -------------------------------------------------------------
⋮----
def _conn(self)
⋮----
@staticmethod
    def _row_to_announcement(row: Dict[str, Any]) -> Announcement
⋮----
# -- public API ----------------------------------------------------------
⋮----
def store_announcements(self, announcements: List[Announcement]) -> int
⋮----
"""Insert one or more announcements. Returns the number of rows submitted.

        Duplicates (same id) are silently skipped via ON CONFLICT DO NOTHING.
        """
⋮----
sql = """
⋮----
conn = self._conn()
⋮----
"""Return announcements with optional filters, newest first."""
clauses: List[str] = []
params: Dict[str, Any] = {"limit": limit, "offset": offset}
⋮----
where = ("WHERE " + " AND ".join(clauses)) if clauses else ""
⋮----
sql = f"""
⋮----
"""Return only material announcements (is_material = TRUE), newest first."""
clauses = ["a.is_material = TRUE"]
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
"""Return announcements for all companies in a sector, newest first."""
clauses = ["c.sector ILIKE %(sector)s"]
params: Dict[str, Any] = {
⋮----
def get_announcement_by_id(self, announcement_id: str) -> Optional[Announcement]
⋮----
"""Return a single announcement by its UUID, or None if not found."""
sql = "SELECT * FROM announcements WHERE id = %(id)s"
⋮----
row = cur.fetchone()
⋮----
"""Return total announcement count with optional filters."""
⋮----
params: Dict[str, Any] = {}
⋮----
sql = f"SELECT COUNT(*) FROM announcements a {where}"
</file>

<file path="services/user_service.py">
"""
User Service
=============
CRUD operations for users, user_watchlists, and user_alerts tables.
Provides user management, watchlist management, and alert management.

Requires a psycopg2 connection factory passed at init.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Data classes
⋮----
@dataclass
class UserProfile
⋮----
"""Mirrors the users table in database/schema.sql."""
⋮----
id: str = field(default_factory=lambda: str(uuid.uuid4()))
auth_provider: str = "local"
auth_provider_id: Optional[str] = None
email: str = ""
display_name: Optional[str] = None
avatar_url: Optional[str] = None
subscription_tier: str = "free"
usage_count: int = 0
last_query_at: Optional[datetime] = None
is_active: bool = True
created_at: Optional[datetime] = None
updated_at: Optional[datetime] = None
⋮----
@dataclass
class Watchlist
⋮----
"""Mirrors the user_watchlists table in database/schema.sql."""
⋮----
user_id: str = ""
name: str = "Default"
tickers: List[str] = field(default_factory=list)
⋮----
@dataclass
class UserAlert
⋮----
"""Mirrors the user_alerts table in database/schema.sql."""
⋮----
ticker: str = ""
alert_type: str = ""  # 'price_above', 'price_below', 'volume_spike', 'event'
threshold_value: Optional[float] = None
⋮----
last_triggered_at: Optional[datetime] = None
⋮----
# Service
⋮----
class UserService
⋮----
"""Service layer for users, user_watchlists, and user_alerts tables.

    Parameters
    ----------
    get_conn : callable
        A zero-argument callable that returns a psycopg2 connection.
        The service calls ``conn.close()`` after each operation.
    """
⋮----
def __init__(self, get_conn)
⋮----
# -- helpers -------------------------------------------------------------
⋮----
def _conn(self)
⋮----
@staticmethod
    def _row_to_user(row: Dict[str, Any]) -> UserProfile
⋮----
@staticmethod
    def _row_to_watchlist(row: Dict[str, Any]) -> Watchlist
⋮----
@staticmethod
    def _row_to_alert(row: Dict[str, Any]) -> UserAlert
⋮----
# -----------------------------------------------------------------------
# User methods
⋮----
"""Return an existing user by email, or create a new one.

        Uses INSERT ... ON CONFLICT (email) DO NOTHING then SELECT
        to handle races safely.
        """
user_id = str(uuid.uuid4())
⋮----
sql_insert = """
sql_select = "SELECT * FROM users WHERE email = %(email)s"
⋮----
conn = self._conn()
⋮----
row = cur.fetchone()
⋮----
def get_user_by_id(self, user_id: str) -> Optional[UserProfile]
⋮----
"""Return a user by UUID, or None."""
sql = "SELECT * FROM users WHERE id = %(id)s"
⋮----
def get_user_by_email(self, email: str) -> Optional[UserProfile]
⋮----
"""Return a user by email, or None."""
sql = "SELECT * FROM users WHERE email = %(email)s"
⋮----
def increment_usage(self, user_id: str) -> None
⋮----
"""Increment usage_count and update last_query_at for a user."""
sql = """
⋮----
# Watchlist methods
⋮----
def get_watchlists(self, user_id: str) -> List[Watchlist]
⋮----
"""Return all watchlists for a user."""
⋮----
"""Create a new watchlist. Returns the created watchlist.

        Raises psycopg2.errors.UniqueViolation if (user_id, name) already exists.
        """
wl_id = str(uuid.uuid4())
tickers = tickers or []
⋮----
"""Update a watchlist's name and/or tickers. Returns updated watchlist or None."""
sets: List[str] = ["updated_at = NOW()"]
params: Dict[str, Any] = {"id": watchlist_id, "user_id": user_id}
⋮----
sql = f"""
⋮----
def delete_watchlist(self, watchlist_id: str, user_id: str) -> bool
⋮----
"""Delete a watchlist. Returns True if a row was deleted."""
⋮----
deleted = cur.rowcount > 0
⋮----
# Alert methods
⋮----
"""Create a new alert. Returns the created alert."""
alert_id = str(uuid.uuid4())
⋮----
"""Return all active alerts for a user, optionally filtered by ticker."""
clauses = ["a.user_id = %(user_id)s", "a.is_active = TRUE"]
params: Dict[str, Any] = {"user_id": user_id}
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
def deactivate_alert(self, alert_id: str, user_id: str) -> bool
⋮----
"""Deactivate an alert. Returns True if a row was updated."""
⋮----
updated = cur.rowcount > 0
</file>

<file path="tests/test_middleware.py">
"""
Middleware Tests
================
Tests for CORS, rate limiting, request logging, and error handler middleware.

Uses FastAPI TestClient for integration-style testing of middleware behavior.
"""
⋮----
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# ===========================================================================
# Helper: create a minimal FastAPI app with a test endpoint
⋮----
def _create_test_app()
⋮----
"""Create a minimal FastAPI app for middleware testing."""
app = FastAPI()
⋮----
@app.get("/test")
    async def test_endpoint()
⋮----
@app.get("/health")
    async def health_endpoint()
⋮----
@app.get("/error")
    async def error_endpoint()
⋮----
# CORS middleware tests
⋮----
class TestCORSMiddleware
⋮----
"""Tests for middleware.cors.setup_cors."""
⋮----
def test_cors_headers_present_for_allowed_origin(self)
⋮----
app = _create_test_app()
⋮----
client = TestClient(app)
⋮----
response = client.get("/test", headers={"Origin": "http://localhost:3000"})
⋮----
def test_cors_preflight_request(self)
⋮----
response = client.options(
⋮----
def test_cors_preflight_patch_with_auth_headers(self)
⋮----
def test_cors_allows_credentials(self)
⋮----
# Rate limiting middleware tests
⋮----
class TestRateLimitMiddleware
⋮----
"""Tests for middleware.rate_limit.RateLimitMiddleware."""
⋮----
def test_requests_within_limit_succeed(self)
⋮----
response = client.get("/test")
⋮----
def test_requests_exceeding_limit_return_429(self)
⋮----
responses = []
⋮----
status_codes = [r.status_code for r in responses]
⋮----
def test_rate_limit_429_response_format(self)
⋮----
client.get("/test")  # First request OK
response = client.get("/test")  # Should be rate limited
⋮----
data = response.json()
⋮----
def test_skip_paths_bypass_rate_limit(self)
⋮----
# First request to /test uses the limit
⋮----
# /health should bypass rate limiting
⋮----
response = client.get("/health")
⋮----
# Request logging middleware tests
⋮----
class TestRequestLoggingMiddleware
⋮----
"""Tests for middleware.request_logging.RequestLoggingMiddleware."""
⋮----
def test_requests_are_logged(self, caplog)
⋮----
def test_log_contains_method_and_status(self, caplog)
⋮----
log_messages = [r.message for r in caplog.records]
matching = [m for m in log_messages if "GET" in m and "200" in m]
⋮----
def test_log_contains_duration(self, caplog)
⋮----
matching = [m for m in log_messages if "ms" in m]
⋮----
def test_skip_paths_not_logged(self, caplog)
⋮----
health_logs = [r for r in caplog.records if "/health" in r.message]
⋮----
# Error handler middleware tests
⋮----
class TestErrorHandlerMiddleware
⋮----
"""Tests for middleware.error_handler.ErrorHandlerMiddleware."""
⋮----
def test_unhandled_exception_returns_500_json(self)
⋮----
client = TestClient(app, raise_server_exceptions=False)
⋮----
response = client.get("/error")
⋮----
def test_normal_requests_pass_through(self)
⋮----
def test_error_handler_does_not_expose_traceback(self)
⋮----
body = response.text
</file>

<file path="tests/test_news_feed_api.py">
"""
News Feed API Tests
====================
Tests for api/routes/news_feed.py endpoints using FastAPI's TestClient.
Uses a temporary SQLite database to isolate test state.
"""
⋮----
# Ensure project root is on sys.path
⋮----
a = {
⋮----
class NewsFeedAPITestCase(unittest.TestCase)
⋮----
"""Base class that sets up a test FastAPI app with a temp DB."""
⋮----
def setUp(self)
⋮----
# Patch get_store to use our temp store
⋮----
def tearDown(self)
⋮----
class TestGetNewsFeed(NewsFeedAPITestCase)
⋮----
"""GET /api/v1/news/feed"""
⋮----
def test_returns_200(self)
⋮----
resp = self.client.get("/api/v1/news/feed")
⋮----
def test_response_format(self)
⋮----
data = resp.json()
⋮----
def test_pagination_limit(self)
⋮----
resp = self.client.get("/api/v1/news/feed?limit=3")
⋮----
def test_pagination_offset(self)
⋮----
resp = self.client.get("/api/v1/news/feed?limit=100&offset=2")
⋮----
def test_source_filter(self)
⋮----
resp = self.client.get("/api/v1/news/feed?source=العربية")
⋮----
def test_items_have_required_fields(self)
⋮----
item = resp.json()["items"][0]
⋮----
class TestGetArticleById(NewsFeedAPITestCase)
⋮----
"""GET /api/v1/news/feed/{article_id}"""
⋮----
def test_returns_article(self)
⋮----
resp = self.client.get("/api/v1/news/feed/feed-test-1")
⋮----
def test_nonexistent_returns_404(self)
⋮----
resp = self.client.get("/api/v1/news/feed/nonexistent-id")
⋮----
class TestGetSources(NewsFeedAPITestCase)
⋮----
"""GET /api/v1/news/sources"""
⋮----
def test_returns_sources(self)
⋮----
resp = self.client.get("/api/v1/news/sources")
⋮----
names = [s["source_name"] for s in data["sources"]]
⋮----
def test_empty_db_returns_empty_sources(self)
⋮----
class TestSearchEndpoint(NewsFeedAPITestCase)
⋮----
"""GET /api/v1/news/search"""
⋮----
def test_search_returns_results(self)
⋮----
resp = self.client.get("/api/v1/news/search?q=أرامكو")
⋮----
def test_search_requires_query(self)
⋮----
resp = self.client.get("/api/v1/news/search")
# FastAPI returns 422 for missing required query param
</file>

<file path="tests/test_news_store.py">
"""
News Store Tests
=================
Tests for services/news_store.py (SQLite-backed news storage).
Uses temporary databases via tempfile to avoid polluting real data.
"""
⋮----
# Ensure project root is on sys.path
⋮----
"""Build a test article dict."""
a = {
⋮----
class TestTableCreation(unittest.TestCase)
⋮----
"""Creating a NewsStore on a new DB file creates the table."""
⋮----
def test_table_created(self)
⋮----
db_path = f.name
⋮----
conn = sqlite3.connect(db_path)
cursor = conn.execute(
row = cursor.fetchone()
⋮----
def test_indexes_created(self)
⋮----
rows = conn.execute(
⋮----
# Expect at least 3 indexes
⋮----
class TestStoreArticles(unittest.TestCase)
⋮----
"""store_articles inserts valid data and returns correct count."""
⋮----
def setUp(self)
⋮----
def tearDown(self)
⋮----
def test_store_returns_inserted_count(self)
⋮----
articles = [_make_article(title="خبر 1"), _make_article(title="خبر 2")]
count = self.store.store_articles(articles)
⋮----
def test_store_empty_list(self)
⋮----
count = self.store.store_articles([])
⋮----
def test_duplicate_prevention_same_title_and_source(self)
⋮----
a = _make_article(title="خبر مكرر", source_name="العربية")
⋮----
count = self.store.store_articles([a])
⋮----
def test_same_title_different_source_allowed(self)
⋮----
a1 = _make_article(title="خبر مشترك", source_name="العربية")
a2 = _make_article(title="خبر مشترك", source_name="أرقام")
⋮----
count = self.store.store_articles([a2])
⋮----
class TestGetLatestNews(unittest.TestCase)
⋮----
"""get_latest_news returns articles with correct ordering and filtering."""
⋮----
def _seed(self, n=5)
⋮----
articles = [
⋮----
def test_returns_list(self)
⋮----
result = self.store.get_latest_news()
⋮----
def test_ordered_by_priority(self)
⋮----
result = self.store.get_latest_news(limit=10)
priorities = [a["priority"] for a in result]
⋮----
def test_limit_parameter(self)
⋮----
result = self.store.get_latest_news(limit=3)
⋮----
def test_offset_parameter(self)
⋮----
all_articles = self.store.get_latest_news(limit=100)
offset_articles = self.store.get_latest_news(limit=100, offset=2)
⋮----
def test_source_filter(self)
⋮----
result = self.store.get_latest_news(source="العربية")
⋮----
def test_source_filter_no_match(self)
⋮----
result = self.store.get_latest_news(source="مصدر غير موجود")
⋮----
class TestGetArticleById(unittest.TestCase)
⋮----
"""get_article_by_id retrieves single articles."""
⋮----
def test_returns_correct_article(self)
⋮----
a = _make_article(title="خبر محدد", id="test-id-1")
⋮----
result = self.store.get_article_by_id("test-id-1")
⋮----
def test_nonexistent_id_returns_none(self)
⋮----
result = self.store.get_article_by_id("nonexistent-id")
⋮----
class TestCountArticles(unittest.TestCase)
⋮----
"""count_articles returns correct totals."""
⋮----
def test_count_total(self)
⋮----
articles = [_make_article(title=f"خبر {i}") for i in range(5)]
⋮----
def test_count_by_source(self)
⋮----
def test_count_empty_db(self)
⋮----
class TestGetSources(unittest.TestCase)
⋮----
"""get_sources returns source names with counts."""
⋮----
def test_returns_sources(self)
⋮----
sources = self.store.get_sources()
⋮----
names = [s["source_name"] for s in sources]
⋮----
def test_counts_are_correct(self)
⋮----
src_dict = {s["source_name"]: s["count"] for s in sources}
⋮----
def test_empty_db_returns_empty(self)
⋮----
class TestCleanupOld(unittest.TestCase)
⋮----
"""cleanup_old removes articles older than N days."""
⋮----
def test_cleanup_removes_old_articles(self)
⋮----
# Insert an article, then manually backdate it
⋮----
old_date = (datetime.utcnow() - timedelta(days=30)).isoformat()
conn = sqlite3.connect(self.db_path)
⋮----
deleted = self.store.cleanup_old(days=7)
⋮----
def test_cleanup_keeps_recent_articles(self)
⋮----
class TestSearchArticles(unittest.TestCase)
⋮----
"""search_articles finds articles by title/body text."""
⋮----
def test_search_by_title(self)
⋮----
result = self.store.search_articles("أرامكو")
⋮----
def test_search_no_match(self)
⋮----
result = self.store.search_articles("nonexistent")
</file>

<file path="api/routes/announcements.py">
"""
Announcements API routes.

Provides read endpoints (public) and a write endpoint (authenticated).
"""
⋮----
# Backward-compatible alias used by tests/test_api_routes.py
class AnnouncementListResponse(BaseModel)
⋮----
"""Legacy response model kept for backward compatibility."""
⋮----
items: List[AnnouncementResponse]
count: int
⋮----
router = APIRouter(prefix="/api/announcements", tags=["announcements"])
⋮----
def _to_response(a: Announcement) -> AnnouncementResponse
⋮----
# ---------------------------------------------------------------------------
# Read endpoints (public)
⋮----
"""Return announcements with optional filters."""
items = await asyncio.to_thread(
total = await asyncio.to_thread(svc.count_announcements, ticker=ticker)
⋮----
"""Return only material announcements."""
⋮----
total = await asyncio.to_thread(
⋮----
"""Return announcements for all companies in a sector."""
⋮----
total = await asyncio.to_thread(svc.count_announcements)
⋮----
"""Return a single announcement by ID."""
item = await asyncio.to_thread(svc.get_announcement_by_id, announcement_id)
⋮----
# Write endpoints (authenticated)
⋮----
"""Create a new announcement. Requires authentication."""
announcement = Announcement(
</file>

<file path="api/routes/news_feed.py">
"""
News Feed API routes (SQLite-backed).

Provides endpoints for the live news feed aggregated by the news scrapers.
Works with any database backend (uses its own SQLite store for news_articles).
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/news", tags=["news-feed"])
⋮----
# ---------------------------------------------------------------------------
# Singleton store -- uses the same directory as the main database
⋮----
_HERE = Path(__file__).resolve().parent.parent.parent
_DB_PATH = str(_HERE / "saudi_stocks.db")
_store = NewsStore(_DB_PATH)
⋮----
def get_store() -> NewsStore
⋮----
"""Return the module-level NewsStore singleton."""
⋮----
# Response models
⋮----
class NewsArticle(BaseModel)
⋮----
id: str
ticker: Optional[str] = None
title: str
body: Optional[str] = None
source_name: Optional[str] = None
source_url: Optional[str] = None
published_at: Optional[str] = None
sentiment_score: Optional[float] = None
sentiment_label: Optional[str] = None
language: str = "ar"
priority: int = 3
created_at: Optional[str] = None
⋮----
class NewsFeedResponse(BaseModel)
⋮----
items: List[NewsArticle]
total: int
page: int
limit: int
⋮----
class NewsSourceInfo(BaseModel)
⋮----
source_name: str
count: int
⋮----
class NewsSourcesResponse(BaseModel)
⋮----
sources: List[NewsSourceInfo]
⋮----
# Routes
⋮----
"""Get latest news articles with optional filtering and pagination."""
store = get_store()
articles = await store.aget_latest_news(
total = await store.acount_articles(
page = (offset // limit) + 1 if limit > 0 else 1
⋮----
"""Get multiple articles by their IDs."""
id_list = [i.strip() for i in ids.split(",") if i.strip()]
⋮----
articles = await store.aget_articles_by_ids(id_list)
⋮----
@router.get("/feed/{article_id}", response_model=NewsArticle, responses=STANDARD_ERRORS)
async def get_article(article_id: str) -> NewsArticle
⋮----
"""Get a single article by ID."""
⋮----
article = await store.aget_article_by_id(article_id)
⋮----
"""Search articles by title or body text with optional filters."""
⋮----
articles = await store.asearch_articles(
total = await store.acount_search(
⋮----
@router.get("/sources", response_model=NewsSourcesResponse, responses=STANDARD_ERRORS)
async def get_sources() -> NewsSourcesResponse
⋮----
"""Get available news sources with article counts."""
⋮----
sources = await store.aget_sources()
</file>

<file path="api/routes/reports.py">
"""
Technical reports API routes.

Provides read endpoints (public) and a write endpoint (authenticated).
"""
⋮----
# Backward-compatible alias used by tests/test_api_routes.py
class ReportListResponse(BaseModel)
⋮----
"""Legacy response model kept for backward compatibility."""
⋮----
items: List[ReportResponse]
count: int
⋮----
router = APIRouter(prefix="/api/reports", tags=["reports"])
⋮----
def _to_response(r: TechnicalReport) -> ReportResponse
⋮----
# ---------------------------------------------------------------------------
# Read endpoints (public)
⋮----
"""Return the latest technical reports."""
reports = await asyncio.to_thread(
total = await asyncio.to_thread(svc.count_reports, recommendation=recommendation)
⋮----
"""Return technical reports for a specific ticker."""
ticker = validate_ticker(ticker)
⋮----
total = await asyncio.to_thread(
⋮----
"""Return a single technical report by ID."""
report = await asyncio.to_thread(svc.get_report_by_id, report_id)
⋮----
# Write endpoints (authenticated)
⋮----
"""Create a new technical report. Requires authentication."""
report = TechnicalReport(
</file>

<file path="api/routes/tasi_index.py">
"""
TASI Index chart data API route.

Provides OHLCV data for the Tadawul All Share Index (^TASI) via yfinance
with in-memory caching and mock fallback. Works with both SQLite and
PostgreSQL backends (no database dependency).
"""
⋮----
router = APIRouter(prefix="/api/v1/charts/tasi", tags=["tasi-index"])
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class TASIOHLCVPoint(BaseModel)
⋮----
time: str
open: float
high: float
low: float
close: float
volume: int
⋮----
class TASIIndexResponse(BaseModel)
⋮----
data: List[TASIOHLCVPoint]
source: Literal["real", "mock", "cached"]
data_freshness: Literal["real-time", "cached", "stale", "mock"] = "real-time"
cache_age_seconds: Optional[int] = None
last_updated: str
symbol: str
period: str
count: int
⋮----
# Route
⋮----
"""Return TASI index OHLCV data for TradingView chart rendering.

    Fetches live data from Yahoo Finance (^TASI) with 5-minute caching.
    Falls back to deterministic mock data if yfinance is unavailable.
    """
⋮----
result = await asyncio.to_thread(fetch_tasi_index, period=period)
⋮----
# Health check
⋮----
logger = logging.getLogger(__name__)
⋮----
class TASIHealthResponse(BaseModel)
⋮----
status: Literal["ok", "degraded"]
message: str
⋮----
@router.get("/health", response_model=TASIHealthResponse)
async def tasi_health() -> TASIHealthResponse
⋮----
"""Return health status for the TASI data pipeline.

    Returns only aggregate status and a human-readable message.
    Internal diagnostics (cache state, circuit breaker, dependency
    availability) are logged server-side but not exposed to clients
    to avoid leaking infrastructure details.
    """
yfinance_available = True
⋮----
import yfinance as yf  # noqa: F401
⋮----
yfinance_available = False
⋮----
cache_info = get_cache_status()
cache_status = cache_info["cache_status"]
cb_info = get_circuit_breaker_status()
⋮----
# Degraded when circuit is open, or cache is not fresh AND yfinance is unavailable
⋮----
status = "degraded"
message = "Data source temporarily unavailable; serving cached data."
⋮----
status = "ok"
message = "TASI data pipeline operating normally."
⋮----
# Full diagnostics logged server-side only
</file>

<file path="auth/dependencies.py">
"""
FastAPI dependency functions for authentication.

Provides injectable dependencies for route handlers:
- get_current_user: extracts and validates the Bearer token, returns user dict
- require_admin: ensures the current user has admin-level access
- get_optional_current_user: like get_current_user but returns None when no token
"""
⋮----
_bearer_scheme = HTTPBearer()
_bearer_scheme_optional = HTTPBearer(auto_error=False)
⋮----
# Tiers that grant admin-level access.  Override via ADMIN_TIERS env var
# (comma-separated) or by importing and reassigning this module attribute.
ADMIN_TIERS: FrozenSet[str] = frozenset({"enterprise", "admin"})
⋮----
"""Extract and validate the access token if present.

    Returns the user dict if a valid token is provided, or None if no
    Authorization header is present.  Used by read endpoints that should
    degrade gracefully for unauthenticated callers.
    """
⋮----
# Delegate to the strict version; let its HTTPExceptions propagate
# (expired / invalid tokens should still be rejected).
⋮----
def _fetch_user_row(user_id: str)
⋮----
"""Synchronous DB lookup -- runs in a thread via asyncio.to_thread()."""
conn = get_db_connection()
⋮----
"""Extract and validate the access token from the Authorization header.

    Returns a dict with user fields from the database:
    id, email, display_name, subscription_tier, usage_count, is_active.

    Raises HTTPException 401 if the token is missing, expired, or invalid,
    or if the user account is deactivated.
    """
token = credentials.credentials
⋮----
payload = decode_token(token, expected_type="access")
⋮----
user_id = payload.get("sub")
⋮----
# Fetch user from database in a thread to avoid blocking the event loop
row = await asyncio.to_thread(_fetch_user_row, user_id)
⋮----
user = {
⋮----
"""Ensure the current user has admin-level access.

    Admin access is granted when the user's ``subscription_tier`` is in
    :data:`ADMIN_TIERS` (default: ``{"enterprise", "admin"}``).  To
    customise allowed tiers, update the module-level ``ADMIN_TIERS``
    constant or set the ``ADMIN_TIERS`` env var (comma-separated).

    This dependency can be injected on any route that should be restricted
    to admin users::

        @router.get("/admin/users", dependencies=[Depends(require_admin)])
        async def list_users(): ...

    Returns the user dict if authorized; raises HTTPException 403 otherwise.
    """
tier = current_user.get("subscription_tier", "")
</file>

<file path="frontend/src/app/news/components/ArticleCard.tsx">
import React, { useState, useRef, useCallback, useEffect } from 'react';
import Link from 'next/link';
import { useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { getNewsArticle } from '@/lib/api-client';
import { useLanguage } from '@/providers/LanguageProvider';
import { getSourceColor, timeAgo, readingTime } from '../utils';
⋮----
// ---------------------------------------------------------------------------
// Sub-components
// ---------------------------------------------------------------------------
⋮----
onClick=
⋮----
e.preventDefault();
e.stopPropagation();
onToggle(id);
⋮----
// ---------------------------------------------------------------------------
// ArticleCard
// ---------------------------------------------------------------------------
⋮----
const handleCardClick = () =>
⋮----
className=
⋮----
{/* Main content */}
⋮----
{/* Sentiment badge + bookmark row */}
⋮----
{/* Title */}
⋮----
<h3 className=
⋮----
{/* Body */}
⋮----
setExpanded((v)
⋮----
{/* Footer */}
⋮----

⋮----
{/* Chevron hint — the whole card is clickable */}
⋮----
{/* Source icon on the left side (appears on right in RTL) */}
</file>

<file path="frontend/src/components/charts/StockOHLCVChart.tsx">
import { useEffect, useRef, useState, useCallback, useMemo } from 'react';
import { cn } from '@/lib/utils';
import {
  createChart,
  ColorType,
  type IChartApi,
  type ISeriesApi,
  type CandlestickData,
  type HistogramData,
  type LineData,
  type Time,
} from 'lightweight-charts';
import {
  RAID_CHART_OPTIONS,
  VOLUME_UP_COLOR,
  VOLUME_DOWN_COLOR,
  MA20_COLOR,
  MA50_COLOR,
  AREA_TOP_COLOR,
  AREA_BOTTOM_COLOR,
  LINE_COLOR,
} from './chart-config';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';
import { DataSourceBadge } from './DataSourceBadge';
import { formatVolume } from '@/lib/formatters';
import { useStockOHLCV } from '@/lib/hooks/use-chart-data';
import type { OHLCVData } from './chart-types';
import dynamic from 'next/dynamic';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Period options
// ---------------------------------------------------------------------------
⋮----
type ChartType = 'candlestick' | 'line' | 'area';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function calculateMA(data: OHLCVData[], period: number): LineData[]
⋮----
function exportCSV(data: OHLCVData[], ticker: string, period: string)
⋮----
// ---------------------------------------------------------------------------
// Props
// ---------------------------------------------------------------------------
⋮----
interface StockOHLCVChartProps {
  ticker: string;
  stockName?: string;
  height?: number;
  className?: string;
}
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Auto-refresh every 5 min during Tadawul trading hours (Sun-Thu, 10:00-15:00 AST/UTC+3)
⋮----
function isTradingHours(): boolean
⋮----
const utcDay = now.getUTCDay(); // 0=Sun
⋮----
// Period % change
⋮----
// Memoize MA series data — only recomputes when data or toggle changes
⋮----
// Responsive chart height
⋮----
function handleResize()
⋮----
// Build chart
⋮----
// Candlestick series
⋮----
// Line series (hidden initially)
⋮----
// Area series (hidden initially)
⋮----
// Volume histogram
⋮----
// MA20 line
⋮----
// MA50 line
⋮----
// Crosshair tooltip
⋮----
// ResizeObserver
⋮----
// Create / destroy
⋮----
}, [loading, data && data.length > 0, buildChart]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
// Update height on resize
⋮----
// Update data + chart type visibility
⋮----
// Set data on all series
⋮----
// Toggle visibility based on chart type
⋮----
// MA lines (pre-computed by useMemo)
⋮----
// PNG download
⋮----
// CSV export
⋮----
// Loading state
⋮----
// Error state
⋮----
// Empty state
⋮----
{/* Toolbar */}
⋮----
{/* Left: Title + source badge + period change */}
⋮----
<DataSourceBadge source=
⋮----
className=
⋮----
{/* Right: Controls */}
⋮----
{/* MA toggles */}
⋮----
onClick=
⋮----
aria-label=
⋮----
{/* Chart type toggle */}
⋮----
{/* Export buttons */}
⋮----
{/* Separator */}
⋮----
{/* Period pill selector */}
⋮----
{/* Crosshair tooltip bar */}
⋮----
{/* Chart container */}
⋮----
// SSR-safe dynamic wrapper
</file>

<file path="frontend/src/components/common/MobileBottomNav.tsx">
import { useState, useEffect } from 'react';
import Link from 'next/link';
import { usePathname } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
// ---------------------------------------------------------------------------
// Nav items
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Detect virtual keyboard by watching viewport height changes
⋮----
const handleResize = () =>
⋮----
// If viewport height is significantly smaller than window height, keyboard is open
⋮----
const isActive = (href: string)
⋮----
className=
⋮----
<span className=
</file>

<file path="frontend/src/components/layout/Footer.tsx">
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
{/* Top row: links */}
⋮----
{/* Divider */}
⋮----
{/* Bottom row: copyright + attribution */}
</file>

<file path="frontend/src/lib/hooks/use-auth.tsx">
import {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useRef,
  useState,
  type ReactNode,
} from 'react';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
export interface User {
  id: string;
  email: string;
  name: string;
  /** Whether this is a guest session (no DB user). */
  isGuest: boolean;
  /** Subscription tier from /api/auth/me (null until profile is fetched). */
  subscriptionTier?: string | null;
}
⋮----
/** Whether this is a guest session (no DB user). */
⋮----
/** Subscription tier from /api/auth/me (null until profile is fetched). */
⋮----
interface AuthContextValue {
  user: User | null;
  loading: boolean;
  login: (email: string, password: string) => Promise<void>;
  register: (email: string, password: string, name: string) => Promise<void>;
  guestLogin: () => Promise<void>;
  logout: () => void;
}
⋮----
/** Token refresh interval: 4 minutes (tokens typically expire in 15-60 min). */
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
/**
 * Decode the payload section of a JWT (base64url) without verifying the
 * signature.  We only need the claims (`sub`, `email`, `exp`) that the
 * backend embeds so we can populate the local User object immediately
 * after login without an extra round-trip to /api/auth/me.
 */
function decodeJwtPayload(token: string): Record<string, unknown>
⋮----
// base64url -> base64 -> decode
⋮----
/** Returns true if the JWT will expire within the given number of seconds. */
function isTokenExpiringSoon(token: string, withinSeconds: number = 120): boolean
⋮----
return true; // If we cannot decode, treat it as expiring
⋮----
type AuthApiResponse = {
  token?: string;
  access_token?: string;
  refresh_token?: string;
  user_id?: string;
  name?: string;
};
⋮----
type TokenRefreshResponse = {
  access_token?: string;
  refresh_token?: string;
};
⋮----
type UserProfileResponse = {
  id: string;
  email: string;
  display_name?: string | null;
  subscription_tier: string;
  usage_count: number;
  is_active: boolean;
  created_at?: string | null;
};
⋮----
function extractAccessToken(data: AuthApiResponse): string
⋮----
// ---------------------------------------------------------------------------
// Context
// ---------------------------------------------------------------------------
⋮----
export function AuthProvider(
⋮----
// Hydrate from localStorage on mount
⋮----
// Backward compat: ensure isGuest field exists
⋮----
// ignore corrupt data
⋮----
// localStorage quota exceeded -- continue with in-memory state
⋮----
// ------------------------------------------------------------------
// Token refresh
// ------------------------------------------------------------------
⋮----
// Only refresh if the access token is expiring within 2 minutes
⋮----
// If refresh fails with 401, session is dead -- log out
⋮----
// localStorage quota -- tokens stay in memory via closure
⋮----
// Network error or aborted -- silent failure, will retry next interval
⋮----
// Set up periodic token refresh when user is logged in
⋮----
// Clean up any existing timer when logged out
⋮----
// Immediate check on login
⋮----
// Periodic refresh
⋮----
// ------------------------------------------------------------------
// Fetch user profile from /api/auth/me (enriches user with tier info)
// ------------------------------------------------------------------
⋮----
// Skip profile fetch for guest users -- /api/auth/me does a DB lookup
// that will fail for guest tokens in SQLite mode.
⋮----
// Profile fetch is best-effort -- fall back to base user
⋮----
// ------------------------------------------------------------------
// Auth actions
// ------------------------------------------------------------------
⋮----
// If claims cannot be decoded, continue with API-provided fields.
⋮----
// Persist immediately so the UI updates, then enrich in the background
⋮----
// Fetch full profile from /api/auth/me to get subscription_tier etc.
⋮----
// Backend accepts both `name` and `display_name`; use the canonical field.
⋮----
// If claims cannot be decoded, continue with API-provided fields.
⋮----
// Fetch full profile from /api/auth/me to get subscription_tier etc.
⋮----
export function useAuth()
</file>

<file path="requirements-dev.txt">
# requirements-dev.txt — Development dependencies (includes prod + test + dev tools)
# Usage: pip install -r requirements-dev.txt
-r requirements.txt
-r requirements-test.txt

# Load testing
locust>=2.20.0,<3.0

# Dependency management
pip-tools>=7.0.0,<8.0
</file>

<file path="services/news_scheduler.py">
"""
News Scheduler
===============
Background scheduler that periodically runs the news scrapers and stores
results into the NewsStore (SQLite).

Usage:
    from services.news_store import NewsStore
    from services.news_scheduler import NewsScheduler

    store = NewsStore("saudi_stocks.db")
    scheduler = NewsScheduler(store)
    scheduler.start()   # non-blocking, runs in a daemon thread
    ...
    scheduler.stop()
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_scraper_cfg = get_settings().scraper
FETCH_INTERVAL_SECONDS = _scraper_cfg.fetch_interval_seconds
⋮----
class NewsScheduler
⋮----
"""Background news fetcher that runs scrapers on a timer."""
⋮----
def __init__(self, store: NewsStore)
⋮----
def start(self) -> None
⋮----
"""Start background thread that fetches news periodically."""
⋮----
def stop(self) -> None
⋮----
"""Stop the scheduler gracefully."""
⋮----
def _run_loop(self) -> None
⋮----
"""Main loop: fetch immediately, then sleep between cycles."""
# Fetch immediately on start
inserted = self._fetch_cycle()
⋮----
# Sleep in small increments so stop() is responsive
⋮----
def get_source_error_counts(self) -> dict[str, int]
⋮----
"""Return per-source error counts for health check output."""
⋮----
def get_stats(self) -> dict
⋮----
"""Return scheduler statistics (run count, last run time, articles stored)."""
⋮----
source_errors = dict(self._source_errors)
⋮----
def _fetch_cycle(self) -> int
⋮----
"""One fetch cycle: run all scrapers, store results, clean old.

        Returns:
            Number of new articles inserted during this cycle.
        """
inserted = 0
⋮----
all_articles: list[dict] = []
cycle_errors: dict[str, int] = {}
⋮----
scraper = scraper_cls()
source = scraper.source_name
⋮----
articles = scraper.fetch_articles()
⋮----
# Update cumulative error counts
⋮----
inserted = self.store.store_articles(all_articles)
</file>

<file path="services/reports_service.py">
"""
Technical Reports Service
=========================
CRUD operations for the technical_reports table. Provides methods to store,
retrieve, and filter analyst research reports by ticker, recommendation,
report type, and date range.

Supports both SQLite and PostgreSQL backends. The backend is detected at
runtime from the connection type returned by ``get_conn``.
"""
⋮----
_HAS_PSYCOPG2 = True
⋮----
_HAS_PSYCOPG2 = False
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Data class
⋮----
@dataclass
class TechnicalReport
⋮----
"""Mirrors the technical_reports table in database/schema.sql."""
⋮----
id: str = field(default_factory=lambda: str(uuid.uuid4()))
ticker: Optional[str] = None
title: str = ""
summary: Optional[str] = None
author: Optional[str] = None
source_name: Optional[str] = None
source_url: Optional[str] = None
published_at: Optional[datetime] = None
recommendation: Optional[str] = None
target_price: Optional[float] = None
current_price_at_report: Optional[float] = None
report_type: Optional[str] = None
created_at: Optional[datetime] = None
⋮----
def to_dict(self) -> Dict[str, Any]
⋮----
"""Convert to dict suitable for database insertion."""
d = asdict(self)
# Remove created_at so the DB default applies
⋮----
# Service
⋮----
class TechnicalReportsService
⋮----
"""Service layer for the technical_reports table.

    Parameters
    ----------
    get_conn : callable
        A zero-argument callable that returns a database connection
        (sqlite3.Connection or psycopg2 connection).
        The service calls ``conn.close()`` after each operation.
    """
⋮----
# SQLite CREATE TABLE (run once per connection when table is missing)
_SQLITE_CREATE_TABLE = """
⋮----
def __init__(self, get_conn)
⋮----
# -- helpers -------------------------------------------------------------
⋮----
def _conn(self)
⋮----
conn = self._get_conn()
⋮----
@staticmethod
    def _is_sqlite(conn) -> bool
⋮----
@classmethod
    def _ensure_table(cls, conn: sqlite3.Connection)
⋮----
"""Create the technical_reports table in SQLite if it doesn't exist."""
⋮----
@staticmethod
    def _fetchall(conn, sql: str, params) -> List[Dict[str, Any]]
⋮----
"""Execute SQL and return all rows as dicts, for either backend."""
⋮----
rows = conn.execute(sql, params).fetchall()
⋮----
# PostgreSQL
⋮----
@staticmethod
    def _fetchone(conn, sql: str, params) -> Optional[Dict[str, Any]]
⋮----
"""Execute SQL and return first row as dict, for either backend."""
⋮----
row = conn.execute(sql, params).fetchone()
⋮----
row = cur.fetchone()
⋮----
@staticmethod
    def _scalar(conn, sql: str, params)
⋮----
"""Execute SQL and return a single scalar value."""
⋮----
@staticmethod
    def _execute(conn, sql: str, params)
⋮----
"""Execute a write statement."""
⋮----
@staticmethod
    def _executemany(conn, sql: str, params_list: list)
⋮----
"""Execute a write statement for many rows."""
⋮----
@staticmethod
    def _row_to_report(row: Dict[str, Any]) -> TechnicalReport
⋮----
# -- SQL builders --------------------------------------------------------
⋮----
@staticmethod
    def _like_op(is_sqlite: bool) -> str
⋮----
"""LIKE is case-insensitive in SQLite by default for ASCII; use LIKE
        for SQLite and ILIKE for PostgreSQL."""
⋮----
@staticmethod
    def _nulls_last(is_sqlite: bool) -> str
⋮----
"""SQLite doesn't support NULLS LAST syntax."""
⋮----
@staticmethod
    def _conflict_ignore(is_sqlite: bool) -> str
⋮----
@staticmethod
    def _on_conflict(is_sqlite: bool) -> str
⋮----
def _build_insert_sql(self, is_sqlite: bool) -> str
⋮----
"""Build INSERT SQL for the active backend."""
cols = (
⋮----
placeholders = ", ".join(["?"] * 12)
⋮----
# PostgreSQL — named params
placeholders = (
⋮----
@staticmethod
    def _to_insert_params(report: TechnicalReport, is_sqlite: bool)
⋮----
"""Convert report to params suitable for the backend."""
d = report.to_dict()
⋮----
# -- public API ----------------------------------------------------------
⋮----
def store_report(self, report: TechnicalReport) -> str
⋮----
"""Insert a single report. Returns the report id.

        Duplicates (same id) are silently skipped.
        """
conn = self._conn()
is_sqlite = self._is_sqlite(conn)
sql = self._build_insert_sql(is_sqlite)
params = self._to_insert_params(report, is_sqlite)
⋮----
def store_reports(self, reports: List[TechnicalReport]) -> int
⋮----
"""Bulk insert reports. Returns the number of rows submitted.

        Duplicates (same id) are silently skipped.
        """
⋮----
params_list = [self._to_insert_params(r, is_sqlite) for r in reports]
⋮----
"""Return the most recent reports across all tickers."""
⋮----
like = self._like_op(is_sqlite)
nulls_last = self._nulls_last(is_sqlite)
⋮----
clauses: List[str] = []
⋮----
params: list = []
⋮----
where = ("WHERE " + " AND ".join(clauses)) if clauses else ""
sql = (
⋮----
pg_params: Dict[str, Any] = {"limit": limit, "offset": offset}
⋮----
params = pg_params  # type: ignore[assignment]
⋮----
rows = self._fetchall(conn, sql, params)
⋮----
"""Return reports for a specific ticker, newest first."""
⋮----
clauses = ["r.ticker = ?"]
params: list = [ticker]
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
clauses_pg = ["r.ticker = %(ticker)s"]
pg_params: Dict[str, Any] = {
⋮----
where = "WHERE " + " AND ".join(clauses_pg)
⋮----
def get_report_by_id(self, report_id: str) -> Optional[TechnicalReport]
⋮----
"""Return a single report by its UUID, or None if not found."""
⋮----
sql = "SELECT * FROM technical_reports WHERE id = ?"
params: Any = (report_id,)
⋮----
sql = "SELECT * FROM technical_reports WHERE id = %(id)s"
params = {"id": report_id}
⋮----
row = self._fetchone(conn, sql, params)
⋮----
"""Return total report count with optional ticker/recommendation filter."""
⋮----
sql = f"SELECT COUNT(*) FROM technical_reports r {where}"
⋮----
clauses_pg: List[str] = []
pg_params: Dict[str, Any] = {}
⋮----
where = ("WHERE " + " AND ".join(clauses_pg)) if clauses_pg else ""
</file>

<file path="tests/conftest.py">
"""
Shared pytest fixtures for TASI AI Platform tests.

Provides reusable fixtures for:
- SQLite test databases with sample data
- PostgreSQL live connections (when POSTGRES_HOST is set)
- Mock Redis clients
- JWT auth tokens
- FastAPI TestClient
- Mock PostgreSQL connection pools
"""
⋮----
# Ensure project root is on sys.path for imports
PROJECT_ROOT = Path(__file__).resolve().parent.parent
⋮----
# test_app_assembly_v2.py is a standalone script (not a pytest-compatible module).
# Its top-level `test_result()` function is mistakenly collected as a pytest fixture.
# Excluding it here prevents a confusing collection error; the CI runs it directly
# via `python tests/test_app_assembly_v2.py`.
collect_ignore = [str(Path(__file__).parent / "test_app_assembly_v2.py")]
⋮----
# ---------------------------------------------------------------------------
# SQLite test database
⋮----
@pytest.fixture
def test_db(tmp_path)
⋮----
"""Create a temporary SQLite database with sample TASI data."""
db_path = tmp_path / "test_saudi_stocks.db"
conn = sqlite3.connect(str(db_path))
cursor = conn.cursor()
⋮----
# Create core tables
⋮----
# PostgreSQL availability & fixtures
⋮----
def _pg_available() -> bool
⋮----
"""Check if PostgreSQL is reachable via POSTGRES_HOST env var."""
⋮----
conn = psycopg2.connect(
⋮----
PG_AVAILABLE = _pg_available()
⋮----
@pytest.fixture(scope="session")
def pg_conn()
⋮----
"""Provide a live PostgreSQL connection for integration tests.

    Skips the test automatically when PostgreSQL is not available.
    The connection is shared across all tests in the session and closed at the end.
    """
⋮----
@pytest.fixture
def pg_cursor(pg_conn)
⋮----
"""Provide a PostgreSQL cursor that rolls back after each test.

    Uses SAVEPOINT/ROLLBACK TO isolate test side effects without
    committing data. Requires the ``pg_conn`` fixture.
    """
⋮----
cur = pg_conn.cursor()
⋮----
@pytest.fixture
def pg_conn_factory()
⋮----
"""Provide a factory callable that returns new PostgreSQL connections.

    Useful for services that accept a ``get_conn`` callable.
    Skips when PostgreSQL is not available.
    """
⋮----
connections = []
⋮----
def _factory()
⋮----
# Mock Redis
⋮----
@pytest.fixture
def mock_redis()
⋮----
"""Provide a mock Redis client with basic get/set/delete."""
client = MagicMock()
_store = {}
⋮----
def mock_get(key)
⋮----
def mock_setex(key, ttl, value)
⋮----
def mock_delete(*keys)
⋮----
count = 0
⋮----
def mock_ping()
⋮----
def mock_scan(cursor=0, match=None, count=100)
⋮----
matching = [k for k in _store if fnmatch.fnmatch(k, match or "*")]
⋮----
# Auth token
⋮----
@pytest.fixture
def auth_settings()
⋮----
"""Return a fixed AuthSettings for deterministic JWT testing."""
⋮----
@pytest.fixture
def auth_token(auth_settings)
⋮----
"""Generate a valid access token for testing."""
⋮----
payload = {
token = jwt.encode(
⋮----
@pytest.fixture
def refresh_token(auth_settings)
⋮----
"""Generate a valid refresh token for testing."""
⋮----
# Mock connection pool
⋮----
@pytest.fixture
def mock_pool()
⋮----
"""Mock psycopg2 ThreadedConnectionPool."""
pool = MagicMock()
mock_conn = MagicMock()
mock_cursor = MagicMock()
⋮----
# Mock DB connection (for services/routes that need psycopg2)
⋮----
@pytest.fixture
def mock_db_conn()
⋮----
"""Mock psycopg2 connection with cursor context manager."""
conn = MagicMock()
cursor = MagicMock()
⋮----
# Support `with conn.cursor() as cur:` pattern
⋮----
# PostgreSQL schema sanity check (S3-I7)
⋮----
#
# Design note: the authoritative PostgreSQL schema lives in database/schema.sql.
# In CI the test-pg job initialises the database with:
⋮----
#   psql -h localhost -U tasi_user -d tasi_platform -f database/schema.sql
⋮----
# before pytest is invoked, so PG integration tests do NOT need to run CREATE
# TABLE statements themselves.  The fixture below acts as a fast sanity check
# that the schema was actually applied before any PG test touches the database.
# It is session-scoped, autouse=False, and silently skips when PostgreSQL is
# unavailable (i.e. in the standard SQLite CI job).
⋮----
@pytest.fixture(scope="session", autouse=False)
def pg_schema_version(pg_conn)
⋮----
"""Verify that database/schema.sql was applied before PG tests run.

    This fixture depends on ``pg_conn`` (which already skips when PostgreSQL is
    unreachable) and checks that the ``companies`` table exists as a proxy for
    a fully-initialised schema.  If the table is absent the test session is
    aborted with a clear message pointing at the correct initialisation step.

    Usage — request this fixture in any PG integration test class or module::

        @pytest.mark.usefixtures("pg_schema_version")
        class TestMyPGService:
            ...

    The CI test-pg job guarantees the schema is present by running::

        psql ... -f database/schema.sql

    before invoking pytest, so this fixture should always pass in CI.
    """
</file>

<file path="AGENTS.md">
# AGENTS.md

This file provides guidance for AI coding agents working on this codebase.

## CRITICAL: First Steps for Every Session

Before making ANY changes to this codebase, you MUST:

1. **Read `AGENTS.md`** (this file) in full and follow all instructions.
2. **Read `CLAUDE.md`** if it exists in the project root or any relevant subdirectory. Its instructions are equally binding.
3. **Load the `/vanna` skill** (invoke it via the Skill tool) and read the Vanna 2.0 documentation and best practices it provides. All Vanna-related code must conform to the patterns, APIs, and conventions described in that skill.

These are non-negotiable prerequisites. Do not skip them, even for "small" changes.

## Project Overview

**Ra'd AI** is a TASI Saudi Stock Market AI Platform. It exposes a FastAPI chat interface powered by [Vanna 2.0](https://vanna.ai/) that lets users query a normalized database of ~500 TASI-listed Saudi companies using natural language. The LLM (Claude Sonnet 4.5 via Anthropic API) generates SQL, executes it, and optionally visualizes results with Plotly charts. The platform supports dual database backends (SQLite for development, PostgreSQL for production) and includes news, announcement, and technical report services.

## Tech Stack

- **Language:** Python 3.11+
- **Framework:** Vanna 2.0 (agent framework) + FastAPI (HTTP server)
- **LLM:** Claude Sonnet 4.5 via `AnthropicLlmService`
- **Database:** SQLite (dev) / PostgreSQL 16 (prod), controlled by `DB_BACKEND` env var
- **Configuration:** `pydantic-settings` (`config/settings.py`)
- **Frontend (legacy):** Single-page HTML (`templates/index.html`) using `<vanna-chat>` web component
- **Frontend (production):** Next.js 14 + TypeScript + Tailwind CSS (`frontend/`) — 15 pages, RTL Arabic, SSE news feed
- **Server:** Uvicorn on port 8084
- **Container:** Docker Compose (PostgreSQL + app + optional pgAdmin)
- **Data Pipeline:** pandas + numpy for CSV normalization

## File Ownership

When working in a team, respect file ownership boundaries:

| Owner | Files |
|---|---|
| **database-architect** | `database/` (incl. `queries.py`), `csv_to_sqlite.py`, `ingestion/` |
| **backend-services** | `app.py`, `services/` (except `health_service.py`), `api/models/`, `api/routes/widgets_stream.py` |
| **frontend-dev** | `frontend/` (incl. `src/app/news/components/`, `src/app/news/hooks/`, `src/components/widgets/`, `src/components/common/`), `templates/` |
| **infra-testing** | `config/`, `tests/`, `.github/`, `docker-compose.yml`, `Dockerfile`, `requirements.txt`, `services/health_service.py`, `CLAUDE.md`, `AGENTS.md`, `README.md` |

Do NOT modify files owned by other agents unless coordinating with them.

## File Structure

```
.
├── app.py                          # Vanna 2.0 FastAPI server (dual backend)
├── csv_to_sqlite.py                # CSV -> normalized SQLite
├── config/
│   ├── __init__.py                 # get_settings() singleton
│   ├── settings.py                 # Pydantic Settings classes
│   └── logging.py                  # JSON/pretty log formatters
├── database/
│   ├── schema.sql                  # PostgreSQL DDL (all tables + indexes + views)
│   ├── queries.py                  # Centralized SQL query strings
│   ├── pool.py                     # PostgreSQL ThreadedConnectionPool singleton
│   ├── postgres_utils.py           # Shared PG helpers: pg_available(), pg_connection_params()
│   ├── migrate_sqlite_to_pg.py     # SQLite -> PostgreSQL migration
│   └── csv_to_postgres.py          # CSV -> PostgreSQL pipeline
├── services/
│   ├── health_service.py           # Health checks (DB, LLM)
│   ├── news_store.py               # SQLite news storage (sync + async)
│   ├── news_scraper.py             # 5-source Arabic news scraper
│   ├── news_scheduler.py           # Background news fetch scheduler
│   ├── news_service.py             # News CRUD (PostgreSQL only)
│   ├── reports_service.py          # Technical reports CRUD
│   ├── announcement_service.py     # Announcement CRUD
│   ├── yfinance_base.py            # Shared yfinance cache + circuit breaker
│   ├── cache_utils.py              # Unified @cache_response decorator
│   └── widgets/
│       ├── __init__.py
│       ├── quotes_hub.py           # QuotesHub orchestrator (Redis pub/sub)
│       └── providers/
│           ├── __init__.py
│           ├── crypto.py           # Cryptocurrency quotes
│           ├── metals.py           # Precious metals quotes
│           ├── oil.py              # Oil & energy quotes
│           └── indices.py          # Global market indices
├── api/
│   ├── models/
│   │   └── widgets.py              # QuoteItem Pydantic model
│   ├── routes/                     # FastAPI async route handlers
│   │   ├── widgets_stream.py       # /api/v1/widgets/stream (SSE)
│   │   └── ...
│   └── db_helper.py                # Async DB wrappers
├── frontend/                       # Next.js 14 app (production)
│   ├── scripts/
│   │   └── lint-rtl.js             # RTL direction class linter
│   ├── src/
│   │   ├── components/
│   │   │   ├── widgets/
│   │   │   │   └── LiveMarketWidgets.tsx  # SSE-powered market ticker
│   │   │   └── common/
│   │   │       └── ConnectionStatusBadge.tsx  # Live/offline indicator
│   │   └── app/
│   │       ├── news/               # News module (decomposed)
│   │       │   ├── components/     # ArticleCard, FilterBar, etc.
│   │       │   ├── hooks/          # useNewsFilters
│   │       │   └── utils.ts        # Shared constants & helpers
│   │       ├── */loading.tsx       # Route loading states (news, market, charts, chat)
│   │       └── */error.tsx         # Route error boundaries (news, market, charts, chat)
├── middleware/
│   ├── chat_auth.py                # ChatAuthMiddleware (JWT on chat endpoints, PG only)
│   ├── request_context.py          # ContextVar request ID + RequestIdFilter
│   ├── error_handler.py            # Unified JSON error responses
│   └── rate_limit.py               # Tiered rate limiting
├── tests/
│   ├── conftest.py                 # Shared fixtures (DB connections, pg_schema_version)
│   ├── test_database.py            # 23 DB integrity tests (moved from root)
│   ├── test_app_assembly_v2.py     # 33 Vanna assembly tests (moved from root)
│   └── ...                         # Unit/integration/security/performance tests
├── templates/index.html            # Legacy vanna-chat UI
├── docker-compose.yml              # PostgreSQL + app + pgAdmin
├── Dockerfile                      # Python 3.11 container
├── requirements.in                 # Unpinned source constraints
├── requirements.txt                # Pinned production deps (generated)
├── requirements-dev.txt            # Development/test deps
├── requirements.lock               # pip-compile lock file
├── .env.example                    # All env vars documented
├── test_app_assembly.py            # Legacy Vanna assembly smoke tests (v1)
├── vanna-skill/                    # Vanna 2.0 reference (read-only)
├── vanna_docs/                     # Scraped Vanna docs (read-only)
├── saudi_stocks.db                 # SQLite DB (generated)
└── saudi_stocks_yahoo_data.csv     # Source data
```

## Database Schema

### Core Tables (SQLite + PostgreSQL)

10 normalized tables derived from a 1062-column flat CSV:

| Table | Rows | Key |
|---|---|---|
| `companies` | 500 | `ticker` (PK) |
| `market_data` | 500 | `ticker` (PK, FK) |
| `valuation_metrics` | 500 | `ticker` (PK, FK) |
| `profitability_metrics` | 500 | `ticker` (PK, FK) |
| `dividend_data` | 500 | `ticker` (PK, FK) |
| `financial_summary` | 500 | `ticker` (PK, FK) |
| `analyst_data` | 500 | `ticker` (PK, FK) |
| `balance_sheet` | ~2,527 | `id` (PK), `ticker` (FK) |
| `income_statement` | ~2,632 | `id` (PK), `ticker` (FK) |
| `cash_flow` | ~2,604 | `id` (PK), `ticker` (FK) |

### PostgreSQL-Only Tables (see `database/schema.sql`)

- `sectors`, `entities` - Reference/enrichment tables
- `filings`, `xbrl_facts`, `computed_metrics` - XBRL financial data
- `price_history` - Daily price/volume history
- `announcements`, `news_articles`, `technical_reports` - Content tables
- `users`, `user_watchlists`, `user_alerts` - User management
- `query_audit_log` - Query tracking

## Setup & Run

### Local Development (SQLite)

```bash
pip install -r requirements.txt
cp .env.example .env
# Edit .env: set ANTHROPIC_API_KEY

python csv_to_sqlite.py   # Build SQLite DB from CSV
python app.py             # Server at http://localhost:8084
```

### Docker (PostgreSQL)

```bash
cp .env.example .env
# Edit .env: set ANTHROPIC_API_KEY and POSTGRES_PASSWORD

docker compose up -d                          # PostgreSQL + app
docker compose --profile tools up -d          # Also start pgAdmin on :5050
```

### Data Migration

```bash
python database/migrate_sqlite_to_pg.py       # SQLite -> PostgreSQL
python database/csv_to_postgres.py            # CSV -> PostgreSQL directly
```

## Testing

```bash
python -m pytest tests/ -q                    # 1571+ backend tests
cd frontend && npx vitest run                 # 231 frontend tests
cd frontend && npx next build                 # 15-page build verification
```

All tests must pass before merging changes.

## Configuration

All settings are managed via environment variables and `.env` file. See `.env.example` for the full list.

| Prefix | Class | Purpose |
|---|---|---|
| `DB_` | `DatabaseSettings` | Backend selection, SQLite path, PG connection |
| `POSTGRES_*` | (alias) | Docker-compatible PG connection vars |
| `LLM_` | `LLMSettings` | Model, API key, max tool iterations (Anthropic only) |
| `SERVER_` | `ServerSettings` | Host, port, debug mode |
| `LOG_LEVEL` | logging | DEBUG, INFO, WARNING, ERROR |
| (none) | `ANTHROPIC_API_KEY` | Backward-compatible API key |

Usage: `from config import get_settings; s = get_settings()`

## Vanna 2.0 Patterns

ALWAYS invoke the `/vanna` skill before writing or modifying any Vanna-related code. The skill contains the authoritative Vanna 2.0 API documentation, best practices, integration patterns, and known pitfalls. Treat its guidance as the source of truth for all Vanna work in this project. If anything in this section conflicts with the skill's documentation, the skill takes precedence.

These are critical patterns specific to Vanna 2.0. Getting them wrong causes runtime errors:

- **Tool registration:** Use `tools.register_local_tool(tool, access_groups=[...])`. Do NOT use `tools.register()` (does not exist).
- **SystemPromptBuilder:** Abstract method signature is `build_system_prompt(self, user, tools)`, not `build()`.
- **Agent constructor requires:** `llm_service`, `tool_registry`, `user_resolver`, `agent_memory` (all required).
- **Agent memory:** Use `DemoAgentMemory(max_items=N)` for in-memory storage.
- **FastAPI compatibility:** Requires FastAPI 0.115.6+ (older versions cause Starlette middleware errors).

## Important Rules

- NEVER commit `.env` or API keys.
- NEVER modify the database schema without updating the system prompt in `app.py` (the `SYSTEM_PROMPT` string documents every column).
- ALWAYS use script-relative paths via `Path(__file__).resolve().parent` for file references (not `./relative`).
- ALWAYS use `try/finally` for database connections to ensure cleanup.
- ALWAYS replace NaN with `None` before writing to SQLite (use `df.where(pd.notnull(df), None)`).
- The system prompt in `app.py` must document ALL database columns. If you add or remove columns, update `SYSTEM_PROMPT` to match.
- When modifying `csv_to_sqlite.py` column mappings, verify against the actual CSV headers.
- ALWAYS read `AGENTS.md` and `CLAUDE.md` at the start of every session before making changes.
- ALWAYS invoke the `/vanna` skill and follow its best practices before writing or modifying any Vanna 2.0 code.
- Services in `services/` that use `psycopg2` require PostgreSQL. Use `news_store.py` for SQLite news operations.
- Configuration changes should go through `config/settings.py`, not raw `os.environ.get()` calls.
- In FastAPI route handlers, use `aget_*` async methods (never sync `get_*` methods) to avoid blocking the event loop.
- Frontend must use Tailwind logical properties (`ms-*`, `me-*`, `ps-*`, `pe-*`), never physical (`ml-*`, `mr-*`, `pl-*`, `pr-*`). Run `npm run lint:rtl` in `frontend/` to verify.
- All SSE endpoints MUST include `request.is_disconnected()` checks to prevent orphaned generators.
- New SQL queries should use `database/queries.py` constants instead of inline strings where practical.
- Use `services/cache_utils.py` `@cache_response` for caching; do not roll custom LRU cache implementations.
- Tadawul trading week is Sunday-Thursday. Friday and Saturday are weekends (not Saturday/Sunday).
- Use `database/postgres_utils.pg_available()` and `pg_connection_params()` for shared PostgreSQL availability checks. Do not duplicate this logic in test files or services.
- When adding a Python dependency, add it to `requirements.in` (not `requirements.txt` directly), then regenerate: `pip-compile requirements.in -o requirements.lock --no-annotate --strip-extras`. The CI verifies the lock file matches.
- Test files live in `tests/`. The project root has only `test_app_assembly.py` (v1 legacy). Never add new test files at root level.
</file>

<file path="api/dependencies.py">
"""
Shared FastAPI dependencies for the TASI AI platform API.

Provides database connection factory and service instances
for dependency injection into route handlers.

Connections are managed through ``DatabaseManager`` which handles pool
integration, backend selection (SQLite/PostgreSQL), and configuration.

For PostgreSQL deployments, ``init_pg_pool()`` must be called at application
startup (from the FastAPI lifespan handler) before any requests are served.
All subsequent calls to ``get_db_connection()`` will draw from that pool.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# PostgreSQL connection pool bootstrap
⋮----
def init_pg_pool(dsn: str, minconn: int = 2, maxconn: int = 10) -> None
⋮----
"""Initialize the module-level PostgreSQL connection pool.

    Delegates to ``database.pool.init_pool`` which manages the singleton
    ``ThreadedConnectionPool``.  This wrapper exists so that application
    startup code (``app.py`` lifespan) has a single, stable import path in
    ``api.dependencies`` regardless of where the pool implementation lives.

    Must be called once during application startup before any request is
    handled.  Subsequent calls are no-ops (pool already initialized).

    Parameters
    ----------
    dsn:
        Full PostgreSQL connection string, e.g.
        ``postgresql://user:pass@host:5432/dbname``.
    minconn:
        Minimum number of connections to keep open (default 2).
    maxconn:
        Maximum number of connections allowed (default 10).
    """
⋮----
# database.pool.init_pool accepts a db_settings object; we pass a thin
# namespace built from the DSN so callers only need the connection string.
⋮----
# Parse DSN into keyword args understood by psycopg2
⋮----
parsed = _urlparse.urlparse(dsn)
_pool_kwargs = dict(
⋮----
# Connection factory
⋮----
def get_db_connection()
⋮----
"""Get a database connection via the centralized DatabaseManager.

    For PostgreSQL backends the connection is drawn from the pool initialized
    by ``init_pg_pool()``.  If the pool has not been initialized a
    ``RuntimeError`` is raised so the misconfiguration is surfaced immediately
    rather than silently opening unbounded direct connections.

    Returns a raw connection. The caller is responsible for closing it
    (typically in a try/finally block). For pool-backed connections,
    ``close()`` returns the connection to the pool with an automatic rollback
    of uncommitted work.

    This function is used both as a direct call in route handlers and as
    a callable passed to services (get_conn=get_db_connection).
    """
⋮----
db = get_database_manager()
⋮----
def get_db_connection_dep() -> Generator
⋮----
"""FastAPI generator dependency that auto-closes connections.

    Use with ``Depends(get_db_connection_dep)`` for route handlers that
    need a connection with guaranteed cleanup::

        @router.get("/items")
        async def list_items(conn=Depends(get_db_connection_dep)):
            cur = conn.cursor()
            ...
    """
conn = get_db_connection()
⋮----
# Service singletons (each holds a reference to get_db_connection)
⋮----
@lru_cache(maxsize=1)
def get_news_service() -> NewsAggregationService
⋮----
@lru_cache(maxsize=1)
def get_reports_service() -> TechnicalReportsService
⋮----
@lru_cache(maxsize=1)
def get_announcement_service() -> AnnouncementService
⋮----
@lru_cache(maxsize=1)
def get_user_service() -> UserService
⋮----
@lru_cache(maxsize=1)
def get_audit_service() -> AuditService
</file>

<file path="api/routes/entities.py">
"""
Entity (company/stock) API routes.

Queries the companies, market_data, valuation_metrics, and profitability_metrics
tables directly via psycopg2 for company lookup and listing.
"""
⋮----
router = APIRouter(prefix="/api/entities", tags=["entities"])
⋮----
def _pg_fetchall(sql: str, params=None) -> List[Dict[str, Any]]
⋮----
conn = get_db_connection()
⋮----
def _pg_fetchone(sql: str, params=None) -> Optional[Dict[str, Any]]
⋮----
row = cur.fetchone()
⋮----
# ---------------------------------------------------------------------------
# Routes
⋮----
"""List companies with basic market data."""
# Always filter out stub entities with no meaningful market data
clauses: list = ["(m.current_price IS NOT NULL OR m.market_cap IS NOT NULL)"]
params: dict = {"limit": limit, "offset": offset}
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
count_sql = f"""
⋮----
sql = f"""
⋮----
def _sync_query()
⋮----
total = cur.fetchone()["cnt"]
⋮----
rows = [dict(r) for r in cur.fetchall()]
⋮----
items = [
⋮----
@router.get("/sectors", response_model=List[SectorInfo])
async def list_sectors() -> List[SectorInfo]
⋮----
"""Return all sectors with company counts (only companies with real market data)."""
sql = """
rows = await asyncio.to_thread(_pg_fetchall, sql)
⋮----
def _normalize_ticker(ticker: str) -> str
⋮----
"""Ensure Saudi ticker has .SR suffix.

    The database stores tickers with the .SR suffix (e.g. '2222.SR').
    Users and internal links often use just the number ('2222'), so
    we append '.SR' when the input is purely numeric.
    """
stripped = ticker.strip()
⋮----
@router.get("/{ticker}", response_model=CompanyDetail)
async def get_entity(ticker: str) -> CompanyDetail
⋮----
"""Return detailed company information with market data, valuation, and profitability."""
ticker = validate_ticker(ticker)
ticker = _normalize_ticker(ticker)
⋮----
row = await asyncio.to_thread(_pg_fetchone, sql, {"ticker": ticker})
⋮----
def _f(val)
⋮----
def _i(val)
⋮----
# expanded fields
</file>

<file path="api/routes/health.py">
"""
Health check API routes.

Provides three endpoints:
  /health        - Full health report with all component checks
  /health/live   - Lightweight liveness probe (always 200 if process is running)
  /health/ready  - Readiness probe (200 only if database is reachable)
"""
⋮----
router = APIRouter(tags=["health"])
⋮----
@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse
⋮----
"""Return structured health status for all platform components."""
report = await asyncio.to_thread(get_health)
pool_stats = await asyncio.to_thread(get_pool_stats)
status_code = 200 if report.status != HealthStatus.UNHEALTHY else 503
⋮----
response = HealthResponse(
⋮----
@router.get("/health/live")
async def liveness()
⋮----
"""Liveness probe for load balancers and orchestrators.

    Returns 200 if the process is running. Does not check external dependencies.
    """
⋮----
@router.get("/health/ready")
async def readiness()
⋮----
"""Readiness probe — returns 200 only when the database is reachable."""
db = await asyncio.to_thread(check_database)
</file>

<file path="api/routes/watchlists.py">
"""
Watchlists and alerts API routes.

All endpoints require JWT authentication. Unauthenticated requests receive 401.
"""
⋮----
router = APIRouter(prefix="/api/watchlists", tags=["watchlists"])
⋮----
# ---------------------------------------------------------------------------
# Watchlist read endpoints (authenticated via JWT)
⋮----
"""Return all watchlists for the authenticated user. Returns 401 if not authenticated."""
wls = await asyncio.to_thread(svc.get_watchlists, user_id=current_user["id"])
⋮----
# Watchlist write endpoints (authenticated via JWT)
⋮----
"""Create a new watchlist for the authenticated user."""
wl = await asyncio.to_thread(
⋮----
"""Add a single ticker to an existing watchlist. Requires authentication."""
# Fetch current watchlist to get existing tickers
⋮----
wl = next((w for w in wls if w.id == watchlist_id), None)
⋮----
tickers = list(wl.tickers)
⋮----
updated = await asyncio.to_thread(
⋮----
"""Update a watchlist's name and/or tickers. Requires authentication."""
⋮----
"""Delete a watchlist. Requires authentication."""
deleted = await asyncio.to_thread(
⋮----
# Alert read endpoints (authenticated via JWT)
⋮----
"""Return active alerts for the authenticated user. Returns 401 if not authenticated."""
alerts = await asyncio.to_thread(
⋮----
# Alert write endpoints (authenticated via JWT)
⋮----
"""Create a new alert for the authenticated user."""
alert = await asyncio.to_thread(
⋮----
"""Deactivate an alert (soft-delete). Requires authentication."""
</file>

<file path="DEVELOPMENT.md">
# Development Guide

This guide covers common development tasks and architectural patterns in the Ra'd AI codebase.

## Async Database Access

### Background

All FastAPI route handlers are `async def`, but the database drivers (sqlite3, psycopg2) are synchronous. Calling sync I/O directly inside an async handler blocks the event loop, degrading throughput for concurrent requests.

The solution: wrap sync calls in `asyncio.to_thread()`, which runs them in a thread pool without blocking the event loop.

### Using `aget_*` Methods (NewsStore)

`services/news_store.py` provides both sync and async methods. **Always use the async variants in route handlers.**

```python
# CORRECT - non-blocking
from api.routes.news_feed import get_store

@router.get("/articles")
async def list_articles(source: str | None = None):
    store = get_store()
    articles = await store.aget_latest_news(limit=20, source=source)
    return {"items": articles}
```

```python
# WRONG - blocks the event loop
@router.get("/articles")
async def list_articles():
    store = get_store()
    articles = store.get_latest_news(limit=20)  # sync call in async handler!
    return {"items": articles}
```

Available async methods:

| Async Method | Sync Equivalent | Description |
|---|---|---|
| `aget_latest_news(**kwargs)` | `get_latest_news(...)` | Fetch articles with filters |
| `acount_articles(**kwargs)` | `count_articles(...)` | Count articles with filters |
| `aget_article_by_id(id)` | `get_article_by_id(id)` | Single article by ID |
| `asearch_articles(**kwargs)` | `search_articles(...)` | Full-text search |
| `acount_search(**kwargs)` | `count_search(...)` | Count search results |
| `aget_sources()` | `get_sources()` | List sources with counts |
| `aget_articles_by_ids(ids)` | `get_articles_by_ids(ids)` | Batch fetch by IDs |

### Using `db_helper` (Generic Queries)

For ad-hoc SQLite queries in route handlers, use `api/db_helper.py`:

```python
from api.db_helper import afetchall, afetchone

@router.get("/stats")
async def get_stats():
    row = await afetchone("SELECT COUNT(*) as total FROM companies")
    return {"total": row["total"]}

@router.get("/sectors")
async def get_sectors():
    rows = await afetchall(
        "SELECT sector, COUNT(*) as count FROM companies GROUP BY sector"
    )
    return {"sectors": rows}
```

### Adding Async Methods to Existing Services

When adding new sync methods that will be called from route handlers, always add an async wrapper:

```python
class NewsStore:
    def get_trending(self, hours: int = 24) -> List[Dict]:
        """Sync implementation."""
        conn = self._connect()
        try:
            # ... query logic
            return [dict(row) for row in rows]
        finally:
            conn.close()

    async def aget_trending(self, **kwargs) -> List[Dict]:
        """Async wrapper for use in FastAPI handlers."""
        return await asyncio.to_thread(self.get_trending, **kwargs)
```

## Adding a News Source

The news scraper system in `services/news_scraper.py` is extensible. Each source is a subclass of `BaseNewsScraper`.

### Step 1: Create the Scraper Class

```python
class MySourceScraper(BaseNewsScraper):
    """Scraper for MySource financial news."""

    @property
    def source_name(self) -> str:
        return "مصدري"  # Arabic name displayed in UI

    @property
    def base_url(self) -> str:
        return "https://mysource.example.com/economy"

    def parse_articles(self, html: str) -> List[Dict]:
        soup = BeautifulSoup(html, "lxml")
        articles = []
        for item in soup.select("article.news-item"):
            title_el = item.select_one("h2 a")
            if not title_el:
                continue
            articles.append({
                "title": title_el.get_text(strip=True),
                "source_url": title_el.get("href", ""),
                "published_at": self._parse_date(
                    item.select_one("time")
                ),
            })
        return articles[:_scraper_cfg.max_articles_per_source]
```

If the source blocks direct requests (Cloudflare, WAF), extend `GoogleNewsRssScraper` instead:

```python
class MySourceScraper(GoogleNewsRssScraper):
    @property
    def source_name(self) -> str:
        return "مصدري"

    @property
    def base_url(self) -> str:
        return "https://mysource.example.com"  # used for Google News site: filter

    @property
    def google_news_query(self) -> str:
        return "سوق الأسهم السعودي site:mysource.example.com"
```

### Step 2: Register the Scraper

Add your class to the `ALL_SCRAPERS` list at the bottom of `news_scraper.py`:

```python
ALL_SCRAPERS: List[type] = [
    AlarabiyaScraper,
    AsharqBusinessScraper,
    ArgaamScraper,
    MaaalScraper,
    MubasherScraper,
    MySourceScraper,         # <-- add here
]
```

### Step 3: Add the Source Filter to the Frontend

Update `frontend/src/app/news/utils.ts` to add the source chip:

```typescript
export const SOURCE_FILTERS = [
  { key: null, label: 'الكل', color: '#D4A84B' },
  { key: 'العربية', label: 'العربية', color: '#C4302B' },
  { key: 'الشرق', label: 'الشرق', color: '#1A73E8' },
  { key: 'أرقام', label: 'أرقام', color: '#00A650' },
  { key: 'معال', label: 'معال', color: '#FF6B00' },
  { key: 'مباشر', label: 'مباشر', color: '#6B21A8' },
  { key: 'مصدري', label: 'مصدري', color: '#FF1493' },  // <-- add here
] as const;
```

Also add the color mapping in the `SOURCE_COLORS` map (same file):

```typescript
const SOURCE_COLORS: Record<string, string> = {
  // ... existing entries
  'مصدري': '#FF1493',
};
```

### Step 4: Configuration

All scraper settings are centralized in `config/settings.py` under `ScraperSettings`:

| Env Variable | Default | Description |
|---|---|---|
| `SCRAPER_REQUEST_TIMEOUT` | `10` | HTTP timeout for source pages (seconds) |
| `SCRAPER_ARTICLE_FETCH_TIMEOUT` | `5` | HTTP timeout for article body fetches |
| `SCRAPER_INTER_REQUEST_DELAY` | `1.5` | Delay between requests (rate limiting) |
| `SCRAPER_MAX_ARTICLES_PER_SOURCE` | `10` | Max articles per source per cycle |
| `SCRAPER_MAX_FULL_ARTICLE_FETCHES` | `5` | Max full-body fetches per source |
| `SCRAPER_FETCH_INTERVAL_SECONDS` | `1800` | Scheduler interval (30 min default) |
| `SCRAPER_CLEANUP_AGE_DAYS` | `7` | Auto-delete articles older than N days |
| `SCRAPER_DEDUP_THRESHOLD` | `0.55` | Title similarity threshold for dedup |

### Step 5: Test

```bash
# Verify the scraper works in isolation
python -c "
from services.news_scraper import MySourceScraper
s = MySourceScraper()
articles = s.fetch_articles()
print(f'Found {len(articles)} articles')
for a in articles[:3]:
    print(f'  - {a[\"title\"][:60]}')
"

# Run the full pipeline
python -c "
from services.news_scraper import fetch_all_news
articles = fetch_all_news()
print(f'Total: {len(articles)} articles from all sources')
"
```

## Frontend Architecture

### Component Decomposition

Large page files have been decomposed into focused subcomponents to improve maintainability:

**Charts Page** (`frontend/src/app/charts/`):

| File | Purpose |
|---|---|
| `page.tsx` | Top-level orchestrator (slim) |
| `components/ChartHeader.tsx` | Page title + stock selector header |
| `components/ChartControls.tsx` | Timeframe picker + chart-type selector |
| `components/CandlestickPanel.tsx` | OHLCV candlestick chart wrapper |
| `components/VolumePanel.tsx` | Volume bar chart panel |
| `components/TASIPanel.tsx` | TASI index lightweight-charts panel |
| `components/ComparisonPanel.tsx` | Multi-stock overlay comparison chart |
| `components/StockSearch.tsx` | Ticker autocomplete search input |
| `components/ChartSkeleton.tsx` | Loading skeleton for chart panels |

**Markets Page** (`frontend/src/app/markets/`):

| File | Purpose |
|---|---|
| `page.tsx` | Top-level orchestrator (slim) |
| `components/MarketHeader.tsx` | Summary stats + breadcrumb |
| `components/SectorFilter.tsx` | Sector filter chip bar |
| `components/SortControls.tsx` | Column sort controls |
| `components/MarketTable.tsx` | Full desktop data table |
| `components/MarketCard.tsx` | Mobile card view for a single stock |
| `components/MarketCardList.tsx` | Mobile card list container |
| `components/PaginationBar.tsx` | Pagination controls |
| `components/MarketSearch.tsx` | Market search input |
| `components/MarketSkeleton.tsx` | Loading skeleton for table/cards |
| `components/MarketError.tsx` | Error state component |
| `components/MarketEmpty.tsx` | Empty state component |

### API Client Modules

`frontend/src/lib/api-client.ts` now re-exports from domain modules under `frontend/src/lib/api/`. Import from the domain module directly for new code:

```typescript
// Preferred: domain-specific import
import { getStockDetail } from '@/lib/api/stocks';
import { getNewsFeed } from '@/lib/api/news';

// Still works: legacy import (backward-compatible shim)
import { getStockDetail, getNewsFeed } from '@/lib/api-client';
```

Domain modules:

| Module | Exports |
|---|---|
| `api/stocks.ts` | `getStockDetail`, `getStockOHLCV`, `searchStocks` |
| `api/news.ts` | `getNewsFeed`, `getNewsArticle`, `getNewsSources`, `searchNews` |
| `api/charts.ts` | `getChartData`, `getTASIIndex` |
| `api/market.ts` | `getMarketOverview`, `getMarketSectors` |
| `api/auth.ts` | `login`, `refreshToken`, `guestLogin`, `getProfile` |
| `api/health.ts` | `getHealth`, `getLiveness`, `getReadiness` |
| `api/widgets.ts` | `getWidgetQuotes` |
| `api/reports.ts` | `getReports`, `getReport` |
| `api/announcements.ts` | `getAnnouncements` |

### Auth System

The auth hook (`frontend/src/lib/hooks/use-auth.ts`) provides:

- **Token refresh**: Automatically refreshes JWT before expiry using a background timer
- **Guest login**: `loginAsGuest()` calls `/api/v1/auth/guest` for anonymous access
- **Profile enrichment**: `fetchProfile()` populates user name, role, and permissions after login

Auth service (`services/auth_service.py`) was updated to support:

- Guest token issuance with limited claims
- Profile endpoint (`/api/v1/auth/me`) returning enriched user data

### Stock Detail Page

The stock detail page (`frontend/src/app/stocks/[ticker]/page.tsx`) now includes:

- **Financials tab**: Balance sheet, income statement, and cash flow in tabbed panels
- **Dividends tab**: Historical dividend table with yield trend sparkline
- **Reports section**: Related analyst reports linked to the ticker
- **News feed**: Latest news articles filtered by ticker/company name
- **Watchlist toggle**: Add/remove from watchlist with toast notification

## Frontend Patterns

### RTL Support

The app uses `dir="rtl"` for Arabic layout. Always use Tailwind logical properties:

| Physical (DO NOT USE) | Logical (USE THIS) |
|---|---|
| `ml-*` | `ms-*` (margin-inline-start) |
| `mr-*` | `me-*` (margin-inline-end) |
| `pl-*` | `ps-*` (padding-inline-start) |
| `pr-*` | `pe-*` (padding-inline-end) |
| `left-*` | `start-*` |
| `right-*` | `end-*` |

### AbortController Pattern

All fetch calls must be cancellable. The `api-client.ts` `request()` function accepts an optional `signal` parameter:

```typescript
import { getNewsFeed } from '@/lib/api-client';

useEffect(() => {
  const controller = new AbortController();
  getNewsFeed({ limit: 20 }, undefined, undefined, controller.signal)
    .then(setData)
    .catch((err) => {
      if (err.name !== 'AbortError') setError(err.message);
    });
  return () => controller.abort();
}, []);
```

For hooks, use the `useAsync` pattern from `lib/hooks/use-api.ts` which handles AbortController automatically.

### Runtime Configuration

Frontend config values live in `frontend/src/lib/config.ts` and are driven by environment variables:

```typescript
import { API_TIMEOUT_MS, HEALTH_POLL_INTERVAL_MS } from '@/lib/config';
```

See `frontend/.env.local.example` for all available variables.

## Live Market Widgets

The widgets system provides live-updating market quotes (crypto, metals, oil, global indices) via Server-Sent Events.

### Architecture

```
QuotesHub (services/widgets/quotes_hub.py)
  ├── CryptoProvider     (providers/crypto.py)
  ├── MetalsProvider     (providers/metals.py)
  ├── OilProvider        (providers/oil.py)
  └── IndicesProvider    (providers/indices.py)
         │
         v
  SSE endpoint (/api/v1/widgets/stream)
         │
         v
  LiveMarketWidgets (React component)
    └── EventSource with reconnection backoff
```

### Adding a New Provider

1. Create a new file in `services/widgets/providers/`:

```python
from api.models.widgets import QuoteItem

async def fetch_my_quotes() -> list[QuoteItem]:
    """Fetch quotes from your data source."""
    # ... fetch logic
    return [
        QuoteItem(
            symbol="XYZ",
            name="My Asset",
            price=100.0,
            change=2.5,
            change_pct=2.56,
            category="my_category",
        )
    ]
```

2. Register the provider in `services/widgets/providers/__init__.py`.

3. Add the category to the `LiveMarketWidgets` component's category filter tabs.

### Redis Pub/Sub (Optional)

The `QuotesHub` supports Redis pub/sub for multi-instance deployments. Configure via:

| Env Variable | Default | Description |
|---|---|---|
| `REDIS_URL` | `redis://localhost:6379/0` | Redis connection URL |
| `CACHE_ENABLED` | `false` | Enable Redis-based caching |

Without Redis, the hub operates in single-process mode with in-memory state.

## Cache Utilities

### Unified Caching Decorator (`services/cache_utils.py`)

Use `@cache_response` for caching service method results:

```python
from services.cache_utils import cache_response

@cache_response(ttl=300, max_size=500)
def get_expensive_data(ticker: str) -> dict:
    # ... expensive computation
    return result
```

The decorator provides:
- **TTL-based expiration**: Entries expire after `ttl` seconds
- **LRU eviction**: Cache is capped at `max_size` entries (default 500)
- **Thread-safe**: Uses `threading.Lock` internally

### YFinance Shared Utilities (`services/yfinance_base.py`)

Common patterns for yfinance API calls are centralized in `yfinance_base.py`:

- **`YFinanceCache`**: Shared LRU cache with configurable TTL and max entries (default 500)
- **`CircuitBreaker`**: Prevents repeated calls to a failing yfinance endpoint; auto-resets after a cooldown period

```python
from services.yfinance_base import YFinanceCache, CircuitBreaker

cache = YFinanceCache(max_size=500, ttl=300)
breaker = CircuitBreaker(failure_threshold=5, reset_timeout=60)
```

## Connection Status Patterns

### SSE Disconnect Detection (Backend)

All SSE endpoints must check for client disconnection to avoid orphaned generators:

```python
@router.get("/stream")
async def stream(request: Request):
    async def event_generator():
        while True:
            if await request.is_disconnected():
                break
            yield {"data": json.dumps(payload)}
            await asyncio.sleep(interval)
    return EventSourceResponse(event_generator())
```

### EventSource Reconnection (Frontend)

The `ConnectionStatusBadge` component (`frontend/src/components/common/ConnectionStatusBadge.tsx`) provides a reusable connection state indicator. SSE consumers should implement exponential backoff on reconnection:

```typescript
const reconnect = useCallback(() => {
  const delay = Math.min(1000 * 2 ** attempt, 30000);
  setTimeout(() => {
    const es = new EventSource(url);
    es.onopen = () => setAttempt(0);
    es.onerror = () => { es.close(); reconnect(); };
  }, delay);
}, [attempt]);
```

States: `live` (connected), `reconnecting` (attempting), `offline` (failed).

### Header Health Polling

The site header polls `/health/live` with AbortController to show a connection indicator. The polling interval is configurable via `HEALTH_POLL_INTERVAL_MS` in `frontend/src/lib/config.ts`.

## Async I/O Notes

In addition to database calls and news store methods, health check routes in `app.py` are also wrapped in `asyncio.to_thread()` to prevent blocking when the database driver performs I/O during health probes.

## RTL Lint Enforcement

Run the RTL lint check to catch physical direction properties (`ml-*`, `mr-*`, `pl-*`, `pr-*`) that should use logical equivalents:

```bash
cd frontend && npm run lint:rtl
```

This runs `scripts/lint-rtl.js`, which scans `.tsx` and `.ts` files for Tailwind physical direction classes and reports violations. The check is also integrated into CI.

## Backend Module (`backend/`)

The `backend/` package contains enterprise-grade infrastructure organized into four subsystems:

### Security Pipeline (`backend/security/`)

SQL injection prevention and query validation for AI-generated SQL:

- **`sanitizer.py`**: Input sanitization for natural-language queries and SQL identifiers. Strips dangerous characters and normalizes whitespace.
- **`allowlist.py`**: Table and column allowlist enforcement (`QueryAllowlist`). Only permits queries against known schema objects.
- **`sql_validator.py`**: Multi-pass SQL query validation (`SqlQueryValidator`). Checks for prohibited keywords, subquery depth, statement type restrictions.
- **`vanna_hook.py`**: Integration hook (`validate_vanna_output`) that intercepts Vanna-generated SQL before execution and runs it through the full validation pipeline.
- **`config.py`**: `SecurityConfig` with tunable thresholds and feature flags.
- **`models.py`**: `ValidationResult` and `ValidatedQuery` Pydantic models.

### Middleware (`backend/middleware/`)

Request-level controls for rate limiting and cost management:

- **`rate_limiter.py`**: Redis-backed sliding window rate limiter (`RateLimiter`) with automatic in-memory fallback when Redis is unavailable.
- **`rate_limit_middleware.py`**: FastAPI middleware (`RateLimitMiddleware`) that enforces per-endpoint rate limits with standard `X-RateLimit-*` response headers.
- **`rate_limit_config.py`**: `RateLimitConfig` and `EndpointRateLimit` for per-route limit customization.
- **`cost_controller.py`**: LLM spend tracking (`CostController`) with configurable daily/monthly limits (`CostLimitConfig`) and usage summaries (`UsageSummary`).
- **`register.py`**: One-call middleware registration (`register_middleware`) and accessors (`get_rate_limiter`, `get_cost_controller`, `shutdown_middleware`).

### Audit & Logging (`backend/services/audit/`)

Structured observability for query lifecycle and security events:

- **`structured_logger.py`**: JSON-formatted logging (`JSONFormatter`, `configure_logging`, `get_logger`) with automatic request-ID injection.
- **`correlation.py`**: `CorrelationMiddleware` for end-to-end request tracing via `X-Request-ID` headers. `get_current_request_id()` accessor for any code path.
- **`query_audit.py`**: `QueryAuditLogger` tracks the full NL-to-SQL lifecycle (query received, SQL generated, executed, results returned).
- **`security_events.py`**: `SecurityEventLogger` records auth failures, SQL injection attempts, rate limit violations, and other security-relevant events.
- **`config.py`**: `AuditConfig` for log levels, output paths, and feature toggles.
- **`models.py`**: `QueryAuditEvent`, `SecurityEvent`, `SecurityEventType`, `SecuritySeverity` Pydantic models.

### Cache Layer (`backend/services/cache/`)

Redis-based caching with compression and connection pooling:

- **`query_cache.py`**: Tiered query cache (`QueryCache`) with configurable TTL per query complexity.
- **`compression.py`**: `GZipCacheMiddleware` for transparent response compression. `compress_bytes` / `decompress_bytes` utilities, `compress_large_response` for threshold-based compression.
- **`redis_client.py`**: `RedisManager` for async Redis connection lifecycle with connection pooling.
- **`db_pool.py`**: `DatabasePoolManager` for database connection pooling with health checks.
- **`maintenance.py`**: `CacheMaintenance` for periodic cache cleanup, stats collection, and eviction.
- **`config.py`**: `CacheConfig` for Redis URLs, pool sizes, TTL tiers, and compression thresholds.
- **`models.py`**: `CachedResult`, `PoolConfig`, `PoolStats`, `TTLTier` Pydantic models.

### Resilience (`backend/services/resilience/`)

Fault tolerance for external service calls (yfinance, LLM APIs, Redis):

- **`circuit_breaker.py`**: `CircuitBreaker` with configurable failure thresholds, half-open probing, and a global registry (`get_or_create`, `get_all_stats`).
- **`retry.py`**: `with_retry` decorator for exponential backoff retries. `with_timeout` decorator for deadline enforcement.
- **`timeout_manager.py`**: `QueryTimeoutManager` with per-query-type timeout configuration (`QueryTimeoutConfig`).
- **`degradation.py`**: `DegradationManager` for graceful feature degradation when dependencies fail. `create_default_manager()` factory.
- **`config.py`**: `ResilienceConfig` with `get_resilience_config()` accessor.

## E2E Testing (Playwright)

End-to-end tests use [Playwright](https://playwright.dev/) and live under `frontend/e2e/`.

### Setup

```bash
cd frontend
npx playwright install --with-deps chromium
```

### Running E2E Tests

```bash
# Run all E2E tests (headless)
cd frontend && npx playwright test

# Run with UI (headed)
cd frontend && npx playwright test --headed

# Run a specific spec file
cd frontend && npx playwright test e2e/markets.spec.ts

# Show last test report
cd frontend && npx playwright show-report
```

### Spec File Locations

| File | What It Covers |
|---|---|
| `e2e/news.spec.ts` | News portal: RTL layout, virtual scroll, SSE stream, source filters |
| `e2e/markets.spec.ts` | Markets page: sector filter chips, column sort, pagination, search, mobile card view |
| `e2e/stock-detail.spec.ts` | Stock detail: financials tab, dividends tab, watchlist toggle, related news, reports |

### Writing New E2E Tests

- Use `page.getByRole()` and `page.getByTestId()` selectors for resilience
- Add `data-testid` attributes to new interactive elements in TSX files
- Use `await page.waitForLoadState('networkidle')` after SSE-dependent page loads
- For RTL assertions check `dir="rtl"` on `<html>` and use `toHaveCSS` for layout direction

## Performance Optimizations

### ConstellationCanvas Animation

The `ConstellationCanvas` component uses `requestAnimationFrame` instead of `setInterval` for its particle animation loop. This aligns animation updates with the browser's repaint cycle (typically 60fps) and avoids frame-rate issues:

```typescript
// Correct pattern (requestAnimationFrame)
const animate = useCallback(() => {
  drawFrame();
  animationRef.current = requestAnimationFrame(animate);
}, [drawFrame]);

useEffect(() => {
  animationRef.current = requestAnimationFrame(animate);
  return () => cancelAnimationFrame(animationRef.current!);
}, [animate]);
```

Child particle components are wrapped in `React.memo` to prevent re-renders when parent state unrelated to particle data changes.

The resize handler is debounced (300ms) to avoid recalculating canvas dimensions on every intermediate resize event:

```typescript
const handleResize = useMemo(
  () => debounce(() => { /* recalculate */ }, 300),
  []
);
```

The canvas element has `will-change: transform` applied via CSS to hint to the browser that it should be composited on a separate GPU layer.

## Running Test Coverage

Generate an HTML coverage report for the backend:

```bash
# Run tests with coverage collection
python -m pytest tests/ --cov=api --cov=services --cov=backend --cov-report=html --cov-report=term-missing

# Open the HTML report
# Windows:
start htmlcov/index.html
# macOS:
open htmlcov/index.html
# Linux:
xdg-open htmlcov/index.html
```

For a quick terminal summary without HTML:

```bash
python -m pytest tests/ --cov=api --cov=services --cov=backend --cov-report=term-missing -q
```

## PostgreSQL Shared Utilities (`database/postgres_utils.py`)

Common PostgreSQL availability and connection helpers are centralized to avoid duplication across test files and services.

```python
from database.postgres_utils import pg_available, pg_connection_params

# Check if PG is reachable (safe to call with no PG configured — returns False)
if pg_available(timeout=3):
    params = pg_connection_params()
    conn = psycopg2.connect(**params, connect_timeout=5)
```

**`pg_available(timeout=3) -> bool`**
- Returns `False` immediately if `POSTGRES_HOST` env var is unset
- Catches all exceptions including invalid port values; logs `debug` on failure
- Safe to use in `@pytest.mark.skipif` decorators

**`pg_connection_params() -> dict`**
- Returns `host`, `port` (int, defaults to 5432 on invalid input), `dbname`, `user`, `password` from env
- Does not include `connect_timeout`; callers add as needed

Import this in any new code that needs to probe PG availability instead of duplicating the try/connect pattern.

## Test Organization

### Backend Tests

All test files live under `tests/`. There are no test files in the project root (except the legacy `test_app_assembly.py` v1, which is not run by pytest).

| Directory | Purpose | Marker |
|---|---|---|
| `tests/` | Unit tests (services, routes, schemas, middleware) | `@pytest.mark.fast` |
| `tests/integration/` | Integration tests (API chains, auth flows, health, PG path) | `@pytest.mark.integration` |
| `tests/security/` | Security tests (SQL injection, auth bypass) | - |
| `tests/performance/` | Load and concurrency tests | `@pytest.mark.performance` |

Key test files:

| File | Purpose |
|---|---|
| `tests/test_database.py` | 23 database integrity tests (dual SQLite + PG backends) |
| `tests/test_app_assembly_v2.py` | 33 Vanna 2.0 assembly tests (run directly with `python tests/test_app_assembly_v2.py`) |
| `tests/conftest.py` | Shared fixtures: `sqlite_db`, `pg_conn`, `pg_schema_version` |

New backend test files added in the quality sprint:

| File | Coverage |
|---|---|
| `tests/test_auth_service.py` | JWT issuance, guest login, token refresh, permission guards |
| `tests/test_widget_system.py` | QuotesHub providers, SSE streaming, Redis pub/sub fallback |
| `tests/test_health_config.py` | Health probes (live/ready), config validation, env startup checks |

### Frontend Tests (Vitest)

Frontend tests live under `frontend/src/__tests__/`. Run with `npx vitest run` from the `frontend/` directory. 231 tests across 20 files.

New frontend test files added in the quality sprint:

| File | Coverage |
|---|---|
| `src/__tests__/hooks/use-auth.test.ts` | Token refresh timer, guest login flow, profile enrichment |
| `src/__tests__/lib/api/stocks.test.ts` | Domain module exports, request shaping, error handling |
| `src/__tests__/lib/api/news.test.ts` | News domain module: feed, article, search, sources |
| `src/__tests__/app/stocks/StockDetail.test.tsx` | Financials tab, dividends tab, watchlist toggle, news section |
| `src/__tests__/lib/api/charts.test.ts` | Chart data fetch, TASI index, error and loading states |
| `src/__tests__/lib/api/entities.test.ts` | Entity search, pagination, empty results |
| `src/__tests__/lib/api/market.test.ts` | Market overview, sector filter, sort parameters |
| `src/__tests__/lib/api/health.test.ts` | Liveness and readiness probe responses |

New backend test files added in the coverage expansion sprint:

| File | Coverage Target | Description |
|---|---|---|
| `tests/test_xbrl_processor.py` | 70%+ (was 26.7%) | XBRL document parsing, field extraction, error handling |
| `tests/test_price_loader.py` | 70%+ (was 33.5%) | Price record loading, batch upsert, date normalization |
| `tests/test_stock_ohlcv.py` | 70%+ (was 18.8%) | OHLCV service: yfinance integration, cache, circuit breaker |
| `tests/test_redis_client.py` | 70%+ (was 22.2%) | Redis client: mocked ops, pub/sub flow, connection pool, error recovery |
| `tests/test_ingestion_scheduler.py` | 70%+ (was 0%) | Scheduler: task registration, run cycle, error isolation |

Run specific test categories:

```bash
# Unit tests only
python -m pytest tests/ --ignore=tests/integration --ignore=tests/security --ignore=tests/performance -q

# Integration tests only
python -m pytest tests/integration/ -q

# Security tests only
python -m pytest tests/security/ -q

# Performance tests only
python -m pytest tests/performance/ -q
```
</file>

<file path="docker-compose.yml">
services:
  # ---------------------------------------------------------------------------
  # PostgreSQL 16
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: tasi-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-tasi_platform}
      POSTGRES_USER: ${POSTGRES_USER:-tasi_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required - set it in .env}
    ports:
      - "127.0.0.1:${POSTGRES_PORT:-5432}:5432"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-tasi_user} -d ${POSTGRES_DB:-tasi_platform}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - tasi-network

  # ---------------------------------------------------------------------------
  # FastAPI application
  # ---------------------------------------------------------------------------
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: tasi-app
    restart: unless-stopped
    env_file:
      - .env
    environment:
      DB_BACKEND: postgres
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      CACHE_REDIS_URL: redis://redis:6379/0
      CACHE_ENABLED: ${CACHE_ENABLED:-false}
      # --- Production env vars (uncomment when deploying) ---
      # ENVIRONMENT: production
      # AUTH_JWT_SECRET: <run: python -c "import secrets; print(secrets.token_urlsafe(64))">
      # SENTRY_DSN: https://xxx@sentry.io/yyy
      # PG_POOL_MIN: 5
      # PG_POOL_MAX: 20
    ports:
      - "${SERVER_PORT:-8084}:8084"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8084/health"]
      interval: 30s
      timeout: 5s
      start_period: 30s
      retries: 3
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - tasi-network

  # ---------------------------------------------------------------------------
  # Redis 7 (cache layer - optional, controlled by CACHE_ENABLED)
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: tasi-redis
    restart: unless-stopped
    command: >
      sh -c '
      if [ -n "$$REDIS_PASSWORD" ]; then
        redis-server --requirepass "$$REDIS_PASSWORD" --appendonly yes
      else
        redis-server --appendonly yes
      fi'
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    ports:
      - "127.0.0.1:${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - tasi-network

  # ---------------------------------------------------------------------------
  # pgAdmin (optional - start with: docker compose --profile tools up)
  # ---------------------------------------------------------------------------
  pgadmin:
    image: dpage/pgadmin4:8
    container_name: tasi-pgadmin
    profiles:
      - tools
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@tasi.local}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:?PGADMIN_DEFAULT_PASSWORD is required - set it in .env}
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - tasi-network

networks:
  tasi-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
</file>

<file path="frontend/package.json">
{
  "name": "frontend",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "test": "vitest",
    "test:run": "vitest run",
    "test:e2e": "playwright test --config=e2e/playwright.config.ts",
    "test:e2e:ui": "playwright test --config=e2e/playwright.config.ts --ui",
    "analyze": "node scripts/analyze-bundle.js",
    "security:audit": "bash scripts/security-audit.sh",
    "lint:rtl": "node scripts/lint-rtl.js"
  },
  "dependencies": {
    "@sentry/nextjs": "^8.0.0",
    "@tanstack/react-virtual": "^3.13.18",
    "clsx": "^2.1.1",
    "idb": "^8.0.2",
    "jspdf": "^2.5.2",
    "jspdf-autotable": "^3.8.4",
    "lightweight-charts": "^4.2.3",
    "next": "14.2.35",
    "nextjs-toploader": "^3.9.17",
    "plotly.js-dist-min": "^3.3.1",
    "react": "^18",
    "react-dom": "^18",
    "react-markdown": "^10.1.0",
    "react-plotly.js": "^2.6.0",
    "react-syntax-highlighter": "^16.1.0",
    "recharts": "^3.7.0",
    "swagger-ui-react": "^5.31.0",
    "swr": "^2.4.0",
    "tailwind-merge": "^3.4.0",
    "web-vitals": "^4.2.0",
    "xlsx": "^0.18.5"
  },
  "devDependencies": {
    "@next/bundle-analyzer": "^16.1.6",
    "@playwright/test": "^1.50.0",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.2",
    "@testing-library/user-event": "^14.6.1",
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "@types/react-syntax-highlighter": "^15.5.13",
    "@types/swagger-ui-react": "^5.18.0",
    "@vitejs/plugin-react": "^5.1.3",
    "eslint": "^8",
    "eslint-config-next": "14.2.35",
    "eslint-plugin-security": "^3.0.1",
    "jsdom": "^28.0.0",
    "msw": "^2.12.9",
    "postcss": "^8",
    "tailwindcss": "^3.4.1",
    "typescript": "^5",
    "vitest": "^4.0.18"
  }
}
</file>

<file path="frontend/src/app/announcements/page.tsx">
import { useState, useEffect, useCallback, useRef } from 'react';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { getAnnouncements, type AnnouncementListResponse } from '@/lib/api-client';
import { Breadcrumb } from '@/components/common/Breadcrumb';
⋮----
// ---------------------------------------------------------------------------
// Types (matches AnnouncementItem in api-client.ts)
// ---------------------------------------------------------------------------
⋮----
interface Announcement {
  id: string;
  ticker: string | null;
  title_ar: string | null;
  title_en: string | null;
  body_ar: string | null;
  body_en: string | null;
  source: string | null;
  announcement_date: string | null;
  category: string | null;
  classification: string | null;
  is_material: boolean;
  source_url: string | null;
  created_at: string | null;
}
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Date formatter
// ---------------------------------------------------------------------------
⋮----
function formatDate(dateStr: string | null, locale: string): string
⋮----
// ---------------------------------------------------------------------------
// Skeleton loader
// ---------------------------------------------------------------------------
⋮----
function SkeletonCard()
⋮----
// ---------------------------------------------------------------------------
// Announcement card
// ---------------------------------------------------------------------------
⋮----
{/* Badges row */}
⋮----
{/* Title */}
⋮----
className=
⋮----
onClick=
⋮----
{/* Footer */}
⋮----
// ---------------------------------------------------------------------------
// Main page
// ---------------------------------------------------------------------------
⋮----
const handleFilterChange = (key: string | null) =>
⋮----
{/* Breadcrumb */}
⋮----
{/* Header */}
⋮----
{/* Filter tabs */}
⋮----
/* PostgreSQL required message */
⋮----
{/* Cards */}
⋮----
{/* Pagination */}
</file>

<file path="frontend/src/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;
⋮----
/* ===================================================================
   CSS Custom Properties - Ra'd AI Design Tokens
   =================================================================== */
⋮----
:root {
⋮----
/* Gold Palette */
⋮----
/* Light mode defaults */
⋮----
.dark {
⋮----
/* ===================================================================
   Base Styles
   =================================================================== */
⋮----
html {
⋮----
body {
⋮----
.dark body {
⋮----
/* ===================================================================
   Custom Scrollbar (Gold-themed)
   =================================================================== */
⋮----
::-webkit-scrollbar {
⋮----
::-webkit-scrollbar-track {
⋮----
.dark ::-webkit-scrollbar-track {
⋮----
::-webkit-scrollbar-thumb {
⋮----
::-webkit-scrollbar-thumb:hover {
⋮----
/* Firefox */
* {
⋮----
/* ===================================================================
   Utility Classes
   =================================================================== */
⋮----
.gold-text {
⋮----
.gold-border {
⋮----
.gold-border-hover:hover {
⋮----
.gold-glow {
⋮----
.gold-glow-sm {
⋮----
.gold-underline {
⋮----
/* ===================================================================
   Skeleton Shimmer Animation
   =================================================================== */
⋮----
@layer utilities {
⋮----
.animate-fade-in-up {
.animate-fade-in {
.press-feedback {
.animate-shimmer {
.animate-float {
⋮----
/* ===================================================================
   Reduced Motion
   =================================================================== */
⋮----
*,
⋮----
/* ===================================================================
   Focus-visible defaults
   =================================================================== */
⋮----
:focus-visible {
⋮----
/* ===================================================================
   RTL Support
   =================================================================== */
⋮----
[dir="rtl"] .flip-rtl {
⋮----
/* ===================================================================
   Print Styles
   =================================================================== */
⋮----
/* Hide non-content elements */
header,
⋮----
/* Page setup */
@page {
⋮----
/* Force readable colors */
body,
⋮----
/* Reset dark backgrounds for print */
main,
⋮----
/* Print header branding */
body::before {
⋮----
/* Main content fills the page */
main {
⋮----
/* Tables */
table {
⋮----
th, td {
⋮----
th {
⋮----
/* Keep charts and tables visible */
table,
⋮----
/* Page breaks */
h1, h2, h3 {
⋮----
/* Clean up card borders for print */
⋮----
/* Remove background gradients */
⋮----
/* Links: no decoration */
a {
⋮----
a[href^="#"]:after,
⋮----
/* Text color overrides for readability */
[class*="text-[var("] {
⋮----
[class*="text-accent-green"] {
⋮----
[class*="text-accent-red"] {
⋮----
[class*="text-gold"] {
⋮----
/* ===================================================================
   Scrollbar Hide Utility
   =================================================================== */
⋮----
.scrollbar-hide {
⋮----
.scrollbar-hide::-webkit-scrollbar {
</file>

<file path="frontend/src/app/layout.tsx">
import type { Metadata, Viewport } from 'next';
import { IBM_Plex_Sans_Arabic, Inter } from 'next/font/google';
⋮----
import { ThemeProvider } from '@/providers/ThemeProvider';
import { LanguageProvider } from '@/providers/LanguageProvider';
import { AuthProvider } from '@/lib/hooks/use-auth';
import { ErrorBoundary } from '@/components/common/error-boundary';
import { GlobalKeyboardShortcuts } from '@/components/common/GlobalKeyboardShortcuts';
import { ScrollToTop } from '@/components/common/ScrollToTop';
import { AppShell } from '@/components/layout/AppShell';
import NextTopLoader from 'nextjs-toploader';
import { BackToTop } from '@/components/common/BackToTop';
⋮----
export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>)
</file>

<file path="frontend/src/components/charts/PreBuiltCharts.tsx">
import { useEffect, useRef, useState, useCallback } from 'react';
import { useLanguage } from '@/providers/LanguageProvider';
import {
  getChartSectorMarketCap,
  getChartTopCompanies,
  getChartSectorPE,
  getChartDividendYieldTop,
  type ChartResponse,
  type ChartDataPoint,
} from '@/lib/api-client';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
type ChartFetcher = (signal?: AbortSignal) => Promise<ChartResponse>;
⋮----
interface ChartCardConfig {
  fetcher: ChartFetcher;
  key: string;
  titleAr: string;
  titleEn: string;
  suffix?: string;
  color: string;
}
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function formatLargeNumber(val: number, suffix?: string): string
⋮----
// ---------------------------------------------------------------------------
// Single chart card
// ---------------------------------------------------------------------------
⋮----
// Extract status from ApiError if available
⋮----
{/* Card header */}
⋮----
{/* Card body */}
⋮----
// ---------------------------------------------------------------------------
// Main grid component
// ---------------------------------------------------------------------------
</file>

<file path="frontend/src/components/charts/StockComparisonChart.tsx">
import { useEffect, useRef, useState, useCallback } from 'react';
import { cn } from '@/lib/utils';
import {
  createChart,
  ColorType,
  type IChartApi,
  type ISeriesApi,
  type LineData,
  type Time,
} from 'lightweight-charts';
import { RAID_CHART_OPTIONS } from './chart-config';
import { ChartSkeleton } from './ChartSkeleton';
import { useLanguage } from '@/providers/LanguageProvider';
import { getOHLCVData } from '@/lib/api-client';
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
interface OHLCVItem {
  time: string;
  open: number;
  high: number;
  low: number;
  close: number;
  volume?: number;
}
⋮----
interface StockComparisonChartProps {
  tickers: string[];
  height?: number;
  className?: string;
}
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
async function fetchOHLCV(ticker: string, period: string, signal?: AbortSignal): Promise<OHLCVItem[]>
⋮----
/** Normalize close prices to base 100 (first close = 100). */
function normalizeToBase100(data: OHLCVItem[]): LineData[]
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Track which tickers actually have data
⋮----
// Abort previous fetch
⋮----
// Clean up previous chart
⋮----
// Fetch all tickers in parallel
⋮----
// Create chart
⋮----
// Add line series for each ticker
⋮----
// Crosshair tooltip
⋮----
// Resize observer
⋮----
{/* Toolbar */}
⋮----
{/* Legend */}
⋮----
{/* Period selector */}
⋮----
className=
⋮----
{/* Crosshair tooltip bar */}
⋮----
{/* Chart container */}
</file>

<file path="frontend/src/components/charts/TASIIndexChart.tsx">
import { useEffect, useRef, useState, useCallback, useMemo } from 'react';
import { cn } from '@/lib/utils';
import {
  createChart,
  ColorType,
  type IChartApi,
  type ISeriesApi,
  type CandlestickData,
  type HistogramData,
  type LineData,
  type Time,
} from 'lightweight-charts';
import {
  RAID_CHART_OPTIONS,
  VOLUME_UP_COLOR,
  VOLUME_DOWN_COLOR,
  MA20_COLOR,
  MA50_COLOR,
  AREA_TOP_COLOR,
  AREA_BOTTOM_COLOR,
  LINE_COLOR,
} from './chart-config';
import { ChartSkeleton } from './ChartSkeleton';
import { ChartError } from './ChartError';
import { ChartEmpty } from './ChartEmpty';
import { DataSourceBadge } from './DataSourceBadge';
import { formatVolume } from '@/lib/formatters';
import { useTasiOHLCV } from '@/lib/hooks/use-chart-data';
import type { OHLCVData } from './chart-types';
import dynamic from 'next/dynamic';
import { useLanguage } from '@/providers/LanguageProvider';
import { useChartIndicators } from './tasi/useChartIndicators';
import { IndicatorToggleBar } from './tasi/IndicatorToggleBar';
import { PeriodSelector } from './tasi/PeriodSelector';
import { ChartExportButton } from './tasi/ChartExportButton';
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function calculateMA(data: OHLCVData[], period: number): LineData[]
⋮----
interface TASIIndexChartProps {
  height?: number;
  className?: string;
}
⋮----
// Auto-refresh every 5 min during Tadawul trading hours (Sun-Thu, 10:00-15:00 AST/UTC+3)
⋮----
function isTradingHours(): boolean
⋮----
function handleResize()
⋮----
}, [loading, data && data.length > 0, buildChart]); // eslint-disable-line react-hooks/exhaustive-deps
⋮----
{/* Toolbar */}
⋮----
{/* Left: Title + last price + source badge + period change */}
⋮----
<span className=
⋮----
{/* Right: Controls */}
⋮----
{/* Crosshair tooltip bar */}
⋮----
{/* Chart container */}
⋮----
// SSR-safe dynamic wrapper
</file>

<file path="frontend/src/components/chat/DataTable.tsx">
import { useMemo, useState } from 'react';
import { cn } from '@/lib/utils';
import type { SSETableData } from '@/lib/types';
import { useLanguage } from '@/providers/LanguageProvider';
⋮----
interface DataTableProps {
  data: SSETableData;
}
⋮----
function exportCSV(columns: string[], rows: (string | number | null)[][])
⋮----
const escape = (val: string | number | null) =>
⋮----
// Add BOM for Arabic text support in Excel
⋮----
function compareValues(a: string | number | null, b: string | number | null): number
⋮----
// Both are numbers
⋮----
// Try numeric comparison for string values that look like numbers
⋮----
// Fall back to string comparison
⋮----
/** Format a numeric cell value with smart formatting. */
function formatNumericCell(num: number, columnName: string): string
⋮----
/** Format a cell value with smart number formatting. */
function formatCell(value: string | number | null, columnName: string): string
⋮----
const handleHeaderClick = (colIdx: number) =>
⋮----
aria-sort=
⋮----
onClick=
⋮----
aria-label=
</file>

<file path="frontend/src/lib/hooks/use-api.ts">
import { useCallback, useEffect, useRef, useState } from 'react';
import {
  getNews,
  getNewsFeed,
  getNewsArticle,
  searchNewsFeed,
  getNewsSources,
  getReports,
  getReportsByTicker,
  getNewsByTicker,
  getEntities,
  getEntityDetail,
  getSectors,
  getAnnouncements,
  getMarketMovers,
  getMarketSummary,
  getSectorPerformance,
  getMarketHeatmap,
  getStockDividends,
  getStockFinancialSummary,
  getStockFinancials,
  compareStocks,
  getBatchQuotes,
  type NewsListResponse,
  type NewsFeedResponse,
  type NewsFeedItem,
  type ReportListResponse,
  type EntityListResponse,
  type CompanyDetail,
  type SectorInfo,
  type AnnouncementListResponse,
  type NewsSourcesResponse,
  type MarketMover,
  type MarketSummary,
  type SectorPerformance,
  type HeatmapItem,
  type StockDividends,
  type FinancialSummary,
  type FinancialsResponse,
  type StockComparison,
  type BatchQuote,
} from '@/lib/api-client';
⋮----
// ---------------------------------------------------------------------------
// Generic async-data hook
// ---------------------------------------------------------------------------
⋮----
interface UseAsyncResult<T> {
  data: T | null;
  loading: boolean;
  /** True when data already exists and a background refresh is in progress. */
  isRefreshing: boolean;
  error: string | null;
  /** Timestamp of the last successful data fetch. */
  lastUpdated: Date | null;
  refetch: () => void;
}
⋮----
/** True when data already exists and a background refresh is in progress. */
⋮----
/** Timestamp of the last successful data fetch. */
⋮----
function useAsync<T>(
  fetcher: (signal: AbortSignal) => Promise<T>,
  deps: unknown[] = [],
  autoRefreshMs?: number,
): UseAsyncResult<T>
⋮----
// First fetch (no data yet) -> show loading skeleton
// Subsequent fetches (data exists) -> show subtle refreshing indicator
⋮----
// eslint-disable-next-line react-hooks/exhaustive-deps
⋮----
// Auto-refresh interval
⋮----
// ---------------------------------------------------------------------------
// Domain hooks
// ---------------------------------------------------------------------------
⋮----
export function useNews(params?:
⋮----
export function useReports(params?: {
  page?: number;
  page_size?: number;
  recommendation?: string;
  report_type?: string;
})
⋮----
export function useEntities(params?: {
  limit?: number;
  offset?: number;
  sector?: string;
  search?: string;
})
⋮----
export function useEntityDetail(ticker: string)
⋮----
export function useSectors()
⋮----
export function useAnnouncements(params?: {
  page?: number;
  page_size?: number;
  ticker?: string;
  category?: string;
})
⋮----
/** Market data with 30-second auto-refresh */
export function useMarketData(params?:
⋮----
export function useStockDetail(ticker: string)
⋮----
export function useNewsFeed(params?: {
  limit?: number;
  offset?: number;
  source?: string;
  sentiment?: string;
  date_from?: string;
  date_to?: string;
})
⋮----
export function useNewsArticle(id: string)
⋮----
export function useNewsSearch(params:
⋮----
export function useNewsSources()
⋮----
// ---------------------------------------------------------------------------
// Market Analytics hooks
// ---------------------------------------------------------------------------
⋮----
/** Top gainers or losers with 30-second auto-refresh */
export function useMarketMovers(type: 'gainers' | 'losers', limit?: number)
⋮----
/** Full market summary with 30-second auto-refresh */
export function useMarketSummary()
⋮----
/** Sector performance breakdown */
export function useSectorPerformance()
⋮----
/** Market heatmap data */
export function useMarketHeatmap()
⋮----
// ---------------------------------------------------------------------------
// Stock data hooks
// ---------------------------------------------------------------------------
⋮----
/** Dividend data for a single stock */
export function useStockDividends(ticker: string)
⋮----
/** Financial summary for a single stock */
export function useStockFinancialSummary(ticker: string)
⋮----
/** Financial statements for a single stock */
export function useStockFinancials(
  ticker: string,
  statement?: string,
  period_type?: string,
)
⋮----
/** Compare multiple stocks across metrics */
export function useStockComparison(tickers: string[], metrics: string[])
⋮----
/** Batch quotes for multiple tickers with 30-second auto-refresh */
export function useBatchQuotes(tickers: string[])
⋮----
/** News articles for a specific ticker */
export function useNewsByTicker(ticker: string, params?:
⋮----
/** Reports for a specific ticker */
export function useReportsByTicker(ticker: string, params?:
</file>

<file path="middleware/error_handler.py">
"""
Global error handler middleware and exception handlers.

Catches unhandled exceptions and returns a safe JSON response with a
consistent ``{"error": {"code": ..., "message": ..., "request_id": ...}}``
structure.

Specific built-in exceptions are mapped to appropriate HTTP status codes:
- ``ValueError`` -> 400 (BAD_REQUEST)
- ``PermissionError`` -> 403 (FORBIDDEN)
- ``FileNotFoundError`` / ``KeyError`` -> 404 (NOT_FOUND)
- ``ConnectionError`` -> 503 (SERVICE_UNAVAILABLE)

Full tracebacks are logged server-side but never exposed to clients.
In debug mode (``SERVER_DEBUG=true``), the error message is included;
in production only a generic message is returned for unexpected errors.

Exception handlers for ``HTTPException`` and ``RequestValidationError``
are registered via ``install_exception_handlers()`` to ensure all error
responses use the same shape.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_DEBUG = os.environ.get("SERVER_DEBUG", "false").lower() in ("true", "1", "yes")
⋮----
_set_request_id_ctx = None
⋮----
# Maps exception types to (http_status, error_code).
_EXCEPTION_MAP: dict[type, tuple[int, str]] = {
⋮----
# HTTP status -> default error code for HTTPException responses
_STATUS_CODE_MAP: dict[int, str] = {
⋮----
def _get_request_id(request: Request) -> str
⋮----
"""Get or generate a request ID for correlation."""
# Check if request already has a request_id (set by logging middleware)
request_id = getattr(request.state, "request_id", None)
⋮----
# Check incoming header
request_id = request.headers.get("x-request-id")
⋮----
# Generate a new one
⋮----
"""Build a consistent error JSON response."""
⋮----
class ErrorHandlerMiddleware(BaseHTTPMiddleware)
⋮----
"""Catches unhandled exceptions and returns a JSON error response.

    Known exception types are mapped to specific HTTP status codes.
    Unknown exceptions always return 500.  Stack traces are logged
    server-side but never included in the HTTP response (unless
    ``SERVER_DEBUG`` is enabled, in which case the error message is
    included for developer convenience).
    """
⋮----
async def dispatch(self, request: Request, call_next)
⋮----
# Ensure request_id is available for the entire request lifecycle
request_id = _get_request_id(request)
⋮----
# Check for a mapped exception type
⋮----
# Unmapped / unexpected exception -> 500
⋮----
message = str(exc) if _DEBUG else "Internal server error"
⋮----
def install_exception_handlers(app: "FastAPI") -> None
⋮----
"""Register custom exception handlers on the FastAPI app.

    This ensures that ``HTTPException`` and ``RequestValidationError``
    responses use the same ``{"error": {...}}`` shape as the middleware.
    Call this **after** creating the app but before starting the server.
    """
⋮----
@app.exception_handler(HTTPException)
    async def _http_exception_handler(request: Request, exc: HTTPException)
⋮----
code = _STATUS_CODE_MAP.get(exc.status_code, "ERROR")
message = exc.detail if isinstance(exc.detail, str) else str(exc.detail)
⋮----
# Summarize validation errors into a human-readable message
errors = exc.errors()
⋮----
parts = []
for err in errors[:5]:  # Limit to first 5 errors
loc = " -> ".join(str(part) for part in err.get("loc", []))
msg = err.get("msg", "invalid")
⋮----
message = "; ".join(parts)
⋮----
message = "Request validation failed"
</file>

<file path="requirements.txt">
# Core framework
vanna>=2.0.2,<3.0
fastapi>=0.115.6,<1.0
uvicorn[standard]>=0.34.0,<1.0
python-dotenv>=1.0.1,<2.0
pydantic-settings>=2.0.0,<3.0
pydantic[email]>=2.5.0,<3.0

# LLM providers
openai>=1.20.0,<3.0
anthropic>=0.41.0,<1.0

# Database
psycopg2-binary>=2.9.10,<3.0
sqlalchemy[asyncio]>=2.0.0,<3.0
aiosqlite>=0.20.0,<1.0

# Data & visualization
pandas>=2.1.0,<3.0
numpy>=1.24.0,<3.0
plotly>=5.20.0,<7.0
yfinance>=0.2.35,<2.0

# Web scraping
lxml>=4.10.0,<7.0
beautifulsoup4>=4.12.0,<5.0
requests>=2.31.0,<3.0

# Auth
pyjwt>=2.8.1,<3.0
bcrypt>=4.1.0,<5.0

# Caching & messaging
redis>=5.0.0,<8.0
msgpack>=1.0.0,<2.0

# Scheduling
apscheduler>=3.10.4,<4.0

# SQL parsing
sqlparse>=0.5.0,<1.0
</file>

<file path="services/stock_ohlcv.py">
"""
Per-stock OHLCV data fetcher service.

Fetches OHLCV data for individual Saudi-listed stocks via yfinance with
in-memory caching, circuit breaker, and deterministic mock fallback.
Saudi tickers get ".SR" suffix automatically (e.g. "2222" -> "2222.SR").
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Shared cache & circuit breaker instances
⋮----
_cache = YFinanceCache(ttl=300, max_entries=500, name="stock_ohlcv")
⋮----
# Per-ticker locks: allow concurrent fetches for different tickers
_ticker_locks: Dict[str, threading.Lock] = {}
_locks_guard = threading.Lock()  # Protects _ticker_locks dict itself
_MAX_LOCKS = 1000  # Prevent unbounded growth
⋮----
def _get_ticker_lock(symbol: str, period: str) -> threading.Lock
⋮----
"""Return a per-(symbol, period) lock, creating one if needed."""
key = f"{symbol}:{period}"
⋮----
# Evict oldest if at capacity
⋮----
# Remove first entry (oldest by insertion order in Python 3.7+)
⋮----
VALID_PERIODS = ("1mo", "3mo", "6mo", "1y", "2y", "5y")
⋮----
_breaker = CircuitBreaker(
⋮----
timeout=900,  # 15 min
⋮----
# Ticker normalization
⋮----
def _normalize_ticker(ticker: str) -> str
⋮----
"""Append .SR suffix for Saudi stocks if not already present."""
ticker = ticker.strip().upper()
⋮----
ticker = ticker + ".SR"
⋮----
# Cache helpers (thin wrappers over YFinanceCache)
⋮----
def _get_cached(ticker: str, period: str) -> Optional[Dict[str, Any]]
⋮----
"""Return cached data if still fresh."""
⋮----
def _get_stale_cached(ticker: str, period: str) -> Optional[Dict[str, Any]]
⋮----
"""Return stale cache entry (for fallback on fetch failure)."""
payload = _cache.get_stale((ticker, period))
⋮----
result = dict(payload)
⋮----
def _set_cache(ticker: str, period: str, payload: Dict[str, Any]) -> None
⋮----
# Circuit breaker helpers (delegates to shared CircuitBreaker)
⋮----
def _is_circuit_open() -> bool
⋮----
"""Return True if the circuit breaker is currently open."""
⋮----
def _record_failure() -> None
⋮----
"""Increment consecutive failure count; open circuit if threshold reached."""
⋮----
def _record_success() -> None
⋮----
"""Reset circuit breaker on a successful fetch."""
⋮----
def get_circuit_breaker_status() -> Dict[str, Any]
⋮----
"""Return circuit breaker diagnostics for the health endpoint."""
⋮----
# Mock data generator (deterministic per ticker)
⋮----
def _generate_mock_data(ticker: str, period: str) -> List[Dict[str, Any]]
⋮----
"""Generate deterministic mock OHLCV data seeded by ticker + period."""
period_days = {
days = period_days.get(period, 252)
⋮----
# Deterministic seed based on ticker so the same ticker always produces
# the same mock data across requests.
seed = sum(ord(c) for c in ticker)
rng = random.Random(seed)
⋮----
# Base price derived from ticker digits (realistic range for Saudi stocks)
digits = "".join(c for c in ticker if c.isdigit())
base_price = float(digits[:4]) / 100.0 if digits else 50.0
base_price = max(10.0, min(base_price, 500.0))
⋮----
data: List[Dict[str, Any]] = []
end_date = datetime.utcnow().date()
start_date = end_date - timedelta(days=int(days * 1.45))
⋮----
current = start_date
price = base_price
count = 0
⋮----
# Skip weekends (Tadawul operates Sun-Thu; Friday=4, Saturday=5)
⋮----
change_pct = rng.gauss(0.0002, 0.015)
price = price * (1 + change_pct)
day_range = price * rng.uniform(0.005, 0.025)
open_price = price + rng.uniform(-day_range / 2, day_range / 2)
high = max(open_price, price) + rng.uniform(0, day_range / 2)
low = min(open_price, price) - rng.uniform(0, day_range / 2)
volume = int(rng.uniform(100_000, 20_000_000))
⋮----
# Main fetcher
⋮----
def fetch_stock_ohlcv(ticker: str, period: str = "1y") -> Dict[str, Any]
⋮----
"""Fetch OHLCV data for a single Saudi stock.

    Tries yfinance first with .SR suffix normalization.
    Falls back to stale cache or deterministic mock data.

    Args:
        ticker: Saudi stock ticker (e.g. "2222", "2222.SR").
        period: One of '1mo', '3mo', '6mo', '1y', '2y', '5y'.

    Returns:
        Dict with keys: data, source, last_updated, symbol, period, count.
    """
symbol = _normalize_ticker(ticker)
t_start = time.monotonic()
⋮----
# Check fresh cache first
cached = _get_cached(symbol, period)
⋮----
duration_ms = round((time.monotonic() - t_start) * 1000, 1)
⋮----
# Serialize yfinance fetches per ticker (different tickers fetch concurrently)
⋮----
# Double-check cache inside the lock
⋮----
# Check circuit breaker
⋮----
# Try yfinance
⋮----
yf_ticker = yf.Ticker(symbol)
df = yf_ticker.history(period=period, auto_adjust=True)
⋮----
df = df.reset_index()
⋮----
date_val = row.get("Date")
⋮----
time_str = date_val.strftime("%Y-%m-%d")
⋮----
time_str = str(date_val)[:10]
⋮----
payload = {
⋮----
exc_type = type(exc).__name__
exc_msg = str(exc)
error_category = "unknown"
⋮----
error_category = "rate_limit"
⋮----
error_category = "network"
⋮----
error_category = "data_error"
⋮----
# Fallback: stale cache
stale = _get_stale_cached(symbol, period)
⋮----
# Fallback: mock data
mock_data = _generate_mock_data(symbol, period)
⋮----
# Convenience alias used by the verification command
get_stock_ohlcv = fetch_stock_ohlcv
⋮----
def get_cache_status() -> Dict[str, Any]
⋮----
"""Return cache diagnostic information for the health endpoint."""
⋮----
entry = _cache.newest_entry()
⋮----
age = time.monotonic() - entry["fetched_at"]
fresh = age < _cache.ttl
</file>

<file path="templates/index.html">
<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ra'd AI - Saudi Stock Market AI Analyst</title>
    <meta name="description" content="AI-powered Saudi stock market analysis. Query TASI data with natural language.">

    <!-- Tajawal Font (Arabic-friendly, great for English) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;500;700&display=swap" rel="stylesheet">

    <!-- Marked.js for markdown rendering in chat responses -->
    <script src="https://cdn.jsdelivr.net/npm/marked@14/marked.min.js" async crossorigin="anonymous"></script>
    <!-- DOMPurify for sanitizing rendered markdown (prevents XSS via innerHTML) -->
    <script src="https://cdn.jsdelivr.net/npm/dompurify@3/dist/purify.min.js" async crossorigin="anonymous"></script>

    <style>
        /* ===================================================================
           CSS CUSTOM PROPERTIES - Ra'd AI Design Tokens
           =================================================================== */
        :root {
            /* Gold Palette */
            --gold-primary: #D4A84B;
            --gold-light: #E8C872;
            --gold-dark: #B8860B;
            --gold-gradient: linear-gradient(135deg, #D4A84B 0%, #E8C872 50%, #B8860B 100%);
            --gold-glow: rgba(212, 168, 75, 0.3);
            --gold-border: rgba(212, 168, 75, 0.2);
            --gold-border-hover: rgba(212, 168, 75, 0.6);

            /* Backgrounds */
            --bg-dark: #0E0E0E;
            --bg-card: #1A1A1A;
            --bg-card-hover: #252525;
            --bg-input: #2A2A2A;
            --bg-page: radial-gradient(ellipse at top, #1a1a1a 0%, #0E0E0E 50%);

            /* Text */
            --text-primary: #FFFFFF;
            --text-secondary: #B0B0B0;
            --text-muted: #999999;

            /* Accent Colors */
            --accent-green: #4CAF50;
            --accent-red: #FF6B6B;
            --accent-blue: #4A9FFF;
            --accent-warning: #FFA726;

            /* Spacing */
            --space-xs: 4px;
            --space-sm: 8px;
            --space-md: 16px;
            --space-lg: 24px;
            --space-xl: 32px;
            --space-2xl: 48px;

            /* Radii */
            --radius-sm: 8px;
            --radius-md: 12px;
            --radius-lg: 16px;
            --radius-pill: 9999px;

            /* Transitions */
            --transition-base: 0.3s ease;

            /* Layout */
            --content-max-width: 960px;
            --header-height: 64px;
        }

        /* ===================================================================
           RESET & BASE
           =================================================================== */
        *, *::before, *::after {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            height: 100%;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        body {
            font-family: 'Tajawal', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: var(--bg-page);
            background-attachment: fixed;
            color: var(--text-primary);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            overflow-x: hidden;
            line-height: 1.6;
        }

        /* ===================================================================
           CUSTOM SCROLLBAR (Gold-themed)
           =================================================================== */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-dark);
        }

        ::-webkit-scrollbar-thumb {
            background: rgba(212, 168, 75, 0.3);
            border-radius: var(--radius-pill);
        }

        ::-webkit-scrollbar-thumb:hover {
            background: rgba(212, 168, 75, 0.5);
        }

        /* Firefox */
        * {
            scrollbar-width: thin;
            scrollbar-color: rgba(212, 168, 75, 0.3) var(--bg-dark);
        }

        /* ===================================================================
           ANIMATIONS
           =================================================================== */
        @keyframes goldPulse {
            0%, 100% { opacity: 0.4; }
            50% { opacity: 1; }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes shimmer {
            0% { background-position: -200% center; }
            100% { background-position: 200% center; }
        }

        @keyframes dotBounce {
            0%, 80%, 100% { transform: scale(0); }
            40% { transform: scale(1); }
        }

        @keyframes subtleFloat {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-4px); }
        }

        /* Chart loading shimmer effect */
        @keyframes chartShimmer {
            0% { background-position: -200% 0; }
            100% { background-position: 200% 0; }
        }

        .chart-loading {
            background: linear-gradient(90deg,
                var(--bg-card) 0%,
                rgba(212, 168, 75, 0.05) 50%,
                var(--bg-card) 100%);
            background-size: 200% 100%;
            animation: chartShimmer 1.5s ease-in-out infinite;
            border-radius: var(--radius-sm);
            min-height: 200px;
        }

        /* ===================================================================
           HEADER
           =================================================================== */
        .app-header {
            position: sticky;
            top: 0;
            z-index: 100;
            background: rgba(14, 14, 14, 0.85);
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            border-bottom: 1px solid var(--gold-border);
            padding: 0 var(--space-lg);
            height: var(--header-height);
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .header-inner {
            width: 100%;
            max-width: var(--content-max-width);
            display: flex;
            align-items: center;
            gap: var(--space-md);
        }

        .brand-mark {
            width: 42px;
            height: 42px;
            background: var(--gold-gradient);
            border-radius: var(--radius-sm);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 18px;
            font-weight: 700;
            color: var(--bg-dark);
            letter-spacing: -0.5px;
            flex-shrink: 0;
            box-shadow: 0 2px 12px rgba(212, 168, 75, 0.25);
        }

        .header-text {
            display: flex;
            flex-direction: column;
            gap: 1px;
        }

        .header-text h1 {
            font-size: 17px;
            font-weight: 700;
            color: var(--text-primary);
            letter-spacing: 0.2px;
            line-height: 1.3;
        }

        .header-text p {
            font-size: 12px;
            font-weight: 400;
            color: var(--text-muted);
            line-height: 1.3;
        }

        .header-status {
            margin-left: auto;
            display: flex;
            align-items: center;
            gap: var(--space-sm);
        }

        .status-dot {
            width: 8px;
            height: 8px;
            background: var(--accent-green);
            border-radius: 50%;
            animation: goldPulse 2s ease-in-out infinite;
        }

        .status-label {
            font-size: 12px;
            font-weight: 500;
            color: var(--accent-green);
        }

        /* ===================================================================
           MAIN CONTENT WRAPPER
           =================================================================== */
        .app-main {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: var(--space-sm) var(--space-md);
            width: 100%;
            min-height: 0;
        }

        .content-column {
            width: 100%;
            max-width: var(--content-max-width);
            display: flex;
            flex-direction: column;
            flex: 1;
            min-height: 0;
            gap: var(--space-sm);
        }

        /* ===================================================================
           HERO / BRAND SECTION
           =================================================================== */
        .hero-section {
            text-align: center;
            padding: var(--space-md) 0 var(--space-xs);
            animation: fadeInUp 0.6s ease-out;
        }

        .hero-title {
            font-size: 28px;
            font-weight: 700;
            background: var(--gold-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            line-height: 1.3;
            margin-bottom: var(--space-xs);
        }

        .hero-subtitle {
            font-size: 14px;
            font-weight: 400;
            color: #C8C8C8;
            margin-bottom: var(--space-md);
        }

        /* ===================================================================
           QUICK STATS BAR
           =================================================================== */
        .stats-bar {
            display: flex;
            justify-content: center;
            gap: var(--space-md);
            flex-wrap: wrap;
            animation: fadeInUp 0.6s ease-out 0.1s both;
        }

        .stat-pill {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            background: var(--bg-card);
            border: 1px solid var(--gold-border);
            border-radius: var(--radius-pill);
            padding: 6px 16px;
            font-size: 13px;
            font-weight: 500;
            color: var(--text-secondary);
            transition: all var(--transition-base);
        }

        .stat-pill:hover {
            border-color: var(--gold-border-hover);
            color: var(--text-primary);
        }

        .stat-pill .stat-icon {
            font-size: 14px;
        }

        .stat-pill .stat-value {
            color: var(--gold-primary);
            font-weight: 700;
        }

        /* ===================================================================
           EXAMPLE QUESTION CHIPS
           =================================================================== */
        .suggestions-section {
            animation: fadeInUp 0.6s ease-out 0.2s both;
        }

        .suggestions-label {
            font-size: 13px;
            font-weight: 600;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 1.5px;
            margin-bottom: var(--space-md);
            text-align: center;
        }

        .suggestions-grid {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: var(--space-sm);
        }

        .suggestion-chip {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            background: var(--bg-card);
            border: 1px solid var(--gold-border);
            border-radius: var(--radius-pill);
            padding: 8px 16px;
            font-family: 'Tajawal', sans-serif;
            font-size: 13px;
            font-weight: 500;
            color: var(--text-secondary);
            cursor: pointer;
            transition: all var(--transition-base);
            text-align: left;
            line-height: 1.3;
            white-space: nowrap;
            position: relative;
            z-index: 10;
        }

        .suggestion-chip:hover {
            background: var(--bg-card-hover);
            border-color: var(--gold-primary);
            color: var(--text-primary);
            box-shadow: 0 4px 20px var(--gold-glow);
            transform: translateY(-2px);
        }

        .suggestion-chip:active {
            transform: translateY(0);
            box-shadow: 0 2px 8px var(--gold-glow);
        }

        .chip-icon {
            flex-shrink: 0;
            font-size: 14px;
        }

        /* ===================================================================
           CHAT AREA
           =================================================================== */
        .chat-container {
            height: calc(100vh - 180px);
            min-height: 500px;
            max-height: 800px;
            background: var(--bg-card);
            border: 1px solid var(--gold-border);
            border-radius: var(--radius-md);
            overflow: hidden;
            animation: fadeInUp 0.6s ease-out 0.3s both;
            position: relative;
            display: flex;
            flex-direction: column;
        }

        /* ===================================================================
           NATIVE CHAT UI
           =================================================================== */
        .raid-messages {
            flex: 1;
            overflow-y: auto;
            padding: var(--space-md);
            display: flex;
            flex-direction: column;
            gap: var(--space-md);
        }

        .raid-message {
            display: flex;
            gap: 10px;
            max-width: 90%;
            animation: fadeInUp 0.3s ease-out;
        }

        .raid-message.user {
            align-self: flex-end;
            flex-direction: row-reverse;
        }

        .raid-message.assistant {
            align-self: flex-start;
        }

        .raid-avatar {
            width: 32px;
            height: 32px;
            border-radius: var(--radius-sm);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 11px;
            font-weight: 700;
            flex-shrink: 0;
            margin-top: 2px;
        }

        .raid-message.assistant .raid-avatar {
            background: var(--gold-gradient);
            color: var(--bg-dark);
        }

        .raid-message.user .raid-avatar {
            background: var(--bg-input);
            color: var(--text-secondary);
        }

        .raid-bubble {
            border-radius: var(--radius-md);
            padding: 10px var(--space-md);
            line-height: 1.6;
            font-size: 14px;
            color: var(--text-primary);
            overflow-wrap: break-word;
            word-break: break-word;
            min-width: 0;
        }

        .raid-message.assistant .raid-bubble {
            background: var(--bg-card-hover);
        }

        .raid-message.user .raid-bubble {
            background: rgba(212, 168, 75, 0.12);
            border: 1px solid rgba(212, 168, 75, 0.25);
        }

        /* Markdown content inside bubbles */
        .raid-bubble p { margin: 0 0 8px; }
        .raid-bubble p:last-child { margin-bottom: 0; }
        .raid-bubble h1, .raid-bubble h2, .raid-bubble h3, .raid-bubble h4 {
            color: var(--gold-primary);
            margin: 12px 0 6px;
            font-weight: 700;
        }
        .raid-bubble h4 { font-size: 15px; }
        .raid-bubble h3 { font-size: 16px; }
        .raid-bubble h2 { font-size: 18px; }
        .raid-bubble h1 { font-size: 20px; }
        .raid-bubble ul, .raid-bubble ol {
            padding-left: 20px;
            margin: 6px 0;
        }
        .raid-bubble li { margin: 3px 0; }
        .raid-bubble strong { color: var(--text-primary); }
        .raid-bubble a {
            color: var(--gold-primary);
            text-decoration: underline;
        }

        .raid-bubble pre {
            background: #111111;
            border: 1px solid rgba(212, 168, 75, 0.12);
            border-radius: var(--radius-sm);
            padding: 10px 14px;
            overflow-x: auto;
            margin: 8px 0;
            font-size: 13px;
            line-height: 1.5;
        }

        .raid-bubble code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 13px;
        }

        .raid-bubble :not(pre) > code {
            background: rgba(212, 168, 75, 0.1);
            padding: 2px 6px;
            border-radius: 4px;
            color: var(--gold-light);
            font-size: 0.9em;
        }

        .raid-bubble table {
            width: 100%;
            border-collapse: collapse;
            margin: 8px 0;
            font-size: 13px;
        }

        .raid-bubble th {
            background: rgba(212, 168, 75, 0.15);
            color: var(--gold-primary);
            font-weight: 600;
            padding: 8px 12px;
            text-align: left;
            border-bottom: 2px solid rgba(212, 168, 75, 0.3);
        }

        .raid-bubble td {
            padding: 6px 12px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
        }

        .raid-bubble tr:hover td {
            background: rgba(212, 168, 75, 0.04);
        }

        .raid-chart-wrap {
            margin: 12px 0;
            border-radius: var(--radius-sm);
            overflow: hidden;
            min-height: 350px;
        }

        .raid-typing {
            display: inline-flex;
            gap: 4px;
            align-items: center;
            padding: 4px 0;
        }

        .raid-typing span {
            width: 8px;
            height: 8px;
            background: var(--gold-primary);
            border-radius: 50%;
            animation: dotBounce 1.4s ease-in-out infinite both;
        }
        .raid-typing span:nth-child(1) { animation-delay: -0.32s; }
        .raid-typing span:nth-child(2) { animation-delay: -0.16s; }
        .raid-typing span:nth-child(3) { animation-delay: 0s; }

        .raid-input-area {
            display: flex;
            align-items: flex-end;
            gap: var(--space-sm);
            padding: 10px var(--space-md);
            border-top: 1px solid var(--gold-border);
            background: var(--bg-card);
            border-radius: 0 0 var(--radius-md) var(--radius-md);
        }

        #raid-input {
            flex: 1;
            background: var(--bg-input);
            border: 1px solid var(--gold-border);
            border-radius: var(--radius-md);
            padding: 10px 16px;
            color: var(--text-primary);
            font-family: 'Tajawal', sans-serif;
            font-size: 14px;
            line-height: 1.5;
            resize: none;
            min-height: 42px;
            max-height: 120px;
            outline: none;
            transition: border-color var(--transition-base);
        }

        #raid-input:focus {
            border-color: var(--gold-primary);
            box-shadow: 0 0 0 2px rgba(212, 168, 75, 0.15);
        }

        #raid-input::placeholder {
            color: var(--text-muted);
        }

        .raid-send-btn {
            width: 42px;
            height: 42px;
            border-radius: var(--radius-md);
            background: var(--gold-gradient);
            border: none;
            color: var(--bg-dark);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            flex-shrink: 0;
            transition: opacity var(--transition-base), transform 0.1s;
        }

        .raid-send-btn:hover { opacity: 0.9; }
        .raid-send-btn:active { transform: scale(0.95); }
        .raid-send-btn:disabled { opacity: 0.4; cursor: not-allowed; }
        .raid-send-btn svg { width: 18px; height: 18px; }

        /* ===================================================================
           PLOTLY DARK THEME - Comprehensive Overrides
           =================================================================== */

        /* Background transparency */
        .js-plotly-plot,
        .plotly,
        .plot-container {
            background: transparent !important;
        }

        .js-plotly-plot .plotly .main-svg {
            background: transparent !important;
        }

        /* Chart title - gold */
        .js-plotly-plot .plotly .g-gtitle text {
            fill: #D4A84B !important;
            font-family: 'Tajawal', sans-serif !important;
        }

        /* Axis titles */
        .js-plotly-plot .plotly .g-xtitle text,
        .js-plotly-plot .plotly .g-ytitle text {
            fill: #E0E0E0 !important;
            font-family: 'Tajawal', sans-serif !important;
        }

        /* Axis tick labels */
        .js-plotly-plot .plotly .xtick text,
        .js-plotly-plot .plotly .ytick text,
        .js-plotly-plot .plotly .ztick text {
            fill: #B0B0B0 !important;
        }

        /* Grid lines */
        .js-plotly-plot .plotly .gridlayer line {
            stroke: rgba(212, 168, 75, 0.08) !important;
        }

        .js-plotly-plot .plotly .zerolinelayer line {
            stroke: rgba(212, 168, 75, 0.15) !important;
        }

        /* Legend */
        .js-plotly-plot .plotly .legend {
            background: rgba(26, 26, 26, 0.9) !important;
        }

        .js-plotly-plot .plotly .legend text {
            fill: #B0B0B0 !important;
        }

        .js-plotly-plot .plotly .legend .bg {
            fill: rgba(26, 26, 26, 0.9) !important;
            stroke: rgba(212, 168, 75, 0.2) !important;
        }

        /* Modebar (toolbar) */
        .js-plotly-plot .plotly .modebar {
            background: rgba(26, 26, 26, 0.9) !important;
            border-radius: 6px;
        }

        .js-plotly-plot .plotly .modebar-btn path {
            fill: #707070 !important;
        }

        .js-plotly-plot .plotly .modebar-btn:hover path {
            fill: #D4A84B !important;
        }

        /* Hover labels */
        .js-plotly-plot .plotly .hoverlayer .hovertext rect {
            fill: rgba(26, 26, 26, 0.95) !important;
            stroke: rgba(212, 168, 75, 0.4) !important;
        }

        .js-plotly-plot .plotly .hoverlayer .hovertext text {
            fill: #E0E0E0 !important;
        }

        /* Plotly table styling */
        .js-plotly-plot .plotly .table .cell > text {
            fill: #E0E0E0 !important;
        }

        .js-plotly-plot .plotly .table .header > text {
            fill: #D4A84B !important;
        }

        /* Colorbar (for heatmaps) */
        .js-plotly-plot .plotly .cbtitle text,
        .js-plotly-plot .plotly .colorbar text {
            fill: #B0B0B0 !important;
        }

        /* Annotation text (for heatmap cell values) */
        .js-plotly-plot .plotly .annotation text {
            fill: #E0E0E0 !important;
        }

        /* ===================================================================
           LOADING DOTS ANIMATION
           =================================================================== */
        .loading-dots {
            display: inline-flex;
            gap: 4px;
            align-items: center;
            padding: 8px 0;
        }

        .loading-dots span {
            width: 8px;
            height: 8px;
            background: var(--gold-primary);
            border-radius: 50%;
            animation: dotBounce 1.4s ease-in-out infinite both;
        }

        .loading-dots span:nth-child(1) { animation-delay: -0.32s; }
        .loading-dots span:nth-child(2) { animation-delay: -0.16s; }
        .loading-dots span:nth-child(3) { animation-delay: 0s; }

        /* ===================================================================
           FOOTER
           =================================================================== */
        .app-footer {
            text-align: center;
            padding: var(--space-md) var(--space-lg);
            border-top: 1px solid rgba(212, 168, 75, 0.08);
        }

        .footer-text {
            font-size: 12px;
            color: #8A8A8A;
            line-height: 1.6;
        }

        .footer-text a {
            color: var(--gold-primary);
            text-decoration: none;
            transition: color var(--transition-base);
        }

        .footer-text a:hover {
            color: var(--gold-light);
        }

        .footer-divider {
            display: inline-block;
            margin: 0 8px;
            color: rgba(112, 112, 112, 0.4);
        }

        /* ===================================================================
           RESPONSIVE DESIGN
           =================================================================== */

        /* Tablets and below */
        @media (max-width: 768px) {
            .hero-title {
                font-size: 20px;
            }

            .hero-subtitle {
                font-size: 13px;
            }

            .stats-bar {
                gap: var(--space-xs);
            }

            .stat-pill {
                padding: 5px 10px;
                font-size: 11px;
            }

            .app-main {
                padding: var(--space-sm) var(--space-sm);
            }

            .hero-section {
                padding: var(--space-xs) 0;
            }

            .chat-container {
                height: calc(100vh - 160px);
                min-height: 450px;
                border-radius: var(--radius-sm);
            }

            .suggestion-chip {
                font-size: 11px;
                padding: 6px 12px;
            }

            .suggestions-label {
                margin-bottom: var(--space-sm);
            }
        }

        /* Mobile */
        @media (max-width: 480px) {
            :root {
                --header-height: 56px;
            }

            .header-text h1 {
                font-size: 15px;
            }

            .header-text p {
                display: none;
            }

            .status-label {
                display: none;
            }

            .hero-title {
                font-size: 20px;
            }

            .chat-container {
                height: calc(100vh - 240px);
                min-height: 400px;
                border-radius: var(--radius-sm);
            }

            .stats-bar {
                flex-direction: column;
                align-items: center;
            }

            .suggestions-grid {
                gap: var(--space-xs);
            }
        }

        /* Very small phones */
        @media (max-width: 360px) {
            .suggestions-grid {
                flex-direction: column;
                align-items: stretch;
            }

            .suggestion-chip {
                white-space: normal;
                text-align: center;
                justify-content: center;
            }
        }

        /* Large screens */
        @media (min-width: 1200px) {
            :root {
                --content-max-width: 1080px;
            }

            .chat-container {
                min-height: 600px;
            }

            .hero-title {
                font-size: 34px;
            }

            .suggestion-chip {
                font-size: 13px;
                padding: 10px 18px;
            }
        }

        @media (min-width: 1600px) {
            :root {
                --content-max-width: 1200px;
            }
        }
    </style>
    <link rel="stylesheet" href="/static/raid-enhancements.css">
</head>
<body>
    <div id="new-portal-banner" style="background: linear-gradient(135deg, #D4A84B, #E8C872); color: #0E0E0E; text-align: center; padding: 10px 16px; font-family: 'Tajawal', sans-serif; font-size: 14px; font-weight: 600; position: relative; z-index: 9999;">
      New Ra'd AI portal is live!
      <a href="{{FRONTEND_URL}}" style="color: #0E0E0E; text-decoration: underline; margin-inline-start: 8px; font-weight: 700;">Visit the new experience &rarr;</a>
      <button onclick="document.getElementById('new-portal-banner').remove()" style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%); background: none; border: none; color: #0E0E0E; font-size: 18px; cursor: pointer; padding: 4px;">&times;</button>
    </div>
    <a href="#main-content" class="skip-to-content">Skip to content</a>

    <!-- ================================================================
         HEADER
         ================================================================ -->
    <header class="app-header" role="banner">
        <div class="header-inner">
            <div class="brand-mark" aria-hidden="true">RA</div>
            <div class="header-text">
                <h1>Ra'd AI</h1>
                <p>Saudi Financial Intelligence Platform</p>
            </div>
            <div class="header-status">
                <span class="status-dot" aria-hidden="true"></span>
                <span class="status-label">Online</span>
            </div>
        </div>
    </header>

    <!-- ================================================================
         MAIN CONTENT
         ================================================================ -->
    <main class="app-main" role="main" id="main-content">
        <div class="content-column">

            <!-- Hero / Brand Title -->
            <section class="hero-section">
                <h2 class="hero-title">Saudi Stock Market AI Analyst</h2>
                <p class="hero-subtitle">
                    Ask questions in plain English about TASI-listed companies, financials, and market data.
                </p>

                <!-- Quick Stats Bar -->
                <div class="stats-bar" id="stats-bar">
                    <div class="stat-pill">
                        <span class="stat-icon" aria-hidden="true">&#xe001;</span>
                        <span class="stat-value">~500</span> Companies
                    </div>
                    <div class="stat-pill">
                        <span class="stat-icon" aria-hidden="true">&#x1F4CA;</span>
                        <span class="stat-value">10</span> Tables
                    </div>
                    <div class="stat-pill">
                        <span class="stat-icon" aria-hidden="true">&#x1F3E6;</span>
                        TASI Data
                    </div>
                    <div class="stat-pill">
                        <span class="stat-icon" aria-hidden="true">&#x26A1;</span>
                        Powered by <span class="stat-value">Gemini</span>
                    </div>
                </div>
            </section>

            <!-- Example Question Chips -->
            <section class="suggestions-section" id="suggestions-section" aria-label="Example questions">
                <p class="suggestions-label">Try asking</p>
                <div class="suggestions-grid">
                    <button class="suggestion-chip" data-query="Chart the top 10 companies by market cap">
                        <span class="chip-icon" aria-hidden="true">&#x1F4CA;</span>
                        Bar chart: Top 10 by market cap
                    </button>
                    <button class="suggestion-chip" data-query="Show a heatmap of ROE, ROA, and profit margin for the top 15 companies by market cap">
                        <span class="chip-icon" aria-hidden="true">&#x1F525;</span>
                        Heatmap: Profitability metrics
                    </button>
                    <button class="suggestion-chip" data-query="Plot the annual revenue trend for Saudi Aramco (2222.SR) over all available periods">
                        <span class="chip-icon" aria-hidden="true">&#x1F4C8;</span>
                        Line chart: Aramco revenue trend
                    </button>
                    <button class="suggestion-chip" data-query="Compare average P/E ratio, P/B ratio, and dividend yield across all sectors in a chart">
                        <span class="chip-icon" aria-hidden="true">&#x1F504;</span>
                        Chart: Sector valuation comparison
                    </button>
                    <button class="suggestion-chip" data-query="Show a scatter plot of market cap vs trailing P/E for all companies that have both values">
                        <span class="chip-icon" aria-hidden="true">&#x2B50;</span>
                        Scatter: Market cap vs P/E
                    </button>
                    <button class="suggestion-chip" data-query="Visualize the distribution of dividend yields across all companies that pay dividends">
                        <span class="chip-icon" aria-hidden="true">&#x1F4B0;</span>
                        Histogram: Dividend yield distribution
                    </button>
                    <button class="suggestion-chip" data-query="Which 5 sectors have the most companies and what is their average market cap? Show as a chart">
                        <span class="chip-icon" aria-hidden="true">&#x1F3E2;</span>
                        Chart: Sectors by company count
                    </button>
                    <button class="suggestion-chip" data-query="Show a heatmap of total debt, total assets, and stockholders equity for the 10 largest Financial Services companies">
                        <span class="chip-icon" aria-hidden="true">&#x1F3E6;</span>
                        Heatmap: Bank balance sheets
                    </button>
                </div>
            </section>

            <div id="raid-status" aria-live="polite" class="sr-only" style="position:absolute;width:1px;height:1px;overflow:hidden;clip:rect(0,0,0,0);"></div>

            <!-- Chat Component (native, no CDN dependency) -->
            <div class="chat-container" id="raid-chat">
                <div class="raid-messages" id="raid-messages">
                    <div class="raid-message assistant">
                        <div class="raid-avatar" aria-hidden="true">RA</div>
                        <div class="raid-bubble">
                            <p>Welcome to <strong>Ra'd AI</strong>. I can help you analyze Saudi stock market data.</p>
                            <p style="font-size:13px;color:var(--text-muted);margin-top:4px;">Click a suggestion above or type your question below.</p>
                        </div>
                    </div>
                </div>
                <div class="raid-input-area">
                    <textarea id="raid-input" placeholder="Ask about Saudi stocks..." rows="1" aria-label="Chat message input"></textarea>
                    <button id="raid-send" class="raid-send-btn" aria-label="Send message">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <line x1="22" y1="2" x2="11" y2="13"></line>
                            <polygon points="22 2 15 22 11 13 2 9 22 2"></polygon>
                        </svg>
                    </button>
                </div>
            </div>

            <div id="raid-onboarding-overlay"></div>

        </div>
    </main>

    <!-- ================================================================
         FOOTER
         ================================================================ -->
    <footer class="app-footer" role="contentinfo">
        <p class="footer-text">
            Ra'd AI
            <span class="footer-divider">|</span>
            Saudi Stock Market Intelligence
            <span class="footer-divider">|</span>
            Data sourced from Tadawul (TASI)
            <span class="footer-divider">|</span>
            <span id="data-freshness-display"></span>
        </p>
    </footer>

    <!-- ================================================================
         JAVASCRIPT - Plotly for charts
         ================================================================ -->
    <script src="https://cdn.plot.ly/plotly-2.35.2.min.js" async crossorigin="anonymous"></script>

    <!-- ================================================================
         JAVASCRIPT - Native Chat Engine
         ================================================================ -->
    <script>
    (function() {
        'use strict';

        var SSE_URL = '/api/vanna/v2/chat_sse';
        var conversationId = null;
        var isStreaming = false;
        var componentElements = {};

        // DOM refs
        var messagesEl = document.getElementById('raid-messages');
        var inputEl = document.getElementById('raid-input');
        var sendBtn = document.getElementById('raid-send');

        // Configure marked.js when available
        function setupMarked() {
            if (window.marked) {
                marked.setOptions({ breaks: true, gfm: true });
            }
        }
        setupMarked();

        // ---- Utility ----
        function esc(text) {
            var d = document.createElement('div');
            d.appendChild(document.createTextNode(text));
            return d.innerHTML;
        }

        function renderMd(text) {
            var html;
            if (window.marked) {
                setupMarked();
                html = marked.parse(text);
            } else {
                // Minimal fallback: escape and convert line breaks
                html = esc(text).replace(/\n/g, '<br>');
            }
            // Sanitize rendered HTML to prevent XSS
            if (window.DOMPurify) {
                return DOMPurify.sanitize(html, { ADD_TAGS: ['iframe'], ADD_ATTR: ['target'] });
            }
            return html;
        }

        function scrollBottom() {
            messagesEl.scrollTop = messagesEl.scrollHeight;
        }

        // ---- Auto-resize textarea ----
        inputEl.addEventListener('input', function() {
            this.style.height = 'auto';
            this.style.height = Math.min(this.scrollHeight, 120) + 'px';
        });

        // ---- Send ----
        function sendMessage(text) {
            if (!text || isStreaming) return;
            inputEl.value = '';
            inputEl.style.height = 'auto';
            addUserMsg(text);
            streamSSE(text);
        }

        sendBtn.addEventListener('click', function() {
            sendMessage(inputEl.value.trim());
        });

        inputEl.addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage(this.value.trim());
            }
        });

        // ---- User message ----
        function addUserMsg(text) {
            var el = document.createElement('div');
            el.className = 'raid-message user';
            el.innerHTML =
                '<div class="raid-avatar" aria-hidden="true">' +
                    '<svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">' +
                        '<path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"/><circle cx="12" cy="7" r="4"/>' +
                    '</svg>' +
                '</div>' +
                '<div class="raid-bubble">' + esc(text) + '</div>';
            messagesEl.appendChild(el);
            scrollBottom();
        }

        // ---- Assistant message container ----
        function createAssistantMsg() {
            var el = document.createElement('div');
            el.className = 'raid-message assistant';
            el.innerHTML =
                '<div class="raid-avatar" aria-hidden="true">RA</div>' +
                '<div class="raid-bubble">' +
                    '<div class="raid-typing"><span></span><span></span><span></span></div>' +
                '</div>';
            messagesEl.appendChild(el);
            scrollBottom();
            return el;
        }

        // ---- SSE streaming ----
        function streamSSE(message) {
            isStreaming = true;
            sendBtn.disabled = true;

            var msgEl = createAssistantMsg();
            var bubbleEl = msgEl.querySelector('.raid-bubble');
            var typingEl = bubbleEl.querySelector('.raid-typing');
            var gotContent = false;

            var body = { message: message };
            if (conversationId) body.conversation_id = conversationId;

            fetch(SSE_URL, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(body)
            }).then(function(response) {
                if (!response.ok) throw new Error('Server returned ' + response.status);

                var reader = response.body.getReader();
                var decoder = new TextDecoder();
                var buffer = '';

                function pump() {
                    return reader.read().then(function(result) {
                        if (result.done) {
                            // Flush remaining buffer
                            if (buffer.trim()) processLine(buffer);
                            finish();
                            return;
                        }

                        buffer += decoder.decode(result.value, { stream: true });
                        var lines = buffer.split('\n');
                        buffer = lines.pop();

                        for (var i = 0; i < lines.length; i++) {
                            processLine(lines[i]);
                        }

                        return pump();
                    });
                }

                function processLine(line) {
                    line = line.trim();
                    if (!line.startsWith('data: ')) return;
                    var payload = line.substring(6);
                    if (payload === '[DONE]') return;

                    try {
                        var chunk = JSON.parse(payload);
                        if (chunk.conversation_id) conversationId = chunk.conversation_id;

                        if (!gotContent && typingEl && typingEl.parentNode) {
                            typingEl.parentNode.removeChild(typingEl);
                            gotContent = true;
                        }

                        handleChunk(chunk, bubbleEl);
                    } catch (e) { /* skip unparseable */ }
                }

                function finish() {
                    if (!gotContent && typingEl && typingEl.parentNode) {
                        typingEl.parentNode.removeChild(typingEl);
                        bubbleEl.innerHTML = '<p style="color:var(--text-muted);">No response received.</p>';
                    }
                    isStreaming = false;
                    sendBtn.disabled = false;
                    inputEl.focus();
                    scrollBottom();
                }

                return pump();

            }).catch(function(err) {
                if (typingEl && typingEl.parentNode) typingEl.parentNode.removeChild(typingEl);
                bubbleEl.innerHTML =
                    '<p style="color:var(--accent-red);">Error: ' + esc(err.message) + '</p>' +
                    '<p style="font-size:12px;color:var(--text-muted);">Please check the server connection and try again.</p>';
                isStreaming = false;
                sendBtn.disabled = false;
                inputEl.focus();
            });
        }

        // ---- Chunk handler ----
        function handleChunk(chunk, bubbleEl) {
            var rich = chunk.rich;
            if (!rich || rich.visible === false) return;

            var type = rich.type;
            var data = rich.data || {};

            switch (type) {
                case 'text':
                    renderText(rich, data, bubbleEl);
                    break;
                case 'chart':
                    renderChart(rich, data, bubbleEl);
                    break;
                case 'artifact':
                    renderArtifact(rich, data, bubbleEl);
                    break;
                case 'dataframe':
                    renderDataframe(rich, data, bubbleEl);
                    break;
                case 'notification':
                    renderNotification(rich, data, bubbleEl);
                    break;
                case 'status_indicator':
                case 'progress_display':
                    renderStatus(rich, data, bubbleEl);
                    break;
                default:
                    // Try to show any text content
                    if (data.content) renderText(rich, data, bubbleEl);
                    else if (data.message) { data.content = data.message; renderText(rich, data, bubbleEl); }
                    break;
            }
            scrollBottom();
        }

        // ---- Text component ----
        function renderText(rich, data, bubbleEl) {
            var content = data.content || '';
            if (!content) return;

            var existing = componentElements[rich.id];
            var html;

            if (data.code_language) {
                html = '<pre><code class="language-' + esc(data.code_language) + '">' + esc(content) + '</code></pre>';
            } else {
                html = renderMd(content);
            }

            if (existing && (rich.lifecycle === 'update' || rich.lifecycle === 'replace')) {
                existing.innerHTML = html;
            } else {
                var el = document.createElement('div');
                el.className = 'raid-text-block';
                el.innerHTML = html;
                bubbleEl.appendChild(el);
                componentElements[rich.id] = el;
            }
        }

        // ---- Chart component (Plotly) ----
        function renderChart(rich, data, bubbleEl) {
            var wrap = document.createElement('div');
            wrap.className = 'raid-chart-wrap';
            var plotId = 'chart_' + rich.id.replace(/[^a-zA-Z0-9_-]/g, '');
            wrap.id = plotId;
            bubbleEl.appendChild(wrap);

            function drawChart() {
                if (!window.Plotly || !data.data) return false;
                var traces = data.data;
                var layout = Object.assign({
                    paper_bgcolor: 'transparent',
                    plot_bgcolor: 'transparent',
                    font: { family: 'Tajawal, sans-serif', color: '#E0E0E0' },
                    margin: { t: 40, b: 60, l: 60, r: 20 }
                }, data.layout || {});
                Plotly.newPlot(plotId, traces, layout, {
                    responsive: true,
                    displayModeBar: true,
                    displaylogo: false
                });
                return true;
            }

            if (!drawChart()) {
                wrap.innerHTML = '<p style="color:var(--text-muted);padding:20px;text-align:center;">Loading chart...</p>';
                var tries = 0;
                var iv = setInterval(function() {
                    if (drawChart() || ++tries > 40) {
                        clearInterval(iv);
                        if (tries > 40) wrap.innerHTML = '<p style="color:var(--accent-red);padding:20px;text-align:center;">Chart library failed to load.</p>';
                    }
                }, 500);
            }
        }

        // ---- Artifact component (HTML/SVG) ----
        function renderArtifact(rich, data, bubbleEl) {
            var el = document.createElement('div');
            el.className = 'raid-artifact';

            if (data.title) {
                var t = document.createElement('p');
                t.style.cssText = 'font-size:12px;color:var(--text-muted);margin-bottom:4px;';
                t.textContent = data.title;
                el.appendChild(t);
            }

            if (data.artifact_type === 'html' || data.artifact_type === 'svg' || data.artifact_type === 'visualization') {
                var iframe = document.createElement('iframe');
                iframe.style.cssText = 'width:100%;min-height:400px;border:1px solid var(--gold-border);border-radius:8px;background:#fff;';
                iframe.sandbox = 'allow-same-origin';
                iframe.srcdoc = data.content || '';
                el.appendChild(iframe);
            } else {
                el.innerHTML += '<pre><code>' + esc(data.content || '') + '</code></pre>';
            }

            bubbleEl.appendChild(el);
            componentElements[rich.id] = el;
        }

        // ---- DataFrame component ----
        function renderDataframe(rich, data, bubbleEl) {
            var columns = data.columns || [];
            var rows = data.data || data.rows || [];
            if (!columns.length && !rows.length) return;

            var html = '<div style="overflow-x:auto;"><table>';
            if (columns.length) {
                html += '<thead><tr>';
                for (var c = 0; c < columns.length; c++) html += '<th>' + esc(String(columns[c])) + '</th>';
                html += '</tr></thead>';
            }
            html += '<tbody>';
            for (var r = 0; r < rows.length; r++) {
                html += '<tr>';
                var row = rows[r];
                if (Array.isArray(row)) {
                    for (var j = 0; j < row.length; j++) html += '<td>' + esc(String(row[j] != null ? row[j] : '')) + '</td>';
                } else if (typeof row === 'object') {
                    for (var k = 0; k < columns.length; k++) {
                        var val = row[columns[k]];
                        html += '<td>' + esc(String(val != null ? val : '')) + '</td>';
                    }
                }
                html += '</tr>';
            }
            html += '</tbody></table></div>';

            var el = document.createElement('div');
            el.className = 'raid-dataframe';
            el.innerHTML = html;
            bubbleEl.appendChild(el);
            componentElements[rich.id] = el;
        }

        // ---- Notification component ----
        function renderNotification(rich, data, bubbleEl) {
            var colors = { error: '#FF6B6B', warning: '#FFA726', success: '#4CAF50', info: '#4A9FFF' };
            var level = data.level || 'info';
            var color = colors[level] || colors.info;
            var el = document.createElement('div');
            el.style.cssText = 'padding:8px 12px;border-radius:8px;border-left:3px solid ' + color + ';background:rgba(0,0,0,0.2);margin:8px 0;font-size:13px;';
            el.innerHTML = '<span style="color:' + color + ';font-weight:600;">' + esc(level.charAt(0).toUpperCase() + level.slice(1)) + ':</span> ' + esc(data.message || '');
            bubbleEl.appendChild(el);
            componentElements[rich.id] = el;
        }

        // ---- Status indicator ----
        function renderStatus(rich, data, bubbleEl) {
            var text = data.label || data.status || data.message || '';
            if (!text) return;
            var sid = 'st_' + rich.id;
            var existing = document.getElementById(sid);
            if (existing) {
                existing.textContent = text;
            } else {
                var el = document.createElement('div');
                el.id = sid;
                el.style.cssText = 'font-size:12px;color:var(--text-muted);font-style:italic;padding:2px 0;';
                el.textContent = text;
                bubbleEl.appendChild(el);
            }
        }

        // ---- Public API (used by suggestion chips and raid-features.js) ----

        /** Send a question from suggestion chips or external callers */
        window.raidAskQuestion = function(query) {
            if (!query || isStreaming) return;
            sendMessage(query);
            collapseSuggestions();
        };

        /** Focus the chat input (for keyboard shortcuts) */
        window.raidFocusInput = function() {
            if (inputEl) inputEl.focus();
        };

        /** Reset the chat (for "New Chat" button) */
        window.raidResetChat = function() {
            conversationId = null;
            componentElements = {};
            messagesEl.innerHTML =
                '<div class="raid-message assistant">' +
                    '<div class="raid-avatar" aria-hidden="true">RA</div>' +
                    '<div class="raid-bubble">' +
                        '<p>Welcome to <strong>Ra\'d AI</strong>. I can help you analyze Saudi stock market data.</p>' +
                        '<p style="font-size:13px;color:var(--text-muted);margin-top:4px;">Click a suggestion above or type your question below.</p>' +
                    '</div>' +
                '</div>';
        };

        function collapseSuggestions() {
            var section = document.querySelector('.suggestions-section');
            if (section) {
                section.style.transition = 'opacity 0.3s ease, max-height 0.4s ease, margin 0.4s ease, padding 0.4s ease';
                section.style.opacity = '0';
                section.style.maxHeight = '0';
                section.style.overflow = 'hidden';
                section.style.margin = '0';
                section.style.padding = '0';
                section.setAttribute('aria-hidden', 'true');
                document.querySelectorAll('.suggestion-chip').forEach(function(chip) {
                    chip.setAttribute('tabindex', '-1');
                });
            }
        }

        // ---- Suggestion chip handlers ----
        document.querySelectorAll('.suggestion-chip[data-query]').forEach(function(chip) {
            chip.addEventListener('click', function(e) {
                e.preventDefault();
                e.stopPropagation();
                var query = this.getAttribute('data-query');
                if (query) window.raidAskQuestion(query);
            });
        });

        // Staggered entrance animation for chips
        document.querySelectorAll('.suggestion-chip').forEach(function(chip, index) {
            chip.style.opacity = '0';
            chip.style.transform = 'translateY(8px)';
            chip.style.transitionDelay = (0.03 * index) + 's';
            requestAnimationFrame(function() {
                requestAnimationFrame(function() {
                    chip.style.opacity = '1';
                    chip.style.transform = 'translateY(0)';
                });
            });
        });

        // Announce to ARIA live region
        var ariaStatus = document.getElementById('raid-status');
        if (ariaStatus) ariaStatus.textContent = 'Chat ready. Type a question or click a suggestion.';

    })();
    </script>

    <script src="/static/raid-features.js" defer></script>

</body>
</html>
</file>

<file path="api/db_helper.py">
"""
Dual-backend database helper.

Provides a unified interface for SQLite and PostgreSQL backends.
Route modules use this instead of importing sqlite3 directly, so they
work on both local dev (SQLite) and Railway/Docker (PostgreSQL).

SQL queries should use ``?`` for positional parameters (SQLite style).
When running against PostgreSQL the helper automatically converts ``?``
to ``%s`` before execution.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Backend detection
⋮----
_DB_BACKEND = os.environ.get("DB_BACKEND", "sqlite")
_HERE = Path(__file__).resolve().parent.parent
_SQLITE_PATH = os.environ.get("DB_SQLITE_PATH", str(_HERE / "saudi_stocks.db"))
⋮----
# Matches single-quoted strings, double-quoted strings, or bare ? placeholders.
# Used by _convert_sql() to skip ? inside string literals.
_PLACEHOLDER_RE = re.compile(r"'[^']*'|\"[^\"]*\"|\?")
⋮----
def is_postgres() -> bool
⋮----
"""Return True when running against PostgreSQL."""
⋮----
# Connection factory
⋮----
def get_conn()
⋮----
"""Return a database connection for the active backend.

    * **SQLite**: opens ``saudi_stocks.db`` with ``sqlite3.Row`` factory.
    * **PostgreSQL**: delegates to ``api.dependencies.get_db_connection()``,
      which uses the connection pool managed by ``database.manager``.

    The caller **must** close the connection in a ``finally`` block.
    """
⋮----
conn = sqlite3.connect(_SQLITE_PATH)
⋮----
# Query helpers
⋮----
def _convert_sql(sql: str) -> str
⋮----
"""Convert ``?`` positional placeholders to ``%s`` for PostgreSQL.

    Skips ``?`` characters inside quoted string literals to avoid
    corrupting LIKE patterns and other literal strings.
    """
⋮----
def _replace(m: re.Match) -> str
⋮----
Params = Union[Tuple, List, Dict[str, Any], None]
⋮----
def fetchall(conn: Any, sql: str, params: Params = None) -> List[Dict[str, Any]]
⋮----
"""Execute *sql* and return every row as a ``dict``.

    Works transparently with both SQLite and PostgreSQL connections.
    """
converted = _convert_sql(sql)
⋮----
rows = conn.execute(sql, params or ()).fetchall()
⋮----
def fetchone(conn: Any, sql: str, params: Params = None) -> Optional[Dict[str, Any]]
⋮----
"""Execute *sql* and return the first row as a ``dict``, or ``None``."""
⋮----
row = cur.fetchone()
⋮----
row = conn.execute(sql, params or ()).fetchone()
⋮----
# Async wrappers (run sync DB I/O in a thread)
⋮----
def _sync_fetchall(sql: str, params: Params = None) -> List[Dict[str, Any]]
⋮----
"""Open a connection, run fetchall, close. Designed for asyncio.to_thread."""
⋮----
conn = get_conn()
⋮----
def _sync_fetchone(sql: str, params: Params = None) -> Optional[Dict[str, Any]]
⋮----
"""Open a connection, run fetchone, close. Designed for asyncio.to_thread."""
⋮----
async def afetchall(sql: str, params: Params = None) -> List[Dict[str, Any]]
⋮----
"""Async wrapper: run a fetchall query in a background thread."""
⋮----
async def afetchone(sql: str, params: Params = None) -> Optional[Dict[str, Any]]
⋮----
"""Async wrapper: run a fetchone query in a background thread."""
</file>

<file path="api/routes/auth.py">
"""
Authentication API routes.

Endpoints for user registration, login, guest access, token refresh,
and profile retrieval.
Delegates database operations to AuthService for separation of concerns.
The guest endpoint works without a database (generates anonymous tokens).
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/auth", tags=["auth"])
⋮----
def _get_auth_service()
⋮----
"""Get AuthService, importing DB dependencies lazily.

    Returns None if the database backend is not available (e.g. SQLite mode).
    """
⋮----
def _build_tokens(user_id: str, email: str) -> Dict[str, str]
⋮----
"""Build access and refresh tokens for a user."""
claims = {"sub": user_id, "email": email}
⋮----
async def register(body: UserCreate)
⋮----
"""Register a new user account.

    Creates a local auth user with bcrypt-hashed password.
    Returns access/refresh tokens and user info so the frontend can
    persist the session without a separate ``/me`` call.

    Raises 409 if the email is already registered.
    Raises 503 if the database backend is not available.
    """
service = _get_auth_service()
⋮----
result = await asyncio.to_thread(
⋮----
tokens = _build_tokens(result.user_id, result.email)
⋮----
@router.post("/login", response_model=AuthResponse)
async def login(body: UserLogin)
⋮----
"""Authenticate a user and return tokens with user info.

    Verifies the email/password against the users table.
    Only works for auth_provider='local' users.

    Raises 401 if credentials are invalid.
    Raises 503 if the database backend is not available.
    """
⋮----
result = await asyncio.to_thread(service.login, body.email, body.password)
⋮----
@router.post("/guest", response_model=AuthResponse)
async def guest_login()
⋮----
"""Generate a guest token for anonymous access.

    No credentials required. Creates a short-lived token with a
    unique guest ID that can be used to access the AI chat and
    other features without registration.
    """
guest_id = f"guest-{uuid.uuid4().hex[:12]}"
guest_email = f"{guest_id}@guest.local"
tokens = _build_tokens(guest_id, guest_email)
⋮----
@router.post("/refresh", response_model=TokenResponse)
async def refresh_token(body: TokenRefreshRequest)
⋮----
"""Exchange a valid refresh token for a new access/refresh token pair.

    Raises 401 if the refresh token is expired or invalid.
    """
⋮----
payload = decode_token(body.refresh_token, expected_type="refresh")
⋮----
user_id = payload.get("sub")
email = payload.get("email")
⋮----
# For guest tokens, skip DB verification
⋮----
result = await asyncio.to_thread(service.verify_user_active, user_id)
⋮----
tokens = _build_tokens(user_id, email)
⋮----
@router.get("/me", response_model=UserProfile)
async def get_me(current_user: Dict[str, Any] = Depends(get_current_user))
⋮----
"""Return the authenticated user's profile."""
</file>

<file path="api/routes/market_analytics.py">
"""
Market analytics API routes (dual-backend: SQLite + PostgreSQL).

Provides market movers, summary, sector breakdown, and heatmap data.
Works with both SQLite (local dev) and PostgreSQL (Railway/Docker) backends.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/market", tags=["market-analytics"])
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class MoverItem(BaseModel)
⋮----
ticker: str
short_name: Optional[str] = None
current_price: Optional[float] = None
previous_close: Optional[float] = None
change_pct: Optional[float] = None
volume: Optional[int] = None
sector: Optional[str] = None
⋮----
class MoversResponse(BaseModel)
⋮----
items: List[MoverItem]
type: str
count: int
⋮----
class MarketSummary(BaseModel)
⋮----
total_market_cap: Optional[float] = None
total_volume: Optional[int] = None
gainers_count: int = 0
losers_count: int = 0
unchanged_count: int = 0
top_gainers: List[MoverItem] = []
top_losers: List[MoverItem] = []
⋮----
class SectorAnalytics(BaseModel)
⋮----
sector: str
avg_change_pct: Optional[float] = None
⋮----
company_count: int = 0
gainers: int = 0
losers: int = 0
⋮----
class HeatmapItem(BaseModel)
⋮----
name: Optional[str] = None
⋮----
market_cap: Optional[float] = None
⋮----
# Alias for backward compatibility within this module
_MOVERS_SQL = MOVERS_BASE
⋮----
def _row_to_mover(row: Dict[str, Any]) -> MoverItem
⋮----
# Routes
⋮----
"""Get top market movers (gainers or losers) by percent change."""
order = "DESC" if type == "gainers" else "ASC"
sql = _MOVERS_SQL + f" ORDER BY change_pct {order} LIMIT ?"
⋮----
rows = await afetchall(sql, (limit,))
⋮----
items = [_row_to_mover(r) for r in rows]
⋮----
@router.get("/summary", response_model=MarketSummary, responses=STANDARD_ERRORS)
async def get_market_summary() -> MarketSummary
⋮----
"""Get overall market summary with totals and top 5 movers."""
⋮----
agg = await afetchone(MARKET_SUMMARY_AGGREGATES)
⋮----
gainers = await afetchall(_MOVERS_SQL + " ORDER BY change_pct DESC LIMIT 5")
losers = await afetchall(_MOVERS_SQL + " ORDER BY change_pct ASC LIMIT 5")
⋮----
@router.get("/sectors", response_model=List[SectorAnalytics], responses=STANDARD_ERRORS)
async def get_sector_analytics() -> List[SectorAnalytics]
⋮----
"""Get per-sector analytics: avg change, volumes, market cap, gainers/losers."""
⋮----
rows = await afetchall(SECTOR_ANALYTICS)
⋮----
@router.get("/heatmap", response_model=List[HeatmapItem], responses=STANDARD_ERRORS)
async def get_heatmap() -> List[HeatmapItem]
⋮----
"""Get all stocks with data suitable for treemap/heatmap visualization."""
⋮----
rows = await afetchall(HEATMAP)
</file>

<file path="api/routes/stock_ohlcv.py">
"""
Per-stock OHLCV chart data API route.

Provides OHLCV data for individual Saudi-listed stocks via yfinance
with in-memory caching, circuit breaker, and mock fallback.
Works with both SQLite and PostgreSQL backends (no database dependency).
"""
⋮----
router = APIRouter(prefix="/api/v1/charts", tags=["stock-ohlcv"])
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class StockOHLCVPoint(BaseModel)
⋮----
time: str
open: float
high: float
low: float
close: float
volume: int
⋮----
class StockOHLCVResponse(BaseModel)
⋮----
data: List[StockOHLCVPoint]
source: Literal["real", "mock", "cached"]
last_updated: str
symbol: str
period: str
count: int
⋮----
class StockHealthResponse(BaseModel)
⋮----
status: Literal["ok", "degraded"]
message: str
⋮----
# Routes
⋮----
"""Return OHLCV data for a Saudi stock for TradingView chart rendering.

    Fetches live data from Yahoo Finance with 5-minute caching.
    Falls back to deterministic mock data if yfinance is unavailable.
    Saudi tickers get .SR suffix automatically.
    """
ticker = validate_ticker(ticker)
⋮----
result = await asyncio.to_thread(fetch_stock_ohlcv, ticker=ticker, period=period)
⋮----
# Health check
⋮----
"""Return health status for the per-stock OHLCV data pipeline.

    Internal diagnostics are logged server-side but not exposed to clients.
    """
⋮----
yfinance_available = True
⋮----
import yfinance as yf  # noqa: F401
⋮----
yfinance_available = False
⋮----
cache_info = get_cache_status()
cache_status = cache_info["cache_status"]
cb_info = get_circuit_breaker_status()
⋮----
status = "degraded"
message = "Data source temporarily unavailable; serving cached data."
⋮----
status = "ok"
message = "Stock OHLCV data pipeline operating normally."
</file>

<file path="Dockerfile">
# =============================================================================
# Stage 1: Builder — install Python dependencies with build tools
# =============================================================================
FROM python:3.11-slim AS builder

WORKDIR /build

# Install build-time system dependencies (gcc for psycopg2, libpq-dev for headers)
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc libpq-dev && \
    rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt

# =============================================================================
# Stage 2: Runtime — minimal image with only runtime dependencies
# =============================================================================
FROM python:3.11-slim

# Install tini for proper PID 1 signal handling, libpq for psycopg2 runtime,
# and postgresql-client for entrypoint.sh schema init via psql
RUN apt-get update && \
    apt-get install -y --no-install-recommends tini libpq5 postgresql-client curl && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy installed Python packages from builder stage
COPY --from=builder /install /usr/local

# Create non-root user before copying application code
RUN useradd -m -u 1000 -s /bin/bash appuser

# Copy application code
COPY app.py csv_to_sqlite.py saudi_stocks_yahoo_data.csv entrypoint.sh ./
COPY config/ config/
COPY templates/ templates/
COPY chart_engine/ chart_engine/
COPY services/ services/
COPY database/ database/
COPY api/ api/
COPY models/ models/
COPY auth/ auth/
COPY cache/ cache/
COPY middleware/ middleware/
COPY ingestion/ ingestion/

RUN chmod +x entrypoint.sh && \
    chown -R appuser:appuser /app

USER appuser

EXPOSE 8084

HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
  CMD curl -f http://localhost:8084/health || exit 1

# Use tini as init process to handle signals (SIGTERM) properly
ENTRYPOINT ["tini", "--"]
CMD ["./entrypoint.sh"]
</file>

<file path="frontend/src/components/layout/Sidebar.tsx">
import { useState, useEffect, useRef } from 'react';
import Link from 'next/link';
import { usePathname } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { Tooltip } from '@/components/ui/Tooltip';
import { prefetchRoute } from '@/lib/performance/utils';
⋮----
interface NavItem {
  label: string;
  labelAr: string;
  href: string;
  icon: React.ReactNode;
}
⋮----
interface SidebarProps {
  mobileOpen?: boolean;
  onMobileClose?: () => void;
}
⋮----
// Close mobile sidebar on route change
⋮----
// Only trigger on pathname change
// eslint-disable-next-line react-hooks/exhaustive-deps
⋮----
// Focus trapping for mobile sidebar
⋮----
// Focus first link in mobile sidebar
⋮----
const handler = (e: KeyboardEvent) =>
⋮----
const isActive = (href: string)
⋮----
{/* Nav Items */}
⋮----
{/* Active indicator bar (end side for RTL) */}
⋮----
className=
⋮----
{/* Bottom section: branding + collapse toggle */}
⋮----
onClick=
⋮----
{/* Desktop sidebar */}
⋮----
{/* Mobile overlay */}
⋮----
aria-label=
</file>

<file path="services/news_scraper.py">
"""
News Scraper Engine
====================
Scrapes Arabic financial news from 5 Saudi market sources.
Each source has a dedicated scraper subclass that handles site-specific
HTML parsing. Articles are filtered for TASI/Saudi market relevance.

Sources 1 and 2 (Al Arabiya, Asharq Bloomberg) use Google News RSS as a
proxy because the original sites block non-browser requests (Cloudflare 403
and AWS WAF challenge respectively). The RSS approach yields the same
articles without requiring JavaScript rendering.

Usage:
    from services.news_scraper import fetch_all_news
    articles = fetch_all_news()
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_scraper_cfg = get_settings().scraper
⋮----
# ---------------------------------------------------------------------------
# Sentiment Analysis
⋮----
# Regular positive keywords (weight 1)
POSITIVE_KEYWORDS = [
⋮----
# Strong positive keywords (weight 2)
STRONG_POSITIVE_KEYWORDS = [
⋮----
# Regular negative keywords (weight 1)
NEGATIVE_KEYWORDS = [
⋮----
# Strong negative keywords (weight 2)
STRONG_NEGATIVE_KEYWORDS = [
⋮----
def analyze_sentiment(title: str, body: str) -> tuple[float, str]
⋮----
"""Analyze Arabic text sentiment using weighted keyword matching.

    Strong keywords contribute 2x weight. Returns (score, label) where:
    - score: float in [-1, 1]
    - label: "إيجابي" / "سلبي" / "محايد"
    """
text = f"{title} {body}"
pos = sum(1 for kw in POSITIVE_KEYWORDS if kw in text)
⋮----
neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text)
⋮----
score = (pos - neg) / (pos + neg + 1)
⋮----
label = "إيجابي"
⋮----
label = "سلبي"
⋮----
label = "محايد"
⋮----
# Ticker Extraction
⋮----
# Hardcoded fallback mapping (most traded TASI stocks)
_FALLBACK_TICKER_MAP: Dict[str, str] = {
⋮----
def _load_ticker_map_from_db(db_path: str) -> Dict[str, str]
⋮----
"""Load company name -> ticker map from the SQLite companies table.

    Returns an empty dict on failure (caller should fall back to hardcoded map).
    """
⋮----
conn = sqlite3.connect(db_path)
rows = conn.execute(
⋮----
ticker_map: Dict[str, str] = {}
⋮----
name = name.strip()
⋮----
# Also index the first word as a shortcut for multi-word names
words = name.split()
⋮----
def _init_ticker_map() -> Dict[str, str]
⋮----
"""Build the ticker map: DB entries merged with the hardcoded fallback."""
db_path = str(Path(__file__).resolve().parent.parent / "saudi_stocks.db")
db_map = _load_ticker_map_from_db(db_path)
# Start with DB entries, then overlay hardcoded map (higher precision)
merged = {**db_map, **_FALLBACK_TICKER_MAP}
⋮----
# Loaded once at module import time
COMPANY_TICKER_MAP: Dict[str, str] = _init_ticker_map()
⋮----
def extract_ticker(title: str, body: str) -> str | None
⋮----
"""Extract a stock ticker from article text by matching company names.

    Returns the first matched ticker code or None.
    """
⋮----
# Constants (loaded from config/settings.py ScraperSettings)
⋮----
REQUEST_TIMEOUT = _scraper_cfg.request_timeout
ARTICLE_FETCH_TIMEOUT = _scraper_cfg.article_fetch_timeout
INTER_REQUEST_DELAY = _scraper_cfg.inter_request_delay
MAX_ARTICLES_PER_SOURCE = _scraper_cfg.max_articles_per_source
MAX_FULL_ARTICLE_FETCHES = _scraper_cfg.max_full_article_fetches
⋮----
DEFAULT_HEADERS = {
⋮----
# Arabic keywords indicating Saudi/TASI market relevance
RELEVANCE_KEYWORDS = [
⋮----
# Base scraper
⋮----
class BaseNewsScraper(ABC)
⋮----
"""Abstract base class for news source scrapers."""
⋮----
source_name: str = ""
source_url: str = ""
priority: int = 0
⋮----
def __init__(self)
⋮----
def fetch_articles(self) -> List[dict]
⋮----
"""Fetch and parse articles from this source.

        Returns a list of article dicts. Never raises -- logs warnings on
        failure and returns an empty list.
        """
⋮----
resp = self._session.get(self.source_url, timeout=REQUEST_TIMEOUT)
⋮----
raw_articles = self._parse_page(resp.text)
⋮----
# Filter for Saudi market relevance
relevant = [a for a in raw_articles if self._is_relevant(a)]
limited = relevant[:MAX_ARTICLES_PER_SOURCE]
⋮----
# Fetch full article bodies for articles with empty/short body
limited = self._enrich_bodies(limited)
⋮----
@abstractmethod
    def _parse_page(self, html: str) -> List[dict]
⋮----
"""Parse HTML and extract article dicts.

        Each dict must contain:
            title, body, source_name, source_url, published_at, priority, language
        """
⋮----
def _fetch_full_article(self, url: str) -> str
⋮----
"""Fetch full article body from the article URL.

        Tries common article content selectors, falling back to paragraph
        extraction. Returns empty string on failure.
        """
⋮----
resp = self._session.get(url, timeout=ARTICLE_FETCH_TIMEOUT)
⋮----
soup = BeautifulSoup(resp.text, "lxml")
⋮----
# Try common article content selectors (most specific first)
⋮----
content = soup.select(selector)
⋮----
text = " ".join(el.get_text(strip=True) for el in content)
⋮----
# Fallback: get all paragraphs from article/main/content areas
paragraphs = soup.select("article p, main p, .content p")
⋮----
text = " ".join(p.get_text(strip=True) for p in paragraphs)
⋮----
def _enrich_bodies(self, articles: List[dict]) -> List[dict]
⋮----
"""Fetch full article bodies for articles with empty body.

        Fetches up to MAX_FULL_ARTICLE_FETCHES articles per source to avoid
        excessive requests. Respects INTER_REQUEST_DELAY between fetches.
        """
fetched_count = 0
⋮----
body = article.get("body", "")
url = article.get("source_url", "")
⋮----
full_body = self._fetch_full_article(url)
⋮----
"""Build a standardized article dict.

        Falls back to current UTC time when published_at is not available.
        """
⋮----
@staticmethod
    def _is_relevant(article: dict) -> bool
⋮----
"""Check if article text contains Saudi market keywords."""
text = (article.get("title", "") + " " + article.get("body", "")).lower()
⋮----
@staticmethod
    def _extract_text(soup_element) -> str
⋮----
"""Safely extract text from a BeautifulSoup element."""
⋮----
@staticmethod
    def _absolute_url(base: str, href: str) -> str
⋮----
"""Convert a relative URL to absolute."""
⋮----
parsed = urlparse(base)
⋮----
# Google News RSS base scraper (used for sources that block direct scraping)
⋮----
class GoogleNewsRssScraper(BaseNewsScraper)
⋮----
"""Scraper that fetches articles via Google News RSS search.

    Both Al Arabiya and Asharq Bloomberg block direct HTTP requests with
    Cloudflare 403 and AWS WAF challenges respectively. Google News RSS
    indexes their articles and provides them in a standard RSS feed, making
    this a reliable proxy for otherwise inaccessible sources.

    Subclasses set ``_rss_queries`` (a list of search queries to try) and
    optionally ``_source_filter`` to filter results to a specific domain.
    """
⋮----
_rss_queries: List[str] = []
_source_filter: str = ""  # domain substring to filter, e.g. "alarabiya"
_google_rss_base = "https://news.google.com/rss/search"
⋮----
"""Fetch articles from Google News RSS, trying each query in order."""
all_articles: List[dict] = []
seen_urls: set = set()
⋮----
encoded_query = quote_plus(query)
rss_url = (
⋮----
resp = self._session.get(rss_url, timeout=REQUEST_TIMEOUT)
⋮----
soup = BeautifulSoup(resp.text, "xml")
items = soup.select("item")
⋮----
title_el = item.select_one("title")
link_el = item.select_one("link")
pub_date_el = item.select_one("pubDate")
source_el = item.select_one("source")
description_el = item.select_one("description")
⋮----
title = title_el.get_text(strip=True) if title_el else ""
link = link_el.get_text(strip=True) if link_el else ""
pub_date = pub_date_el.get_text(strip=True) if pub_date_el else None
source_text = source_el.get_text(strip=True) if source_el else ""
source_href = source_el.get("url", "") if source_el else ""
body = ""
⋮----
# Google wraps description in HTML; extract text
desc_soup = BeautifulSoup(
body = desc_soup.get_text(strip=True)
⋮----
# Filter by source domain if specified
⋮----
matches_source = (
⋮----
# Parse RFC 2822 date from Google RSS
published_at = None
⋮----
dt = parsedate_to_datetime(pub_date)
published_at = dt.isoformat()
⋮----
published_at = pub_date
⋮----
break  # Got results from this query, skip remaining
⋮----
relevant = [a for a in all_articles if self._is_relevant(a)]
⋮----
def _parse_page(self, html: str) -> List[dict]
⋮----
"""Not used for RSS scrapers -- fetch_articles is overridden."""
⋮----
# Source 1: Al Arabiya Markets via Google News RSS (priority 1)
⋮----
class AlarabiyaScraper(GoogleNewsRssScraper)
⋮----
"""Fetches Al Arabiya market news via Google News RSS.

    Al Arabiya (alarabiya.net) blocks direct HTTP requests with Cloudflare 403.
    We use Google News RSS with ``site:alarabiya.net`` queries instead.
    """
⋮----
source_name = "العربية"
source_url = "https://www.alarabiya.net/aswaq"
priority = 1
⋮----
_source_filter = "alarabiya"
_rss_queries = [
⋮----
# Source 2: Asharq Business / Bloomberg Saudi via Google News RSS (priority 2)
⋮----
class AsharqBusinessScraper(GoogleNewsRssScraper)
⋮----
"""Fetches Asharq Bloomberg business news via Google News RSS.

    Asharq Business (asharqbusiness.com) uses AWS WAF with a JavaScript
    challenge (HTTP 202) that cannot be bypassed with plain requests.
    We use Google News RSS with ``site:asharqbusiness.com`` queries instead.
    """
⋮----
source_name = "الشرق بلومبرغ"
source_url = "https://www.asharqbusiness.com/"
priority = 2
⋮----
_source_filter = "asharq"
⋮----
# Source 3: Argaam (priority 3)
⋮----
class ArgaamScraper(BaseNewsScraper)
⋮----
source_name = "أرقام"
source_url = "https://www.argaam.com/"
priority = 3
⋮----
soup = BeautifulSoup(html, "lxml")
articles = []
⋮----
# Strategy 1: structured containers
⋮----
href = item.get("href", "")
⋮----
url = self._absolute_url(self.source_url, href)
⋮----
title_el = item.select_one("h2, h3, h4, [class*='title'], span")
title = (
⋮----
summary_el = item.select_one("p, [class*='summary'], [class*='desc']")
body = self._extract_text(summary_el) if summary_el else ""
⋮----
time_el = item.select_one("time, [class*='date'], [class*='time']")
published = (
⋮----
# Source 4: Maaal (priority 4)
⋮----
class MaaalScraper(BaseNewsScraper)
⋮----
source_name = "معال"
source_url = "https://maaal.com/"
priority = 4
⋮----
# Try multiple paths since the /news path may have changed
_alt_urls = [
⋮----
"""Override to try multiple URLs for Maaal."""
⋮----
articles = super().fetch_articles()
⋮----
# Strategy 1: WordPress-style article/post containers
⋮----
link = item.select_one("a[href]")
⋮----
href = link.get("href", "")
⋮----
title_el = item.select_one(
⋮----
summary_el = item.select_one(
⋮----
time_el = item.select_one(
⋮----
# Strategy 2: Fallback - plain links to article-like URLs
⋮----
title = self._extract_text(link)
⋮----
# Source 5: Mubasher (priority 5)
⋮----
class MubasherScraper(BaseNewsScraper)
⋮----
source_name = "مباشر"
source_url = "https://www.mubasher.info/"
priority = 5
⋮----
title = self._extract_text(item)
link = item
⋮----
summary_el = (
⋮----
time_el = (
⋮----
# Strategy 2: direct news links
⋮----
# Registry of all scrapers
⋮----
ALL_SCRAPERS: List[type] = [
⋮----
# Deduplication
⋮----
def _title_word_overlap(title_a: str, title_b: str) -> float
⋮----
"""Return the fraction of shared words between two titles (Jaccard-like).

    Computes ``|intersection| / |union|`` over whitespace-split word sets.
    Returns 0.0 when both titles are empty.
    """
words_a = set(title_a.split())
words_b = set(title_b.split())
⋮----
intersection = words_a & words_b
union = words_a | words_b
⋮----
"""Remove near-duplicate articles based on title similarity.

    Two-pass deduplication:
    1. SequenceMatcher on full titles with a 0.55 threshold (lowered from 0.7
       to catch paraphrased duplicates that share most of the same phrasing).
    2. Word-overlap check on titles: if >50% of the words are shared between
       two article titles, they are considered duplicates.

    Keeps the article from the higher-priority source (lower priority number).
    """
unique: List[dict] = []
⋮----
is_dup = False
⋮----
# Check 1: SequenceMatcher ratio
seq_similarity = SequenceMatcher(
# Check 2: word overlap in titles
word_overlap = _title_word_overlap(article["title"], existing["title"])
⋮----
is_dup = True
# Keep the one with higher priority (lower number)
⋮----
# Top-level aggregator
⋮----
def fetch_all_news() -> List[dict]
⋮----
"""Run all scrapers, paraphrase, deduplicate, and sort results.

    Returns articles sorted by priority (ascending) then published_at
    (descending, most recent first).
    """
⋮----
scraper = scraper_cls()
articles = scraper.fetch_articles()
⋮----
# Paraphrase each article and enrich with sentiment + ticker
paraphrased = []
⋮----
article = paraphrase_article(article)
⋮----
# Sentiment analysis
title = article.get("title", "")
⋮----
# Ticker extraction
ticker = extract_ticker(title, body)
⋮----
# Deduplicate by title similarity
unique = _deduplicate(paraphrased)
⋮----
# Sort by priority (asc), then articles with dates before those without
def _sort_key(a)
⋮----
pri = a.get("priority", 99)
pub = a.get("published_at") or ""
# Articles with published_at sort before those without (0 < 1)
has_date = 0 if pub else 1
# Reverse date order: newer dates (lexicographically larger) first
</file>

<file path="services/tasi_index.py">
"""
TASI Index data fetcher service.

Fetches TASI (Tadawul All Share Index) OHLCV data via yfinance with
in-memory caching, circuit breaker, and deterministic mock fallback.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# Shared cache & circuit breaker instances
⋮----
_cache = YFinanceCache(ttl=300, max_entries=500, name="tasi_index")
_CACHE_TTL = _cache.ttl  # backward-compatible alias for tests
⋮----
# Per-period locks: allow concurrent fetches for different periods
_period_locks: Dict[str, threading.Lock] = {}
_locks_guard = threading.Lock()  # Protects _period_locks dict itself
_MAX_LOCKS = 100  # Prevent unbounded growth
⋮----
def _get_period_lock(period: str) -> threading.Lock
⋮----
"""Return a per-period lock, creating one if needed."""
⋮----
VALID_PERIODS = ("1mo", "3mo", "6mo", "1y", "2y", "5y")
⋮----
_breaker = CircuitBreaker(
⋮----
timeout=300,  # 5 min
⋮----
SYMBOL_RETRY_DELAY = 0.5  # seconds between symbol retries
⋮----
# Backward-compatible module-level circuit breaker state
# (used by test fixtures that reset state via ``mod._consecutive_failures = 0``)
CIRCUIT_BREAKER_THRESHOLD = _breaker.threshold
CIRCUIT_BREAKER_TIMEOUT = _breaker.timeout
_consecutive_failures: int = 0
_circuit_open_until: float = 0.0
_circuit_lock = threading.Lock()
⋮----
# Cache helpers (thin wrappers with TASI-specific metadata enrichment)
⋮----
def _get_cached(period: str) -> Optional[Dict[str, Any]]
⋮----
"""Return cached data if still fresh, enriched with freshness metadata."""
payload = _cache.get(period)
⋮----
# Get entry for age calculation
⋮----
entry = _cache._store.get(period)
⋮----
age = time.monotonic() - entry["fetched_at"]
result = dict(payload)
⋮----
def _get_stale_cached(period: str) -> Optional[Dict[str, Any]]
⋮----
"""Return stale cache entry (for fallback on fetch failure).

    Enriches the payload with staleness metadata so consumers (especially
    the frontend) can display an appropriate warning about data age.
    """
payload = _cache.get_stale(period)
⋮----
age_seconds = round(time.monotonic() - entry["fetched_at"]) if entry else 0
⋮----
def _set_cache(period: str, payload: Dict[str, Any]) -> None
⋮----
# Circuit breaker helpers
# Uses shared CircuitBreaker internally but keeps module-level state
# in sync for backward compatibility with test fixtures.
⋮----
def _sync_to_breaker() -> None
⋮----
"""Push module-level state into the CircuitBreaker instance.

    Must be called while holding ``_circuit_lock``.
    """
⋮----
def _is_circuit_open() -> bool
⋮----
"""Return True if the circuit breaker is currently open (yfinance skipped)."""
⋮----
def _record_failure() -> None
⋮----
"""Increment consecutive failure count; open circuit if threshold reached."""
⋮----
# Sync back
_consecutive_failures = _breaker._consecutive_failures
_circuit_open_until = _breaker._open_until
⋮----
def _record_success() -> None
⋮----
"""Reset circuit breaker on a successful fetch."""
⋮----
def get_circuit_breaker_status() -> Dict[str, Any]
⋮----
"""Return circuit breaker diagnostics for the health endpoint."""
⋮----
# Mock data generator (deterministic)
⋮----
def _generate_mock_data(period: str) -> List[Dict[str, Any]]
⋮----
"""Generate deterministic mock TASI data seeded by period string."""
period_days = {
days = period_days.get(period, 252)
rng = random.Random(42)  # deterministic seed
⋮----
data = []
base_price = 11500.0
end_date = datetime.utcnow().date()
start_date = end_date - timedelta(days=int(days * 1.45))  # account for weekends
⋮----
current = start_date
price = base_price
count = 0
⋮----
# Skip weekends (Tadawul operates Sun-Thu; Friday=4, Saturday=5)
⋮----
change_pct = rng.gauss(0.0002, 0.012)
price = price * (1 + change_pct)
day_range = price * rng.uniform(0.005, 0.02)
open_price = price + rng.uniform(-day_range / 2, day_range / 2)
high = max(open_price, price) + rng.uniform(0, day_range / 2)
low = min(open_price, price) - rng.uniform(0, day_range / 2)
volume = int(rng.uniform(80_000_000, 300_000_000))
⋮----
# Main fetcher
⋮----
def fetch_tasi_index(period: str = "1y") -> Dict[str, Any]
⋮----
"""Fetch TASI index OHLCV data.

    Tries yfinance first (^TASI, then TASI.SR fallback).
    Falls back to stale cache or deterministic mock data.

    Args:
        period: One of '1mo', '3mo', '6mo', '1y', '2y', '5y'.

    Returns:
        Dict with keys: data, source, last_updated, symbol.
    """
t_start = time.monotonic()
⋮----
# Check fresh cache first
cached = _get_cached(period)
⋮----
duration_ms = round((time.monotonic() - t_start) * 1000, 1)
⋮----
# Serialize yfinance fetches per period (different periods fetch concurrently)
⋮----
# Double-check cache inside the lock (another thread may have filled it)
⋮----
# Check circuit breaker -- skip yfinance entirely if open
⋮----
# Try yfinance
symbols = ["^TASI", "TASI.SR"]
⋮----
ticker = yf.Ticker(symbol)
df = ticker.history(period=period, auto_adjust=True)
⋮----
df = df.reset_index()
⋮----
date_val = row.get("Date")
⋮----
time_str = date_val.strftime("%Y-%m-%d")
⋮----
time_str = str(date_val)[:10]
⋮----
payload = {
⋮----
exc_type = type(exc).__name__
exc_msg = str(exc)
# Classify the error
error_category = "unknown"
⋮----
error_category = "rate_limit"
⋮----
error_category = "network"
⋮----
error_category = "data_error"
⋮----
# Add delay between symbol retries (but not after the last symbol)
⋮----
# Fallback: stale cache
stale = _get_stale_cached(period)
⋮----
# Fallback: mock data
mock_data = _generate_mock_data(period)
⋮----
# Convenience alias used by the verification command
get_tasi_data = fetch_tasi_index
⋮----
def get_cache_status() -> Dict[str, Any]
⋮----
"""Return cache diagnostic information for the health endpoint."""
⋮----
entry = _cache.newest_entry()
⋮----
fresh = age < _cache.ttl
</file>

<file path="config/settings.py">
"""
Configuration module for TASI AI Platform.
Uses pydantic-settings for typed, validated configuration loaded from environment variables and .env files.
"""
⋮----
_log = logging.getLogger(__name__)
⋮----
class DatabaseSettings(BaseSettings)
⋮----
"""
    Database connection settings.

    Env vars use the DB_ prefix for app-level settings.
    PostgreSQL connection vars also accept POSTGRES_* names for Docker compatibility.
    """
⋮----
model_config = SettingsConfigDict(env_prefix="DB_")
⋮----
backend: Literal["sqlite", "postgres"] = "sqlite"
# SQLite settings
sqlite_path: str = "saudi_stocks.db"
# PostgreSQL settings — accept both DB_PG_* and POSTGRES_* env vars
pg_host: str = Field(
pg_port: int = Field(
pg_database: str = Field(
pg_user: str = Field(
pg_password: str = Field(
⋮----
@property
    def pg_connection_string(self) -> str
⋮----
@property
    def resolved_sqlite_path(self) -> Path
⋮----
"""Return absolute path to SQLite DB, resolved relative to project root."""
p = Path(self.sqlite_path)
⋮----
class LLMSettings(BaseSettings)
⋮----
"""LLM provider settings."""
⋮----
model_config = SettingsConfigDict(env_prefix="LLM_")
⋮----
provider: Literal["anthropic", "gemini", "openai"] = Field(
model: str = Field(
api_key: str = ""
max_tool_iterations: int = 10
⋮----
class PoolSettings(BaseSettings)
⋮----
"""PostgreSQL connection pool settings. All env vars prefixed with PG_POOL_."""
⋮----
model_config = SettingsConfigDict(env_prefix="PG_POOL_")
⋮----
min: int = 2
max: int = 10
⋮----
class CacheSettings(BaseSettings)
⋮----
"""Redis cache settings. All env vars use explicit field names."""
⋮----
model_config = SettingsConfigDict(env_prefix="CACHE_")
⋮----
redis_url: str = Field(
enabled: bool = False
default_ttl: int = 300
⋮----
class AuthSettings(BaseSettings)
⋮----
"""JWT authentication settings. All env vars prefixed with AUTH_."""
⋮----
model_config = SettingsConfigDict(env_prefix="AUTH_")
⋮----
# In production, set AUTH_JWT_SECRET to a stable value.
# The default generates a random secret on each startup (fine for dev).
jwt_secret: str = Field(default_factory=lambda: secrets.token_urlsafe(32))
jwt_algorithm: str = "HS256"
access_token_expire_minutes: int = 30
refresh_token_expire_days: int = 7
_is_auto_generated: bool = True
⋮----
@model_validator(mode="after")
    def _validate_jwt_secret(self) -> "AuthSettings"
⋮----
"""Warn if JWT secret is auto-generated; fail in production."""
⋮----
# If the secret was explicitly provided via env var, it won't match
# the auto-generated default (the default_factory runs only when
# no value is supplied).  We detect this by checking whether
# AUTH_JWT_SECRET is actually set in the environment.
explicitly_set = bool(os.environ.get("AUTH_JWT_SECRET"))
⋮----
# Auto-generated secret -- fine for dev, risky in production.
environment = os.environ.get("ENVIRONMENT", "development").lower()
is_production = environment == "production"
⋮----
class MiddlewareSettings(BaseSettings)
⋮----
"""Middleware settings. All env vars prefixed with MW_."""
⋮----
model_config = SettingsConfigDict(env_prefix="MW_")
⋮----
cors_origins: str = "http://localhost:3000,http://localhost:8084,https://frontend-two-nu-83.vercel.app,https://raid-ai-app-production.up.railway.app"
rate_limit_per_minute: int = 60
log_skip_paths: str = "/health,/favicon.ico"
⋮----
@property
    def cors_origins_list(self) -> list[str]
⋮----
"""Parse comma-separated CORS origins into a list."""
⋮----
@property
    def log_skip_paths_list(self) -> list[str]
⋮----
"""Parse comma-separated skip paths into a list."""
⋮----
class ScraperSettings(BaseSettings)
⋮----
"""News scraper settings. All env vars prefixed with SCRAPER_."""
⋮----
model_config = SettingsConfigDict(env_prefix="SCRAPER_")
⋮----
request_timeout: int = 10
article_fetch_timeout: int = 5
inter_request_delay: float = 1.5
max_articles_per_source: int = 10
max_full_article_fetches: int = 5
fetch_interval_seconds: int = 1800
cleanup_age_days: int = 7
user_agent: str = (
dedup_threshold: float = 0.55
⋮----
class ServerSettings(BaseSettings)
⋮----
"""FastAPI server settings. All env vars prefixed with SERVER_."""
⋮----
model_config = SettingsConfigDict(env_prefix="SERVER_")
⋮----
host: str = "0.0.0.0"
port: int = 8084
debug: bool = False
environment: str = Field(
⋮----
class Settings(BaseSettings)
⋮----
"""
    Top-level application settings.
    Loads from .env file and environment variables.
    Nested settings use their own env prefixes (DB_, LLM_, SERVER_).
    """
⋮----
model_config = SettingsConfigDict(
⋮----
# Backward compatibility: existing .env uses ANTHROPIC_API_KEY
anthropic_api_key: str = ""
⋮----
# Nested settings
db: DatabaseSettings = DatabaseSettings()
llm: LLMSettings = LLMSettings()
server: ServerSettings = ServerSettings()
pool: PoolSettings = PoolSettings()
cache: CacheSettings = CacheSettings()
auth: AuthSettings = AuthSettings()
middleware: MiddlewareSettings = MiddlewareSettings()
scraper: ScraperSettings = ScraperSettings()
⋮----
def get_llm_api_key(self) -> str
⋮----
"""Return the effective LLM API key for the configured provider."""
⋮----
@lru_cache(maxsize=1)
def get_settings() -> Settings
⋮----
"""Return a cached singleton Settings instance."""
</file>

<file path="frontend/src/app/charts/page.tsx">
import { useState, useCallback, useEffect } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { Breadcrumb } from '@/components/common/Breadcrumb';
import {
  ChartTabNavigation,
  StocksTab,
  CompareTab,
  AnalyticsTab,
  StockChartPanel,
  type TabId,
} from './components';
⋮----
// ---------------------------------------------------------------------------
// Main Charts page
// ---------------------------------------------------------------------------
⋮----
// Comparison tickers
⋮----
// Read URL params on mount (e.g. /charts?ticker=2222)
⋮----
// Add to comparison handler
⋮----
// Escape key exits fullscreen
⋮----
function handleKeyDown(e: KeyboardEvent)
⋮----
// Fullscreen layout
⋮----
{/* Breadcrumb */}
⋮----
{/* Header */}
⋮----
{/* Tab bar */}
⋮----
{/* Tab content */}
⋮----
{/* AI Chat CTA */}
</file>

<file path="frontend/src/app/reports/page.tsx">
import { useCallback, useEffect, useRef, useState } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { getReports, type ReportItem } from '@/lib/api-client';
import { LoadingSpinner } from '@/components/common/loading-spinner';
import { ErrorDisplay } from '@/components/common/error-display';
import { useLanguage } from '@/providers/LanguageProvider';
import { Breadcrumb } from '@/components/common/Breadcrumb';
⋮----
// ---------------------------------------------------------------------------
// Filter types
// ---------------------------------------------------------------------------
⋮----
type TypeFilter = 'all' | 'technical' | 'fundamental' | 'sector' | 'macro';
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Paginated response from backend (matches PaginatedResponse[ReportResponse])
// ---------------------------------------------------------------------------
⋮----
interface PaginatedReportResponse {
  items: ReportItem[];
  total: number;
  page: number;
  page_size: number;
  total_pages: number;
}
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
function handleFilterChange(newFilter: TypeFilter)
⋮----
{/* Breadcrumb */}
⋮----
{/* Header */}
⋮----
{/* Search */}
⋮----
onChange=
placeholder=
⋮----
{/* Type Filters */}
⋮----
{/* Reports Grid */}
⋮----
{/* Type badge */}
⋮----
<span className=
⋮----
{/* Content */}
⋮----
{/* Footer */}
⋮----
{/* Target price */}
⋮----
<span className="text-[var(--text-muted)]">
⋮----
{/* Pagination */}
⋮----
onClick=
</file>

<file path="frontend/src/app/watchlist/page.tsx">
import { useEffect, useState, useCallback, useRef } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import {
  getWatchlists,
  createWatchlist,
  updateWatchlist as apiUpdateWatchlist,
  deleteWatchlist as apiDeleteWatchlist,
  getEntityDetail,
  getBatchQuotes,
  ApiError,
  type WatchlistItem,
} from '@/lib/api-client';
import { useAuth } from '@/lib/hooks/use-auth';
import { LoadingSpinner } from '@/components/common/loading-spinner';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector } from '@/lib/stock-translations';
import { Breadcrumb } from '@/components/common/Breadcrumb';
import { loadLocalWatchlists, saveLocalWatchlists, type LocalWatchlist } from '@/lib/hooks/useWatchlist';
import { formatPrice, formatChangePercent } from '@/lib/formatters';
⋮----
// ---------------------------------------------------------------------------
// LocalStorage fallback (when API is not available)
// ---------------------------------------------------------------------------
⋮----
/** Unified quote data that works with both BatchQuote and CompanyDetail. */
interface QuoteData {
  ticker: string;
  short_name: string | null;
  sector: string | null;
  current_price: number | null;
  previous_close: number | null;
  change_pct: number | null;
  volume: number | null;
}
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Load watchlists: try API if authenticated, fall back to localStorage
⋮----
// Not authenticated - use localStorage only
⋮----
// API available but no watchlists, use local
⋮----
// Token expired or invalid - use local fallback
⋮----
// Save to localStorage whenever watchlists change (for local mode)
⋮----
// Fetch quotes for tickers in the active watchlist
// Uses batch quotes first (single request), falls back to individual entity
// detail calls if batch endpoint is unavailable.
⋮----
// Strategy 1: Try batch quotes endpoint (single request, most efficient)
⋮----
if (fetchId !== quoteFetchRef.current) return; // stale request
⋮----
sector: null, // batch quotes don't include sector
⋮----
// If batch returned results but is missing sector info, try to fill
// from individual entity detail for the found tickers
⋮----
// Fire-and-forget sector enrichment (don't block rendering)
⋮----
// Batch endpoint failed -- fall through to individual fetches
⋮----
// Strategy 2: Fallback to individual getEntityDetail calls
⋮----
if (fetchId !== quoteFetchRef.current) return; // stale request
⋮----
// Trigger quote fetching when the active list changes
⋮----
// Fallback to local
⋮----
// continue with local delete
⋮----
// Auto-append .SR if user enters just a number (e.g. "2222" -> "2222.SR")
⋮----
// continue with local update
⋮----
// continue with local update
⋮----
{/* Breadcrumb */}
⋮----
{/* Sync banner for anonymous users */}
⋮----
{/* Header */}
⋮----
onClick=
⋮----
{/* Create list form */}
⋮----
onChange=
⋮----
placeholder=
⋮----
{/* Watchlist Tabs */}
⋮----
{/* Active List Content */}
⋮----
{/* Add ticker */}
⋮----
{/* Error banner */}
⋮----
{/* Ticker List */}
⋮----
aria-label=
</file>

<file path="README.md">
[![CI](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/ci.yml/badge.svg)](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/ci.yml)

# Ra'd AI - TASI Saudi Stock Market Platform

AI-powered financial analyst for the Saudi stock market (TASI - Tadawul All Share Index). Ask questions in natural language, get SQL-backed answers with interactive Plotly charts.

Built on [Vanna 2.0](https://vanna.ai/) with Google Gemini, supporting dual SQLite/PostgreSQL backends.

## Features

- Natural language to SQL query generation via Google Gemini
- Interactive Plotly chart visualization (bar, line, scatter, heatmap)
- Comprehensive data for ~500 Saudi-listed companies
- Financial statements (balance sheet, income statement, cash flow) with multi-period history
- Market data, valuation metrics, analyst consensus, dividends
- **Live Market Widgets**: Real-time crypto, metals, oil, and global indices via SSE with auto-reconnection
- Real-time news feed with Server-Sent Events (SSE) from 5 Arabic sources
- **Connection status indicators**: Live/reconnecting/offline badge on SSE streams
- Full Arabic RTL support with Tailwind CSS logical properties and lint enforcement
- **Navigation progress bar**: Gold-themed top-loading indicator for page transitions
- Virtual scrolling for high-performance list rendering
- **Mobile-responsive market view**: Card layout for small screens
- **GZip compression**: Automatic response compression for payloads >1KB
- **Route-level loading/error states**: Dedicated loading skeletons and error boundaries per route
- News aggregation, announcement tracking, technical reports
- Dual database backend: SQLite for development, PostgreSQL for production
- Async I/O wrappers (`asyncio.to_thread`) for non-blocking database access
- JWT authentication with production secret enforcement
- **SQL security pipeline**: Input sanitization, table/column allowlisting, query validation with Vanna integration hook
- **Resilience**: Circuit breaker, retry with backoff, timeout management, graceful degradation for external services
- **Query caching**: Tiered Redis cache with GZip compression and connection pooling
- **Audit logging**: Structured JSON logging, correlation IDs, query audit trail, security event tracking
- **Cost control**: LLM spend tracking middleware with configurable limits
- **Component decomposition**: Charts and Markets pages split into focused subcomponents for maintainability
- **Modular API client**: Domain-scoped API modules under `lib/api/` with backward-compatible shim
- **Auth enhancements**: Token refresh, guest login, profile enrichment
- **Enriched stock detail**: Financials, dividends, reports, news, and watchlist in one page
- **Prometheus metrics**: `/metrics` endpoint via `prometheus-fastapi-instrumentator` (graceful fallback if not installed)
- **Pool stats**: Connection pool size (SQLite or PostgreSQL) reported in `/health` response
- **Structured request tracing**: `ContextVar`-based request ID injected into all log records within a request
- **Security scan CI**: bandit static analysis with medium severity/confidence thresholds
- **Type check CI**: mypy type checking across all backend modules
- **Dependency lockfile**: `pip-compile`-managed `requirements.lock` with CI verification

## Quick Start

### Local Development (SQLite)

```bash
# Clone and install
git clone <repo-url>
cd vanna-ai-testing
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env and set GEMINI_API_KEY

# Build database and start
python csv_to_sqlite.py
python app.py
```

Open http://localhost:8084

### Docker (PostgreSQL)

```bash
cp .env.example .env
# Edit .env: set GEMINI_API_KEY and POSTGRES_PASSWORD

docker compose up -d
```

Services:
- App: http://localhost:8084
- pgAdmin (optional): `docker compose --profile tools up -d` then http://localhost:5050

## Architecture

```
        +-------------------+          +-------------------+
        |  Next.js 14 (3000)|          |   Browser/User    |
        |  - RTL Arabic UI  |          |   (Legacy UI)     |
        |  - SSE News Feed  |          +--------+----------+
        |  - Market Widgets |                   |
        |  - Virtual Scroll |                   |
        +--------+----------+                   |
                 |                              |
                 +----------+  +----------------+
                            |  |
                   +--------v--v-------+
                   |  FastAPI (8084)    |
                   |  - GZip Middleware |
                   |  - Chat SSE       |
                   |  - News Stream    |
                   |  - Widgets Stream |
                   |  - REST API       |
                   +----+--------+-----+
                        |        |
           +------------+        +------------+
           |                                  |
  +--------v----------+            +----------v--------+
  |  Vanna 2.0 Agent  |            |   QuotesHub       |
  |  - Gemini LLM     |            |   - 4 Providers   |
  |  - RunSqlTool     |            |   (crypto, metals,|
  |  - VisualizeTool  |            |   oil, indices)   |
  +--------+----------+            |   - Redis pub/sub |
           |                       +-------------------+
  +--------v----------+
  | backend/ module   |
  | - SQL validation  |
  | - Cost controller |
  | - Circuit breaker |
  | - Query cache     |
  | - Audit logging   |
  +--------+----------+
           |
  +--------v----------+
  |  Async I/O Layer  |
  |  asyncio.to_thread|
  +--------+----------+
           |
  +--------+--------------+
  |                        |
  +--------v--------+  +--v--------------+
  |     SQLite       |  |   PostgreSQL    |
  | (saudi_stocks.db)|  |  (tasi_platform)|
  |  10 core tables  |  |  10 core + 14   |
  +-----------------+  |  extended tables |
                        +-----------------+
```

### Backend Module (`backend/`)

Enterprise-grade infrastructure providing security, resilience, caching, and observability:

| Package | Purpose |
|---|---|
| `backend/security/` | SQL validation pipeline: input sanitizer, table/column allowlist, SQL query validator, Vanna output hook |
| `backend/middleware/` | Cost controller (LLM spend tracking), Redis-backed sliding window rate limiter, FastAPI middleware registration |
| `backend/services/audit/` | Query audit logging, security event tracking, structured JSON logger, correlation ID middleware |
| `backend/services/cache/` | Tiered query cache, GZip compression, Redis connection management, database connection pooling, cache maintenance |
| `backend/services/resilience/` | Circuit breaker, retry with backoff, timeout manager, graceful degradation |

## Database

### Core Tables (10 tables, both backends)

| Table | Description | Rows |
|---|---|---|
| `companies` | Company info, sector, industry | 500 |
| `market_data` | Price, volume, market cap, beta | 500 |
| `valuation_metrics` | PE, PB, EV ratios, EPS | 500 |
| `profitability_metrics` | Margins, ROA, ROE, growth | 500 |
| `dividend_data` | Yields, payout ratio, dates | 500 |
| `financial_summary` | Revenue, debt, cash flow summary | 500 |
| `analyst_data` | Target prices, recommendations | 500 |
| `balance_sheet` | Assets, liabilities, equity (multi-period) | ~2,527 |
| `income_statement` | Revenue, expenses, net income (multi-period) | ~2,632 |
| `cash_flow` | Operating, investing, financing (multi-period) | ~2,604 |

### PostgreSQL Extended Tables

The PostgreSQL schema (`database/schema.sql`) adds tables for XBRL data, price history, news/announcements, user management, and audit logging.

## Configuration

All settings via environment variables. See `.env.example` for the complete reference.

| Variable | Default | Description |
|---|---|---|
| `GEMINI_API_KEY` | (required) | Gemini API key |
| `ANTHROPIC_API_KEY` | (optional) | Legacy fallback key |
| `DB_BACKEND` | `sqlite` | Database backend (`sqlite` or `postgres`) |
| `DB_SQLITE_PATH` | `saudi_stocks.db` | SQLite file path |
| `POSTGRES_HOST` | `localhost` | PostgreSQL host |
| `POSTGRES_PORT` | `5432` | PostgreSQL port |
| `POSTGRES_DB` | `tasi_platform` | PostgreSQL database name |
| `POSTGRES_USER` | `tasi_user` | PostgreSQL user |
| `POSTGRES_PASSWORD` | (required for PG) | PostgreSQL password |
| `SERVER_PORT` | `8084` | FastAPI server port |
| `LOG_LEVEL` | `INFO` | Logging level |
| `REDIS_URL` | `redis://localhost:6379/0` | Redis URL (optional, for widgets pub/sub) |
| `CACHE_ENABLED` | `false` | Enable Redis-based caching |
| `JWT_SECRET_KEY` | (required in prod) | JWT signing secret (enforced in PG mode) |

## Testing

1571+ backend tests (unit, integration, security, performance) and 231 frontend Vitest tests plus Playwright E2E specs.

```bash
# Backend tests (all)
python -m pytest tests/ -q

# Backend tests with coverage
python -m pytest tests/ --cov=api --cov=services --cov=backend --cov-report=term-missing

# Frontend unit tests (Vitest)
cd frontend && npx vitest run

# Frontend E2E tests (Playwright)
cd frontend && npx playwright test

# Frontend build verification (15 pages)
cd frontend && npx next build

# RTL lint check (catch physical direction classes)
cd frontend && npm run lint:rtl
```

### E2E Test Specs

Playwright specs under `frontend/e2e/`:

| Spec | Coverage |
|---|---|
| `news.spec.ts` | News portal: RTL, virtual scroll, SSE, source filters |
| `markets.spec.ts` | Markets: sector filter, sort, pagination, search, mobile card view |
| `stock-detail.spec.ts` | Stock detail: financials, dividends, watchlist, news, reports |

## Project Structure

See `CLAUDE.md` for the full directory tree and detailed architecture documentation.

## Contributing

1. Read `AGENTS.md` and `CLAUDE.md` before making changes
2. For Vanna 2.0 code, consult `vanna-skill/references/` first
3. All tests must pass before merging
4. Never commit `.env` or API keys
5. Update the system prompt in `app.py` if you change the database schema

## License

Proprietary. All rights reserved.
</file>

<file path="services/news_store.py">
"""
News Store (SQLite)
====================
SQLite-compatible news storage service. Schema mirrors the PostgreSQL
``news_articles`` table from ``database/schema.sql`` so the same data
model works in both development (SQLite) and production (PostgreSQL).

Thread-safe: reuses one SQLite connection per thread via threading.local().

Usage:
    from services.news_store import NewsStore
    store = NewsStore("saudi_stocks.db")
    store.store_articles([{...}, ...])
    articles = store.get_latest_news(limit=20)
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_CREATE_TABLE_SQL = """\
⋮----
_CREATE_INDEXES_SQL = [
⋮----
class NewsStore
⋮----
"""SQLite-backed news article storage."""
⋮----
def __init__(self, db_path: str)
⋮----
def _connect(self) -> sqlite3.Connection
⋮----
"""Return a per-thread cached connection.

        Reuses the same connection within a thread to avoid the overhead of
        opening/closing SQLite connections on every operation. Each thread
        still gets its own connection (via threading.local), so there is no
        cross-thread contention.
        """
conn = getattr(self._local, "conn", None)
⋮----
# Connection was closed externally
⋮----
conn = sqlite3.connect(self.db_path)
⋮----
def close(self) -> None
⋮----
"""Close the current thread's cached connection, if any."""
⋮----
def _ensure_table(self) -> None
⋮----
"""Create the news_articles table and indexes if they don't exist."""
conn = self._connect()
⋮----
def store_articles(self, articles: List[Dict]) -> int
⋮----
"""Insert articles, skipping duplicates. Returns count of newly inserted.

        Uses a single ``executemany`` call (all-or-nothing batch). Duplicate
        rows are silently skipped by ``INSERT OR IGNORE`` at the SQLite engine
        level — no ``IntegrityError`` is raised for duplicates.  Any other
        unexpected error rolls back the entire batch.
        """
⋮----
rows = [
before = conn.total_changes
⋮----
inserted = conn.total_changes - before
⋮----
"""Build WHERE clause fragments and params for common filters.

        Returns (clauses: list[str], params: list).
        """
clauses: List[str] = []
params: list = []
⋮----
# Include the full day by comparing with the next day boundary
⋮----
"""Get latest news, optionally filtered by source, sentiment, and date range.

        .. deprecated:: Use :meth:`aget_latest_news` in async contexts.
        """
⋮----
where = (" WHERE " + " AND ".join(clauses)) if clauses else ""
⋮----
rows = conn.execute(
⋮----
def get_article_by_id(self, article_id: str) -> Optional[Dict]
⋮----
"""Get a single article by ID.

        .. deprecated:: Use :meth:`aget_article_by_id` in async contexts.
        """
⋮----
row = conn.execute(
⋮----
def get_articles_by_ids(self, ids: List[str]) -> List[Dict]
⋮----
"""Get multiple articles by their IDs.

        .. deprecated:: Use :meth:`aget_articles_by_ids` in async contexts.
        """
⋮----
placeholders = ",".join("?" for _ in ids)
⋮----
"""Count articles, optionally filtered by source, sentiment, and date range.

        .. deprecated:: Use :meth:`acount_articles` in async contexts.
        """
⋮----
"""Search articles by title or body text, with optional filters.

        .. deprecated:: Use :meth:`asearch_articles` in async contexts.
        """
⋮----
escaped = query.replace("%", "\\%").replace("_", "\\_")
pattern = f"%{escaped}%"
⋮----
where = "(title LIKE ? ESCAPE '\\' OR body LIKE ? ESCAPE '\\')"
params: list = [pattern, pattern]
⋮----
"""Count total articles matching a search query with optional filters.

        .. deprecated:: Use :meth:`acount_search` in async contexts.
        """
⋮----
def get_sources(self) -> List[Dict]
⋮----
"""Get list of sources with article counts.

        .. deprecated:: Use :meth:`aget_sources` in async contexts.
        """
⋮----
def cleanup_old(self, days: int = 7) -> int
⋮----
"""Delete articles older than N days. Returns count deleted."""
cutoff = (datetime.utcnow() - timedelta(days=days)).isoformat()
⋮----
deleted = conn.execute("SELECT changes()").fetchone()[0]
⋮----
# ------------------------------------------------------------------
# Async wrappers (run sync I/O in a thread)
⋮----
async def aget_latest_news(self, **kwargs) -> List[Dict]
⋮----
async def acount_articles(self, **kwargs) -> int
⋮----
async def aget_article_by_id(self, article_id: str) -> Optional[Dict]
⋮----
async def asearch_articles(self, **kwargs) -> List[Dict]
⋮----
async def acount_search(self, **kwargs) -> int
⋮----
async def aget_sources(self) -> List[Dict]
⋮----
async def aget_articles_by_ids(self, ids: List[str]) -> List[Dict]
</file>

<file path="api/routes/charts_analytics.py">
"""
Dual-backend chart analytics API routes.

Queries either SQLite (local dev) or PostgreSQL (Railway/Docker) depending on
the DB_BACKEND environment variable.  Connection handling and parameter
conversion are delegated to ``api.db_helper``.

Endpoints match the frontend PreBuiltCharts.tsx expectations:
  - /api/charts/sector-market-cap
  - /api/charts/top-companies
  - /api/charts/sector-pe
  - /api/charts/dividend-yield-top
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/charts", tags=["charts-analytics"])
⋮----
# ---------------------------------------------------------------------------
# Routes
⋮----
async def sector_market_cap() -> ChartResponse
⋮----
"""Return total market cap by sector for a pie/bar chart."""
⋮----
rows = await afetchall(SECTOR_MARKET_CAP)
⋮----
"""Return top N companies by market cap."""
clauses = ["m.market_cap IS NOT NULL"]
params: list = []
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
sql = f"""
⋮----
rows = await afetchall(sql, params)
⋮----
@router.get("/sector-pe", response_model=ChartResponse, responses=STANDARD_ERRORS)
async def sector_avg_pe() -> ChartResponse
⋮----
"""Return average trailing P/E ratio by sector."""
⋮----
rows = await afetchall(SECTOR_AVG_PE)
⋮----
"""Return top N companies by dividend yield."""
⋮----
rows = await afetchall(DIVIDEND_YIELD_TOP, (limit,))
</file>

<file path="api/routes/sqlite_entities.py">
"""
Dual-backend entity (company/stock) API routes.

Provides entity endpoints that work with both SQLite and PostgreSQL backends
via the shared ``api.db_helper`` module. Used as a fallback when the PG-specific
entities router is unavailable, or as the primary backend when DB_BACKEND=sqlite.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/entities", tags=["entities"])
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class CompanySummary(BaseModel)
⋮----
ticker: str
short_name: Optional[str] = None
sector: Optional[str] = None
industry: Optional[str] = None
current_price: Optional[float] = None
market_cap: Optional[float] = None
change_pct: Optional[float] = None
⋮----
class EntityListResponse(BaseModel)
⋮----
items: List[CompanySummary]
count: int
total: int = 0
⋮----
class SectorInfo(BaseModel)
⋮----
sector: str
company_count: int
⋮----
class CompanyFullDetail(BaseModel)
⋮----
"""Full stock detail joining all available tables."""
⋮----
# companies
⋮----
exchange: Optional[str] = None
currency: Optional[str] = None
# market_data
⋮----
previous_close: Optional[float] = None
open_price: Optional[float] = None
day_high: Optional[float] = None
day_low: Optional[float] = None
week_52_high: Optional[float] = None
week_52_low: Optional[float] = None
avg_50d: Optional[float] = None
avg_200d: Optional[float] = None
volume: Optional[int] = None
avg_volume: Optional[int] = None
beta: Optional[float] = None
⋮----
shares_outstanding: Optional[float] = None
pct_held_insiders: Optional[float] = None
pct_held_institutions: Optional[float] = None
# valuation_metrics
trailing_pe: Optional[float] = None
forward_pe: Optional[float] = None
price_to_book: Optional[float] = None
price_to_sales: Optional[float] = None
enterprise_value: Optional[float] = None
ev_to_revenue: Optional[float] = None
ev_to_ebitda: Optional[float] = None
peg_ratio: Optional[float] = None
trailing_eps: Optional[float] = None
forward_eps: Optional[float] = None
book_value: Optional[float] = None
# profitability_metrics
roa: Optional[float] = None
roe: Optional[float] = None
profit_margin: Optional[float] = None
operating_margin: Optional[float] = None
gross_margin: Optional[float] = None
ebitda_margin: Optional[float] = None
earnings_growth: Optional[float] = None
revenue_growth: Optional[float] = None
# dividend_data
dividend_rate: Optional[float] = None
dividend_yield: Optional[float] = None
payout_ratio: Optional[float] = None
ex_dividend_date: Optional[str] = None
# financial_summary
total_revenue: Optional[float] = None
total_debt: Optional[float] = None
debt_to_equity: Optional[float] = None
current_ratio: Optional[float] = None
free_cashflow: Optional[float] = None
operating_cashflow: Optional[float] = None
ebitda: Optional[float] = None
# analyst_data
recommendation: Optional[str] = None
target_mean_price: Optional[float] = None
target_high_price: Optional[float] = None
target_low_price: Optional[float] = None
target_median_price: Optional[float] = None
analyst_count: Optional[int] = None
⋮----
# Routes
⋮----
"""List companies with basic market data."""
# Always filter out stub entities with no meaningful market data
clauses: list = ["(m.current_price IS NOT NULL OR m.market_cap IS NOT NULL)"]
params: list = []
⋮----
where = "WHERE " + " AND ".join(clauses)
⋮----
count_row = await afetchone(
total = count_row["cnt"] if count_row else 0
⋮----
sql = f"""
rows = await afetchall(sql, params + [limit, offset])
⋮----
items = [
⋮----
@router.get("/sectors", response_model=List[SectorInfo], responses=STANDARD_ERRORS)
async def list_sectors() -> List[SectorInfo]
⋮----
"""Return all sectors with company counts (only companies with real market data)."""
⋮----
rows = await afetchall(SECTOR_LIST)
⋮----
def _normalize_ticker(ticker: str) -> str
⋮----
"""Ensure Saudi ticker has .SR suffix.

    The database stores tickers with the .SR suffix (e.g. '2222.SR').
    Users and internal links often use just the number ('2222'), so
    we append '.SR' when the input is purely numeric.
    """
stripped = ticker.strip()
⋮----
@router.get("/{ticker}", response_model=CompanyFullDetail, responses=STANDARD_ERRORS)
async def get_entity(ticker: str) -> CompanyFullDetail
⋮----
"""Return full stock detail joining all available tables."""
ticker = validate_ticker(ticker)
ticker = _normalize_ticker(ticker)
⋮----
row_dict = await afetchone(ENTITY_FULL_DETAIL, (ticker,))
⋮----
def _f(val)
⋮----
def _i(val)
</file>

<file path="api/routes/stock_data.py">
"""
Stock data API routes (dual-backend: SQLite + PostgreSQL).

Provides per-stock dividends, financial summary, financial statements,
stock comparison, and batch quotes.
Works with both SQLite and PostgreSQL backends via db_helper.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
router = APIRouter(prefix="/api/v1/stocks", tags=["stock-data"])
⋮----
# Allowed financial statement tables to prevent SQL injection
_STATEMENT_TABLES = {"balance_sheet", "income_statement", "cash_flow"}
⋮----
# Allowed metric columns across tables for the compare endpoint.
# Maps metric name -> (table, column).
_METRIC_MAP: Dict[str, tuple] = {}
⋮----
# Build from valuation_metrics
⋮----
# Build from profitability_metrics
⋮----
# Build from market_data
⋮----
# Build from dividend_data
⋮----
# Build from financial_summary
⋮----
# Build from analyst_data
⋮----
# ---------------------------------------------------------------------------
# Response models
⋮----
class DividendData(BaseModel)
⋮----
ticker: str
dividend_rate: Optional[float] = None
dividend_yield: Optional[float] = None
payout_ratio: Optional[float] = None
trailing_annual_dividend_rate: Optional[float] = None
trailing_annual_dividend_yield: Optional[float] = None
avg_dividend_yield_5y: Optional[float] = None
ex_dividend_date: Optional[str] = None
last_dividend_value: Optional[float] = None
last_dividend_date: Optional[str] = None
⋮----
class FinancialSummaryData(BaseModel)
⋮----
total_revenue: Optional[float] = None
total_cash: Optional[float] = None
total_cash_per_share: Optional[float] = None
total_debt: Optional[float] = None
debt_to_equity: Optional[float] = None
current_ratio: Optional[float] = None
quick_ratio: Optional[float] = None
free_cashflow: Optional[float] = None
operating_cashflow: Optional[float] = None
ebitda: Optional[float] = None
gross_profits: Optional[float] = None
net_income_to_common: Optional[float] = None
⋮----
class FinancialPeriod(BaseModel)
⋮----
period_type: Optional[str] = None
period_index: Optional[int] = None
period_date: Optional[str] = None
data: Dict[str, Any] = {}
⋮----
class FinancialsResponse(BaseModel)
⋮----
statement: str
periods: List[FinancialPeriod] = []
⋮----
class CompareMetrics(BaseModel)
⋮----
short_name: Optional[str] = None
metrics: Dict[str, Any] = {}
⋮----
class CompareResponse(BaseModel)
⋮----
tickers: List[CompareMetrics]
⋮----
class QuoteItem(BaseModel)
⋮----
current_price: Optional[float] = None
previous_close: Optional[float] = None
change_pct: Optional[float] = None
volume: Optional[int] = None
⋮----
# Routes
⋮----
async def get_dividends(ticker: str) -> DividendData
⋮----
"""Get dividend data for a specific stock."""
ticker = validate_ticker(ticker)
exists = await afetchone(COMPANY_EXISTS, (ticker,))
⋮----
row = await afetchone(DIVIDEND_DATA_BY_TICKER, (ticker,))
⋮----
async def get_financial_summary(ticker: str) -> FinancialSummaryData
⋮----
"""Get financial summary for a specific stock."""
⋮----
row = await afetchone(FINANCIAL_SUMMARY_BY_TICKER, (ticker,))
⋮----
"""Get financial statement periods for a specific stock."""
⋮----
# Table name is validated above against _STATEMENT_TABLES whitelist.
# SELECT * is intentional here: financial statement tables have dynamic
# columns that vary by statement type, and the handler strips metadata
# fields (id, ticker, period_type, period_index, period_date) below,
# returning everything else in the generic `data` dict.
rows = await afetchall(
⋮----
periods = []
⋮----
# Extract metadata fields
pt = row_dict.pop("period_type", None)
pi = row_dict.pop("period_index", None)
pd_ = row_dict.pop("period_date", None)
⋮----
"""Compare 2-5 stocks side-by-side on specified metrics."""
ticker_list = validate_ticker_list(tickers, min_count=2, max_count=5)
metric_list = [m.strip() for m in metrics.split(",") if m.strip()]
⋮----
# Validate metrics
invalid_metrics = [m for m in metric_list if m not in _METRIC_MAP]
⋮----
# Group metrics by table for efficient queries
table_columns: Dict[str, List[str]] = {}
⋮----
def _sync_compare()
⋮----
placeholders = ",".join("?" for _ in ticker_list)
name_rows = fetchall(
name_map = {r["ticker"]: r["short_name"] for r in name_rows}
⋮----
result_data: Dict[str, Dict[str, Any]] = {t: {} for t in ticker_list}
⋮----
col_list = ", ".join(["ticker"] + columns)
rows = fetchall(
⋮----
tk = row_dict.pop("ticker")
⋮----
items = []
⋮----
"""Get batch quotes for multiple tickers."""
ticker_list = validate_ticker_list(tickers, min_count=1, max_count=50)
⋮----
rows = await afetchall(BATCH_QUOTES_SQL.format(placeholders=placeholders), tuple(ticker_list))
</file>

<file path="CLAUDE.md">
# CLAUDE.md

[![CI](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/ci.yml/badge.svg)](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/ci.yml)
[![Deploy](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/deploy.yml/badge.svg)](https://github.com/Recuba/TASI-vanna-dev_1/actions/workflows/deploy.yml)

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Required Reading (Mandatory)

**At the start of every session**, before making any changes, you MUST read:

1. **`AGENTS.md`** - Read this FIRST, every time. Contains agent configuration rules, constraints, and behavioral guidelines that govern how you operate in this repo.

2. **`vanna-skill/SKILL.md`** and **`vanna-skill/references/`** - The authoritative source for Vanna 2.0 API patterns, correct method signatures, tool registration, and integration best practices. All Vanna-related code MUST conform to these references. When in doubt, read the relevant reference file before writing code.

3. **`vanna_docs/`** - Scraped official Vanna documentation (JSON pages and raw HTML). Consult these when implementing new features, debugging issues, or working with any Vanna API you haven't used before.

**Hard rules:**
- Never guess at Vanna API signatures or patterns -- look them up in the skill references and docs first.
- If a pattern in the codebase conflicts with the skill/docs, flag it rather than silently propagating the incorrect pattern.
- Always cross-reference `vanna-skill/references/` for the correct way to register tools, build system prompts, configure agents, set up servers, and integrate LLMs/databases.

## Project Overview

**Ra'd AI** is a TASI Saudi Stock Market AI Platform built on the **Vanna 2.0** framework. It supports dual database backends (SQLite for development, PostgreSQL for production). Natural language queries are converted to SQL against a normalized database of ~500 Saudi-listed companies, with Plotly chart generation. The platform includes news aggregation, announcement tracking, and technical report services.

## Directory Structure

```
.
├── app.py                          # Vanna 2.0 FastAPI server (dual SQLite/PostgreSQL backend)
├── csv_to_sqlite.py                # CSV-to-normalized-SQLite converter
├── config/
│   ├── __init__.py                 # Singleton get_settings() + re-exports
│   ├── settings.py                 # Pydantic Settings (DatabaseSettings, LLMSettings, ServerSettings)
│   ├── lifecycle.py                # on_startup() / on_shutdown() with pool + Prometheus logging
│   ├── env_validator.py            # Startup env validation with fail-fast enforcement
│   └── logging_config.py          # JSON (prod) / pretty (dev) logging configuration
├── database/
│   ├── schema.sql                  # Full PostgreSQL schema (DDL for all tables + indexes + views)
│   ├── queries.py                  # Centralized SQL query strings
│   ├── pool.py                     # PostgreSQL ThreadedConnectionPool singleton
│   ├── postgres_utils.py           # Shared PG helpers: pg_available(), pg_connection_params()
│   ├── migrate_sqlite_to_pg.py     # SQLite -> PostgreSQL data migration
│   └── csv_to_postgres.py          # CSV -> PostgreSQL direct pipeline
├── services/
│   ├── __init__.py
│   ├── health_service.py           # Health checks (DB connectivity, LLM status)
│   ├── news_store.py               # SQLite news storage (sync + async wrappers)
│   ├── news_scraper.py             # 5-source Arabic news scraper
│   ├── news_scheduler.py           # Background news fetch scheduler
│   ├── news_paraphraser.py         # Arabic synonym substitution
│   ├── news_service.py             # News CRUD (PostgreSQL only)
│   ├── reports_service.py          # CRUD for technical_reports table
│   ├── announcement_service.py     # CRUD for announcements table
│   ├── auth_service.py             # JWT authentication service
│   ├── db_compat.py                # SQLite/PostgreSQL abstraction layer
│   ├── stock_ohlcv.py              # OHLCV data service
│   ├── tasi_index.py               # TASI index data service
│   ├── yfinance_base.py            # Shared yfinance cache + circuit breaker
│   ├── cache_utils.py              # Unified @cache_response decorator (LRU + TTL)
│   └── widgets/
│       ├── __init__.py
│       ├── quotes_hub.py           # QuotesHub: market quote orchestrator (Redis pub/sub)
│       └── providers/
│           ├── __init__.py
│           ├── crypto.py           # Cryptocurrency quotes
│           ├── metals.py           # Precious metals quotes
│           ├── oil.py              # Oil & energy quotes
│           └── indices.py          # Global market indices
├── api/
│   ├── models/
│   │   └── widgets.py              # QuoteItem Pydantic model
│   ├── routes/                     # FastAPI route handlers (async)
│   │   ├── news_feed.py            # /api/v1/news/feed (SQLite news API)
│   │   ├── news_stream.py          # /api/v1/news/stream (SSE endpoint)
│   │   ├── widgets_stream.py       # /api/v1/widgets/stream (SSE market quotes)
│   │   ├── charts_analytics.py     # Chart data endpoints
│   │   ├── market_analytics.py     # Market analytics endpoints
│   │   ├── stock_data.py           # Stock data endpoints
│   │   ├── sqlite_entities.py      # Entity search (SQLite)
│   │   └── ...                     # auth, health, reports, announcements
│   └── db_helper.py                # Async DB query wrappers (asyncio.to_thread)
├── frontend/                       # Next.js 14 app (production)
│   ├── src/
│   │   ├── app/                    # Next.js app router pages
│   │   │   ├── news/               # News feed (decomposed)
│   │   │   │   ├── page.tsx        # Main news page (~500 lines)
│   │   │   │   ├── utils.ts        # Shared constants & helpers
│   │   │   │   ├── hooks/
│   │   │   │   │   └── useNewsFilters.ts
│   │   │   │   ├── components/
│   │   │   │   │   ├── ArticleCard.tsx
│   │   │   │   │   ├── FilterBar.tsx
│   │   │   │   │   ├── NewArticlesBanner.tsx
│   │   │   │   │   ├── SearchInput.tsx
│   │   │   │   │   ├── SkeletonCard.tsx
│   │   │   │   │   └── index.ts
│   │   │   │   └── [id]/page.tsx   # Article detail page
│   │   │   ├── charts/             # TradingView + TASI charts
│   │   │   ├── market/             # Market overview
│   │   │   ├── chat/               # AI chat interface
│   │   │   └── ...                 # admin, login, reports, etc.
│   │   ├── components/             # Shared components
│   │   │   ├── layout/             # Header, Footer, Sidebar
│   │   │   ├── charts/             # Chart wrappers
│   │   │   ├── chat/               # AI chat components
│   │   │   ├── widgets/            # LiveMarketWidgets (SSE market ticker)
│   │   │   └── common/             # Command palette, ConnectionStatusBadge, etc.
│   │   ├── lib/
│   │   │   ├── api-client.ts       # API functions with AbortController
│   │   │   ├── config.ts           # Runtime config (env-driven)
│   │   │   ├── hooks/use-api.ts    # Data fetching hooks
│   │   │   └── utils.ts            # Utility functions
│   │   ├── providers/              # ThemeProvider, LanguageProvider
│   │   └── styles/design-system.ts # Gold/dark design tokens
│   └── package.json
├── middleware/
│   ├── chat_auth.py                # ChatAuthMiddleware: JWT enforcement on chat SSE/poll endpoints
│   ├── request_context.py          # ContextVar request ID + RequestIdFilter for structured logging
│   ├── error_handler.py            # Unified JSON error responses + request_id propagation
│   ├── rate_limit.py               # Tiered rate limiting (10/30/60 rpm)
│   └── cors.py                     # CORS configuration
├── templates/
│   └── index.html                  # Legacy frontend UI (vanna-chat web component)
├── ingestion/                      # Data ingestion pipelines (in progress)
├── docker-compose.yml              # PostgreSQL 16 + app + pgAdmin (optional)
├── Dockerfile                      # Python 3.11 FastAPI container
├── requirements.in                 # Unpinned source constraints (edit this)
├── requirements.txt                # Pinned production dependencies (generated)
├── requirements-dev.txt            # Development/test dependencies
├── requirements.lock               # pip-compile lock file (verified in CI)
├── .env.example                    # All environment variables documented
├── .dockerignore
├── test_app_assembly.py            # Legacy Vanna assembly smoke tests (v1)
├── vanna-skill/                    # Vanna 2.0 API reference (read-only)
│   ├── SKILL.md
│   └── references/
├── vanna_docs/                     # Scraped Vanna docs (read-only)
├── saudi_stocks.db                 # SQLite database (generated, not committed)
├── saudi_stocks_yahoo_data.csv     # Source data (500 stocks, 1062 columns)
├── AGENTS.md                       # Agent behavioral rules
└── CLAUDE.md                       # This file
```

## Commands

```bash
# Start server (port 8084, SQLite backend)
python app.py

# Start with PostgreSQL via Docker
docker compose up -d

# Start with pgAdmin included
docker compose --profile tools up -d

# Run backend tests (1571+ tests)
python -m pytest tests/ -q

# Run frontend tests (231 tests)
cd frontend && npx vitest run

# Frontend production build (15 pages)
cd frontend && npx next build

# Rebuild SQLite database from CSV
python csv_to_sqlite.py

# Migrate SQLite data to PostgreSQL
python database/migrate_sqlite_to_pg.py

# Load CSV directly into PostgreSQL
python database/csv_to_postgres.py

# Lint frontend for RTL violations (physical direction classes)
cd frontend && npm run lint:rtl
```

**Environment setup:** Copy `.env.example` to `.env` and configure. At minimum set `GEMINI_API_KEY`. See `.env.example` for all available settings. For the frontend, copy `frontend/.env.local.example` to `frontend/.env.local`.

## Architecture

### Dual Database Backend

The app supports two database backends controlled by `DB_BACKEND` env var:
- **SQLite** (default): Uses `saudi_stocks.db` via `SqliteRunner`. Good for local development.
- **PostgreSQL**: Uses `PostgresRunner` with `POSTGRES_*` env vars. Used in Docker/production. Full schema in `database/schema.sql`.

### Configuration Module (`config/`)

Typed settings via `pydantic-settings`:
- `DatabaseSettings` (env prefix `DB_`): backend selection, SQLite path, PostgreSQL connection. Accepts both `DB_PG_*` and `POSTGRES_*` env var names for Docker compatibility.
- `LLMSettings` (env prefix `LLM_`): model, API key, max tool iterations (Anthropic only).
- `ServerSettings` (env prefix `SERVER_`): host, port, debug mode.
- `Settings`: top-level aggregator with `.env` file loading and backward-compatible `ANTHROPIC_API_KEY`.
- `get_settings()`: cached singleton accessor.

### Data Pipeline (`csv_to_sqlite.py`)
Transforms a 1062-column flat CSV into 10 normalized SQLite tables:
- **7 simple tables** (1 row per ticker): companies, market_data, valuation_metrics, profitability_metrics, dividend_data, financial_summary, analyst_data
- **3 financial statement tables** (multiple rows per ticker, unpivoted from wide to tall): balance_sheet, income_statement, cash_flow

Financial statements use `period_type` ('annual'/'quarterly'/'ttm') and `period_index` (0=most recent) for time-series querying. Column mappings are declarative dicts at the top of the file. The unpivot logic in `unpivot_financial()` converts prefixed columns (e.g., `bs_y0_Total_Assets`) into normalized rows.

### Server (`app.py`)
Assembles a Vanna 2.0 `Agent` with 5 components:
1. `AnthropicLlmService` - Claude Sonnet 4.5
2. `ToolRegistry` with `RunSqlTool` + `VisualizeDataTool` (access_groups: admin, user)
3. `DefaultUserResolver` - returns single default user (no auth)
4. `DemoAgentMemory` - in-memory conversation storage
5. `SaudiStocksSystemPromptBuilder` - schema documentation (includes PostgreSQL notes when using PG backend)

The `VannaFastAPIServer.create_app()` creates the FastAPI app. Vanna's default "/" route is **explicitly removed** before registering the custom template route, because FastAPI uses first-match routing.

### Services (`services/`)

**SQLite services** (work with both backends):
- `news_store.py` - SQLite news storage with sync methods + async wrappers (`aget_*` via `asyncio.to_thread`). The sync methods are deprecated in favor of their async counterparts for use in FastAPI handlers.
- `news_scraper.py` - Scrapes 5 Arabic news sources (config-driven via `ScraperSettings`)
- `news_scheduler.py` - Background daemon thread for periodic news fetching
- `news_paraphraser.py` - Arabic synonym substitution for content diversity
- `db_compat.py` - SQLite/PostgreSQL abstraction layer
- `health_service.py` - Structured health checks (database connectivity, LLM availability)

**PostgreSQL-only CRUD services** (using `psycopg2`):
- `news_service.py` - News article aggregation and retrieval (PostgreSQL)
- `announcement_service.py` - CMA/Tadawul announcement tracking

**Dual-backend CRUD services** (SQLite + PostgreSQL):
- `reports_service.py` - Technical/analyst report management. Auto-creates the `technical_reports` table in SQLite.

### Async I/O Layer

All route handlers are `async def`. Synchronous database calls (sqlite3, psycopg2) are wrapped in `asyncio.to_thread()` to prevent blocking the event loop:
- `api/db_helper.py` - `afetchall()` and `afetchone()` async wrappers
- `services/news_store.py` - `aget_latest_news()`, `acount_articles()`, etc.
- `database/manager.py` - `aconnection()` async context manager

### Frontend
- **Legacy** (`templates/index.html`): Custom Ra'd AI design with gold palette (#D4A84B), dark background (#0E0E0E), Tajawal font. Embeds `<vanna-chat>` web component loaded as ES module from CDN.
- **Production** (`frontend/`): Next.js 14 app with TypeScript, Tailwind CSS, gold/dark design system. 15 pages. Features include:
  - Full Arabic RTL support via Tailwind logical properties (`ms-*`, `me-*`, `ps-*`, `pe-*`) with lint enforcement (`npm run lint:rtl`)
  - Real-time news feed via SSE (`/api/v1/news/stream`)
  - Live market widgets via SSE (`/api/v1/widgets/stream`) with reconnection backoff
  - Navigation progress bar (NextTopLoader, gold #D4A84B)
  - Connection status badge (live/reconnecting/offline)
  - AbortController on all fetch calls (race-condition-safe), including header health polling
  - Virtual scrolling for large lists
  - Mobile-responsive card view for market data tables
  - Route-level `loading.tsx` and `error.tsx` for news, market, charts, and chat
  - localStorage quota resilience in chat hook
  - Env-driven runtime config (`frontend/src/lib/config.ts`)

### Live Market Widgets (`services/widgets/`)

Real-time market quotes delivered via SSE:
- **`QuotesHub`** (`quotes_hub.py`): Orchestrates 4 provider fetchers and broadcasts updates. Supports optional Redis pub/sub for multi-instance deployments.
- **Providers** (`providers/`): Modular fetchers for crypto, precious metals, oil, and global indices.
- **`QuoteItem`** (`api/models/widgets.py`): Pydantic model for quote data (symbol, price, change, category).
- **SSE endpoint** (`api/routes/widgets_stream.py`): Streams quote updates to the frontend at `/api/v1/widgets/stream`.
- **Frontend** (`frontend/src/components/widgets/LiveMarketWidgets.tsx`): React component with EventSource, exponential backoff reconnection, and category filter tabs.
- **`ConnectionStatusBadge`** (`frontend/src/components/common/ConnectionStatusBadge.tsx`): Reusable live/reconnecting/offline indicator used by both the widgets ticker and the header.

The hub is started as a background task during FastAPI lifespan and its route is registered in `app.py`.

### Caching & Shared Utilities

- **`@cache_response`** (`services/cache_utils.py`): Unified caching decorator with TTL expiration and LRU eviction (max 500 entries by default).
- **`YFinanceCache`** (`services/yfinance_base.py`): Shared LRU cache for yfinance API calls with configurable TTL.
- **`CircuitBreaker`** (`services/yfinance_base.py`): Prevents cascading failures when yfinance endpoints are down.

### Middleware

- **GZipMiddleware**: Compresses responses larger than 1000 bytes (added in `app.py`).
- **X-Request-ID**: Added to all response headers for request tracing.
- **`ChatAuthMiddleware`** (`middleware/chat_auth.py`): Enforces JWT on `/api/vanna/v2/chat_sse` and `/api/vanna/v2/chat_poll`. Only registered when `DB_BACKEND=postgres`.
- **`RequestIdFilter`** (`middleware/request_context.py`): Injects `request_id` from a `ContextVar` into every log record. Installed at startup with a duplicate guard. `error_handler.py` sets the ContextVar on each request so all downstream log calls share the same ID.

### Docker (`docker-compose.yml`)
- **postgres**: PostgreSQL 16 Alpine, auto-initialized with `database/schema.sql`, health-checked
- **app**: Python 3.11 FastAPI container, auto-connects to postgres
- **pgadmin**: Optional (via `--profile tools`), accessible on port 5050

## Key Vanna 2.0 Patterns

- **Tool registration**: Use `tools.register_local_tool(tool, access_groups=[...])` - the `.register()` method does NOT exist in Vanna 2.0.2
- **SystemPromptBuilder**: Abstract method signature is `build_system_prompt(self, user, tools)`, not `build()`
- **Agent constructor requires all of**: `llm_service`, `tool_registry`, `user_resolver`, `agent_memory`
- **Streaming**: `AgentConfig(stream_responses=True)` enables SSE streaming; `max_tool_iterations=10` caps tool calls per query
- **vanna-chat script tag**: Must use `type="module"` or the web component won't register

## Gotchas

- The system prompt in `app.py` documents the full database schema. If schema changes, update both the column mappings AND the system prompt.
- `csv_to_sqlite.py` skips financial statement rows where `period_date` is null -- some companies have fewer periods than others (~71% coverage, not 100%).
- All test files (`tests/test_database.py`, `tests/test_app_assembly_v2.py`, etc.) use `DB_SQLITE_PATH` env var for the database path. Fallback is `Path(__file__).resolve().parent.parent / "saudi_stocks.db"` which resolves correctly from `tests/` to the project root.
- The `<vanna-chat>` component requires internet (loaded from CDN).
- Database path in app.py is script-relative via `Path(__file__).resolve().parent / "saudi_stocks.db"`.
- PostgreSQL-only services (`news_service.py`, `announcement_service.py`) use `psycopg2` and are not available with SQLite backend. Use `news_store.py` for SQLite news operations. `reports_service.py` supports both backends.
- `config/settings.py` uses `validation_alias` for POSTGRES_* env vars so the same `.env` file works for both Docker Compose and the config module.
- In FastAPI route handlers, always use `aget_*` async methods from `news_store.py`, never the sync `get_*` methods (they block the event loop).
- Frontend uses Tailwind logical properties (`ms-*`, `me-*`, `ps-*`, `pe-*`) for RTL. Do NOT use `ml-*`, `mr-*`, `pl-*`, `pr-*` for horizontal spacing. Run `npm run lint:rtl` to check.
- The Live Market Widgets system works without Redis (single-process mode). Redis (`REDIS_URL`, `CACHE_ENABLED=true`) is only needed for multi-instance deployments.
- All SSE endpoints must include `request.is_disconnected()` checks in their generator loops to avoid orphaned server-side generators.
- The `QuotesHub` background task is started during FastAPI lifespan. If you add new SSE producers, register them similarly in `app.py`'s lifespan handler.
- Tadawul trading days are Sunday-Thursday. Friday and Saturday are weekends (not the Western Saturday/Sunday).
- Health check routes are wrapped in `asyncio.to_thread()` to prevent blocking during database health probes.
- JWT secret is enforced at startup in production PostgreSQL mode -- missing `JWT_SECRET_KEY` raises `RuntimeError`.
- SQL query strings are centralized in `database/queries.py`. Prefer using these constants over inline SQL in route handlers.
- Pagination `limit` parameters have an upper bound of 100 (`le=100`) across all list endpoints.
- `database/postgres_utils.py` provides `pg_available()` and `pg_connection_params()` shared helpers. Do not duplicate PG connection logic in test files — import from there instead.
- `config/lifecycle.py` `on_startup()` logs connection pool size and Prometheus availability at startup. All checks are wrapped in `try/except` and cannot block startup.
- Prometheus metrics are exposed at `/metrics` when `prometheus-fastapi-instrumentator` is installed. The app starts normally without it (graceful `ImportError` fallback).
- `RequestIdFilter` is installed with a duplicate guard: `if not any(isinstance(f, RequestIdFilter) for f in root_logger.filters)`. Do not remove this guard or metrics and hot-reload will double-install it.
- `requirements.lock` is verified in CI via `pip-compile`. If you add a dependency to `requirements.in`, regenerate with: `pip-compile requirements.in -o requirements.lock --no-annotate --strip-extras`
</file>

<file path="frontend/next.config.mjs">
/** @type {import('next').NextConfig} */
⋮----
async headers()
⋮----
// Disable buffering for SSE endpoints so chunks stream through immediately
⋮----
// Disable buffering for news SSE stream
⋮----
async rewrites()
</file>

<file path="frontend/src/app/market/page.tsx">
import { useState, useMemo, useRef } from 'react';
import Link from 'next/link';
import { useSearchParams, useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useSectors, useEntities } from '@/lib/hooks/use-api';
import { AreaChart, ChartWrapper, TradingViewAttribution, ChartErrorBoundary } from '@/components/charts';
import { LazySparkline } from '@/components/charts/LazySparkline';
import { useMarketIndex } from '@/lib/hooks/use-chart-data';
import { LoadingSpinner } from '@/components/common/loading-spinner';
import { ErrorDisplay } from '@/components/common/error-display';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector, findTickersByAlias, matchesSearch } from '@/lib/stock-translations';
import { Breadcrumb } from '@/components/common/Breadcrumb';
import MarketMoversPanel from './components/MarketMoversPanel';
import { useKeyboardNav } from '@/lib/hooks/use-keyboard-nav';
import { formatNumber, formatMarketCap } from '@/lib/utils/arabic-numbers';
// ---------------------------------------------------------------------------
// Sort types
// ---------------------------------------------------------------------------
⋮----
type SortField = 'short_name' | 'ticker' | 'current_price' | 'change_pct' | 'market_cap';
type SortDir = 'asc' | 'desc';
⋮----
// ---------------------------------------------------------------------------
// Sort icon
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// 52-week range bar
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Market overview page
// ---------------------------------------------------------------------------
⋮----
// Expand aliases: if search matches an alias (e.g. "Aramco" -> "2222.SR"),
// send the matched ticker to the backend for precise results.
⋮----
// If the search matches aliases, use the first matched ticker (without .SR)
// so the backend LIKE query will match.
⋮----
// Apply client-side alias filtering when the user typed an alias search.
// The backend already returned matching items via the expanded ticker search,
// but if the search was by original text and aliases matched, we further filter.
⋮----
{/* Breadcrumb */}
⋮----
{/* Header */}
⋮----
{/* TASI Index Chart */}
⋮----
<ChartWrapper title=
⋮----
{/* Search + Sector Filter Row */}
⋮----
{/* Search */}
⋮----
<svg className=
⋮----
onChange=
placeholder=
⋮----
className=
⋮----

⋮----
{/* Sector Chips */}
⋮----
{/* Market Movers Panel */}
⋮----
{/* Companies Table */}
⋮----
{/* Desktop table */}
⋮----
onClick=
⋮----
<span className=
⋮----
{/* Mobile card layout */}
⋮----
{/* Top row: name + price */}
⋮----
{/* Bottom row: metrics */}
⋮----
{/* Pagination */}
⋮----
setSearch('');
handleSectorChange(undefined);
⋮----
{/* AI Chat CTA */}
</file>

<file path="frontend/src/app/news/[id]/page.tsx">
import { useState, useEffect, useCallback, useRef } from 'react';
import Link from 'next/link';
import { useParams, useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useNewsArticle, useNewsFeed } from '@/lib/hooks/use-api';
import { searchNewsFeed, type NewsFeedItem } from '@/lib/api-client';
import { useLanguage } from '@/providers/LanguageProvider';
import { getSourceColor, timeAgo, readingTime } from '../utils';
⋮----
// Extended type for extra API fields
type ArticleExtras = {
  sentiment_score?: number | null;
  sentiment_label?: string | null;
  ticker?: string | null;
};
⋮----
// ---------------------------------------------------------------------------
// Arabic date formatter (detail page specific)
// ---------------------------------------------------------------------------
⋮----
function formatDate(dateStr: string | null, language: string): string
⋮----
// ---------------------------------------------------------------------------
// Priority label
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Large source badge
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Sentiment badge (larger for detail view)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Stock ticker badge
// ---------------------------------------------------------------------------
⋮----
className=
⋮----
// ---------------------------------------------------------------------------
// Reading progress bar
// ---------------------------------------------------------------------------
⋮----
const handleScroll = () =>
⋮----
// ---------------------------------------------------------------------------
// Share button with toast
// ---------------------------------------------------------------------------
⋮----
// Use native share on mobile if available
⋮----
// User cancelled or share failed -- fall through to clipboard copy
⋮----
// Clipboard copy fallback
⋮----

⋮----
// ---------------------------------------------------------------------------
// Loading skeleton
// ---------------------------------------------------------------------------
⋮----
{/* Badges row */}
⋮----
{/* Title */}
⋮----
{/* Meta row: source badge + date */}
⋮----
{/* Divider */}
⋮----
{/* Body lines */}
⋮----
{/* Shimmer overlay */}
⋮----
// ---------------------------------------------------------------------------
// Related article card (compact)
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// URL validation helper
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Article detail page
// ---------------------------------------------------------------------------
⋮----
// Escape key navigates back to news list
⋮----
const handler = (e: KeyboardEvent) =>
⋮----
// Auto-retry once on non-404 network errors
⋮----
// Minimum loading duration to prevent skeleton flash
⋮----
// Fetch related articles: by ticker if available, otherwise by source
⋮----
// Search by ticker when article has one
⋮----
// Fallback: same-source articles (only fetched when no ticker)
⋮----
// Use ticker-based results if available, otherwise same-source
⋮----
{/* Reading progress bar */}
⋮----
{/* Breadcrumb */}
⋮----
{/* Content */}
⋮----
{/* Priority badge + sentiment + stock + share row */}
⋮----
{/* Title */}
⋮----
{/* Meta row: large source badge + date */}
⋮----
{/* Divider */}
⋮----
{/* Body -- improved typography with drop cap */}
⋮----
{/* Gold gradient top accent */}
⋮----
{/* Source link -- only show when body exists (when no body, link is inside the alert box above) */}
⋮----
{/* Enhanced metadata footer */}
⋮----
{/* Related articles */}
</file>

<file path="frontend/src/app/page.tsx">
import { useState, useEffect } from 'react';
import Link from 'next/link';
import { cn } from '@/lib/utils';
import { useSectors, useMarketData } from '@/lib/hooks/use-api';
import { MiniSparkline, TradingViewAttribution, ChartErrorBoundary } from '@/components/charts';
import { useMiniChartData } from '@/lib/hooks/use-chart-data';
import { LoadingSpinner } from '@/components/common/loading-spinner';
import { ErrorDisplay } from '@/components/common/error-display';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector } from '@/lib/stock-translations';
⋮----
// ---------------------------------------------------------------------------
// Quick action cards - bilingual
// ---------------------------------------------------------------------------
⋮----
// Sector display colors by index
⋮----
// ---------------------------------------------------------------------------
// Mini sparkline wrapper
// ---------------------------------------------------------------------------
⋮----
function StockSparkline(
⋮----
// ---------------------------------------------------------------------------
// Page
// ---------------------------------------------------------------------------
⋮----
// Migrate old key name
⋮----
function dismissOnboarding()
⋮----
{/* Onboarding Banner */}
⋮----
<div className=
⋮----
{/* Hero Section */}
⋮----
{/* Quick Action Cards */}
⋮----
{/* Two-column: Sectors + Top Movers */}
⋮----
{/* Sector Cards */}
⋮----
{/* Top Movers */}
⋮----
<p className=
⋮----
{/* About Section */}
⋮----
{/* AI Chat CTA */}
⋮----
{/* TradingView Attribution */}
</file>

<file path="frontend/src/lib/use-sse-chat.ts">
import { useCallback, useEffect, useRef, useState } from 'react';
import type { ChatMessage, SSEEvent, Vanna2RawEvent } from './types';
⋮----
/** How often (ms) to flush batched SSE events to React state */
⋮----
// Migrate old key name
⋮----
/** Serializable version of ChatMessage for localStorage */
interface StoredMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: string;
  components?: SSEEvent[];
  isError?: boolean;
}
⋮----
function saveMessages(messages: ChatMessage[])
⋮----
// Only store completed, non-streaming messages; cap at MAX
⋮----
// Trim to last 50 messages and retry once
⋮----
try { localStorage.setItem(STORAGE_KEY, JSON.stringify(trimmed)); } catch { /* give up */ }
⋮----
// Other errors (unavailable, security) - silently ignore
⋮----
function loadMessages(): ChatMessage[]
⋮----
// ---------------------------------------------------------------------------
// Vanna 2.0 SSE event normalization
// ---------------------------------------------------------------------------
⋮----
/**
 * Checks whether a parsed JSON object is a Vanna 2.0 raw event
 * (i.e. has the rich/simple envelope).
 */
function isVanna2RawEvent(obj: unknown): obj is Vanna2RawEvent
⋮----
/**
 * Checks whether a parsed JSON object already conforms to the normalized
 * SSEEvent shape (has `type` string and `data` object).
 */
function isNormalizedSSEEvent(obj: unknown): obj is SSEEvent
⋮----
/**
 * Normalize a raw parsed JSON payload from the Vanna 2.0 backend into the
 * frontend SSEEvent format. Handles three cases:
 *   1. Already in normalized {type, data} format -> pass through.
 *   2. Vanna 2.0 rich/simple envelope -> map rich.type to SSEEvent.
 *   3. Unknown shape -> wrap as text.
 *
 * Returns null if the event should be silently skipped (e.g. UI-only
 * control messages like chat_input_update).
 */
function normalizeSSEEvent(raw: unknown): SSEEvent | null
⋮----
// Case 2: Vanna 2.0 rich/simple envelope
⋮----
// Status bar progress updates
⋮----
// Skip "error" status bar updates that duplicate status_card errors
// -- they don't carry new info and the status_card handles the error UI
⋮----
// Task tracker updates (e.g. "Execute run_sql", "Load conversation context")
⋮----
// Only show meaningful task updates (new tasks and completions with detail)
⋮----
// Skip silent/redundant task tracker updates
⋮----
// Status cards (info, error, success states)
⋮----
// Error status cards -> render as text so the error is visible
⋮----
// Dataframes -> table
⋮----
// Convert array-of-objects into array-of-arrays
⋮----
// SQL code blocks
⋮----
// Plotly chart — Vanna 2.0 puts chart fields (data, layout, etc.)
// directly in richData, not wrapped in plotly_json or fig.
⋮----
// Text / markdown content
⋮----
// UI-only control messages that don't produce visible content
⋮----
// Unknown rich type -> fallback to text using simple text or stringified rich
⋮----
// Case 1: Already in normalized format
⋮----
// Case 3: Unknown shape -> wrap as text
⋮----
// Try to extract any text field
⋮----
/**
 * Hook that manages the chat conversation with the Vanna SSE backend.
 * Includes localStorage persistence, error recovery, retry, and
 * batched SSE event processing for reduced React re-renders.
 */
export function useSSEChat()
⋮----
/** The latest progress message from the SSE stream, updated independently */
⋮----
// --- Event batching refs ---
/** Accumulates SSE events between flushes */
⋮----
/** Accumulates text content between flushes */
⋮----
/** Timer handle for the flush interval */
⋮----
/** ID of the assistant message currently being streamed into */
⋮----
// Restore messages from localStorage on mount
⋮----
// Update messageIdRef to avoid collisions
⋮----
// Persist messages whenever they change (skip initial empty + skip while streaming)
⋮----
// Cleanup flush timer and abort controller on unmount to prevent leaks
⋮----
function nextId(): string
⋮----
/**
   * Flush accumulated events from refs into React state in a single
   * setMessages call, dramatically reducing re-renders during streaming.
   */
⋮----
// Reset refs before the state update
⋮----
// Add user message
⋮----
// Abort any previous request
⋮----
// Start the periodic flush timer
⋮----
// Skip null events (silently ignored control messages)
⋮----
// Progress events: update the live progress text (shown in the
// loading indicator) but DON'T accumulate in components -- they
// are transient status messages, not final content.
⋮----
// Batch the event into refs (flushed periodically)
⋮----
// Skip malformed JSON
⋮----
// Bilingual error message based on stored language preference
⋮----
// Stop the flush timer and do one final flush
⋮----
// Mark streaming complete
⋮----
// ignore
⋮----
// Final flush of any pending events
⋮----
/** Retry the last failed query by re-sending the last user message */
⋮----
// Find the last user message
⋮----
// Remove the last assistant message (the error one)
⋮----
// Remove the last user message too since sendMessage will re-add it
⋮----
// Re-send
// Use setTimeout so state updates flush first
</file>

<file path="frontend/src/components/common/CommandPalette.tsx">
import {
  useState,
  useEffect,
  useCallback,
  useRef,
  useMemo,
  useDeferredValue,
} from 'react';
import { useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector, matchesSearch } from '@/lib/stock-translations';
import { getMarketHeatmap, getEntities } from '@/lib/api-client';
⋮----
// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------
⋮----
interface PaletteItem {
  id: string;
  label: string;
  sublabel?: string;
  section: string;
  href: string;
}
⋮----
interface CachedStock {
  ticker: string;
  name: string;
  sector: string;
}
⋮----
// ---------------------------------------------------------------------------
// Constants
// ---------------------------------------------------------------------------
⋮----
// Migrate old key names
⋮----
// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------
⋮----
function getRecentSearches(): string[]
⋮----
function saveRecentSearch(query: string)
⋮----
function getCachedStocks(): CachedStock[]
⋮----
// ignore
⋮----
function setCachedStocks(data: CachedStock[])
⋮----
function fuzzyMatch(text: string, query: string): boolean
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
// Bilingual labels for page items
⋮----
// Load stocks on first open
⋮----
// Keyboard shortcut: Ctrl+K / Cmd+K
⋮----
function handleKeyDown(e: KeyboardEvent)
⋮----
// Auto-focus input when opened
⋮----
// Detect if a query looks like a natural language question for the AI
⋮----
// Contains question mark
⋮----
// Arabic question particles
⋮----
// English question words
⋮----
// More than 5 words and doesn't look like a ticker search
⋮----
// Build filtered results
⋮----
// Show pages + actions when no query
⋮----
// Smart query routing: if it looks like a question, add "Ask Ra'd" at the top
⋮----
// Filter stocks (with alias matching for common names like "Aramco")
⋮----
// Filter pages
⋮----
// Filter actions
⋮----
// Reset index when results change
⋮----
// Navigate to selected item
⋮----
// Keyboard navigation
⋮----
// Scroll selected item into view
⋮----
// Group results by section for display
⋮----
{/* Backdrop */}
⋮----
{/* Palette modal */}
⋮----
className=
⋮----
{/* Search input */}
⋮----
{/* Recent searches */}
⋮----
{/* Results list */}
⋮----
{/* Icon based on section */}
⋮----
{/* Footer hints */}
</file>

<file path="frontend/src/app/stock/[ticker]/StockDetailClient.tsx">
import { useEffect, useState, useCallback } from 'react';
import Link from 'next/link';
import { useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useStockDetail } from '@/lib/hooks/use-api';
import { CandlestickChart, ChartWrapper, TradingViewAttribution, ChartErrorBoundary } from '@/components/charts';
import { useOHLCVData } from '@/lib/hooks/use-chart-data';
import { LoadingSpinner } from '@/components/common/loading-spinner';
import { ErrorDisplay } from '@/components/common/error-display';
import { useLanguage } from '@/providers/LanguageProvider';
import { translateSector } from '@/lib/stock-translations';
import { Tooltip } from '@/components/ui/Tooltip';
import { useToast } from '@/components/common/Toast';
import { StockFinancials } from './components/StockFinancials';
import { StockDividends } from './components/StockDividends';
import { StockNewsSection } from './components/StockNewsSection';
import { StockReportsSection } from './components/StockReportsSection';
⋮----
// ---------------------------------------------------------------------------
// Watchlist helpers
// ---------------------------------------------------------------------------
⋮----
function getWatchlistTickers(): string[]
⋮----
function setWatchlistTickers(tickers: string[])
⋮----
// ---------------------------------------------------------------------------
// Shared helpers
// ---------------------------------------------------------------------------
⋮----
function formatNumber(val: number | null | undefined, opts?:
⋮----
function formatPct(val: number | null | undefined): string
⋮----
// ---------------------------------------------------------------------------
// Ticker normalization + tab types
// ---------------------------------------------------------------------------
⋮----
// ---------------------------------------------------------------------------
// Component
// ---------------------------------------------------------------------------
⋮----
const handler = (e: KeyboardEvent) =>
⋮----
try { await navigator.share({ title: shareTitle, url: window.location.href }); return; } catch { /* fall through */ }
⋮----
try { await navigator.clipboard.writeText(window.location.href); setShareCopied(true); setTimeout(() => setShareCopied(false), 2000); } catch { /* ignore */ }
⋮----
{/* Print-only header */}
⋮----
{/* Breadcrumbs */}
⋮----

⋮----
{/* Company Header */}
⋮----
<Tooltip text=
⋮----
{/* Price Summary Row */}
⋮----
<MetricCard label=
⋮----
{/* Tab Navigation */}
⋮----
<button key=
⋮----
{/* Overview Tab */}
⋮----
<ChartWrapper title=
⋮----
<Link href=
⋮----
{/* AI Chat CTA */}
</file>

<file path="frontend/src/components/layout/Header.tsx">
import { useState, useEffect } from 'react';
import Link from 'next/link';
import { usePathname } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useTheme } from '@/providers/ThemeProvider';
import { useLanguage } from '@/providers/LanguageProvider';
import { useAuth } from '@/lib/hooks/use-auth';
import { HEALTH_POLL_INTERVAL_MS } from '@/lib/config';
import { Tooltip } from '@/components/ui/Tooltip';
import { prefetchRoute } from '@/lib/performance/utils';
⋮----
interface HeaderProps {
  onToggleMobileSidebar?: () => void;
}
⋮----
const checkHealth = async () =>
⋮----
{/* Mobile hamburger */}
⋮----
{/* Brand Mark */}
⋮----
{/* Desktop nav links */}
⋮----
onMouseEnter=
⋮----
className=
⋮----
{/* Search / Command palette hint */}
⋮----
{/* Language toggle */}
⋮----
{/* Theme toggle */}
⋮----
{/* Status indicator */}
⋮----
{/* User auth section */}
</file>

<file path="frontend/src/components/chat/AIChatInterface.tsx">
import { useRef, useEffect, useState, useCallback, FormEvent } from 'react';
import { useSearchParams, useRouter } from 'next/navigation';
import { cn } from '@/lib/utils';
import { useSSEChat } from '@/lib/use-sse-chat';
import { LoadingDots } from './LoadingDots';
import { HelpPanel } from './HelpPanel';
import { MessageThread } from './MessageThread';
import { useLanguage } from '@/providers/LanguageProvider';
import { useConversationHistory, getActiveConvId, setActiveConvId } from './hooks/useConversationHistory';
⋮----
// Auto-scroll to bottom on new messages
⋮----
// Auto-focus input
⋮----
// Handle ?q= pre-fill from URL
⋮----
// M-23: Auto-expanding textarea
⋮----
const handleSubmit = (e: FormEvent) =>
⋮----
const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) =>
⋮----
{/* Header with clear button */}
⋮----
{/* History toggle */}
⋮----
{/* Clear chat */}
⋮----
{/* Conversation history dropdown */}
⋮----
<div key=
⋮----
<button onClick=
⋮----
{/* Messages area */}
⋮----
/* Welcome screen */
⋮----
{/* Input area */}
⋮----
placeholder=
⋮----
<button type="button" onClick=
</file>

<file path="frontend/src/lib/api-client.ts">
/**
 * Legacy compatibility shim.
 *
 * All types and functions have moved to '@/lib/api/' domain modules.
 * Import from '@/lib/api' for new code. This file re-exports everything
 * so existing imports from '@/lib/api-client' continue to work.
 */
</file>

<file path="frontend/src/app/news/page.tsx">
import { useReducer, useCallback, useEffect, useRef, useMemo } from 'react';
import { useVirtualizer } from '@tanstack/react-virtual';
import { cn } from '@/lib/utils';
import { useNewsFeed, useNewsSources } from '@/lib/hooks/use-api';
import { searchNewsFeed, getNewsFeedByIds, type NewsFeedItem } from '@/lib/api-client';
import { useLanguage } from '@/providers/LanguageProvider';
import { PAGE_SIZE, POLLING_FALLBACK_INTERVAL, getBookmarks, saveBookmarks } from './utils';
import { useNewsFilters } from './hooks/useNewsFilters';
import { ArticleCard, FilterBar, NewArticlesBanner, SkeletonCard } from './components';
import { ConnectionStatusBadge } from '@/components/common/ConnectionStatusBadge';
import { useToast } from '@/components/common/Toast';
⋮----
// ---------------------------------------------------------------------------
// Types & reducer (outside component)
// ---------------------------------------------------------------------------
⋮----
interface NewsState {
  allArticles: NewsFeedItem[];
  bookmarks: Set<string>;
  loadingMore: boolean;
  searchResults: NewsFeedItem[] | null;
  searchLoading: boolean;
  isSticky: boolean;
  newArticleCount: number;
  savedArticles: NewsFeedItem[];
  savedLoading: boolean;
  columnCount: number;
  sseStatus: 'live' | 'reconnecting' | 'offline';
  retrying: boolean;
  minLoadElapsed: boolean;
}
⋮----
type NewsAction =
  | { type: 'SET_ALL_ARTICLES'; payload: NewsFeedItem[] }
  | { type: 'APPEND_ARTICLES'; payload: NewsFeedItem[] }
  | { type: 'SET_BOOKMARKS'; payload: Set<string> }
  | { type: 'SET_LOADING_MORE'; payload: boolean }
  | { type: 'SET_SEARCH_RESULTS'; payload: NewsFeedItem[] | null }
  | { type: 'SET_SEARCH_LOADING'; payload: boolean }
  | { type: 'SET_IS_STICKY'; payload: boolean }
  | { type: 'SET_NEW_ARTICLE_COUNT'; payload: number }
  | { type: 'SET_SAVED_ARTICLES'; payload: NewsFeedItem[] }
  | { type: 'SET_SAVED_LOADING'; payload: boolean }
  | { type: 'SET_COLUMN_COUNT'; payload: number }
  | { type: 'SET_SSE_STATUS'; payload: 'live' | 'reconnecting' | 'offline' }
  | { type: 'SET_RETRYING'; payload: boolean }
  | { type: 'SET_MIN_LOAD_ELAPSED'; payload: boolean };
⋮----
function newsReducer(state: NewsState, action: NewsAction): NewsState
⋮----
// ---------------------------------------------------------------------------
// Main page
// ---------------------------------------------------------------------------
⋮----
// Filters hook — pass a reset callback that dispatches SET_ALL_ARTICLES with []
⋮----
// Load bookmarks from localStorage on mount
⋮----
// Minimum loading duration (200ms) to prevent skeleton flash
⋮----
// Scroll restoration: save position before navigating away
⋮----
const saveScroll = () =>
⋮----
const handleScroll = () =>
⋮----
// Scroll restoration: restore on mount
⋮----
// Fetch saved articles from backend when showSaved is active
⋮----
// Sticky observer
⋮----
// Track container width to determine column count for virtual rows
⋮----
// Match Tailwind breakpoints: xl(1280)=3cols, md(768)=2cols, else 1col
⋮----
// Auto-refresh: SSE stream with polling fallback
⋮----
const startPollingFallback = () =>
⋮----
// Silently ignore polling errors
⋮----
// Try SSE first
⋮----
// Ignore malformed events
⋮----
// SSE failed -- close and fall back to polling
⋮----
// EventSource constructor failed (e.g. SSR) -- fall back to polling
⋮----
// Track known IDs for auto-refresh detection
⋮----
// Source counts
⋮----
// Accumulate articles for "load more"
⋮----
// Infinite scroll: observe sentinel element
⋮----
// Search handling with debounce
⋮----
// Don't treat abort errors as search failures
⋮----
// Determine which articles to display
⋮----
// Group articles into rows based on column count for virtualization
⋮----
// Approximate height of one ArticleCard row (card ~180px + 12px gap + padding).
// measureElement is used via ref={rowVirtualizer.measureElement} on each
// virtual row div below, so actual measurements replace this estimate
// after initial render.
⋮----
{/* Header */}
⋮----
{/* Sentinel for sticky detection */}
⋮----
{/* Sticky search + filters */}
⋮----
{/* New articles banner */}
⋮----
/* Loading skeletons */
⋮----
/* Error state */
⋮----
dispatch(
refetch();
setTimeout(() => dispatch(
⋮----
className=
⋮----
/* Empty states */
⋮----
/* Search empty state */
⋮----
{/* Magnifying glass */}
⋮----
{/* Document inside the lens */}
⋮----

⋮----
onClick=
⋮----
/* Saved empty state */
⋮----
{/* Bookmark shape */}
⋮----
/* No news empty state */
⋮----
{/* Gold gradient circle behind icon */}
⋮----
{/* Newspaper / RSS icon */}
⋮----
{/* Search results count */}
⋮----
{/* Virtualized Articles */}
⋮----
{/* Infinite scroll: loading spinner */}
⋮----
{/* Infinite scroll sentinel + keyboard-accessible Load More */}
</file>

<file path="services/health_service.py">
"""
Health check service for TASI AI Platform.
Provides structured health status for database connectivity, LLM availability,
Redis cache status, entities, market data, news pipeline, TASI index cache,
and news scraper scheduler.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
_STARTUP_TIME = datetime.utcnow()
⋮----
class HealthStatus(str, Enum)
⋮----
HEALTHY = "healthy"
DEGRADED = "degraded"
UNHEALTHY = "unhealthy"
⋮----
@dataclass
class ComponentHealth
⋮----
name: str
status: HealthStatus
latency_ms: Optional[float] = None
message: str = ""
⋮----
@dataclass
class HealthReport
⋮----
status: HealthStatus = HealthStatus.HEALTHY
service: str = "raid-ai-tasi"
version: str = "1.0.0"
uptime_seconds: float = 0.0
components: list = field(default_factory=list)
build_info: Dict[str, Any] = field(default_factory=dict)
⋮----
def to_dict(self) -> dict
⋮----
result = {
⋮----
def check_database() -> ComponentHealth
⋮----
"""Check database connectivity.

    For PostgreSQL, uses the connection pool exclusively.  If the pool has not
    been initialized the check returns ``unhealthy`` immediately — the
    application should not be serving traffic without a healthy pool, and
    falling back to a direct connection would mask a real misconfiguration.

    For SQLite, opens a direct connection to the configured database file.
    """
settings = get_settings()
start = time.monotonic()
⋮----
latency = (time.monotonic() - start) * 1000
⋮----
# Pool is available — use it exclusively; no direct-connection fallback.
⋮----
cur = conn.cursor()
⋮----
db_path = settings.db.resolved_sqlite_path
⋮----
conn = sqlite3.connect(str(db_path), timeout=5)
⋮----
def check_llm() -> ComponentHealth
⋮----
"""Check LLM API key is configured (does not make a live API call).

    Checks ANTHROPIC_API_KEY / LLM_API_KEY for the Claude Sonnet 4.5 provider.
    """
⋮----
api_key = settings.get_llm_api_key()
⋮----
def check_redis() -> ComponentHealth
⋮----
"""Check Redis cache connectivity.

    Redis is optional -- if caching is disabled the component reports HEALTHY
    with a note that it is disabled. If enabled but unreachable, reports DEGRADED
    (not UNHEALTHY, since the app works without cache).
    """
⋮----
available = is_redis_available()
⋮----
def _get_sqlite_path() -> Path
⋮----
"""Resolve the SQLite database path."""
⋮----
def _sqlite_query(sql: str, db_path: Optional[Path] = None)
⋮----
"""Execute a read-only SQLite query and return all rows."""
path = db_path or _get_sqlite_path()
conn = sqlite3.connect(str(path), timeout=5)
⋮----
def _is_pg_backend() -> bool
⋮----
"""Return True when the active backend is PostgreSQL."""
⋮----
def _scalar_query(sql: str) -> Any
⋮----
"""Execute a scalar query against the active backend.

    Uses the connection pool for PostgreSQL and direct SQLite otherwise.
    """
⋮----
conn = get_read_connection()
⋮----
db_path = _get_sqlite_path()
rows = _sqlite_query(sql, db_path)
⋮----
def check_entities() -> ComponentHealth
⋮----
"""Check if entities/companies data is accessible."""
⋮----
count = _scalar_query("SELECT COUNT(*) FROM companies") or 0
⋮----
def check_market_data() -> ComponentHealth
⋮----
"""Check if market_data table has data with non-null current_price."""
⋮----
total = _scalar_query("SELECT COUNT(*) FROM market_data") or 0
with_price = (
⋮----
status = HealthStatus.DEGRADED
msg = "No market data rows"
⋮----
msg = f"{total} rows but no current_price data"
⋮----
status = HealthStatus.HEALTHY
msg = f"{total} rows, {with_price} with current_price"
⋮----
def check_news() -> ComponentHealth
⋮----
"""Check if news_articles table exists and has data.

    Returns HEALTHY if articles exist, DEGRADED if no articles,
    UNHEALTHY if the table is missing.
    """
⋮----
article_count = (
source_count = (
⋮----
# Check if table exists (SQLite)
table_check = _sqlite_query(
⋮----
count_rows = _sqlite_query("SELECT COUNT(*) FROM news_articles", db_path)
article_count = count_rows[0][0] if count_rows else 0
source_rows = _sqlite_query(
source_count = source_rows[0][0] if source_rows else 0
⋮----
def check_tasi_index() -> ComponentHealth
⋮----
"""Check TASI index data cache and circuit breaker status.

    Reports on whether TASI data is cached, cache age, and whether the
    yfinance circuit breaker is open.  This check is non-fatal: if the
    ``services.tasi_index`` module is unavailable, the component reports
    DEGRADED rather than UNHEALTHY.
    """
⋮----
cache_info = get_cache_status()
cb_info = get_circuit_breaker_status()
⋮----
cache_status = cache_info.get("cache_status", "unknown")
cache_age = cache_info.get("cache_age_seconds")
circuit_state = cb_info.get("circuit_state", "unknown")
consecutive_failures = cb_info.get("consecutive_failures", 0)
⋮----
# Build a descriptive message
parts = [f"cache={cache_status}"]
⋮----
message = ", ".join(parts)
⋮----
# Determine status
⋮----
def check_news_scraper() -> ComponentHealth
⋮----
"""Check news scraper scheduler status.

    Inspects the running ``NewsScheduler`` instance (via ``app._news_scheduler``)
    to determine if the scheduler thread is alive. Falls back to querying the
    ``news_articles`` table in ``news_feed`` (SQLite) for recent scrape activity.

    This check complements the existing ``check_news()`` function which checks
    the ``news_articles`` table data. This one focuses on the *scraper process*
    health: is the scheduler running, when was the last scrape, and how many
    sources are producing articles.
    """
⋮----
scheduler_running = False
scheduler_info = "scheduler not found"
⋮----
# Try to access the scheduler instance from app module
⋮----
scheduler = getattr(_app_module, "_news_scheduler", None)
⋮----
scheduler_running = getattr(scheduler, "_running", False)
thread = getattr(scheduler, "_thread", None)
thread_alive = thread.is_alive() if thread else False
⋮----
scheduler_info = "running"
⋮----
scheduler_info = "flag=running but thread dead"
⋮----
scheduler_info = "stopped"
⋮----
scheduler_info = "not initialized"
⋮----
scheduler_info = "unable to inspect"
⋮----
# Check recent scrape activity from the news_articles table
active_sources = 0
last_scrape_age = None
⋮----
last_created = scalar_compat(
⋮----
# PG returns datetime objects
last_scrape_age = (
⋮----
last_dt = datetime.fromisoformat(str(last_created))
⋮----
active_sources = (
⋮----
rows = _sqlite_query(
last_created = rows[0][0] if rows and rows[0][0] else None
⋮----
last_dt = datetime.fromisoformat(last_created)
⋮----
active_sources = source_rows[0][0] if source_rows else 0
⋮----
pass  # DB query failures are non-fatal here
⋮----
# Build message
parts = [f"scheduler={scheduler_info}"]
⋮----
age_str = f"{int(last_scrape_age / 60)}m ago"
⋮----
age_str = f"{last_scrape_age / 3600:.1f}h ago"
⋮----
age_str = f"{last_scrape_age / 86400:.1f}d ago"
⋮----
# Running but no recent articles from any source
⋮----
# Scheduler not running but we have recent articles (maybe just started)
⋮----
def _get_build_info() -> Dict[str, Any]
⋮----
"""Collect version and build information for the health report.

    Includes LLM model name, database backend type, and Python version.
    """
⋮----
def get_pool_stats() -> Dict[str, Any]
⋮----
"""Return connection pool statistics for the active backend.

    For SQLite: returns pool_size from the SQLitePool instance.
    For PostgreSQL: returns minconn/maxconn from ThreadedConnectionPool.
    Always returns a dict and never raises.
    """
⋮----
backend = settings.db.backend
⋮----
backend = "sqlite"
⋮----
pool_size = _sq_pool.pool_size
⋮----
def get_uptime_seconds() -> float
⋮----
"""Return seconds since the health service module was loaded."""
⋮----
def get_health() -> HealthReport
⋮----
"""Run all health checks and return a structured report."""
report = HealthReport()
⋮----
# Overall status is the worst component status
statuses = [c.status for c in report.components]
⋮----
# Attach version/build metadata
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    name: Lint (Python)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install ruff
        run: pip install ruff

      - name: Run ruff check
        run: ruff check .

      - name: Run ruff format check
        run: ruff format --check .

      - name: Install pip-tools
        run: pip install pip-tools

      - name: Verify lock file is up to date
        run: |
          pip-compile requirements.in -o /tmp/requirements.lock.check --no-annotate --strip-extras --quiet
          if ! diff -q requirements.lock /tmp/requirements.lock.check > /dev/null 2>&1; then
            echo "requirements.lock is stale. Run: pip-compile requirements.in -o requirements.lock"
            diff requirements.lock /tmp/requirements.lock.check | head -30
            exit 1
          fi
          echo "requirements.lock is up to date"

  security:
    name: Security Audit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install pip-audit
        run: pip install pip-audit

      - name: Audit dependencies for known vulnerabilities
        # requirements.lock provides the reproducible pinned set audited here.
        # Do not use || true — real CVEs must be explicitly acknowledged with valid IDs.
        run: pip-audit -r requirements.txt

  security-scan:
    name: Security Scan (bandit)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install bandit
        run: pip install bandit[toml]

      - name: Run bandit
        run: >
          bandit -r .
          --exclude ./tests,./venv,./.venv,./frontend,./vanna_docs,./vanna-skill,./.git,./__pycache__
          --severity-level medium
          --confidence-level medium
          -f txt

  type-check:
    name: Type Check (mypy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install mypy types-requests types-beautifulsoup4 types-redis types-PyYAML

      - name: Run mypy
        run: >
          mypy config/ services/ api/ middleware/ database/ backend/
          --ignore-missing-imports
          --no-strict-optional
          --follow-imports=silent

  python-tests:
    name: Test (SQLite)
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Build SQLite database from CSV
        run: python csv_to_sqlite.py

      - name: Run database integrity tests
        run: python -m pytest tests/test_database.py -v

      - name: Run Vanna assembly tests
        run: python tests/test_app_assembly_v2.py

      - name: Run unit and integration tests
        run: python -m pytest tests/ -v --timeout=60 --ignore=tests/test_services.py --ignore=tests/test_api_routes.py --ignore=tests/performance/ --cov=api --cov=services --cov=middleware --cov=config --cov=backend --cov=ingestion --cov-report=term-missing --cov-fail-under=75
        timeout-minutes: 10

  test-pg:
    name: Test (PostgreSQL)
    runs-on: ubuntu-latest
    needs: lint
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: tasi_platform
          POSTGRES_USER: tasi_user
          POSTGRES_PASSWORD: tasi_pass
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U tasi_user -d tasi_platform"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      POSTGRES_HOST: localhost
      POSTGRES_PORT: 5432
      POSTGRES_DB: tasi_platform
      POSTGRES_USER: tasi_user
      POSTGRES_PASSWORD: tasi_pass
      DB_BACKEND: postgres
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Build SQLite database (needed for migration)
        run: python csv_to_sqlite.py

      - name: Initialize PostgreSQL schema
        run: psql -h localhost -U tasi_user -d tasi_platform -f database/schema.sql
        env:
          PGPASSWORD: tasi_pass

      - name: Migrate SQLite data to PostgreSQL
        run: >
          python database/migrate_sqlite_to_pg.py
          --skip-schema
          --pg-host localhost
          --pg-dbname tasi_platform
          --pg-user tasi_user
          --pg-password tasi_pass

      - name: Run database integrity tests (PostgreSQL)
        run: python -m pytest tests/test_database.py -v

      - name: Run service integration tests
        run: python -m pytest tests/test_services.py -v

      - name: Run API route integration tests
        run: python -m pytest tests/test_api_routes.py -v

  docker:
    name: Docker Build
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image (with cache)
        uses: docker/build-push-action@v6
        with:
          context: .
          push: false
          load: true
          tags: tasi-app:ci
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Verify non-root user in image
        run: |
          USER=$(docker inspect --format='{{.Config.User}}' tasi-app:ci)
          if [ -z "$USER" ] || [ "$USER" = "root" ]; then
            echo "ERROR: Container runs as root"
            exit 1
          fi
          echo "Container runs as user: $USER"

      - name: Verify HEALTHCHECK is defined
        run: |
          HC=$(docker inspect --format='{{.Config.Healthcheck.Test}}' tasi-app:ci)
          if [ -z "$HC" ] || [ "$HC" = "[]" ]; then
            echo "ERROR: No HEALTHCHECK defined"
            exit 1
          fi
          echo "HEALTHCHECK: $HC"

  frontend-lint:
    name: Frontend Lint
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: ESLint
        run: npm run lint

  frontend-build:
    name: Frontend Build & Test
    runs-on: ubuntu-latest
    needs: frontend-lint
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Type check and build
        run: npm run build

      - name: Run Vitest tests
        run: npx vitest run
</file>

<file path="app.py">
"""
Vanna 2.0 Saudi Stock Market Analyst
=====================================
Connects to a SQLite or PostgreSQL database of ~500 Saudi-listed companies
and exposes a FastAPI chat interface powered by Claude Sonnet 4.5
via the Anthropic API.

Set DB_BACKEND=postgres (with POSTGRES_* env vars) to use PostgreSQL.
Default is SQLite.
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# ---------------------------------------------------------------------------
# 0. Configuration (fail fast on invalid configuration)
⋮----
_settings = get_settings()
⋮----
_HERE = Path(__file__).resolve().parent
⋮----
# 1. LLM -- Claude Sonnet 4.5 via Anthropic API
⋮----
_llm_model = _settings.llm.model if _settings else "claude-sonnet-4-5-20250929"
_llm_api_key = (
⋮----
llm = AnthropicLlmService(
⋮----
# 2. SQL runner -- SQLite (default) or PostgreSQL
⋮----
DB_BACKEND = (_settings.db.backend if _settings else "sqlite").lower()
⋮----
def _create_sql_runner()
⋮----
"""Create the SQL runner based on configuration."""
⋮----
# Fallback only when settings module failed to load entirely
⋮----
sql_runner = _create_sql_runner()
⋮----
# 3. Tool registry
⋮----
tools = ToolRegistry()
⋮----
# 4. User resolver (JWT-aware; auth required in PostgreSQL mode)
⋮----
class JWTUserResolver(UserResolver)
⋮----
"""Resolve user identity from JWT token.

    Authentication is optional: if a valid token is present, the user
    identity is extracted from it. Otherwise, an anonymous user is returned.
    Invalid tokens are still rejected to prevent confusion.
    """
⋮----
async def resolve_user(self, request_context: RequestContext) -> User
⋮----
auth_header = request_context.get_header("authorization")
⋮----
token = None
⋮----
token = auth_header[7:]
⋮----
payload = decode_token(token, expected_type="access")
user_id = payload.get("sub", "authenticated_user")
email = payload.get("email", "user@localhost")
⋮----
# 5. System prompt builder -- uses extracted prompt from config/prompts.py
⋮----
class SaudiStocksSystemPromptBuilder(SystemPromptBuilder)
⋮----
"""Provides the LLM with full schema documentation for the Saudi stocks DB."""
⋮----
# 6. Agent configuration
⋮----
_max_iterations = _settings.llm.max_tool_iterations if _settings else 10
⋮----
config = AgentConfig(
⋮----
# 7. Assemble the agent
⋮----
agent = Agent(
⋮----
# Production-safe memory limit: each item can contain large SQL results
# or Plotly chart JSON. 500 items ~= 50 conversations * 10 turns each.
# For persistent storage, replace with a database-backed AgentMemory.
⋮----
# 8. FastAPI server
⋮----
server = VannaFastAPIServer(agent)
app = server.create_app()
⋮----
# 8.05. Prometheus metrics (optional -- gracefully skipped if not installed)
⋮----
_pfi = _PIInstrumentator(should_group_status_codes=True, should_group_untemplated=True)
⋮----
# 8.1. OpenAPI metadata
⋮----
# Remove Vanna's default "/" route so our custom template takes precedence
⋮----
# Remove Vanna's default CORSMiddleware (allow_origins=["*"]) so our
# configured CORS settings (from MiddlewareSettings) take effect.
⋮----
# 8a. Middleware (outermost first: error_handler -> request_logging -> rate_limit -> CORS)
⋮----
_mw_settings = _settings.middleware if _settings else None
⋮----
_cors_origins = (
⋮----
# Dynamically add origins from environment variables if not already present
⋮----
_frontend_url = _os.environ.get("FRONTEND_URL", "").strip().rstrip("/")
⋮----
_railway_domain = _os.environ.get("RAILWAY_PUBLIC_DOMAIN", "").strip()
⋮----
_railway_origin = (
⋮----
_rate_limit = _mw_settings.rate_limit_per_minute if _mw_settings else 60
_skip_paths = (
_debug_mode = _settings.server.debug if _settings else False
⋮----
# CORS is applied via FastAPI's add_middleware (innermost)
⋮----
# GZip compression (after CORS, before rate limiter)
⋮----
# Rate limiter (skip in debug mode)
⋮----
# Request logging
⋮----
# Error handler (outermost -- catches everything)
⋮----
# Register exception handlers for HTTPException and RequestValidationError
# so all error responses use the same {"error": {...}} shape.
⋮----
# Chat auth: validate JWT for Vanna chat endpoints (anonymous access allowed)
⋮----
# 9. API routers -- always registered, with graceful degradation
⋮----
# Health route works with both backends (health_service checks backend internally)
⋮----
# Reports route works with both SQLite and PostgreSQL backends.
⋮----
# PG-backed service routes: register in postgres mode for news,
# announcements, watchlists. Entities and charts have SQLite fallbacks
# registered below (section 9g/9h), so PG versions are optional.
⋮----
# SQLite mode: register stub routers that return 503 for PG-only endpoints
# (news, announcements, watchlists). Entities and charts have
# real SQLite handlers registered in sections 9g/9h below.
⋮----
_pg_stub_configs = [
⋮----
_stub = _APIRouter(prefix=_prefix, tags=[_tag])
⋮----
# Capture prefix in closure
def _make_stub_handler(prefix: str)
⋮----
async def _stub_handler()
⋮----
_all_methods = ["GET", "POST", "PUT", "PATCH", "DELETE"]
⋮----
# Auth routes (guest endpoint works with any backend; login/register need PG)
⋮----
# 9b. TASI index route (no database dependency -- works with any backend)
⋮----
# 9c. Per-stock OHLCV route (no database dependency -- works with any backend)
⋮----
# 9d. News feed route (SQLite-backed -- works with any backend)
⋮----
# 9d-2. News SSE stream route (real-time push via Server-Sent Events)
⋮----
# 9d-3. Live market widgets SSE stream (Redis-backed)
⋮----
# 9d-4. Market overview route (World 360 page -- no database dependency)
⋮----
# 9e2. Market movers routes — registered BEFORE market_analytics so the
#      combined /api/v1/market/movers endpoint wins (FastAPI first-match).
⋮----
# 9e. Market analytics routes (Dual-backend (SQLite/PostgreSQL) -- works with any backend)
⋮----
# 9f. Stock data routes (Dual-backend (SQLite/PostgreSQL) -- works with any backend)
⋮----
# 9g. SQLite entities routes (registered only when using SQLite backend)
⋮----
# 9h. Chart analytics routes (Dual-backend (SQLite/PostgreSQL))
⋮----
# 10. Custom routes and static files
⋮----
_TEMPLATE_RAW = (_HERE / "templates" / "index.html").read_text(encoding="utf-8")
_FRONTEND_URL = os.environ.get("FRONTEND_URL", "http://localhost:3000").strip().rstrip("/")
_TEMPLATE_HTML = _TEMPLATE_RAW.replace("{{FRONTEND_URL}}", _FRONTEND_URL)
⋮----
@app.get("/", response_class=HTMLResponse)
async def custom_index()
⋮----
# Serve static assets (logo, favicon, etc.)
_TEMPLATES_DIR = _HERE / "templates"
⋮----
@app.get("/favicon.ico")
async def favicon()
⋮----
favicon_path = _HERE / "templates" / "favicon.svg"
⋮----
# 11. Lifespan (replaces deprecated on_event startup/shutdown)
⋮----
@asynccontextmanager
async def lifespan(app)
⋮----
"""Initialize resources on startup and clean up on shutdown."""
# Setup logging
⋮----
# Install request_id filter on root logger so all log records carry request_id
⋮----
_root_logger = logging.getLogger()
⋮----
# Initialize error tracking
⋮----
# Lifecycle startup diagnostics (version, env validation, backend info)
⋮----
# Initialize PostgreSQL connection pool
⋮----
_pool_min = _settings.pool.min if _settings else 2
_pool_max = _settings.pool.max if _settings else 10
_db_settings = _settings.db if _settings else None
⋮----
pg_dsn = _db_settings.pg_connection_string
⋮----
# Initialize SQLite connection pool (WAL mode, 5 connections)
⋮----
_sqlite_db_path = (
⋮----
# Initialize Redis (if enabled)
_cache_enabled = _settings.cache.enabled if _settings else False
_redis_status = "disabled"
⋮----
_redis_url = (
⋮----
_redis_status = "connected"
⋮----
_redis_status = "unavailable"
⋮----
_redis_status = "error"
⋮----
# -----------------------------------------------------------------------
# Startup diagnostics
⋮----
# SA-06: Warn if debug mode is enabled (rate limiting disabled)
_is_debug = globals().get("_debug_mode", False)
⋮----
# Start news scheduler (SQLite-backed, works with any backend)
_news_scheduler = None
⋮----
_news_store = _NewsStore(str(_HERE / "saudi_stocks.db"))
_news_scheduler = _NewsScheduler(_news_store)
⋮----
# Start quotes hub background task (Redis or in-memory fallback)
_quotes_hub_task = None
⋮----
_redis_for_hub = None
⋮----
_redis_for_hub = _get_redis_client()
⋮----
_quotes_hub_task = asyncio.create_task(run_quotes_hub(_redis_for_hub))
_hub_mode = "Redis" if _redis_for_hub else "in-memory"
⋮----
# Non-blocking yfinance reachability check
⋮----
def _check_yfinance()
⋮----
ticker = yf.Ticker("^TASI")
df = ticker.history(period="5d", auto_adjust=True, timeout=5)
⋮----
# Shutdown: cancel quotes hub background task
⋮----
# Shutdown: stop news scheduler
⋮----
# Shutdown: close connection pool and Redis
⋮----
# Lifecycle shutdown (flush logs, log uptime)
⋮----
# Register the lifespan with the app
⋮----
_port = _settings.server.port if _settings else int(os.environ.get("PORT", "8084"))
_host = _settings.server.host if _settings else "0.0.0.0"
</file>

</files>
